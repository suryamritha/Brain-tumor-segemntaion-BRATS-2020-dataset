{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb904106",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-05T04:04:31.116991Z",
     "iopub.status.busy": "2024-05-05T04:04:31.116599Z",
     "iopub.status.idle": "2024-05-05T04:04:45.333443Z",
     "shell.execute_reply": "2024-05-05T04:04:45.332500Z"
    },
    "papermill": {
     "duration": 14.231193,
     "end_time": "2024-05-05T04:04:45.335787",
     "exception": false,
     "start_time": "2024-05-05T04:04:31.104594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting monai==0.6\r\n",
      "  Downloading monai-0.6.0-202107081903-py3-none-any.whl.metadata (6.0 kB)\r\n",
      "Requirement already satisfied: torch>=1.5 in /opt/conda/lib/python3.10/site-packages (from monai==0.6) (2.1.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from monai==0.6) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai==0.6) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai==0.6) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai==0.6) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai==0.6) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai==0.6) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.5->monai==0.6) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.5->monai==0.6) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.5->monai==0.6) (1.3.0)\r\n",
      "Downloading monai-0.6.0-202107081903-py3-none-any.whl (584 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.8/584.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: monai\r\n",
      "Successfully installed monai-0.6.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install monai==0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7de322",
   "metadata": {
    "papermill": {
     "duration": 0.009838,
     "end_time": "2024-05-05T04:04:45.356294",
     "exception": false,
     "start_time": "2024-05-05T04:04:45.346456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d6cc2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:04:45.378763Z",
     "iopub.status.busy": "2024-05-05T04:04:45.378105Z",
     "iopub.status.idle": "2024-05-05T04:05:04.355420Z",
     "shell.execute_reply": "2024-05-05T04:05:04.354432Z"
    },
    "papermill": {
     "duration": 18.991225,
     "end_time": "2024-05-05T04:05:04.357912",
     "exception": false,
     "start_time": "2024-05-05T04:04:45.366687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srini\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "c:\\Users\\srini\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from easydict import EasyDict\n",
    "from typing import List, Dict, Union, Sequence, Callable\n",
    "\n",
    "from monai.data import DataLoader, CacheDataset, decollate_batch\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    Randomizable,\n",
    "    Resized,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.utils import set_determinism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d431c1",
   "metadata": {
    "papermill": {
     "duration": 0.009995,
     "end_time": "2024-05-05T04:05:04.378974",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.368979",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715326f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.401378Z",
     "iopub.status.busy": "2024-05-05T04:05:04.400236Z",
     "iopub.status.idle": "2024-05-05T04:05:04.407178Z",
     "shell.execute_reply": "2024-05-05T04:05:04.406280Z"
    },
    "papermill": {
     "duration": 0.020003,
     "end_time": "2024-05-05T04:05:04.409161",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.389158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = EasyDict()\n",
    "cfg.epoch = 100\n",
    "cfg.learning_rate = 1e-4\n",
    "cfg.weight_decay = 1e-5\n",
    "\n",
    "cfg.unetr = EasyDict()\n",
    "cfg.unetr.img_shape = (96, 96, 96)\n",
    "cfg.unetr.input_dim = 4\n",
    "cfg.unetr.output_dim = 3\n",
    "cfg.unetr.patch_size = 16\n",
    "cfg.unetr.embed_dim = 768\n",
    "cfg.unetr.num_layers = 12\n",
    "cfg.unetr.num_heads = 12\n",
    "cfg.unetr.mlp_dim = 2048\n",
    "cfg.unetr.extract_layers = [3, 6, 9, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31646b8",
   "metadata": {
    "papermill": {
     "duration": 0.010076,
     "end_time": "2024-05-05T04:05:04.429749",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.419673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e4b744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.453257Z",
     "iopub.status.busy": "2024-05-05T04:05:04.452578Z",
     "iopub.status.idle": "2024-05-05T04:05:04.457022Z",
     "shell.execute_reply": "2024-05-05T04:05:04.455991Z"
    },
    "papermill": {
     "duration": 0.018075,
     "end_time": "2024-05-05T04:05:04.459090",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.441015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = \"C:/Users/srini/Downloads/archive (1)/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
    "output_dir = \"C:/Users/srini/Downloads/archive (1)/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b499c8",
   "metadata": {
    "papermill": {
     "duration": 0.010231,
     "end_time": "2024-05-05T04:05:04.479859",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.469628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c54300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.503826Z",
     "iopub.status.busy": "2024-05-05T04:05:04.503459Z",
     "iopub.status.idle": "2024-05-05T04:05:04.510495Z",
     "shell.execute_reply": "2024-05-05T04:05:04.509493Z"
    },
    "papermill": {
     "duration": 0.021919,
     "end_time": "2024-05-05T04:05:04.512681",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.490762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ad9b4",
   "metadata": {
    "papermill": {
     "duration": 0.010272,
     "end_time": "2024-05-05T04:05:04.533953",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.523681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conv blocks classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a9b039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.556868Z",
     "iopub.status.busy": "2024-05-05T04:05:04.556488Z",
     "iopub.status.idle": "2024-05-05T04:05:04.562581Z",
     "shell.execute_reply": "2024-05-05T04:05:04.561686Z"
    },
    "papermill": {
     "duration": 0.019541,
     "end_time": "2024-05-05T04:05:04.564531",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.544990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SingleDeconv3DBlock(nn.Module):\n",
    "    '''\n",
    "    The green box in the architecture\n",
    "    '''\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super().__init__()\n",
    "        self.block = nn.ConvTranspose3d(in_planes, out_planes, kernel_size=2, stride=2, padding=0, output_padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6895e63c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.586734Z",
     "iopub.status.busy": "2024-05-05T04:05:04.586425Z",
     "iopub.status.idle": "2024-05-05T04:05:04.592128Z",
     "shell.execute_reply": "2024-05-05T04:05:04.591180Z"
    },
    "papermill": {
     "duration": 0.018692,
     "end_time": "2024-05-05T04:05:04.593904",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.575212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SingleConv3DBlock(nn.Module):\n",
    "    '''\n",
    "    The gray box in the architecture\n",
    "    '''\n",
    "    def __init__(self, in_planes, out_planes, kernel_size):\n",
    "        super().__init__()\n",
    "        self.block = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
    "                               padding=((kernel_size - 1) // 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eedc7af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.616208Z",
     "iopub.status.busy": "2024-05-05T04:05:04.615491Z",
     "iopub.status.idle": "2024-05-05T04:05:04.621281Z",
     "shell.execute_reply": "2024-05-05T04:05:04.620334Z"
    },
    "papermill": {
     "duration": 0.019131,
     "end_time": "2024-05-05T04:05:04.623381",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.604250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conv3DBlock(nn.Module):\n",
    "    '''\n",
    "    The yellow box in the architecture\n",
    "    Cnv3d 3x3x3 + Batch Normalization + ReLU\n",
    "    '''\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            SingleConv3DBlock(in_planes, out_planes, kernel_size),\n",
    "            nn.BatchNorm3d(out_planes),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "543a4143",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.645268Z",
     "iopub.status.busy": "2024-05-05T04:05:04.644805Z",
     "iopub.status.idle": "2024-05-05T04:05:04.650354Z",
     "shell.execute_reply": "2024-05-05T04:05:04.649539Z"
    },
    "papermill": {
     "duration": 0.018369,
     "end_time": "2024-05-05T04:05:04.652220",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.633851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Deconv3DBlock(nn.Module):\n",
    "    '''\n",
    "    The blue box in the architecture\n",
    "    Deconv 2x2x2 + Conv 3x3x3 + Batch Normalization + ReLU\n",
    "    '''\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            SingleDeconv3DBlock(in_planes, out_planes),\n",
    "            SingleConv3DBlock(out_planes, out_planes, kernel_size),\n",
    "            nn.BatchNorm3d(out_planes),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f5bf8",
   "metadata": {
    "papermill": {
     "duration": 0.010421,
     "end_time": "2024-05-05T04:05:04.673354",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.662933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transformer classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "068ac498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.696179Z",
     "iopub.status.busy": "2024-05-05T04:05:04.695403Z",
     "iopub.status.idle": "2024-05-05T04:05:04.709850Z",
     "shell.execute_reply": "2024-05-05T04:05:04.708983Z"
    },
    "papermill": {
     "duration": 0.028008,
     "end_time": "2024-05-05T04:05:04.711901",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.683893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_heads\n",
    "        self.attention_head_size = int(embed_dim / num_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.query = nn.Linear(embed_dim, self.all_head_size)\n",
    "        self.key = nn.Linear(embed_dim, self.all_head_size)\n",
    "        self.value = nn.Linear(embed_dim, self.all_head_size)\n",
    "\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.vis = False\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return attention_output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c269854d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.734735Z",
     "iopub.status.busy": "2024-05-05T04:05:04.734163Z",
     "iopub.status.idle": "2024-05-05T04:05:04.740107Z",
     "shell.execute_reply": "2024-05-05T04:05:04.739268Z"
    },
    "papermill": {
     "duration": 0.019308,
     "end_time": "2024-05-05T04:05:04.742009",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.722701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(mlp_dim, embedding_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd582815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.764797Z",
     "iopub.status.busy": "2024-05-05T04:05:04.764175Z",
     "iopub.status.idle": "2024-05-05T04:05:04.771892Z",
     "shell.execute_reply": "2024-05-05T04:05:04.770995Z"
    },
    "papermill": {
     "duration": 0.021285,
     "end_time": "2024-05-05T04:05:04.773895",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.752610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, cube_size, patch_size, dropout):\n",
    "        super().__init__()\n",
    "        self.n_patches = int((cube_size[0] * cube_size[1] * cube_size[2]) / (patch_size * patch_size * patch_size))\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        # They used convolution operation for making patches. Where kernel size is the patch_size, anstride is patch_size.\n",
    "        # In normal ViT, there is a flattening layer followed by a Dense layer\"\n",
    "        self.patch_embeddings = nn.Conv3d(in_channels=input_dim, out_channels=embed_dim,\n",
    "                                          kernel_size=patch_size, stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.n_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        x = x.flatten(2) # the output of conv layer(a matrix) is then flattened.\n",
    "        x = x.transpose(-1, -2)\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f4a803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.797768Z",
     "iopub.status.busy": "2024-05-05T04:05:04.797199Z",
     "iopub.status.idle": "2024-05-05T04:05:04.805021Z",
     "shell.execute_reply": "2024-05-05T04:05:04.804168Z"
    },
    "papermill": {
     "duration": 0.022022,
     "end_time": "2024-05-05T04:05:04.806971",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.784949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, cube_size, patch_size):\n",
    "        super().__init__()\n",
    "        self.attention_norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mlp_norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mlp_dim = int((cube_size[0] * cube_size[1] * cube_size[2]) / (patch_size * patch_size * patch_size))\n",
    "        self.mlp = MLP(embed_dim, cfg.unetr.mlp_dim)\n",
    "        self.attn = MultiHeadAttention(num_heads, embed_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        x = x + h\n",
    "        \n",
    "        h = x\n",
    "        x = self.mlp_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + h\n",
    "        \n",
    "        return x, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421355ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.830929Z",
     "iopub.status.busy": "2024-05-05T04:05:04.830325Z",
     "iopub.status.idle": "2024-05-05T04:05:04.837994Z",
     "shell.execute_reply": "2024-05-05T04:05:04.837159Z"
    },
    "papermill": {
     "duration": 0.021254,
     "end_time": "2024-05-05T04:05:04.839741",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.818487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, cube_size, patch_size, num_heads, num_layers, dropout, extract_layers):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(input_dim, embed_dim, cube_size, patch_size, dropout)\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.extract_layers = extract_layers\n",
    "        for _ in range(num_layers):\n",
    "            layer = TransformerBlock(embed_dim, num_heads, dropout, cube_size, patch_size)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, x):\n",
    "        extract_layers = []\n",
    "        hidden_states = self.embeddings(x)\n",
    "\n",
    "        for depth, layer_block in enumerate(self.layer):\n",
    "            hidden_states, _ = layer_block(hidden_states)\n",
    "            if depth + 1 in self.extract_layers:\n",
    "                extract_layers.append(hidden_states)\n",
    "\n",
    "        return extract_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4cf275",
   "metadata": {
    "papermill": {
     "duration": 0.010335,
     "end_time": "2024-05-05T04:05:04.860753",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.850418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# UNETR architecture class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2f9731f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.883889Z",
     "iopub.status.busy": "2024-05-05T04:05:04.883381Z",
     "iopub.status.idle": "2024-05-05T04:05:04.901256Z",
     "shell.execute_reply": "2024-05-05T04:05:04.900332Z"
    },
    "papermill": {
     "duration": 0.031562,
     "end_time": "2024-05-05T04:05:04.903171",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.871609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UNETR(nn.Module):\n",
    "    def __init__(self, img_shape=cfg.unetr.img_shape, input_dim=cfg.unetr.input_dim, output_dim=cfg.unetr.output_dim, embed_dim=cfg.unetr.embed_dim, patch_size=cfg.unetr.patch_size, num_heads=cfg.unetr.num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.img_shape = img_shape\n",
    "        self.patch_size = patch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = cfg.unetr.num_layers\n",
    "        self.ext_layers = cfg.unetr.extract_layers\n",
    "\n",
    "        self.patch_dim = [int(x / patch_size) for x in img_shape]\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer = \\\n",
    "            Transformer(\n",
    "                input_dim=input_dim,\n",
    "                embed_dim=embed_dim,\n",
    "                cube_size=img_shape,\n",
    "                patch_size=patch_size,\n",
    "                num_heads=num_heads,\n",
    "                num_layers=self.num_layers,\n",
    "                dropout=dropout,\n",
    "                extract_layers=self.ext_layers\n",
    "            )\n",
    "\n",
    "        # U-Net Decoder\n",
    "        self.decoder0 = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(input_dim, 32, 3),\n",
    "                Conv3DBlock(32, 64, 3)\n",
    "            )\n",
    "\n",
    "        self.decoder3 = \\\n",
    "            nn.Sequential(\n",
    "                Deconv3DBlock(embed_dim, 512),\n",
    "                Deconv3DBlock(512, 256),\n",
    "                Deconv3DBlock(256, 128)\n",
    "            )\n",
    "\n",
    "        self.decoder6 = \\\n",
    "            nn.Sequential(\n",
    "                Deconv3DBlock(embed_dim, 512),\n",
    "                Deconv3DBlock(512, 256),\n",
    "            )\n",
    "\n",
    "        self.decoder9 = \\\n",
    "            Deconv3DBlock(embed_dim, 512)\n",
    "\n",
    "        self.decoder12_upsampler = \\\n",
    "            SingleDeconv3DBlock(embed_dim, 512)\n",
    "\n",
    "        self.decoder9_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(1024, 512),\n",
    "                Conv3DBlock(512, 512),\n",
    "                Conv3DBlock(512, 512),\n",
    "                SingleDeconv3DBlock(512, 256)\n",
    "            )\n",
    "\n",
    "        self.decoder6_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(512, 256),\n",
    "                Conv3DBlock(256, 256),\n",
    "                SingleDeconv3DBlock(256, 128)\n",
    "            )\n",
    "\n",
    "        self.decoder3_upsampler = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(256, 128),\n",
    "                Conv3DBlock(128, 128),\n",
    "                SingleDeconv3DBlock(128, 64)\n",
    "            )\n",
    "\n",
    "        self.decoder0_header = \\\n",
    "            nn.Sequential(\n",
    "                Conv3DBlock(128, 64),\n",
    "                Conv3DBlock(64, 64),\n",
    "                SingleConv3DBlock(64, output_dim, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.transformer(x)\n",
    "        z0, z3, z6, z9, z12 = x, *z\n",
    "        z3 = z3.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        z6 = z6.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        z9 = z9.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        z12 = z12.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "\n",
    "        z12 = self.decoder12_upsampler(z12)\n",
    "        z9 = self.decoder9(z9)\n",
    "        z9 = self.decoder9_upsampler(torch.cat([z9, z12], dim=1))\n",
    "        z6 = self.decoder6(z6)\n",
    "        z6 = self.decoder6_upsampler(torch.cat([z6, z9], dim=1))\n",
    "        z3 = self.decoder3(z3)\n",
    "        z3 = self.decoder3_upsampler(torch.cat([z3, z6], dim=1))\n",
    "        z0 = self.decoder0(z0)\n",
    "        output = self.decoder0_header(torch.cat([z0, z3], dim=1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e9d82",
   "metadata": {
    "papermill": {
     "duration": 0.011043,
     "end_time": "2024-05-05T04:05:04.925490",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.914447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21290143",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:04.949073Z",
     "iopub.status.busy": "2024-05-05T04:05:04.948731Z",
     "iopub.status.idle": "2024-05-05T04:05:04.970255Z",
     "shell.execute_reply": "2024-05-05T04:05:04.969347Z"
    },
    "papermill": {
     "duration": 0.036135,
     "end_time": "2024-05-05T04:05:04.972461",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.936326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_distribution(num_train: int, num_val: int, num_test: int):\n",
    "    \"\"\"Plot number of data for train-set, val-set, and test-set after splitted\"\"\"\n",
    "\n",
    "    # Create the bar chart\n",
    "    bars = plt.bar([\"Train\", \"Val\", \"Test\"],\n",
    "            [num_train, num_val, num_test], align='center', color=['green', 'red', 'blue'])\n",
    "\n",
    "    # Add the data value on top of each bar\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, yval, ha='center', va='bottom')\n",
    "\n",
    "    plt.ylabel('Number of images')\n",
    "    plt.title('Data distribution')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_datalist(\n",
    "    root_dir: str\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load image/label paths of dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    datalist = []\n",
    "    for data in os.listdir(root_dir):\n",
    "        data_dir_path = os.path.join(root_dir, data)\n",
    "        if os.path.isdir(data_dir_path):\n",
    "            model_scans = [\"flair\", \"t1\", \"t1ce\", \"t2\"]\n",
    "            image_paths = [os.path.join(data_dir_path, f\"{data}_{model}.nii\") for model in model_scans]\n",
    "            label_path = os.path.join(data_dir_path, f\"{data}_seg.nii\")\n",
    "\n",
    "            if (all(os.path.exists(path) for path in [*image_paths, label_path])):\n",
    "                datalist.append({\n",
    "                    \"image\": image_paths,\n",
    "                    \"label\": label_path\n",
    "                })\n",
    "\n",
    "    return datalist\n",
    "\n",
    "\n",
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats classes:\n",
    "    label 1 is the necrotic and non-enhancing tumor core\n",
    "    label 2 is the peritumoral edema\n",
    "    label 4 is the GD-enhancing tumor\n",
    "    The possible classes are TC (Tumor core), WT (Whole tumor)\n",
    "    and ET (Enhancing tumor).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 1 and label 4 to construct TC\n",
    "            result.append(np.logical_or(d[key] == 1, d[key] == 4))\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            result.append(\n",
    "                np.logical_or(\n",
    "                    np.logical_or(d[key] == 1, d[key] == 4), d[key] == 2\n",
    "                )\n",
    "            )\n",
    "            # label 4 is ET\n",
    "            result.append(d[key] == 4)\n",
    "            d[key] = np.stack(result, axis=0).astype(\"float32\")\n",
    "        return d\n",
    "    \n",
    "    \n",
    "class BratsDataset(Randomizable, CacheDataset):\n",
    "    \"\"\"\n",
    "    Generate items for training, validation or test.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        section: str,\n",
    "        transform: Union[Sequence[Callable], Callable] = (),\n",
    "        val_frac: float = 0.15,\n",
    "        test_frac: float = 0.05,\n",
    "        seed: int = 0,\n",
    "        cache_num: int = sys.maxsize,\n",
    "        cache_rate: float = 1.0,\n",
    "        num_workers: int = 0,\n",
    "    ) -> None:\n",
    "        if not os.path.isdir(root_dir) or not os.path.exists(root_dir):\n",
    "            raise RuntimeError(\n",
    "                f\"Cannot find dataset directory: {root_dir}.\"\n",
    "            )\n",
    "\n",
    "        self.section = section\n",
    "        self.val_frac = val_frac\n",
    "        self.test_frac = test_frac\n",
    "        self.set_random_state(seed=seed)\n",
    "        self.indices: np.ndarray = np.array([])\n",
    "        \n",
    "        data = self._generate_data_list(root_dir)\n",
    "        CacheDataset.__init__(\n",
    "            self, data, transform, cache_num=cache_num, cache_rate=cache_rate, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    def get_indices(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the indices of datalist used in this dataset.\n",
    "        \"\"\"\n",
    "        return self.indices\n",
    "\n",
    "    def randomize(self, data: List[int]) -> None:\n",
    "        self.R.shuffle(data)\n",
    "\n",
    "    def _generate_data_list(self, root_dir: str) -> List[Dict]:\n",
    "        datalist = load_datalist(root_dir)\n",
    "        return self._split_datalist(datalist)\n",
    "\n",
    "    def _split_datalist(self, datalist: List[Dict]) -> List[Dict]:\n",
    "        length = len(datalist)\n",
    "        indices = np.arange(length)\n",
    "        self.randomize(indices)\n",
    "\n",
    "        val_length = int(length * self.val_frac)\n",
    "        test_length = int(length * self.test_frac)\n",
    "        if self.section == \"training\":\n",
    "            self.indices = indices[val_length+test_length:]\n",
    "        elif self.section == \"validation\":\n",
    "            self.indices = indices[test_length:val_length+test_length]\n",
    "        else:\n",
    "            self.indices = indices[:test_length]\n",
    "\n",
    "        return [datalist[i] for i in self.indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88539e33",
   "metadata": {
    "papermill": {
     "duration": 0.010785,
     "end_time": "2024-05-05T04:05:04.994309",
     "exception": false,
     "start_time": "2024-05-05T04:05:04.983524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup transforms for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d40b524b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:05.018384Z",
     "iopub.status.busy": "2024-05-05T04:05:05.017677Z",
     "iopub.status.idle": "2024-05-05T04:05:05.028688Z",
     "shell.execute_reply": "2024-05-05T04:05:05.027744Z"
    },
    "papermill": {
     "duration": 0.025272,
     "end_time": "2024-05-05T04:05:05.030591",
     "exception": false,
     "start_time": "2024-05-05T04:05:05.005319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=cfg.unetr.img_shape, mode=\"nearest\"),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93686ff",
   "metadata": {
    "papermill": {
     "duration": 0.011211,
     "end_time": "2024-05-05T04:05:05.053276",
     "exception": false,
     "start_time": "2024-05-05T04:05:05.042065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0153780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:05.077343Z",
     "iopub.status.busy": "2024-05-05T04:05:05.076544Z",
     "iopub.status.idle": "2024-05-05T04:05:15.427535Z",
     "shell.execute_reply": "2024-05-05T04:05:15.426754Z"
    },
    "papermill": {
     "duration": 10.365574,
     "end_time": "2024-05-05T04:05:15.429856",
     "exception": false,
     "start_time": "2024-05-05T04:05:05.064282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = BratsDataset(\n",
    "    root_dir=root_dir,\n",
    "    section=\"training\",\n",
    "    transform=transform,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_ds = BratsDataset(\n",
    "    root_dir=root_dir,\n",
    "    section=\"validation\",\n",
    "    transform=transform,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f980a88d",
   "metadata": {
    "papermill": {
     "duration": 0.01112,
     "end_time": "2024-05-05T04:05:15.452410",
     "exception": false,
     "start_time": "2024-05-05T04:05:15.441290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check data shape and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f55d63eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:15.476463Z",
     "iopub.status.busy": "2024-05-05T04:05:15.476128Z",
     "iopub.status.idle": "2024-05-05T04:05:19.396410Z",
     "shell.execute_reply": "2024-05-05T04:05:19.395370Z"
    },
    "papermill": {
     "duration": 3.934533,
     "end_time": "2024-05-05T04:05:19.398451",
     "exception": false,
     "start_time": "2024-05-05T04:05:15.463918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([4, 96, 96, 96])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape: torch.Size([3, 96, 96, 96])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_channels = ['FLAIR', 'T1w', 'T1gd', 'T2w']\n",
    "label_channels = ['Tumor Core', 'Whole Tumor', 'Enhancing Tumor']\n",
    "\n",
    "# pick one image from BratsDataset to visualize and check the 4 channels\n",
    "val_data_example = val_ds[1]\n",
    "print(f\"image shape: {val_data_example['image'].shape}\")\n",
    "plt.figure(\"image\", (24, 6))\n",
    "for i in range(len(image_channels)):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.title(f\"{image_channels[i]} image\", weight='bold')\n",
    "    plt.imshow(val_data_example[\"image\"][i, :, :, val_data_example['image'].shape[-1] // 2], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# also visualize the 3 channels label corresponding to this image\n",
    "print(f\"label shape: {val_data_example['label'].shape}\")\n",
    "plt.figure(\"label\", (18, 6))\n",
    "for i in range(len(label_channels)):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(f\"{label_channels[i]} label\", weight='bold')\n",
    "    plt.imshow(val_data_example[\"label\"][i, :, :, val_data_example['image'].shape[-1] // 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f38432",
   "metadata": {
    "papermill": {
     "duration": 0.0125,
     "end_time": "2024-05-05T04:05:19.423776",
     "exception": false,
     "start_time": "2024-05-05T04:05:19.411276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check number of data\n",
    "\n",
    "Check number of data used for training / testing / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeb27e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:19.451044Z",
     "iopub.status.busy": "2024-05-05T04:05:19.450191Z",
     "iopub.status.idle": "2024-05-05T04:05:19.934944Z",
     "shell.execute_reply": "2024-05-05T04:05:19.933981Z"
    },
    "papermill": {
     "duration": 0.50057,
     "end_time": "2024-05-05T04:05:19.937072",
     "exception": false,
     "start_time": "2024-05-05T04:05:19.436502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_frac = val_ds.val_frac\n",
    "test_frac = val_ds.test_frac\n",
    "\n",
    "num_train = len(train_ds)\n",
    "num_val = len(val_ds)\n",
    "num_test = int(test_frac * num_val / val_frac)\n",
    "\n",
    "plot_data_distribution(num_train, num_val, num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a938fd7",
   "metadata": {
    "papermill": {
     "duration": 0.013321,
     "end_time": "2024-05-05T04:05:19.963712",
     "exception": false,
     "start_time": "2024-05-05T04:05:19.950391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0723040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:19.992144Z",
     "iopub.status.busy": "2024-05-05T04:05:19.991188Z",
     "iopub.status.idle": "2024-05-05T04:05:21.946478Z",
     "shell.execute_reply": "2024-05-05T04:05:21.945471Z"
    },
    "papermill": {
     "duration": 1.972106,
     "end_time": "2024-05-05T04:05:21.948955",
     "exception": false,
     "start_time": "2024-05-05T04:05:19.976849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srini\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device(\"cpu:0\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "print(device)\n",
    "\n",
    "model = UNETR(\n",
    "    img_shape=cfg.unetr.img_shape,\n",
    "    input_dim=cfg.unetr.input_dim,\n",
    "    output_dim=cfg.unetr.output_dim,\n",
    "    embed_dim=cfg.unetr.embed_dim,\n",
    "    patch_size=cfg.unetr.patch_size,\n",
    "    num_heads=cfg.unetr.num_heads,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epoch)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold_values=True, logit_thresh=0.5)])\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=cfg.unetr.img_shape,\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40f700",
   "metadata": {
    "papermill": {
     "duration": 0.013076,
     "end_time": "2024-05-05T04:05:21.975810",
     "exception": false,
     "start_time": "2024-05-05T04:05:21.962734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Execute a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd0974ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T04:05:22.004416Z",
     "iopub.status.busy": "2024-05-05T04:05:22.004066Z",
     "iopub.status.idle": "2024-05-05T14:47:22.954762Z",
     "shell.execute_reply": "2024-05-05T14:47:22.953421Z"
    },
    "papermill": {
     "duration": 38523.342339,
     "end_time": "2024-05-05T14:47:25.331264",
     "exception": false,
     "start_time": "2024-05-05T04:05:21.988925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n",
      "1/295, train_loss: 0.9673, step time: 94.0479\n",
      "2/295, train_loss: 0.9487, step time: 1.0393\n",
      "3/295, train_loss: 0.9791, step time: 1.0454\n",
      "4/295, train_loss: 0.9574, step time: 1.0452\n",
      "5/295, train_loss: 0.9534, step time: 1.0326\n",
      "6/295, train_loss: 0.9824, step time: 1.0404\n",
      "7/295, train_loss: 0.9918, step time: 1.0697\n",
      "8/295, train_loss: 0.8475, step time: 1.0593\n",
      "9/295, train_loss: 0.9496, step time: 1.0565\n",
      "10/295, train_loss: 0.9875, step time: 1.0641\n",
      "11/295, train_loss: 0.9769, step time: 1.0333\n",
      "12/295, train_loss: 0.9424, step time: 1.0396\n",
      "13/295, train_loss: 0.9444, step time: 1.0359\n",
      "14/295, train_loss: 0.8786, step time: 1.0312\n",
      "15/295, train_loss: 0.9584, step time: 1.0328\n",
      "16/295, train_loss: 0.9761, step time: 1.0556\n",
      "17/295, train_loss: 0.8587, step time: 1.0364\n",
      "18/295, train_loss: 0.8989, step time: 1.0353\n",
      "19/295, train_loss: 0.9213, step time: 1.0345\n",
      "20/295, train_loss: 0.9811, step time: 1.0306\n",
      "21/295, train_loss: 0.9840, step time: 1.0399\n",
      "22/295, train_loss: 0.9192, step time: 1.0452\n",
      "23/295, train_loss: 0.8905, step time: 1.0316\n",
      "24/295, train_loss: 0.9876, step time: 1.0356\n",
      "25/295, train_loss: 0.9750, step time: 1.0324\n",
      "26/295, train_loss: 0.9955, step time: 1.0528\n",
      "27/295, train_loss: 0.8448, step time: 1.0417\n",
      "28/295, train_loss: 0.9488, step time: 1.0570\n",
      "29/295, train_loss: 0.9330, step time: 1.0346\n",
      "30/295, train_loss: 0.9780, step time: 1.0311\n",
      "31/295, train_loss: 0.8825, step time: 1.0447\n",
      "32/295, train_loss: 0.9170, step time: 1.0443\n",
      "33/295, train_loss: 0.9581, step time: 1.0363\n",
      "34/295, train_loss: 0.9124, step time: 1.0418\n",
      "35/295, train_loss: 0.8646, step time: 1.0313\n",
      "36/295, train_loss: 0.9052, step time: 1.0397\n",
      "37/295, train_loss: 0.9649, step time: 1.0656\n",
      "38/295, train_loss: 0.9173, step time: 1.0400\n",
      "39/295, train_loss: 0.9381, step time: 1.0382\n",
      "40/295, train_loss: 0.8779, step time: 1.0417\n",
      "41/295, train_loss: 0.8559, step time: 1.0447\n",
      "42/295, train_loss: 0.8982, step time: 1.0507\n",
      "43/295, train_loss: 0.8952, step time: 1.0320\n",
      "44/295, train_loss: 0.8876, step time: 1.0439\n",
      "45/295, train_loss: 0.9863, step time: 1.0331\n",
      "46/295, train_loss: 0.9249, step time: 1.0419\n",
      "47/295, train_loss: 0.8997, step time: 1.0623\n",
      "48/295, train_loss: 0.8931, step time: 1.0777\n",
      "49/295, train_loss: 0.8717, step time: 1.0302\n",
      "50/295, train_loss: 0.9544, step time: 1.0393\n",
      "51/295, train_loss: 0.9035, step time: 1.0549\n",
      "52/295, train_loss: 0.8829, step time: 1.0355\n",
      "53/295, train_loss: 0.9444, step time: 1.0400\n",
      "54/295, train_loss: 0.8858, step time: 1.0346\n",
      "55/295, train_loss: 0.9093, step time: 1.0502\n",
      "56/295, train_loss: 0.9470, step time: 1.0585\n",
      "57/295, train_loss: 0.9270, step time: 1.0549\n",
      "58/295, train_loss: 0.9807, step time: 1.0313\n",
      "59/295, train_loss: 0.8564, step time: 1.0389\n",
      "60/295, train_loss: 0.9512, step time: 1.0335\n",
      "61/295, train_loss: 0.9213, step time: 1.0409\n",
      "62/295, train_loss: 0.8618, step time: 1.0331\n",
      "63/295, train_loss: 0.9567, step time: 1.0460\n",
      "64/295, train_loss: 0.8284, step time: 1.0482\n",
      "65/295, train_loss: 0.8662, step time: 1.0561\n",
      "66/295, train_loss: 0.9524, step time: 1.0326\n",
      "67/295, train_loss: 0.9336, step time: 1.0320\n",
      "68/295, train_loss: 0.9838, step time: 1.0360\n",
      "69/295, train_loss: 0.9398, step time: 1.0329\n",
      "70/295, train_loss: 0.8509, step time: 1.0346\n",
      "71/295, train_loss: 0.9705, step time: 1.0365\n",
      "72/295, train_loss: 0.8843, step time: 1.0357\n",
      "73/295, train_loss: 0.9327, step time: 1.0739\n",
      "74/295, train_loss: 0.9056, step time: 1.0349\n",
      "75/295, train_loss: 0.9122, step time: 1.0327\n",
      "76/295, train_loss: 0.9167, step time: 1.0383\n",
      "77/295, train_loss: 0.8330, step time: 1.0560\n",
      "78/295, train_loss: 0.9094, step time: 1.0300\n",
      "79/295, train_loss: 0.8785, step time: 1.0330\n",
      "80/295, train_loss: 0.9225, step time: 1.0651\n",
      "81/295, train_loss: 0.9748, step time: 1.0321\n",
      "82/295, train_loss: 0.9113, step time: 1.0318\n",
      "83/295, train_loss: 0.9439, step time: 1.0559\n",
      "84/295, train_loss: 0.9008, step time: 1.0327\n",
      "85/295, train_loss: 0.8032, step time: 1.0645\n",
      "86/295, train_loss: 0.8792, step time: 1.0735\n",
      "87/295, train_loss: 0.9282, step time: 1.0351\n",
      "88/295, train_loss: 0.9063, step time: 1.0757\n",
      "89/295, train_loss: 0.8700, step time: 1.0485\n",
      "90/295, train_loss: 0.9202, step time: 1.0626\n",
      "91/295, train_loss: 0.9218, step time: 1.0382\n",
      "92/295, train_loss: 0.9070, step time: 1.0752\n",
      "93/295, train_loss: 0.8427, step time: 1.0652\n",
      "94/295, train_loss: 0.8561, step time: 1.0366\n",
      "95/295, train_loss: 0.8826, step time: 1.0313\n",
      "96/295, train_loss: 0.9667, step time: 1.0351\n",
      "97/295, train_loss: 0.9796, step time: 1.0478\n",
      "98/295, train_loss: 0.9392, step time: 1.0562\n",
      "99/295, train_loss: 0.8608, step time: 1.0589\n",
      "100/295, train_loss: 0.9859, step time: 1.0352\n",
      "101/295, train_loss: 0.9103, step time: 1.0366\n",
      "102/295, train_loss: 0.8977, step time: 1.0311\n",
      "103/295, train_loss: 0.8873, step time: 1.0518\n",
      "104/295, train_loss: 0.8538, step time: 1.0624\n",
      "105/295, train_loss: 0.8556, step time: 1.0598\n",
      "106/295, train_loss: 0.9114, step time: 1.0343\n",
      "107/295, train_loss: 0.9432, step time: 1.0347\n",
      "108/295, train_loss: 0.8446, step time: 1.0567\n",
      "109/295, train_loss: 0.9386, step time: 1.0347\n",
      "110/295, train_loss: 0.9119, step time: 1.0468\n",
      "111/295, train_loss: 0.9025, step time: 1.0492\n",
      "112/295, train_loss: 0.8350, step time: 1.0308\n",
      "113/295, train_loss: 0.9185, step time: 1.0385\n",
      "114/295, train_loss: 0.9573, step time: 1.0783\n",
      "115/295, train_loss: 0.8684, step time: 1.0351\n",
      "116/295, train_loss: 0.9710, step time: 1.0433\n",
      "117/295, train_loss: 0.8698, step time: 1.0401\n",
      "118/295, train_loss: 0.8199, step time: 1.0393\n",
      "119/295, train_loss: 0.9157, step time: 1.0354\n",
      "120/295, train_loss: 0.9718, step time: 1.0571\n",
      "121/295, train_loss: 0.9048, step time: 1.0506\n",
      "122/295, train_loss: 0.8719, step time: 1.0533\n",
      "123/295, train_loss: 0.8779, step time: 1.0309\n",
      "124/295, train_loss: 0.9258, step time: 1.0616\n",
      "125/295, train_loss: 0.9135, step time: 1.0325\n",
      "126/295, train_loss: 0.9790, step time: 1.0449\n",
      "127/295, train_loss: 0.8367, step time: 1.0353\n",
      "128/295, train_loss: 0.9115, step time: 1.0436\n",
      "129/295, train_loss: 0.8693, step time: 1.0354\n",
      "130/295, train_loss: 0.8670, step time: 1.0317\n",
      "131/295, train_loss: 0.8115, step time: 1.0293\n",
      "132/295, train_loss: 0.8835, step time: 1.0558\n",
      "133/295, train_loss: 0.9746, step time: 1.0418\n",
      "134/295, train_loss: 0.8171, step time: 1.0484\n",
      "135/295, train_loss: 0.9115, step time: 1.0482\n",
      "136/295, train_loss: 0.8491, step time: 1.0681\n",
      "137/295, train_loss: 0.9176, step time: 1.0373\n",
      "138/295, train_loss: 0.8321, step time: 1.0385\n",
      "139/295, train_loss: 0.7911, step time: 1.0595\n",
      "140/295, train_loss: 0.9725, step time: 1.0489\n",
      "141/295, train_loss: 0.9144, step time: 1.0397\n",
      "142/295, train_loss: 0.8644, step time: 1.0617\n",
      "143/295, train_loss: 0.9799, step time: 1.0515\n",
      "144/295, train_loss: 0.8488, step time: 1.0447\n",
      "145/295, train_loss: 0.9183, step time: 1.0338\n",
      "146/295, train_loss: 0.8782, step time: 1.0426\n",
      "147/295, train_loss: 0.9175, step time: 1.0673\n",
      "148/295, train_loss: 0.8641, step time: 1.0431\n",
      "149/295, train_loss: 0.8415, step time: 1.0408\n",
      "150/295, train_loss: 0.8578, step time: 1.0348\n",
      "151/295, train_loss: 0.9369, step time: 1.0409\n",
      "152/295, train_loss: 0.8778, step time: 1.0608\n",
      "153/295, train_loss: 0.9012, step time: 1.0394\n",
      "154/295, train_loss: 0.9311, step time: 1.1051\n",
      "155/295, train_loss: 0.8055, step time: 1.0371\n",
      "156/295, train_loss: 0.9045, step time: 1.0458\n",
      "157/295, train_loss: 0.9544, step time: 1.0482\n",
      "158/295, train_loss: 0.9436, step time: 1.0405\n",
      "159/295, train_loss: 0.8891, step time: 1.0494\n",
      "160/295, train_loss: 0.8606, step time: 1.0681\n",
      "161/295, train_loss: 0.8003, step time: 1.0391\n",
      "162/295, train_loss: 0.8681, step time: 1.0346\n",
      "163/295, train_loss: 0.8479, step time: 1.0330\n",
      "164/295, train_loss: 0.9423, step time: 1.0600\n",
      "165/295, train_loss: 0.8490, step time: 1.0417\n",
      "166/295, train_loss: 0.9140, step time: 1.0422\n",
      "167/295, train_loss: 0.8995, step time: 1.0457\n",
      "168/295, train_loss: 0.8891, step time: 1.0818\n",
      "169/295, train_loss: 0.8385, step time: 1.0315\n",
      "170/295, train_loss: 0.9152, step time: 1.0637\n",
      "171/295, train_loss: 0.8488, step time: 1.0350\n",
      "172/295, train_loss: 0.9511, step time: 1.0387\n",
      "173/295, train_loss: 0.8585, step time: 1.0329\n",
      "174/295, train_loss: 0.8117, step time: 1.0314\n",
      "175/295, train_loss: 0.9006, step time: 1.0412\n",
      "176/295, train_loss: 0.9678, step time: 1.0377\n",
      "177/295, train_loss: 0.9578, step time: 1.0633\n",
      "178/295, train_loss: 0.8715, step time: 1.0548\n",
      "179/295, train_loss: 0.9689, step time: 1.0313\n",
      "180/295, train_loss: 0.9453, step time: 1.0589\n",
      "181/295, train_loss: 0.9410, step time: 1.0319\n",
      "182/295, train_loss: 0.8177, step time: 1.0384\n",
      "183/295, train_loss: 0.9346, step time: 1.0328\n",
      "184/295, train_loss: 0.9459, step time: 1.0324\n",
      "185/295, train_loss: 0.9190, step time: 1.0622\n",
      "186/295, train_loss: 0.8855, step time: 1.0597\n",
      "187/295, train_loss: 0.9318, step time: 1.0325\n",
      "188/295, train_loss: 0.9655, step time: 1.0369\n",
      "189/295, train_loss: 0.7720, step time: 1.0482\n",
      "190/295, train_loss: 0.8511, step time: 1.0340\n",
      "191/295, train_loss: 0.8611, step time: 1.0685\n",
      "192/295, train_loss: 0.7999, step time: 1.0396\n",
      "193/295, train_loss: 0.8687, step time: 1.0663\n",
      "194/295, train_loss: 0.9429, step time: 1.0513\n",
      "195/295, train_loss: 0.8704, step time: 1.0480\n",
      "196/295, train_loss: 0.9417, step time: 1.0457\n",
      "197/295, train_loss: 0.7893, step time: 1.0560\n",
      "198/295, train_loss: 0.7837, step time: 1.0505\n",
      "199/295, train_loss: 0.7975, step time: 1.0627\n",
      "200/295, train_loss: 0.8039, step time: 1.0488\n",
      "201/295, train_loss: 0.7766, step time: 1.0591\n",
      "202/295, train_loss: 0.8665, step time: 1.0588\n",
      "203/295, train_loss: 0.7823, step time: 1.0376\n",
      "204/295, train_loss: 0.8342, step time: 1.0420\n",
      "205/295, train_loss: 0.8458, step time: 1.0326\n",
      "206/295, train_loss: 0.9790, step time: 1.0330\n",
      "207/295, train_loss: 0.9630, step time: 1.0751\n",
      "208/295, train_loss: 0.9361, step time: 1.0634\n",
      "209/295, train_loss: 0.9136, step time: 1.0410\n",
      "210/295, train_loss: 0.9285, step time: 1.0449\n",
      "211/295, train_loss: 0.7629, step time: 1.0376\n",
      "212/295, train_loss: 0.8233, step time: 1.0456\n",
      "213/295, train_loss: 0.8176, step time: 1.0705\n",
      "214/295, train_loss: 0.8890, step time: 1.1274\n",
      "215/295, train_loss: 0.8683, step time: 1.0327\n",
      "216/295, train_loss: 0.8504, step time: 1.0330\n",
      "217/295, train_loss: 0.9626, step time: 1.0346\n",
      "218/295, train_loss: 0.8546, step time: 1.0423\n",
      "219/295, train_loss: 0.9071, step time: 1.0505\n",
      "220/295, train_loss: 0.9168, step time: 1.0328\n",
      "221/295, train_loss: 0.8811, step time: 1.0418\n",
      "222/295, train_loss: 0.9668, step time: 1.0405\n",
      "223/295, train_loss: 0.9802, step time: 1.0335\n",
      "224/295, train_loss: 0.8658, step time: 1.0436\n",
      "225/295, train_loss: 0.8787, step time: 1.0330\n",
      "226/295, train_loss: 0.8447, step time: 1.0343\n",
      "227/295, train_loss: 0.8452, step time: 1.0318\n",
      "228/295, train_loss: 0.8701, step time: 1.0454\n",
      "229/295, train_loss: 0.8951, step time: 1.0374\n",
      "230/295, train_loss: 0.8225, step time: 1.0391\n",
      "231/295, train_loss: 0.9595, step time: 1.0341\n",
      "232/295, train_loss: 0.9545, step time: 1.0465\n",
      "233/295, train_loss: 0.7980, step time: 1.0651\n",
      "234/295, train_loss: 0.9340, step time: 1.0312\n",
      "235/295, train_loss: 0.7878, step time: 1.0582\n",
      "236/295, train_loss: 0.8749, step time: 1.0305\n",
      "237/295, train_loss: 0.8500, step time: 1.0414\n",
      "238/295, train_loss: 0.9050, step time: 1.0365\n",
      "239/295, train_loss: 0.8318, step time: 1.0341\n",
      "240/295, train_loss: 0.9732, step time: 1.0435\n",
      "241/295, train_loss: 0.8227, step time: 1.0465\n",
      "242/295, train_loss: 0.8016, step time: 1.0543\n",
      "243/295, train_loss: 0.7736, step time: 1.0420\n",
      "244/295, train_loss: 0.8321, step time: 1.0636\n",
      "245/295, train_loss: 0.7840, step time: 1.0338\n",
      "246/295, train_loss: 0.7990, step time: 1.0449\n",
      "247/295, train_loss: 0.8145, step time: 1.0341\n",
      "248/295, train_loss: 0.8555, step time: 1.0442\n",
      "249/295, train_loss: 0.7665, step time: 1.0462\n",
      "250/295, train_loss: 0.9172, step time: 1.0345\n",
      "251/295, train_loss: 0.8925, step time: 1.0449\n",
      "252/295, train_loss: 0.7453, step time: 1.0628\n",
      "253/295, train_loss: 0.7893, step time: 1.0613\n",
      "254/295, train_loss: 0.9060, step time: 1.0440\n",
      "255/295, train_loss: 0.8194, step time: 1.0410\n",
      "256/295, train_loss: 0.9107, step time: 1.0328\n",
      "257/295, train_loss: 0.7753, step time: 1.0387\n",
      "258/295, train_loss: 0.8563, step time: 1.0672\n",
      "259/295, train_loss: 0.8260, step time: 1.0599\n",
      "260/295, train_loss: 0.9422, step time: 1.0656\n",
      "261/295, train_loss: 0.8321, step time: 1.0421\n",
      "262/295, train_loss: 0.8194, step time: 1.0347\n",
      "263/295, train_loss: 0.7831, step time: 1.0386\n",
      "264/295, train_loss: 0.8259, step time: 1.0549\n",
      "265/295, train_loss: 0.9489, step time: 1.0505\n",
      "266/295, train_loss: 0.7400, step time: 1.0379\n",
      "267/295, train_loss: 0.9233, step time: 1.0797\n",
      "268/295, train_loss: 0.8100, step time: 1.0388\n",
      "269/295, train_loss: 0.9150, step time: 1.0364\n",
      "270/295, train_loss: 0.7681, step time: 1.0475\n",
      "271/295, train_loss: 0.9856, step time: 1.0624\n",
      "272/295, train_loss: 0.7703, step time: 1.0384\n",
      "273/295, train_loss: 0.9709, step time: 1.0327\n",
      "274/295, train_loss: 0.9330, step time: 1.0353\n",
      "275/295, train_loss: 0.9332, step time: 1.0350\n",
      "276/295, train_loss: 0.8983, step time: 1.0344\n",
      "277/295, train_loss: 0.9782, step time: 1.0534\n",
      "278/295, train_loss: 0.7764, step time: 1.0688\n",
      "279/295, train_loss: 0.9442, step time: 1.0449\n",
      "280/295, train_loss: 0.9341, step time: 1.0577\n",
      "281/295, train_loss: 0.7493, step time: 1.0384\n",
      "282/295, train_loss: 0.8717, step time: 1.0591\n",
      "283/295, train_loss: 0.8839, step time: 1.0435\n",
      "284/295, train_loss: 0.9207, step time: 1.0410\n",
      "285/295, train_loss: 0.8665, step time: 1.0352\n",
      "286/295, train_loss: 0.7961, step time: 1.0815\n",
      "287/295, train_loss: 0.8817, step time: 1.1096\n",
      "288/295, train_loss: 0.8804, step time: 1.0378\n",
      "289/295, train_loss: 0.8423, step time: 1.0345\n",
      "290/295, train_loss: 0.8812, step time: 1.0280\n",
      "291/295, train_loss: 0.8388, step time: 1.0285\n",
      "292/295, train_loss: 0.7669, step time: 1.0310\n",
      "293/295, train_loss: 0.9596, step time: 1.0289\n",
      "294/295, train_loss: 0.9327, step time: 1.0284\n",
      "295/295, train_loss: 0.8429, step time: 1.0281\n",
      "epoch 1 average loss: 0.8921\n",
      "saved new best metric model\n",
      "current epoch: 1 current mean dice: 0.3578 tc: 0.2520 wt: 0.7659 et: 0.0143\n",
      "best mean dice: 0.3578 at epoch: 1\n",
      "time consuming of epoch 1 is: 476.0190\n",
      "----------\n",
      "epoch 2/100\n",
      "1/295, train_loss: 0.7668, step time: 1.0932\n",
      "2/295, train_loss: 0.7852, step time: 1.1962\n",
      "3/295, train_loss: 0.9490, step time: 1.0508\n",
      "4/295, train_loss: 0.7346, step time: 1.0513\n",
      "5/295, train_loss: 0.7293, step time: 1.0452\n",
      "6/295, train_loss: 0.8891, step time: 1.0374\n",
      "7/295, train_loss: 0.7848, step time: 1.1080\n",
      "8/295, train_loss: 0.9764, step time: 1.0501\n",
      "9/295, train_loss: 0.7454, step time: 1.0767\n",
      "10/295, train_loss: 0.7124, step time: 1.0553\n",
      "11/295, train_loss: 0.7882, step time: 1.0437\n",
      "12/295, train_loss: 0.8806, step time: 1.0384\n",
      "13/295, train_loss: 0.7127, step time: 1.0382\n",
      "14/295, train_loss: 0.8413, step time: 1.0592\n",
      "15/295, train_loss: 0.9139, step time: 1.0369\n",
      "16/295, train_loss: 0.7719, step time: 1.0571\n",
      "17/295, train_loss: 0.8499, step time: 1.0407\n",
      "18/295, train_loss: 0.8703, step time: 1.0607\n",
      "19/295, train_loss: 0.7682, step time: 1.0483\n",
      "20/295, train_loss: 0.9478, step time: 1.0430\n",
      "21/295, train_loss: 0.8108, step time: 1.1051\n",
      "22/295, train_loss: 0.8935, step time: 1.1179\n",
      "23/295, train_loss: 0.9290, step time: 1.0343\n",
      "24/295, train_loss: 0.9651, step time: 1.0309\n",
      "25/295, train_loss: 0.7757, step time: 1.0351\n",
      "26/295, train_loss: 0.8949, step time: 1.0397\n",
      "27/295, train_loss: 0.7473, step time: 1.0565\n",
      "28/295, train_loss: 0.7415, step time: 1.0287\n",
      "29/295, train_loss: 0.8161, step time: 1.0455\n",
      "30/295, train_loss: 0.7597, step time: 1.0343\n",
      "31/295, train_loss: 0.7310, step time: 1.0312\n",
      "32/295, train_loss: 0.9008, step time: 1.0669\n",
      "33/295, train_loss: 0.8860, step time: 1.0304\n",
      "34/295, train_loss: 0.8205, step time: 1.0291\n",
      "35/295, train_loss: 0.8706, step time: 1.0326\n",
      "36/295, train_loss: 0.7941, step time: 1.0312\n",
      "37/295, train_loss: 0.8968, step time: 1.0941\n",
      "38/295, train_loss: 0.8852, step time: 1.0719\n",
      "39/295, train_loss: 0.8847, step time: 1.0535\n",
      "40/295, train_loss: 0.8358, step time: 1.0597\n",
      "41/295, train_loss: 0.7419, step time: 1.0298\n",
      "42/295, train_loss: 0.9498, step time: 1.0309\n",
      "43/295, train_loss: 0.7892, step time: 1.0329\n",
      "44/295, train_loss: 0.7743, step time: 1.0327\n",
      "45/295, train_loss: 0.7859, step time: 1.0579\n",
      "46/295, train_loss: 0.8308, step time: 1.0362\n",
      "47/295, train_loss: 0.7272, step time: 1.0311\n",
      "48/295, train_loss: 0.7226, step time: 1.0386\n",
      "49/295, train_loss: 0.7384, step time: 1.0469\n",
      "50/295, train_loss: 0.8548, step time: 1.0787\n",
      "51/295, train_loss: 0.8585, step time: 1.0325\n",
      "52/295, train_loss: 0.8335, step time: 1.0565\n",
      "53/295, train_loss: 0.8600, step time: 1.0321\n",
      "54/295, train_loss: 0.8560, step time: 1.0320\n",
      "55/295, train_loss: 0.7368, step time: 1.0667\n",
      "56/295, train_loss: 0.7190, step time: 1.0468\n",
      "57/295, train_loss: 0.7151, step time: 1.0491\n",
      "58/295, train_loss: 0.8952, step time: 1.0294\n",
      "59/295, train_loss: 0.7564, step time: 1.0296\n",
      "60/295, train_loss: 0.7877, step time: 1.0497\n",
      "61/295, train_loss: 0.7654, step time: 1.1058\n",
      "62/295, train_loss: 0.6735, step time: 1.0374\n",
      "63/295, train_loss: 0.8643, step time: 1.0447\n",
      "64/295, train_loss: 0.9070, step time: 1.0358\n",
      "65/295, train_loss: 0.8886, step time: 1.0356\n",
      "66/295, train_loss: 0.8998, step time: 1.0454\n",
      "67/295, train_loss: 0.7014, step time: 1.0720\n",
      "68/295, train_loss: 0.7763, step time: 1.0618\n",
      "69/295, train_loss: 0.8034, step time: 1.0313\n",
      "70/295, train_loss: 0.9077, step time: 1.0359\n",
      "71/295, train_loss: 0.7937, step time: 1.0464\n",
      "72/295, train_loss: 0.7723, step time: 1.0666\n",
      "73/295, train_loss: 0.7070, step time: 1.0318\n",
      "74/295, train_loss: 0.8915, step time: 1.0622\n",
      "75/295, train_loss: 0.7572, step time: 1.0383\n",
      "76/295, train_loss: 0.7039, step time: 1.0373\n",
      "77/295, train_loss: 0.7505, step time: 1.0573\n",
      "78/295, train_loss: 0.7117, step time: 1.0308\n",
      "79/295, train_loss: 0.7363, step time: 1.0353\n",
      "80/295, train_loss: 0.7191, step time: 1.0339\n",
      "81/295, train_loss: 0.7693, step time: 1.0530\n",
      "82/295, train_loss: 0.9461, step time: 1.0368\n",
      "83/295, train_loss: 0.6840, step time: 1.0330\n",
      "84/295, train_loss: 0.6768, step time: 1.0704\n",
      "85/295, train_loss: 0.8901, step time: 1.0573\n",
      "86/295, train_loss: 0.8214, step time: 1.0349\n",
      "87/295, train_loss: 0.7548, step time: 1.0318\n",
      "88/295, train_loss: 0.7169, step time: 1.0391\n",
      "89/295, train_loss: 0.7329, step time: 1.0323\n",
      "90/295, train_loss: 0.6407, step time: 1.0602\n",
      "91/295, train_loss: 0.9493, step time: 1.0666\n",
      "92/295, train_loss: 0.7474, step time: 1.0334\n",
      "93/295, train_loss: 0.7506, step time: 1.0423\n",
      "94/295, train_loss: 0.6803, step time: 1.0526\n",
      "95/295, train_loss: 0.6350, step time: 1.1051\n",
      "96/295, train_loss: 0.7154, step time: 1.0381\n",
      "97/295, train_loss: 0.7524, step time: 1.0460\n",
      "98/295, train_loss: 0.7420, step time: 1.0373\n",
      "99/295, train_loss: 0.7263, step time: 1.0363\n",
      "100/295, train_loss: 0.7398, step time: 1.0379\n",
      "101/295, train_loss: 0.6933, step time: 1.0399\n",
      "102/295, train_loss: 0.7069, step time: 1.0505\n",
      "103/295, train_loss: 0.7985, step time: 1.0475\n",
      "104/295, train_loss: 0.8600, step time: 1.0396\n",
      "105/295, train_loss: 0.7561, step time: 1.0892\n",
      "106/295, train_loss: 0.7172, step time: 1.0297\n",
      "107/295, train_loss: 0.7572, step time: 1.0304\n",
      "108/295, train_loss: 0.7204, step time: 1.0505\n",
      "109/295, train_loss: 0.7593, step time: 1.0366\n",
      "110/295, train_loss: 0.7765, step time: 1.0739\n",
      "111/295, train_loss: 0.6724, step time: 1.0372\n",
      "112/295, train_loss: 0.8995, step time: 1.0698\n",
      "113/295, train_loss: 0.8252, step time: 1.0285\n",
      "114/295, train_loss: 0.7109, step time: 1.0385\n",
      "115/295, train_loss: 0.9378, step time: 1.0313\n",
      "116/295, train_loss: 0.6805, step time: 1.0503\n",
      "117/295, train_loss: 0.9012, step time: 1.0579\n",
      "118/295, train_loss: 0.7237, step time: 1.0406\n",
      "119/295, train_loss: 0.7426, step time: 1.0418\n",
      "120/295, train_loss: 0.8274, step time: 1.0474\n",
      "121/295, train_loss: 0.7176, step time: 1.0987\n",
      "122/295, train_loss: 0.6373, step time: 1.0606\n",
      "123/295, train_loss: 0.7102, step time: 1.0580\n",
      "124/295, train_loss: 0.7729, step time: 1.0544\n",
      "125/295, train_loss: 0.6973, step time: 1.0288\n",
      "126/295, train_loss: 0.7974, step time: 1.0419\n",
      "127/295, train_loss: 0.8721, step time: 1.0354\n",
      "128/295, train_loss: 0.5971, step time: 1.0466\n",
      "129/295, train_loss: 0.7217, step time: 1.0648\n",
      "130/295, train_loss: 0.8521, step time: 1.0333\n",
      "131/295, train_loss: 0.6160, step time: 1.0651\n",
      "132/295, train_loss: 0.6730, step time: 1.0685\n",
      "133/295, train_loss: 0.7693, step time: 1.0329\n",
      "134/295, train_loss: 0.8484, step time: 1.0402\n",
      "135/295, train_loss: 0.7022, step time: 1.0353\n",
      "136/295, train_loss: 0.6986, step time: 1.0556\n",
      "137/295, train_loss: 0.6814, step time: 1.0749\n",
      "138/295, train_loss: 0.7862, step time: 1.0371\n",
      "139/295, train_loss: 0.8363, step time: 1.0353\n",
      "140/295, train_loss: 0.8891, step time: 1.0408\n",
      "141/295, train_loss: 0.9075, step time: 1.0767\n",
      "142/295, train_loss: 0.7931, step time: 1.0361\n",
      "143/295, train_loss: 0.6842, step time: 1.0386\n",
      "144/295, train_loss: 0.7393, step time: 1.0480\n",
      "145/295, train_loss: 0.6308, step time: 1.0373\n",
      "146/295, train_loss: 0.6365, step time: 1.0584\n",
      "147/295, train_loss: 0.8956, step time: 1.0370\n",
      "148/295, train_loss: 0.7709, step time: 1.0318\n",
      "149/295, train_loss: 0.6806, step time: 1.0398\n",
      "150/295, train_loss: 0.6972, step time: 1.0367\n",
      "151/295, train_loss: 0.6885, step time: 1.0546\n",
      "152/295, train_loss: 0.7998, step time: 1.0407\n",
      "153/295, train_loss: 0.7957, step time: 1.0387\n",
      "154/295, train_loss: 0.6331, step time: 1.0714\n",
      "155/295, train_loss: 0.7088, step time: 1.0322\n",
      "156/295, train_loss: 0.7344, step time: 1.0360\n",
      "157/295, train_loss: 0.7356, step time: 1.0328\n",
      "158/295, train_loss: 0.7408, step time: 1.0388\n",
      "159/295, train_loss: 0.6828, step time: 1.0321\n",
      "160/295, train_loss: 0.7125, step time: 1.0672\n",
      "161/295, train_loss: 0.7503, step time: 1.0687\n",
      "162/295, train_loss: 0.8157, step time: 1.0560\n",
      "163/295, train_loss: 0.6376, step time: 1.0450\n",
      "164/295, train_loss: 0.8749, step time: 1.0301\n",
      "165/295, train_loss: 0.5684, step time: 1.0302\n",
      "166/295, train_loss: 0.8187, step time: 1.0574\n",
      "167/295, train_loss: 0.7253, step time: 1.0592\n",
      "168/295, train_loss: 0.6323, step time: 1.0375\n",
      "169/295, train_loss: 0.5992, step time: 1.0377\n",
      "170/295, train_loss: 0.6637, step time: 1.0375\n",
      "171/295, train_loss: 0.6723, step time: 1.0370\n",
      "172/295, train_loss: 0.7242, step time: 1.0370\n",
      "173/295, train_loss: 0.7951, step time: 1.0359\n",
      "174/295, train_loss: 0.9149, step time: 1.0762\n",
      "175/295, train_loss: 0.6666, step time: 1.0563\n",
      "176/295, train_loss: 0.6082, step time: 1.0377\n",
      "177/295, train_loss: 0.7001, step time: 1.0543\n",
      "178/295, train_loss: 0.7029, step time: 1.0667\n",
      "179/295, train_loss: 0.6492, step time: 1.0573\n",
      "180/295, train_loss: 0.8177, step time: 1.0453\n",
      "181/295, train_loss: 0.7031, step time: 1.0442\n",
      "182/295, train_loss: 0.6015, step time: 1.0406\n",
      "183/295, train_loss: 0.5622, step time: 1.0340\n",
      "184/295, train_loss: 0.6283, step time: 1.0330\n",
      "185/295, train_loss: 0.7913, step time: 1.0350\n",
      "186/295, train_loss: 0.8276, step time: 1.0374\n",
      "187/295, train_loss: 0.8855, step time: 1.0467\n",
      "188/295, train_loss: 0.7812, step time: 1.0366\n",
      "189/295, train_loss: 0.6558, step time: 1.0372\n",
      "190/295, train_loss: 0.7170, step time: 1.0347\n",
      "191/295, train_loss: 0.7072, step time: 1.0380\n",
      "192/295, train_loss: 0.7003, step time: 1.0428\n",
      "193/295, train_loss: 0.7438, step time: 1.0354\n",
      "194/295, train_loss: 0.6869, step time: 1.0543\n",
      "195/295, train_loss: 0.8056, step time: 1.0318\n",
      "196/295, train_loss: 0.9022, step time: 1.0326\n",
      "197/295, train_loss: 0.7478, step time: 1.0607\n",
      "198/295, train_loss: 0.6452, step time: 1.0390\n",
      "199/295, train_loss: 0.6386, step time: 1.0685\n",
      "200/295, train_loss: 0.8917, step time: 1.0870\n",
      "201/295, train_loss: 0.6399, step time: 1.0374\n",
      "202/295, train_loss: 0.5994, step time: 1.0632\n",
      "203/295, train_loss: 0.9312, step time: 1.0378\n",
      "204/295, train_loss: 0.6306, step time: 1.0620\n",
      "205/295, train_loss: 0.6709, step time: 1.0442\n",
      "206/295, train_loss: 0.7098, step time: 1.0581\n",
      "207/295, train_loss: 0.5843, step time: 1.0455\n",
      "208/295, train_loss: 0.6782, step time: 1.0414\n",
      "209/295, train_loss: 0.5742, step time: 1.0666\n",
      "210/295, train_loss: 0.7959, step time: 1.0344\n",
      "211/295, train_loss: 0.6156, step time: 1.0380\n",
      "212/295, train_loss: 0.6041, step time: 1.0537\n",
      "213/295, train_loss: 0.8245, step time: 1.0378\n",
      "214/295, train_loss: 0.7363, step time: 1.0340\n",
      "215/295, train_loss: 0.9494, step time: 1.0474\n",
      "216/295, train_loss: 0.7151, step time: 1.0421\n",
      "217/295, train_loss: 0.5639, step time: 1.0446\n",
      "218/295, train_loss: 0.6523, step time: 1.0368\n",
      "219/295, train_loss: 0.6202, step time: 1.0338\n",
      "220/295, train_loss: 0.6428, step time: 1.0344\n",
      "221/295, train_loss: 0.8717, step time: 1.0828\n",
      "222/295, train_loss: 0.7222, step time: 1.0437\n",
      "223/295, train_loss: 0.6822, step time: 1.0321\n",
      "224/295, train_loss: 0.7510, step time: 1.0348\n",
      "225/295, train_loss: 0.8419, step time: 1.0304\n",
      "226/295, train_loss: 0.5422, step time: 1.0373\n",
      "227/295, train_loss: 0.6215, step time: 1.0339\n",
      "228/295, train_loss: 0.6895, step time: 1.0400\n",
      "229/295, train_loss: 0.6702, step time: 1.0401\n",
      "230/295, train_loss: 0.6940, step time: 1.0429\n",
      "231/295, train_loss: 0.7134, step time: 1.0913\n",
      "232/295, train_loss: 0.7567, step time: 1.0572\n",
      "233/295, train_loss: 0.9590, step time: 1.0364\n",
      "234/295, train_loss: 0.7053, step time: 1.0349\n",
      "235/295, train_loss: 0.6646, step time: 1.0353\n",
      "236/295, train_loss: 0.7631, step time: 1.0717\n",
      "237/295, train_loss: 0.6935, step time: 1.0617\n",
      "238/295, train_loss: 0.5894, step time: 1.0593\n",
      "239/295, train_loss: 0.8277, step time: 1.0302\n",
      "240/295, train_loss: 0.6712, step time: 1.0371\n",
      "241/295, train_loss: 0.6998, step time: 1.0344\n",
      "242/295, train_loss: 0.6075, step time: 1.0370\n",
      "243/295, train_loss: 0.6625, step time: 1.0756\n",
      "244/295, train_loss: 0.5858, step time: 1.0534\n",
      "245/295, train_loss: 0.7577, step time: 1.0379\n",
      "246/295, train_loss: 0.4757, step time: 1.0372\n",
      "247/295, train_loss: 0.7625, step time: 1.0356\n",
      "248/295, train_loss: 0.5307, step time: 1.0366\n",
      "249/295, train_loss: 0.5481, step time: 1.0399\n",
      "250/295, train_loss: 0.4704, step time: 1.0350\n",
      "251/295, train_loss: 0.8152, step time: 1.0578\n",
      "252/295, train_loss: 0.6050, step time: 1.0321\n",
      "253/295, train_loss: 0.8446, step time: 1.0380\n",
      "254/295, train_loss: 0.6499, step time: 1.0650\n",
      "255/295, train_loss: 0.6753, step time: 1.0364\n",
      "256/295, train_loss: 0.6214, step time: 1.0372\n",
      "257/295, train_loss: 0.6843, step time: 1.0973\n",
      "258/295, train_loss: 0.6087, step time: 1.0327\n",
      "259/295, train_loss: 0.5595, step time: 1.0450\n",
      "260/295, train_loss: 0.5641, step time: 1.0369\n",
      "261/295, train_loss: 0.6496, step time: 1.0369\n",
      "262/295, train_loss: 0.8139, step time: 1.0622\n",
      "263/295, train_loss: 0.5191, step time: 1.0333\n",
      "264/295, train_loss: 0.7319, step time: 1.0616\n",
      "265/295, train_loss: 0.9104, step time: 1.0526\n",
      "266/295, train_loss: 0.8699, step time: 1.0369\n",
      "267/295, train_loss: 0.5148, step time: 1.0341\n",
      "268/295, train_loss: 0.8845, step time: 1.0306\n",
      "269/295, train_loss: 0.7881, step time: 1.0374\n",
      "270/295, train_loss: 0.6785, step time: 1.0306\n",
      "271/295, train_loss: 0.6487, step time: 1.0396\n",
      "272/295, train_loss: 0.7174, step time: 1.0392\n",
      "273/295, train_loss: 0.4567, step time: 1.0985\n",
      "274/295, train_loss: 0.8380, step time: 1.0410\n",
      "275/295, train_loss: 0.6879, step time: 1.0382\n",
      "276/295, train_loss: 0.7104, step time: 1.0378\n",
      "277/295, train_loss: 0.3777, step time: 1.0438\n",
      "278/295, train_loss: 0.8114, step time: 1.0368\n",
      "279/295, train_loss: 0.6602, step time: 1.0410\n",
      "280/295, train_loss: 0.8460, step time: 1.0359\n",
      "281/295, train_loss: 0.6287, step time: 1.0402\n",
      "282/295, train_loss: 0.5548, step time: 1.0751\n",
      "283/295, train_loss: 0.7391, step time: 1.0344\n",
      "284/295, train_loss: 0.6344, step time: 1.0340\n",
      "285/295, train_loss: 0.6644, step time: 1.0319\n",
      "286/295, train_loss: 0.5226, step time: 1.0346\n",
      "287/295, train_loss: 0.8220, step time: 1.0355\n",
      "288/295, train_loss: 0.6060, step time: 1.0284\n",
      "289/295, train_loss: 0.5198, step time: 1.0282\n",
      "290/295, train_loss: 0.5320, step time: 1.0284\n",
      "291/295, train_loss: 0.7164, step time: 1.0277\n",
      "292/295, train_loss: 0.7103, step time: 1.0274\n",
      "293/295, train_loss: 0.7554, step time: 1.0281\n",
      "294/295, train_loss: 0.6525, step time: 1.0274\n",
      "295/295, train_loss: 0.4267, step time: 1.0276\n",
      "epoch 2 average loss: 0.7405\n",
      "saved new best metric model\n",
      "current epoch: 2 current mean dice: 0.5804 tc: 0.4957 wt: 0.8286 et: 0.3885\n",
      "best mean dice: 0.5804 at epoch: 2\n",
      "time consuming of epoch 2 is: 381.9814\n",
      "----------\n",
      "epoch 3/100\n",
      "1/295, train_loss: 0.8544, step time: 1.1007\n",
      "2/295, train_loss: 0.7828, step time: 1.0788\n",
      "3/295, train_loss: 0.6375, step time: 1.0627\n",
      "4/295, train_loss: 0.6853, step time: 1.0751\n",
      "5/295, train_loss: 0.4964, step time: 1.0322\n",
      "6/295, train_loss: 0.6137, step time: 1.0458\n",
      "7/295, train_loss: 0.5538, step time: 1.0299\n",
      "8/295, train_loss: 0.6460, step time: 1.0327\n",
      "9/295, train_loss: 0.8379, step time: 1.0337\n",
      "10/295, train_loss: 0.7478, step time: 1.0730\n",
      "11/295, train_loss: 0.6498, step time: 1.1005\n",
      "12/295, train_loss: 0.8180, step time: 1.0525\n",
      "13/295, train_loss: 0.4830, step time: 1.0558\n",
      "14/295, train_loss: 0.7174, step time: 1.0580\n",
      "15/295, train_loss: 0.5000, step time: 1.0832\n",
      "16/295, train_loss: 0.4935, step time: 1.0445\n",
      "17/295, train_loss: 0.6649, step time: 1.0298\n",
      "18/295, train_loss: 0.7224, step time: 1.0336\n",
      "19/295, train_loss: 0.5085, step time: 1.0386\n",
      "20/295, train_loss: 0.5210, step time: 1.1081\n",
      "21/295, train_loss: 0.6869, step time: 1.0316\n",
      "22/295, train_loss: 0.4685, step time: 1.0723\n",
      "23/295, train_loss: 0.3934, step time: 1.0491\n",
      "24/295, train_loss: 0.7892, step time: 1.0364\n",
      "25/295, train_loss: 0.4804, step time: 1.0294\n",
      "26/295, train_loss: 0.5850, step time: 1.0353\n",
      "27/295, train_loss: 0.5834, step time: 1.0444\n",
      "28/295, train_loss: 0.6968, step time: 1.0332\n",
      "29/295, train_loss: 0.4256, step time: 1.0340\n",
      "30/295, train_loss: 0.6185, step time: 1.0298\n",
      "31/295, train_loss: 0.9069, step time: 1.0295\n",
      "32/295, train_loss: 0.4586, step time: 1.0295\n",
      "33/295, train_loss: 0.7724, step time: 1.0328\n",
      "34/295, train_loss: 0.5298, step time: 1.0336\n",
      "35/295, train_loss: 0.7178, step time: 1.0302\n",
      "36/295, train_loss: 0.7874, step time: 1.0358\n",
      "37/295, train_loss: 0.7798, step time: 1.0677\n",
      "38/295, train_loss: 0.5173, step time: 1.0525\n",
      "39/295, train_loss: 0.4601, step time: 1.0564\n",
      "40/295, train_loss: 0.5537, step time: 1.0449\n",
      "41/295, train_loss: 0.4597, step time: 1.1035\n",
      "42/295, train_loss: 0.5951, step time: 1.0357\n",
      "43/295, train_loss: 0.4833, step time: 1.0670\n",
      "44/295, train_loss: 0.4307, step time: 1.0458\n",
      "45/295, train_loss: 0.4526, step time: 1.0383\n",
      "46/295, train_loss: 0.4485, step time: 1.0349\n",
      "47/295, train_loss: 0.5757, step time: 1.0336\n",
      "48/295, train_loss: 0.5886, step time: 1.0301\n",
      "49/295, train_loss: 0.7422, step time: 1.0370\n",
      "50/295, train_loss: 0.4434, step time: 1.0609\n",
      "51/295, train_loss: 0.3497, step time: 1.0376\n",
      "52/295, train_loss: 0.7273, step time: 1.0532\n",
      "53/295, train_loss: 0.6316, step time: 1.0641\n",
      "54/295, train_loss: 0.6339, step time: 1.0472\n",
      "55/295, train_loss: 0.5510, step time: 1.0638\n",
      "56/295, train_loss: 0.3641, step time: 1.0363\n",
      "57/295, train_loss: 0.3150, step time: 1.0387\n",
      "58/295, train_loss: 0.4603, step time: 1.0894\n",
      "59/295, train_loss: 0.5124, step time: 1.0358\n",
      "60/295, train_loss: 0.4437, step time: 1.0366\n",
      "61/295, train_loss: 0.5318, step time: 1.0424\n",
      "62/295, train_loss: 0.4104, step time: 1.0833\n",
      "63/295, train_loss: 0.5468, step time: 1.0300\n",
      "64/295, train_loss: 0.5254, step time: 1.0507\n",
      "65/295, train_loss: 0.4625, step time: 1.0484\n",
      "66/295, train_loss: 0.7873, step time: 1.0373\n",
      "67/295, train_loss: 0.8173, step time: 1.0470\n",
      "68/295, train_loss: 0.5411, step time: 1.0694\n",
      "69/295, train_loss: 0.5786, step time: 1.0336\n",
      "70/295, train_loss: 0.6546, step time: 1.0346\n",
      "71/295, train_loss: 0.8005, step time: 1.0600\n",
      "72/295, train_loss: 0.3674, step time: 1.0548\n",
      "73/295, train_loss: 0.7301, step time: 1.0822\n",
      "74/295, train_loss: 0.5702, step time: 1.0546\n",
      "75/295, train_loss: 0.5254, step time: 1.0484\n",
      "76/295, train_loss: 0.4684, step time: 1.0415\n",
      "77/295, train_loss: 0.4983, step time: 1.0388\n",
      "78/295, train_loss: 0.6625, step time: 1.0331\n",
      "79/295, train_loss: 0.5498, step time: 1.0326\n",
      "80/295, train_loss: 0.4803, step time: 1.0328\n",
      "81/295, train_loss: 0.5136, step time: 1.0338\n",
      "82/295, train_loss: 0.4209, step time: 1.0448\n",
      "83/295, train_loss: 0.7108, step time: 1.0770\n",
      "84/295, train_loss: 0.4226, step time: 1.0391\n",
      "85/295, train_loss: 0.6378, step time: 1.0585\n",
      "86/295, train_loss: 0.8953, step time: 1.0306\n",
      "87/295, train_loss: 0.5160, step time: 1.0407\n",
      "88/295, train_loss: 0.7552, step time: 1.0926\n",
      "89/295, train_loss: 0.3945, step time: 1.0357\n",
      "90/295, train_loss: 0.3173, step time: 1.0400\n",
      "91/295, train_loss: 0.3688, step time: 1.0776\n",
      "92/295, train_loss: 0.3878, step time: 1.0336\n",
      "93/295, train_loss: 0.6876, step time: 1.0355\n",
      "94/295, train_loss: 0.4194, step time: 1.0773\n",
      "95/295, train_loss: 0.6001, step time: 1.0541\n",
      "96/295, train_loss: 0.4706, step time: 1.0757\n",
      "97/295, train_loss: 0.4039, step time: 1.0361\n",
      "98/295, train_loss: 0.3965, step time: 1.0314\n",
      "99/295, train_loss: 0.2802, step time: 1.0395\n",
      "100/295, train_loss: 0.6621, step time: 1.0975\n",
      "101/295, train_loss: 0.8143, step time: 1.0355\n",
      "102/295, train_loss: 0.5140, step time: 1.0432\n",
      "103/295, train_loss: 0.4226, step time: 1.0428\n",
      "104/295, train_loss: 0.3025, step time: 1.0306\n",
      "105/295, train_loss: 0.4935, step time: 1.0321\n",
      "106/295, train_loss: 0.3820, step time: 1.0317\n",
      "107/295, train_loss: 0.5906, step time: 1.0617\n",
      "108/295, train_loss: 0.7295, step time: 1.0399\n",
      "109/295, train_loss: 0.5145, step time: 1.0407\n",
      "110/295, train_loss: 0.6526, step time: 1.0328\n",
      "111/295, train_loss: 0.4442, step time: 1.0422\n",
      "112/295, train_loss: 0.4414, step time: 1.0933\n",
      "113/295, train_loss: 0.4979, step time: 1.0309\n",
      "114/295, train_loss: 0.7463, step time: 1.0431\n",
      "115/295, train_loss: 0.6127, step time: 1.0687\n",
      "116/295, train_loss: 0.6221, step time: 1.0420\n",
      "117/295, train_loss: 0.6312, step time: 1.0336\n",
      "118/295, train_loss: 0.6078, step time: 1.0406\n",
      "119/295, train_loss: 0.4346, step time: 1.0699\n",
      "120/295, train_loss: 0.7305, step time: 1.0391\n",
      "121/295, train_loss: 0.8195, step time: 1.0471\n",
      "122/295, train_loss: 0.4512, step time: 1.0359\n",
      "123/295, train_loss: 0.4342, step time: 1.0654\n",
      "124/295, train_loss: 0.3901, step time: 1.0351\n",
      "125/295, train_loss: 0.2893, step time: 1.0634\n",
      "126/295, train_loss: 0.3183, step time: 1.0318\n",
      "127/295, train_loss: 0.2290, step time: 1.0385\n",
      "128/295, train_loss: 0.6309, step time: 1.0587\n",
      "129/295, train_loss: 0.6913, step time: 1.0931\n",
      "130/295, train_loss: 0.4407, step time: 1.0321\n",
      "131/295, train_loss: 0.4662, step time: 1.0955\n",
      "132/295, train_loss: 0.4537, step time: 1.0341\n",
      "133/295, train_loss: 0.6065, step time: 1.0351\n",
      "134/295, train_loss: 0.3553, step time: 1.0345\n",
      "135/295, train_loss: 0.5056, step time: 1.0386\n",
      "136/295, train_loss: 0.4046, step time: 1.0487\n",
      "137/295, train_loss: 0.3578, step time: 1.0399\n",
      "138/295, train_loss: 0.2927, step time: 1.0388\n",
      "139/295, train_loss: 0.4764, step time: 1.0501\n",
      "140/295, train_loss: 0.4669, step time: 1.0431\n",
      "141/295, train_loss: 0.5448, step time: 1.0359\n",
      "142/295, train_loss: 0.5596, step time: 1.0513\n",
      "143/295, train_loss: 0.2179, step time: 1.0396\n",
      "144/295, train_loss: 0.6608, step time: 1.0453\n",
      "145/295, train_loss: 0.3499, step time: 1.0572\n",
      "146/295, train_loss: 0.7786, step time: 1.0382\n",
      "147/295, train_loss: 0.3268, step time: 1.0465\n",
      "148/295, train_loss: 0.4637, step time: 1.0347\n",
      "149/295, train_loss: 0.4100, step time: 1.0422\n",
      "150/295, train_loss: 0.4207, step time: 1.0358\n",
      "151/295, train_loss: 0.3725, step time: 1.0361\n",
      "152/295, train_loss: 0.7716, step time: 1.0475\n",
      "153/295, train_loss: 0.4152, step time: 1.1306\n",
      "154/295, train_loss: 0.2838, step time: 1.0397\n",
      "155/295, train_loss: 0.2702, step time: 1.0357\n",
      "156/295, train_loss: 0.5379, step time: 1.1156\n",
      "157/295, train_loss: 0.4658, step time: 1.0475\n",
      "158/295, train_loss: 0.4541, step time: 1.0400\n",
      "159/295, train_loss: 0.4605, step time: 1.0327\n",
      "160/295, train_loss: 0.6471, step time: 1.0342\n",
      "161/295, train_loss: 0.4860, step time: 1.0342\n",
      "162/295, train_loss: 0.6236, step time: 1.0336\n",
      "163/295, train_loss: 0.4492, step time: 1.0337\n",
      "164/295, train_loss: 0.6981, step time: 1.0422\n",
      "165/295, train_loss: 0.4253, step time: 1.0436\n",
      "166/295, train_loss: 0.5504, step time: 1.0718\n",
      "167/295, train_loss: 0.5619, step time: 1.0471\n",
      "168/295, train_loss: 0.2708, step time: 1.0351\n",
      "169/295, train_loss: 0.2551, step time: 1.0692\n",
      "170/295, train_loss: 0.2091, step time: 1.0562\n",
      "171/295, train_loss: 0.3382, step time: 1.0413\n",
      "172/295, train_loss: 0.6890, step time: 1.0391\n",
      "173/295, train_loss: 0.3493, step time: 1.0496\n",
      "174/295, train_loss: 0.3630, step time: 1.0523\n",
      "175/295, train_loss: 0.5288, step time: 1.0359\n",
      "176/295, train_loss: 0.4091, step time: 1.0447\n",
      "177/295, train_loss: 0.6019, step time: 1.1349\n",
      "178/295, train_loss: 0.4762, step time: 1.0385\n",
      "179/295, train_loss: 0.3092, step time: 1.0750\n",
      "180/295, train_loss: 0.7531, step time: 1.0526\n",
      "181/295, train_loss: 0.4988, step time: 1.0321\n",
      "182/295, train_loss: 0.6032, step time: 1.0511\n",
      "183/295, train_loss: 0.5490, step time: 1.0356\n",
      "184/295, train_loss: 0.3089, step time: 1.0631\n",
      "185/295, train_loss: 0.5160, step time: 1.0683\n",
      "186/295, train_loss: 0.4478, step time: 1.0401\n",
      "187/295, train_loss: 0.4656, step time: 1.0542\n",
      "188/295, train_loss: 0.3572, step time: 1.0374\n",
      "189/295, train_loss: 0.7740, step time: 1.0397\n",
      "190/295, train_loss: 0.3464, step time: 1.1112\n",
      "191/295, train_loss: 0.3723, step time: 1.0342\n",
      "192/295, train_loss: 0.2630, step time: 1.0430\n",
      "193/295, train_loss: 0.2822, step time: 1.0414\n",
      "194/295, train_loss: 0.4238, step time: 1.1208\n",
      "195/295, train_loss: 0.2917, step time: 1.0761\n",
      "196/295, train_loss: 0.6510, step time: 1.0576\n",
      "197/295, train_loss: 0.3609, step time: 1.0532\n",
      "198/295, train_loss: 0.2835, step time: 1.0335\n",
      "199/295, train_loss: 0.4462, step time: 1.0403\n",
      "200/295, train_loss: 0.3754, step time: 1.0416\n",
      "201/295, train_loss: 0.2718, step time: 1.0372\n",
      "202/295, train_loss: 0.2420, step time: 1.0356\n",
      "203/295, train_loss: 0.6129, step time: 1.0596\n",
      "204/295, train_loss: 0.8431, step time: 1.0509\n",
      "205/295, train_loss: 0.2744, step time: 1.0427\n",
      "206/295, train_loss: 0.3736, step time: 1.0588\n",
      "207/295, train_loss: 0.4339, step time: 1.0338\n",
      "208/295, train_loss: 0.5414, step time: 1.0332\n",
      "209/295, train_loss: 0.6307, step time: 1.0914\n",
      "210/295, train_loss: 0.2000, step time: 1.0812\n",
      "211/295, train_loss: 0.4805, step time: 1.0362\n",
      "212/295, train_loss: 0.2404, step time: 1.0565\n",
      "213/295, train_loss: 0.4134, step time: 1.0643\n",
      "214/295, train_loss: 0.4928, step time: 1.0356\n",
      "215/295, train_loss: 0.3031, step time: 1.0522\n",
      "216/295, train_loss: 0.5120, step time: 1.0820\n",
      "217/295, train_loss: 0.2825, step time: 1.0427\n",
      "218/295, train_loss: 0.1901, step time: 1.0849\n",
      "219/295, train_loss: 0.6382, step time: 1.0307\n",
      "220/295, train_loss: 0.7611, step time: 1.0425\n",
      "221/295, train_loss: 0.3813, step time: 1.0424\n",
      "222/295, train_loss: 0.3064, step time: 1.0317\n",
      "223/295, train_loss: 0.2371, step time: 1.0323\n",
      "224/295, train_loss: 0.4499, step time: 1.0375\n",
      "225/295, train_loss: 0.5167, step time: 1.0597\n",
      "226/295, train_loss: 0.4005, step time: 1.0453\n",
      "227/295, train_loss: 0.2148, step time: 1.0540\n",
      "228/295, train_loss: 0.3528, step time: 1.0317\n",
      "229/295, train_loss: 0.5279, step time: 1.0335\n",
      "230/295, train_loss: 0.6557, step time: 1.0748\n",
      "231/295, train_loss: 0.1709, step time: 1.0372\n",
      "232/295, train_loss: 0.3267, step time: 1.0439\n",
      "233/295, train_loss: 0.8817, step time: 1.0770\n",
      "234/295, train_loss: 0.4781, step time: 1.0395\n",
      "235/295, train_loss: 0.5015, step time: 1.0456\n",
      "236/295, train_loss: 0.3036, step time: 1.0708\n",
      "237/295, train_loss: 0.3061, step time: 1.0350\n",
      "238/295, train_loss: 0.7543, step time: 1.0342\n",
      "239/295, train_loss: 0.4503, step time: 1.0332\n",
      "240/295, train_loss: 0.2120, step time: 1.0306\n",
      "241/295, train_loss: 0.2580, step time: 1.0539\n",
      "242/295, train_loss: 0.2010, step time: 1.0606\n",
      "243/295, train_loss: 0.2988, step time: 1.0551\n",
      "244/295, train_loss: 0.3782, step time: 1.0420\n",
      "245/295, train_loss: 0.5135, step time: 1.0366\n",
      "246/295, train_loss: 0.3683, step time: 1.0319\n",
      "247/295, train_loss: 0.4486, step time: 1.0388\n",
      "248/295, train_loss: 0.6225, step time: 1.0923\n",
      "249/295, train_loss: 0.5058, step time: 1.0411\n",
      "250/295, train_loss: 0.1375, step time: 1.0531\n",
      "251/295, train_loss: 0.2134, step time: 1.0331\n",
      "252/295, train_loss: 0.5465, step time: 1.0518\n",
      "253/295, train_loss: 0.5838, step time: 1.0317\n",
      "254/295, train_loss: 0.2037, step time: 1.0435\n",
      "255/295, train_loss: 0.3722, step time: 1.0534\n",
      "256/295, train_loss: 0.2914, step time: 1.0352\n",
      "257/295, train_loss: 0.5842, step time: 1.0631\n",
      "258/295, train_loss: 0.3681, step time: 1.0591\n",
      "259/295, train_loss: 0.4052, step time: 1.0521\n",
      "260/295, train_loss: 0.2235, step time: 1.0649\n",
      "261/295, train_loss: 0.3151, step time: 1.0322\n",
      "262/295, train_loss: 0.6491, step time: 1.0358\n",
      "263/295, train_loss: 0.1500, step time: 1.0434\n",
      "264/295, train_loss: 0.1574, step time: 1.0885\n",
      "265/295, train_loss: 0.4289, step time: 1.0950\n",
      "266/295, train_loss: 0.2634, step time: 1.0558\n",
      "267/295, train_loss: 0.4218, step time: 1.0356\n",
      "268/295, train_loss: 0.6936, step time: 1.0605\n",
      "269/295, train_loss: 0.5508, step time: 1.0808\n",
      "270/295, train_loss: 0.3763, step time: 1.0541\n",
      "271/295, train_loss: 0.5157, step time: 1.0488\n",
      "272/295, train_loss: 0.5438, step time: 1.0339\n",
      "273/295, train_loss: 0.5005, step time: 1.0367\n",
      "274/295, train_loss: 0.4125, step time: 1.0353\n",
      "275/295, train_loss: 0.3310, step time: 1.0359\n",
      "276/295, train_loss: 0.1984, step time: 1.0356\n",
      "277/295, train_loss: 0.4481, step time: 1.0596\n",
      "278/295, train_loss: 0.3370, step time: 1.0679\n",
      "279/295, train_loss: 0.3594, step time: 1.0805\n",
      "280/295, train_loss: 0.2306, step time: 1.0327\n",
      "281/295, train_loss: 0.1595, step time: 1.0358\n",
      "282/295, train_loss: 0.5854, step time: 1.0335\n",
      "283/295, train_loss: 0.2846, step time: 1.0530\n",
      "284/295, train_loss: 0.3013, step time: 1.0337\n",
      "285/295, train_loss: 0.2708, step time: 1.0314\n",
      "286/295, train_loss: 0.4829, step time: 1.0393\n",
      "287/295, train_loss: 0.6043, step time: 1.0483\n",
      "288/295, train_loss: 0.3103, step time: 1.0458\n",
      "289/295, train_loss: 0.3938, step time: 1.0281\n",
      "290/295, train_loss: 0.5095, step time: 1.0276\n",
      "291/295, train_loss: 0.2614, step time: 1.0270\n",
      "292/295, train_loss: 0.2903, step time: 1.0272\n",
      "293/295, train_loss: 0.3159, step time: 1.0276\n",
      "294/295, train_loss: 0.1941, step time: 1.0278\n",
      "295/295, train_loss: 0.7107, step time: 1.0272\n",
      "epoch 3 average loss: 0.4848\n",
      "saved new best metric model\n",
      "current epoch: 3 current mean dice: 0.7167 tc: 0.6704 wt: 0.8271 et: 0.6425\n",
      "best mean dice: 0.7167 at epoch: 3\n",
      "time consuming of epoch 3 is: 378.2855\n",
      "----------\n",
      "epoch 4/100\n",
      "1/295, train_loss: 0.6839, step time: 1.0990\n",
      "2/295, train_loss: 0.5207, step time: 1.0912\n",
      "3/295, train_loss: 0.3950, step time: 1.0535\n",
      "4/295, train_loss: 0.2160, step time: 1.0465\n",
      "5/295, train_loss: 0.6148, step time: 1.0341\n",
      "6/295, train_loss: 0.3390, step time: 1.0392\n",
      "7/295, train_loss: 0.1405, step time: 1.0382\n",
      "8/295, train_loss: 0.1909, step time: 1.0516\n",
      "9/295, train_loss: 0.4258, step time: 1.0449\n",
      "10/295, train_loss: 0.1399, step time: 1.1067\n",
      "11/295, train_loss: 0.3353, step time: 1.0493\n",
      "12/295, train_loss: 0.1467, step time: 1.0452\n",
      "13/295, train_loss: 0.5274, step time: 1.0330\n",
      "14/295, train_loss: 0.1407, step time: 1.0378\n",
      "15/295, train_loss: 0.2092, step time: 1.0538\n",
      "16/295, train_loss: 0.5046, step time: 1.0359\n",
      "17/295, train_loss: 0.2749, step time: 1.0378\n",
      "18/295, train_loss: 0.1528, step time: 1.0390\n",
      "19/295, train_loss: 0.3916, step time: 1.0400\n",
      "20/295, train_loss: 0.2921, step time: 1.0293\n",
      "21/295, train_loss: 0.6281, step time: 1.0292\n",
      "22/295, train_loss: 0.1092, step time: 1.0303\n",
      "23/295, train_loss: 0.1227, step time: 1.0525\n",
      "24/295, train_loss: 0.5672, step time: 1.0365\n",
      "25/295, train_loss: 0.6785, step time: 1.0439\n",
      "26/295, train_loss: 0.4932, step time: 1.0384\n",
      "27/295, train_loss: 0.5652, step time: 1.0516\n",
      "28/295, train_loss: 0.4339, step time: 1.0294\n",
      "29/295, train_loss: 0.1715, step time: 1.0297\n",
      "30/295, train_loss: 0.2054, step time: 1.0572\n",
      "31/295, train_loss: 0.1766, step time: 1.0564\n",
      "32/295, train_loss: 0.2437, step time: 1.0434\n",
      "33/295, train_loss: 0.4018, step time: 1.0374\n",
      "34/295, train_loss: 0.5927, step time: 1.0449\n",
      "35/295, train_loss: 0.1692, step time: 1.0655\n",
      "36/295, train_loss: 0.2081, step time: 1.0558\n",
      "37/295, train_loss: 0.5513, step time: 1.0281\n",
      "38/295, train_loss: 0.3894, step time: 1.0286\n",
      "39/295, train_loss: 0.2407, step time: 1.0432\n",
      "40/295, train_loss: 0.1299, step time: 1.0305\n",
      "41/295, train_loss: 0.2693, step time: 1.0429\n",
      "42/295, train_loss: 0.5205, step time: 1.0325\n",
      "43/295, train_loss: 0.2430, step time: 1.0446\n",
      "44/295, train_loss: 0.6048, step time: 1.0589\n",
      "45/295, train_loss: 0.1680, step time: 1.0316\n",
      "46/295, train_loss: 0.2422, step time: 1.0475\n",
      "47/295, train_loss: 0.1195, step time: 1.0333\n",
      "48/295, train_loss: 0.1349, step time: 1.0342\n",
      "49/295, train_loss: 0.3910, step time: 1.0696\n",
      "50/295, train_loss: 0.6864, step time: 1.0749\n",
      "51/295, train_loss: 0.3099, step time: 1.0290\n",
      "52/295, train_loss: 0.6762, step time: 1.0441\n",
      "53/295, train_loss: 0.2355, step time: 1.0340\n",
      "54/295, train_loss: 0.6414, step time: 1.0361\n",
      "55/295, train_loss: 0.2024, step time: 1.0462\n",
      "56/295, train_loss: 0.1865, step time: 1.1177\n",
      "57/295, train_loss: 0.2566, step time: 1.0409\n",
      "58/295, train_loss: 0.1879, step time: 1.0332\n",
      "59/295, train_loss: 0.2920, step time: 1.0333\n",
      "60/295, train_loss: 0.2421, step time: 1.0414\n",
      "61/295, train_loss: 0.2808, step time: 1.0330\n",
      "62/295, train_loss: 0.4889, step time: 1.0289\n",
      "63/295, train_loss: 0.1273, step time: 1.0297\n",
      "64/295, train_loss: 0.1736, step time: 1.0298\n",
      "65/295, train_loss: 0.2196, step time: 1.0307\n",
      "66/295, train_loss: 0.4203, step time: 1.0307\n",
      "67/295, train_loss: 0.3908, step time: 1.0370\n",
      "68/295, train_loss: 0.4811, step time: 1.0294\n",
      "69/295, train_loss: 0.1548, step time: 1.0384\n",
      "70/295, train_loss: 0.3382, step time: 1.0767\n",
      "71/295, train_loss: 0.1714, step time: 1.0805\n",
      "72/295, train_loss: 0.2945, step time: 1.0642\n",
      "73/295, train_loss: 0.1146, step time: 1.0685\n",
      "74/295, train_loss: 0.4694, step time: 1.0375\n",
      "75/295, train_loss: 0.1354, step time: 1.0398\n",
      "76/295, train_loss: 0.2950, step time: 1.0301\n",
      "77/295, train_loss: 0.3802, step time: 1.0341\n",
      "78/295, train_loss: 0.2782, step time: 1.0379\n",
      "79/295, train_loss: 0.3123, step time: 1.0533\n",
      "80/295, train_loss: 0.2193, step time: 1.0302\n",
      "81/295, train_loss: 0.1572, step time: 1.0365\n",
      "82/295, train_loss: 0.7209, step time: 1.0664\n",
      "83/295, train_loss: 0.4325, step time: 1.0669\n",
      "84/295, train_loss: 0.2089, step time: 1.0306\n",
      "85/295, train_loss: 0.1667, step time: 1.0559\n",
      "86/295, train_loss: 0.2086, step time: 1.0562\n",
      "87/295, train_loss: 0.4589, step time: 1.0305\n",
      "88/295, train_loss: 0.1606, step time: 1.0712\n",
      "89/295, train_loss: 0.1747, step time: 1.0379\n",
      "90/295, train_loss: 0.1803, step time: 1.0553\n",
      "91/295, train_loss: 0.1663, step time: 1.0888\n",
      "92/295, train_loss: 0.3359, step time: 1.0299\n",
      "93/295, train_loss: 0.3172, step time: 1.0584\n",
      "94/295, train_loss: 0.1690, step time: 1.0394\n",
      "95/295, train_loss: 0.5528, step time: 1.0290\n",
      "96/295, train_loss: 0.3516, step time: 1.0295\n",
      "97/295, train_loss: 0.1184, step time: 1.0316\n",
      "98/295, train_loss: 0.4335, step time: 1.0327\n",
      "99/295, train_loss: 0.1243, step time: 1.1008\n",
      "100/295, train_loss: 0.6548, step time: 1.0416\n",
      "101/295, train_loss: 0.3343, step time: 1.0762\n",
      "102/295, train_loss: 0.2238, step time: 1.0348\n",
      "103/295, train_loss: 0.1379, step time: 1.0460\n",
      "104/295, train_loss: 0.1447, step time: 1.1066\n",
      "105/295, train_loss: 0.3413, step time: 1.0305\n",
      "106/295, train_loss: 0.4547, step time: 1.0397\n",
      "107/295, train_loss: 0.6027, step time: 1.0287\n",
      "108/295, train_loss: 0.1797, step time: 1.0298\n",
      "109/295, train_loss: 0.2775, step time: 1.0313\n",
      "110/295, train_loss: 0.6335, step time: 1.0434\n",
      "111/295, train_loss: 0.1887, step time: 1.0482\n",
      "112/295, train_loss: 0.3847, step time: 1.0420\n",
      "113/295, train_loss: 0.1654, step time: 1.0370\n",
      "114/295, train_loss: 0.1935, step time: 1.0291\n",
      "115/295, train_loss: 0.1905, step time: 1.0371\n",
      "116/295, train_loss: 0.4500, step time: 1.0321\n",
      "117/295, train_loss: 0.2171, step time: 1.0340\n",
      "118/295, train_loss: 0.3306, step time: 1.0739\n",
      "119/295, train_loss: 0.4386, step time: 1.0577\n",
      "120/295, train_loss: 0.3009, step time: 1.0462\n",
      "121/295, train_loss: 0.1596, step time: 1.0396\n",
      "122/295, train_loss: 0.1339, step time: 1.0319\n",
      "123/295, train_loss: 0.3653, step time: 1.0422\n",
      "124/295, train_loss: 0.2248, step time: 1.0282\n",
      "125/295, train_loss: 0.4862, step time: 1.0285\n",
      "126/295, train_loss: 0.2285, step time: 1.0298\n",
      "127/295, train_loss: 0.2412, step time: 1.0392\n",
      "128/295, train_loss: 0.2170, step time: 1.0584\n",
      "129/295, train_loss: 0.2054, step time: 1.0312\n",
      "130/295, train_loss: 0.0990, step time: 1.0497\n",
      "131/295, train_loss: 0.1384, step time: 1.0327\n",
      "132/295, train_loss: 0.1051, step time: 1.0546\n",
      "133/295, train_loss: 0.1542, step time: 1.0509\n",
      "134/295, train_loss: 0.2224, step time: 1.0570\n",
      "135/295, train_loss: 0.2355, step time: 1.0465\n",
      "136/295, train_loss: 0.3245, step time: 1.0499\n",
      "137/295, train_loss: 0.2130, step time: 1.0602\n",
      "138/295, train_loss: 0.5239, step time: 1.0324\n",
      "139/295, train_loss: 0.4746, step time: 1.0324\n",
      "140/295, train_loss: 0.2007, step time: 1.0343\n",
      "141/295, train_loss: 0.1949, step time: 1.0785\n",
      "142/295, train_loss: 0.3550, step time: 1.0418\n",
      "143/295, train_loss: 0.3974, step time: 1.0331\n",
      "144/295, train_loss: 0.2278, step time: 1.0751\n",
      "145/295, train_loss: 0.4252, step time: 1.1025\n",
      "146/295, train_loss: 0.3099, step time: 1.0337\n",
      "147/295, train_loss: 0.2532, step time: 1.0508\n",
      "148/295, train_loss: 0.1970, step time: 1.0371\n",
      "149/295, train_loss: 0.2387, step time: 1.0489\n",
      "150/295, train_loss: 0.2863, step time: 1.0335\n",
      "151/295, train_loss: 0.1036, step time: 1.0425\n",
      "152/295, train_loss: 0.2442, step time: 1.0395\n",
      "153/295, train_loss: 0.3784, step time: 1.1101\n",
      "154/295, train_loss: 0.3231, step time: 1.0409\n",
      "155/295, train_loss: 0.1005, step time: 1.0337\n",
      "156/295, train_loss: 0.1737, step time: 1.0735\n",
      "157/295, train_loss: 0.3617, step time: 1.0432\n",
      "158/295, train_loss: 0.3780, step time: 1.0652\n",
      "159/295, train_loss: 0.2591, step time: 1.0489\n",
      "160/295, train_loss: 0.8688, step time: 1.0460\n",
      "161/295, train_loss: 0.6733, step time: 1.0320\n",
      "162/295, train_loss: 0.3792, step time: 1.0725\n",
      "163/295, train_loss: 0.1370, step time: 1.0439\n",
      "164/295, train_loss: 0.7757, step time: 1.0380\n",
      "165/295, train_loss: 0.6875, step time: 1.0469\n",
      "166/295, train_loss: 0.3016, step time: 1.1080\n",
      "167/295, train_loss: 0.1550, step time: 1.0476\n",
      "168/295, train_loss: 0.1409, step time: 1.0390\n",
      "169/295, train_loss: 0.1542, step time: 1.0494\n",
      "170/295, train_loss: 0.3662, step time: 1.0532\n",
      "171/295, train_loss: 0.4815, step time: 1.0340\n",
      "172/295, train_loss: 0.6018, step time: 1.0687\n",
      "173/295, train_loss: 0.4508, step time: 1.0353\n",
      "174/295, train_loss: 0.2062, step time: 1.0617\n",
      "175/295, train_loss: 0.2745, step time: 1.0602\n",
      "176/295, train_loss: 0.1688, step time: 1.0449\n",
      "177/295, train_loss: 0.1475, step time: 1.0395\n",
      "178/295, train_loss: 0.4915, step time: 1.0353\n",
      "179/295, train_loss: 0.1244, step time: 1.0389\n",
      "180/295, train_loss: 0.4537, step time: 1.0316\n",
      "181/295, train_loss: 0.3035, step time: 1.0752\n",
      "182/295, train_loss: 0.1922, step time: 1.0377\n",
      "183/295, train_loss: 0.3933, step time: 1.0383\n",
      "184/295, train_loss: 0.5010, step time: 1.1043\n",
      "185/295, train_loss: 0.1508, step time: 1.0293\n",
      "186/295, train_loss: 0.7255, step time: 1.0463\n",
      "187/295, train_loss: 0.2094, step time: 1.0544\n",
      "188/295, train_loss: 0.2573, step time: 1.0329\n",
      "189/295, train_loss: 0.1060, step time: 1.0329\n",
      "190/295, train_loss: 0.4272, step time: 1.0431\n",
      "191/295, train_loss: 0.4714, step time: 1.0478\n",
      "192/295, train_loss: 0.5244, step time: 1.0290\n",
      "193/295, train_loss: 0.3552, step time: 1.0293\n",
      "194/295, train_loss: 0.3279, step time: 1.0293\n",
      "195/295, train_loss: 0.2899, step time: 1.0284\n",
      "196/295, train_loss: 0.1674, step time: 1.0289\n",
      "197/295, train_loss: 0.3084, step time: 1.0291\n",
      "198/295, train_loss: 0.1104, step time: 1.0400\n",
      "199/295, train_loss: 0.1987, step time: 1.0337\n",
      "200/295, train_loss: 0.1863, step time: 1.0350\n",
      "201/295, train_loss: 0.5282, step time: 1.1005\n",
      "202/295, train_loss: 0.6950, step time: 1.0301\n",
      "203/295, train_loss: 0.1394, step time: 1.0309\n",
      "204/295, train_loss: 0.2655, step time: 1.0340\n",
      "205/295, train_loss: 0.3289, step time: 1.0505\n",
      "206/295, train_loss: 0.2443, step time: 1.0358\n",
      "207/295, train_loss: 0.2237, step time: 1.0370\n",
      "208/295, train_loss: 0.1733, step time: 1.0675\n",
      "209/295, train_loss: 0.3090, step time: 1.0361\n",
      "210/295, train_loss: 0.5536, step time: 1.0327\n",
      "211/295, train_loss: 0.2599, step time: 1.0643\n",
      "212/295, train_loss: 0.2504, step time: 1.0452\n",
      "213/295, train_loss: 0.4327, step time: 1.0319\n",
      "214/295, train_loss: 0.2247, step time: 1.0359\n",
      "215/295, train_loss: 0.6603, step time: 1.0398\n",
      "216/295, train_loss: 0.4119, step time: 1.0349\n",
      "217/295, train_loss: 0.1694, step time: 1.0435\n",
      "218/295, train_loss: 0.3652, step time: 1.0624\n",
      "219/295, train_loss: 0.2007, step time: 1.0333\n",
      "220/295, train_loss: 0.2876, step time: 1.0318\n",
      "221/295, train_loss: 0.2932, step time: 1.0491\n",
      "222/295, train_loss: 0.4423, step time: 1.0349\n",
      "223/295, train_loss: 0.2559, step time: 1.0343\n",
      "224/295, train_loss: 0.2879, step time: 1.0638\n",
      "225/295, train_loss: 0.1514, step time: 1.0518\n",
      "226/295, train_loss: 0.1788, step time: 1.0358\n",
      "227/295, train_loss: 0.1116, step time: 1.0335\n",
      "228/295, train_loss: 0.3609, step time: 1.0625\n",
      "229/295, train_loss: 0.3127, step time: 1.0594\n",
      "230/295, train_loss: 0.1031, step time: 1.0463\n",
      "231/295, train_loss: 0.2037, step time: 1.0519\n",
      "232/295, train_loss: 0.1667, step time: 1.0323\n",
      "233/295, train_loss: 0.4789, step time: 1.0343\n",
      "234/295, train_loss: 0.2525, step time: 1.0324\n",
      "235/295, train_loss: 0.2777, step time: 1.0305\n",
      "236/295, train_loss: 0.1571, step time: 1.0553\n",
      "237/295, train_loss: 0.1809, step time: 1.0729\n",
      "238/295, train_loss: 0.1879, step time: 1.0324\n",
      "239/295, train_loss: 0.4142, step time: 1.0306\n",
      "240/295, train_loss: 0.4635, step time: 1.0397\n",
      "241/295, train_loss: 0.1627, step time: 1.0401\n",
      "242/295, train_loss: 0.2946, step time: 1.0439\n",
      "243/295, train_loss: 0.1286, step time: 1.0354\n",
      "244/295, train_loss: 0.3131, step time: 1.0721\n",
      "245/295, train_loss: 0.1716, step time: 1.0419\n",
      "246/295, train_loss: 0.2544, step time: 1.0345\n",
      "247/295, train_loss: 0.3222, step time: 1.0301\n",
      "248/295, train_loss: 0.1567, step time: 1.0348\n",
      "249/295, train_loss: 0.2352, step time: 1.0514\n",
      "250/295, train_loss: 0.1423, step time: 1.0782\n",
      "251/295, train_loss: 0.4042, step time: 1.0341\n",
      "252/295, train_loss: 0.3326, step time: 1.0368\n",
      "253/295, train_loss: 0.5248, step time: 1.0453\n",
      "254/295, train_loss: 0.5490, step time: 1.0448\n",
      "255/295, train_loss: 0.2408, step time: 1.0356\n",
      "256/295, train_loss: 0.4988, step time: 1.0599\n",
      "257/295, train_loss: 0.1625, step time: 1.0575\n",
      "258/295, train_loss: 0.2354, step time: 1.0375\n",
      "259/295, train_loss: 0.2322, step time: 1.0290\n",
      "260/295, train_loss: 0.6600, step time: 1.0376\n",
      "261/295, train_loss: 0.4102, step time: 1.0320\n",
      "262/295, train_loss: 0.5008, step time: 1.0540\n",
      "263/295, train_loss: 0.4725, step time: 1.1740\n",
      "264/295, train_loss: 0.7111, step time: 1.0368\n",
      "265/295, train_loss: 0.1866, step time: 1.0367\n",
      "266/295, train_loss: 0.1608, step time: 1.0568\n",
      "267/295, train_loss: 0.3029, step time: 1.0500\n",
      "268/295, train_loss: 0.1942, step time: 1.0529\n",
      "269/295, train_loss: 0.1607, step time: 1.0367\n",
      "270/295, train_loss: 0.3090, step time: 1.0754\n",
      "271/295, train_loss: 0.1029, step time: 1.0653\n",
      "272/295, train_loss: 0.1513, step time: 1.0525\n",
      "273/295, train_loss: 0.1046, step time: 1.0573\n",
      "274/295, train_loss: 0.3522, step time: 1.0385\n",
      "275/295, train_loss: 0.4376, step time: 1.0520\n",
      "276/295, train_loss: 0.1363, step time: 1.0343\n",
      "277/295, train_loss: 0.3216, step time: 1.0719\n",
      "278/295, train_loss: 0.2934, step time: 1.0379\n",
      "279/295, train_loss: 0.4644, step time: 1.0349\n",
      "280/295, train_loss: 0.5454, step time: 1.0366\n",
      "281/295, train_loss: 0.0869, step time: 1.0378\n",
      "282/295, train_loss: 0.1434, step time: 1.1082\n",
      "283/295, train_loss: 0.3064, step time: 1.0324\n",
      "284/295, train_loss: 0.2332, step time: 1.0537\n",
      "285/295, train_loss: 0.2061, step time: 1.0549\n",
      "286/295, train_loss: 0.4457, step time: 1.0403\n",
      "287/295, train_loss: 0.2350, step time: 1.0625\n",
      "288/295, train_loss: 0.7165, step time: 1.0288\n",
      "289/295, train_loss: 0.1435, step time: 1.0283\n",
      "290/295, train_loss: 0.5695, step time: 1.0286\n",
      "291/295, train_loss: 0.2446, step time: 1.0277\n",
      "292/295, train_loss: 0.5607, step time: 1.0277\n",
      "293/295, train_loss: 0.1316, step time: 1.0266\n",
      "294/295, train_loss: 0.4119, step time: 1.0270\n",
      "295/295, train_loss: 0.1234, step time: 1.0283\n",
      "epoch 4 average loss: 0.3107\n",
      "saved new best metric model\n",
      "current epoch: 4 current mean dice: 0.7230 tc: 0.6706 wt: 0.8151 et: 0.6779\n",
      "best mean dice: 0.7230 at epoch: 4\n",
      "time consuming of epoch 4 is: 373.0407\n",
      "----------\n",
      "epoch 5/100\n",
      "1/295, train_loss: 0.3142, step time: 1.0673\n",
      "2/295, train_loss: 0.3768, step time: 1.0701\n",
      "3/295, train_loss: 0.3894, step time: 1.0641\n",
      "4/295, train_loss: 0.4698, step time: 1.0456\n",
      "5/295, train_loss: 0.1610, step time: 1.0716\n",
      "6/295, train_loss: 0.1391, step time: 1.0310\n",
      "7/295, train_loss: 0.3276, step time: 1.0537\n",
      "8/295, train_loss: 0.1871, step time: 1.0841\n",
      "9/295, train_loss: 0.3985, step time: 1.0378\n",
      "10/295, train_loss: 0.5159, step time: 1.0305\n",
      "11/295, train_loss: 0.6150, step time: 1.0401\n",
      "12/295, train_loss: 0.1285, step time: 1.0770\n",
      "13/295, train_loss: 0.2666, step time: 1.0433\n",
      "14/295, train_loss: 0.2476, step time: 1.0334\n",
      "15/295, train_loss: 0.1087, step time: 1.0382\n",
      "16/295, train_loss: 0.5086, step time: 1.0581\n",
      "17/295, train_loss: 0.1238, step time: 1.0600\n",
      "18/295, train_loss: 0.1594, step time: 1.0503\n",
      "19/295, train_loss: 0.2190, step time: 1.0442\n",
      "20/295, train_loss: 0.4440, step time: 1.0790\n",
      "21/295, train_loss: 0.1588, step time: 1.0467\n",
      "22/295, train_loss: 0.4168, step time: 1.0343\n",
      "23/295, train_loss: 0.1661, step time: 1.0377\n",
      "24/295, train_loss: 0.1591, step time: 1.0437\n",
      "25/295, train_loss: 0.0990, step time: 1.0440\n",
      "26/295, train_loss: 0.1313, step time: 1.0302\n",
      "27/295, train_loss: 0.1543, step time: 1.0384\n",
      "28/295, train_loss: 0.2542, step time: 1.0461\n",
      "29/295, train_loss: 0.6717, step time: 1.0422\n",
      "30/295, train_loss: 0.3910, step time: 1.0661\n",
      "31/295, train_loss: 0.6161, step time: 1.0896\n",
      "32/295, train_loss: 0.4349, step time: 1.0479\n",
      "33/295, train_loss: 0.1056, step time: 1.0359\n",
      "34/295, train_loss: 0.1624, step time: 1.0345\n",
      "35/295, train_loss: 0.2597, step time: 1.0426\n",
      "36/295, train_loss: 0.6923, step time: 1.0507\n",
      "37/295, train_loss: 0.6080, step time: 1.0409\n",
      "38/295, train_loss: 0.1712, step time: 1.0403\n",
      "39/295, train_loss: 0.1507, step time: 1.0354\n",
      "40/295, train_loss: 0.2444, step time: 1.0423\n",
      "41/295, train_loss: 0.2024, step time: 1.0504\n",
      "42/295, train_loss: 0.2378, step time: 1.0347\n",
      "43/295, train_loss: 0.1531, step time: 1.0424\n",
      "44/295, train_loss: 0.2826, step time: 1.0539\n",
      "45/295, train_loss: 0.1494, step time: 1.0774\n",
      "46/295, train_loss: 0.0979, step time: 1.0825\n",
      "47/295, train_loss: 0.2238, step time: 1.0408\n",
      "48/295, train_loss: 0.5422, step time: 1.0288\n",
      "49/295, train_loss: 0.1849, step time: 1.0565\n",
      "50/295, train_loss: 0.5375, step time: 1.0386\n",
      "51/295, train_loss: 0.1089, step time: 1.0358\n",
      "52/295, train_loss: 0.6133, step time: 1.0499\n",
      "53/295, train_loss: 0.2765, step time: 1.0352\n",
      "54/295, train_loss: 0.2407, step time: 1.0412\n",
      "55/295, train_loss: 0.0801, step time: 1.0314\n",
      "56/295, train_loss: 0.1909, step time: 1.0428\n",
      "57/295, train_loss: 0.2881, step time: 1.0711\n",
      "58/295, train_loss: 0.5274, step time: 1.0553\n",
      "59/295, train_loss: 0.1250, step time: 1.0387\n",
      "60/295, train_loss: 0.2119, step time: 1.0581\n",
      "61/295, train_loss: 0.4293, step time: 1.0337\n",
      "62/295, train_loss: 0.1942, step time: 1.0552\n",
      "63/295, train_loss: 0.1136, step time: 1.0383\n",
      "64/295, train_loss: 0.1681, step time: 1.0665\n",
      "65/295, train_loss: 0.1860, step time: 1.0338\n",
      "66/295, train_loss: 0.1435, step time: 1.0358\n",
      "67/295, train_loss: 0.2375, step time: 1.0355\n",
      "68/295, train_loss: 0.1391, step time: 1.0483\n",
      "69/295, train_loss: 0.2429, step time: 1.0394\n",
      "70/295, train_loss: 0.1283, step time: 1.0715\n",
      "71/295, train_loss: 0.1783, step time: 1.0300\n",
      "72/295, train_loss: 0.3473, step time: 1.0421\n",
      "73/295, train_loss: 0.5130, step time: 1.0429\n",
      "74/295, train_loss: 0.3192, step time: 1.0497\n",
      "75/295, train_loss: 0.2073, step time: 1.0461\n",
      "76/295, train_loss: 0.1937, step time: 1.0355\n",
      "77/295, train_loss: 0.0987, step time: 1.0348\n",
      "78/295, train_loss: 0.2148, step time: 1.0721\n",
      "79/295, train_loss: 0.2951, step time: 1.0380\n",
      "80/295, train_loss: 0.2772, step time: 1.0408\n",
      "81/295, train_loss: 0.1707, step time: 1.0614\n",
      "82/295, train_loss: 0.0712, step time: 1.0308\n",
      "83/295, train_loss: 0.5134, step time: 1.0345\n",
      "84/295, train_loss: 0.1497, step time: 1.0699\n",
      "85/295, train_loss: 0.4943, step time: 1.0328\n",
      "86/295, train_loss: 0.2108, step time: 1.0678\n",
      "87/295, train_loss: 0.1810, step time: 1.0315\n",
      "88/295, train_loss: 0.1763, step time: 1.0376\n",
      "89/295, train_loss: 0.3705, step time: 1.0828\n",
      "90/295, train_loss: 0.1392, step time: 1.0525\n",
      "91/295, train_loss: 0.3081, step time: 1.0472\n",
      "92/295, train_loss: 0.4301, step time: 1.0287\n",
      "93/295, train_loss: 0.2140, step time: 1.0301\n",
      "94/295, train_loss: 0.5121, step time: 1.0350\n",
      "95/295, train_loss: 0.4386, step time: 1.0517\n",
      "96/295, train_loss: 0.2251, step time: 1.0910\n",
      "97/295, train_loss: 0.7178, step time: 1.0496\n",
      "98/295, train_loss: 0.1861, step time: 1.0634\n",
      "99/295, train_loss: 0.2129, step time: 1.0759\n",
      "100/295, train_loss: 0.1480, step time: 1.0290\n",
      "101/295, train_loss: 0.0971, step time: 1.0437\n",
      "102/295, train_loss: 0.1906, step time: 1.0366\n",
      "103/295, train_loss: 0.1078, step time: 1.0297\n",
      "104/295, train_loss: 0.1088, step time: 1.0358\n",
      "105/295, train_loss: 0.1234, step time: 1.0415\n",
      "106/295, train_loss: 0.2856, step time: 1.0589\n",
      "107/295, train_loss: 0.2321, step time: 1.1021\n",
      "108/295, train_loss: 0.4143, step time: 1.0349\n",
      "109/295, train_loss: 0.0822, step time: 1.0296\n",
      "110/295, train_loss: 0.6279, step time: 1.0954\n",
      "111/295, train_loss: 0.1277, step time: 1.0294\n",
      "112/295, train_loss: 0.4144, step time: 1.0297\n",
      "113/295, train_loss: 0.1826, step time: 1.0353\n",
      "114/295, train_loss: 0.1464, step time: 1.0376\n",
      "115/295, train_loss: 0.1505, step time: 1.0415\n",
      "116/295, train_loss: 0.1718, step time: 1.0402\n",
      "117/295, train_loss: 0.1333, step time: 1.0368\n",
      "118/295, train_loss: 0.2293, step time: 1.0359\n",
      "119/295, train_loss: 0.1250, step time: 1.0442\n",
      "120/295, train_loss: 0.4157, step time: 1.0349\n",
      "121/295, train_loss: 0.1494, step time: 1.0395\n",
      "122/295, train_loss: 0.1457, step time: 1.0522\n",
      "123/295, train_loss: 0.2288, step time: 1.0340\n",
      "124/295, train_loss: 0.3977, step time: 1.0553\n",
      "125/295, train_loss: 0.3975, step time: 1.0464\n",
      "126/295, train_loss: 0.2996, step time: 1.0339\n",
      "127/295, train_loss: 0.1548, step time: 1.0362\n",
      "128/295, train_loss: 0.0981, step time: 1.0305\n",
      "129/295, train_loss: 0.2689, step time: 1.0320\n",
      "130/295, train_loss: 0.4595, step time: 1.0300\n",
      "131/295, train_loss: 0.2564, step time: 1.0452\n",
      "132/295, train_loss: 0.1427, step time: 1.0345\n",
      "133/295, train_loss: 0.0965, step time: 1.0306\n",
      "134/295, train_loss: 0.2497, step time: 1.0840\n",
      "135/295, train_loss: 0.1356, step time: 1.0417\n",
      "136/295, train_loss: 0.2448, step time: 1.0576\n",
      "137/295, train_loss: 0.1447, step time: 1.0493\n",
      "138/295, train_loss: 0.2079, step time: 1.0533\n",
      "139/295, train_loss: 0.4013, step time: 1.0324\n",
      "140/295, train_loss: 0.1233, step time: 1.0294\n",
      "141/295, train_loss: 0.2874, step time: 1.0317\n",
      "142/295, train_loss: 0.2996, step time: 1.0650\n",
      "143/295, train_loss: 0.2302, step time: 1.0947\n",
      "144/295, train_loss: 0.1304, step time: 1.0367\n",
      "145/295, train_loss: 0.1848, step time: 1.0370\n",
      "146/295, train_loss: 0.1478, step time: 1.0590\n",
      "147/295, train_loss: 0.1234, step time: 1.1007\n",
      "148/295, train_loss: 0.4250, step time: 1.0366\n",
      "149/295, train_loss: 0.1815, step time: 1.0416\n",
      "150/295, train_loss: 0.2368, step time: 1.0408\n",
      "151/295, train_loss: 0.0812, step time: 1.0443\n",
      "152/295, train_loss: 0.1396, step time: 1.0282\n",
      "153/295, train_loss: 0.2763, step time: 1.0302\n",
      "154/295, train_loss: 0.4476, step time: 1.0783\n",
      "155/295, train_loss: 0.1104, step time: 1.0547\n",
      "156/295, train_loss: 0.0849, step time: 1.0383\n",
      "157/295, train_loss: 0.2466, step time: 1.0303\n",
      "158/295, train_loss: 0.4193, step time: 1.0527\n",
      "159/295, train_loss: 0.0651, step time: 1.0423\n",
      "160/295, train_loss: 0.1130, step time: 1.0383\n",
      "161/295, train_loss: 0.0702, step time: 1.0383\n",
      "162/295, train_loss: 0.1388, step time: 1.0335\n",
      "163/295, train_loss: 0.2921, step time: 1.0656\n",
      "164/295, train_loss: 0.1920, step time: 1.0512\n",
      "165/295, train_loss: 0.7599, step time: 1.0119\n",
      "166/295, train_loss: 0.1345, step time: 1.0637\n",
      "167/295, train_loss: 0.0907, step time: 1.0298\n",
      "168/295, train_loss: 0.5244, step time: 1.0342\n",
      "169/295, train_loss: 0.4258, step time: 1.0453\n",
      "170/295, train_loss: 0.1182, step time: 1.0549\n",
      "171/295, train_loss: 0.0892, step time: 1.1087\n",
      "172/295, train_loss: 0.2422, step time: 1.0818\n",
      "173/295, train_loss: 0.3066, step time: 1.0323\n",
      "174/295, train_loss: 0.4929, step time: 1.0511\n",
      "175/295, train_loss: 0.1592, step time: 1.0329\n",
      "176/295, train_loss: 0.5379, step time: 1.0580\n",
      "177/295, train_loss: 0.4528, step time: 1.0326\n",
      "178/295, train_loss: 0.1121, step time: 1.0338\n",
      "179/295, train_loss: 0.3398, step time: 1.0677\n",
      "180/295, train_loss: 0.4773, step time: 1.0297\n",
      "181/295, train_loss: 0.2391, step time: 1.0547\n",
      "182/295, train_loss: 0.5658, step time: 1.0339\n",
      "183/295, train_loss: 0.0910, step time: 1.0449\n",
      "184/295, train_loss: 0.5689, step time: 1.0579\n",
      "185/295, train_loss: 0.3091, step time: 1.0380\n",
      "186/295, train_loss: 0.1723, step time: 1.0370\n",
      "187/295, train_loss: 0.1512, step time: 1.0360\n",
      "188/295, train_loss: 0.4879, step time: 1.0306\n",
      "189/295, train_loss: 0.3209, step time: 1.0375\n",
      "190/295, train_loss: 0.2361, step time: 1.0428\n",
      "191/295, train_loss: 0.2896, step time: 1.1259\n",
      "192/295, train_loss: 0.2056, step time: 1.0367\n",
      "193/295, train_loss: 0.1152, step time: 1.0302\n",
      "194/295, train_loss: 0.3064, step time: 1.0352\n",
      "195/295, train_loss: 0.5952, step time: 1.0313\n",
      "196/295, train_loss: 0.2200, step time: 1.0353\n",
      "197/295, train_loss: 0.1582, step time: 1.0600\n",
      "198/295, train_loss: 0.4786, step time: 1.0619\n",
      "199/295, train_loss: 0.4607, step time: 1.0695\n",
      "200/295, train_loss: 0.1762, step time: 1.0292\n",
      "201/295, train_loss: 0.0773, step time: 1.0358\n",
      "202/295, train_loss: 0.7210, step time: 1.0385\n",
      "203/295, train_loss: 0.3030, step time: 1.0503\n",
      "204/295, train_loss: 0.2214, step time: 1.0308\n",
      "205/295, train_loss: 0.4840, step time: 1.0373\n",
      "206/295, train_loss: 0.0862, step time: 1.0315\n",
      "207/295, train_loss: 0.2201, step time: 1.0626\n",
      "208/295, train_loss: 0.0715, step time: 1.0557\n",
      "209/295, train_loss: 0.1891, step time: 1.0311\n",
      "210/295, train_loss: 0.3570, step time: 1.0500\n",
      "211/295, train_loss: 0.1373, step time: 1.0466\n",
      "212/295, train_loss: 0.2390, step time: 1.0549\n",
      "213/295, train_loss: 0.2763, step time: 1.0314\n",
      "214/295, train_loss: 0.3002, step time: 1.0399\n",
      "215/295, train_loss: 0.0892, step time: 1.0307\n",
      "216/295, train_loss: 0.3624, step time: 1.0439\n",
      "217/295, train_loss: 0.1158, step time: 1.0584\n",
      "218/295, train_loss: 0.0971, step time: 1.0587\n",
      "219/295, train_loss: 0.0964, step time: 1.0643\n",
      "220/295, train_loss: 0.1512, step time: 1.0399\n",
      "221/295, train_loss: 0.1947, step time: 1.0532\n",
      "222/295, train_loss: 0.2153, step time: 1.0339\n",
      "223/295, train_loss: 0.1617, step time: 1.0294\n",
      "224/295, train_loss: 0.1567, step time: 1.0308\n",
      "225/295, train_loss: 0.1997, step time: 1.0858\n",
      "226/295, train_loss: 0.1511, step time: 1.0387\n",
      "227/295, train_loss: 0.2229, step time: 1.0344\n",
      "228/295, train_loss: 0.5023, step time: 1.0338\n",
      "229/295, train_loss: 0.2346, step time: 1.0393\n",
      "230/295, train_loss: 0.1190, step time: 1.0531\n",
      "231/295, train_loss: 0.1259, step time: 1.0320\n",
      "232/295, train_loss: 0.2033, step time: 1.0434\n",
      "233/295, train_loss: 0.1276, step time: 1.0463\n",
      "234/295, train_loss: 0.2468, step time: 1.0494\n",
      "235/295, train_loss: 0.2872, step time: 1.0320\n",
      "236/295, train_loss: 0.1328, step time: 1.0719\n",
      "237/295, train_loss: 0.5728, step time: 1.0325\n",
      "238/295, train_loss: 0.1735, step time: 1.0565\n",
      "239/295, train_loss: 0.3943, step time: 1.0718\n",
      "240/295, train_loss: 0.5244, step time: 1.0342\n",
      "241/295, train_loss: 0.1799, step time: 1.0387\n",
      "242/295, train_loss: 0.3816, step time: 1.0378\n",
      "243/295, train_loss: 0.2107, step time: 1.0384\n",
      "244/295, train_loss: 0.3129, step time: 1.0358\n",
      "245/295, train_loss: 0.0748, step time: 1.0328\n",
      "246/295, train_loss: 0.3465, step time: 1.0491\n",
      "247/295, train_loss: 0.4154, step time: 1.0438\n",
      "248/295, train_loss: 0.2487, step time: 1.0385\n",
      "249/295, train_loss: 0.1651, step time: 1.0331\n",
      "250/295, train_loss: 0.1955, step time: 1.0321\n",
      "251/295, train_loss: 0.4935, step time: 1.0295\n",
      "252/295, train_loss: 0.4210, step time: 1.0282\n",
      "253/295, train_loss: 0.2377, step time: 1.0367\n",
      "254/295, train_loss: 0.1581, step time: 1.0307\n",
      "255/295, train_loss: 0.1788, step time: 1.1004\n",
      "256/295, train_loss: 0.2912, step time: 1.0509\n",
      "257/295, train_loss: 0.1104, step time: 1.0421\n",
      "258/295, train_loss: 0.1507, step time: 1.0341\n",
      "259/295, train_loss: 0.2147, step time: 1.0395\n",
      "260/295, train_loss: 0.1470, step time: 1.0559\n",
      "261/295, train_loss: 0.1563, step time: 1.0370\n",
      "262/295, train_loss: 0.1190, step time: 1.0299\n",
      "263/295, train_loss: 0.2673, step time: 1.0379\n",
      "264/295, train_loss: 0.4320, step time: 1.0531\n",
      "265/295, train_loss: 0.2188, step time: 1.0402\n",
      "266/295, train_loss: 0.1735, step time: 1.0558\n",
      "267/295, train_loss: 0.2890, step time: 1.0324\n",
      "268/295, train_loss: 0.1654, step time: 1.0356\n",
      "269/295, train_loss: 0.2822, step time: 1.1248\n",
      "270/295, train_loss: 0.2718, step time: 1.0316\n",
      "271/295, train_loss: 0.1517, step time: 1.0435\n",
      "272/295, train_loss: 0.2576, step time: 1.0388\n",
      "273/295, train_loss: 0.5347, step time: 1.0528\n",
      "274/295, train_loss: 0.4463, step time: 1.0551\n",
      "275/295, train_loss: 0.2021, step time: 1.0350\n",
      "276/295, train_loss: 0.3335, step time: 1.0569\n",
      "277/295, train_loss: 0.1562, step time: 1.0384\n",
      "278/295, train_loss: 0.0782, step time: 1.0502\n",
      "279/295, train_loss: 0.0817, step time: 1.0331\n",
      "280/295, train_loss: 0.1152, step time: 1.0597\n",
      "281/295, train_loss: 0.6460, step time: 1.0338\n",
      "282/295, train_loss: 0.5076, step time: 1.0312\n",
      "283/295, train_loss: 0.1468, step time: 1.0581\n",
      "284/295, train_loss: 0.2468, step time: 1.0390\n",
      "285/295, train_loss: 0.1835, step time: 1.0397\n",
      "286/295, train_loss: 0.1088, step time: 1.1175\n",
      "287/295, train_loss: 0.2786, step time: 1.0417\n",
      "288/295, train_loss: 0.2655, step time: 1.0294\n",
      "289/295, train_loss: 0.0989, step time: 1.0276\n",
      "290/295, train_loss: 0.2264, step time: 1.0267\n",
      "291/295, train_loss: 0.1920, step time: 1.0276\n",
      "292/295, train_loss: 0.1028, step time: 1.0274\n",
      "293/295, train_loss: 0.1926, step time: 1.0273\n",
      "294/295, train_loss: 0.1159, step time: 1.0280\n",
      "295/295, train_loss: 0.1763, step time: 1.0271\n",
      "epoch 5 average loss: 0.2574\n",
      "current epoch: 5 current mean dice: 0.7192 tc: 0.6695 wt: 0.8108 et: 0.6802\n",
      "best mean dice: 0.7230 at epoch: 4\n",
      "time consuming of epoch 5 is: 376.6365\n",
      "----------\n",
      "epoch 6/100\n",
      "1/295, train_loss: 0.2027, step time: 1.0590\n",
      "2/295, train_loss: 0.1572, step time: 1.1036\n",
      "3/295, train_loss: 0.2411, step time: 1.0535\n",
      "4/295, train_loss: 0.4782, step time: 1.0582\n",
      "5/295, train_loss: 0.3686, step time: 1.0808\n",
      "6/295, train_loss: 0.1798, step time: 1.0310\n",
      "7/295, train_loss: 0.1278, step time: 1.0532\n",
      "8/295, train_loss: 0.1182, step time: 1.0602\n",
      "9/295, train_loss: 0.4919, step time: 1.0431\n",
      "10/295, train_loss: 0.2610, step time: 1.0637\n",
      "11/295, train_loss: 0.1622, step time: 1.1249\n",
      "12/295, train_loss: 0.1843, step time: 1.0457\n",
      "13/295, train_loss: 0.0795, step time: 1.0320\n",
      "14/295, train_loss: 0.2368, step time: 1.0312\n",
      "15/295, train_loss: 0.2348, step time: 1.0517\n",
      "16/295, train_loss: 0.2077, step time: 1.0893\n",
      "17/295, train_loss: 0.3108, step time: 1.0317\n",
      "18/295, train_loss: 0.2290, step time: 1.0384\n",
      "19/295, train_loss: 0.2046, step time: 1.0736\n",
      "20/295, train_loss: 0.5403, step time: 1.0429\n",
      "21/295, train_loss: 0.3394, step time: 1.0348\n",
      "22/295, train_loss: 0.1005, step time: 1.0404\n",
      "23/295, train_loss: 0.0759, step time: 1.0826\n",
      "24/295, train_loss: 0.4876, step time: 1.0451\n",
      "25/295, train_loss: 0.1314, step time: 1.0398\n",
      "26/295, train_loss: 0.0881, step time: 1.0448\n",
      "27/295, train_loss: 0.1160, step time: 1.0325\n",
      "28/295, train_loss: 0.5539, step time: 1.0407\n",
      "29/295, train_loss: 0.2025, step time: 1.0458\n",
      "30/295, train_loss: 0.4560, step time: 1.0342\n",
      "31/295, train_loss: 0.2083, step time: 1.0519\n",
      "32/295, train_loss: 0.1915, step time: 1.0355\n",
      "33/295, train_loss: 0.0723, step time: 1.0306\n",
      "34/295, train_loss: 0.2262, step time: 1.0312\n",
      "35/295, train_loss: 0.1501, step time: 1.0427\n",
      "36/295, train_loss: 0.1094, step time: 1.0358\n",
      "37/295, train_loss: 0.1627, step time: 1.0533\n",
      "38/295, train_loss: 0.6947, step time: 1.0490\n",
      "39/295, train_loss: 0.2685, step time: 1.0535\n",
      "40/295, train_loss: 0.2485, step time: 1.0444\n",
      "41/295, train_loss: 0.1815, step time: 1.0477\n",
      "42/295, train_loss: 0.4495, step time: 1.0422\n",
      "43/295, train_loss: 0.4329, step time: 1.0314\n",
      "44/295, train_loss: 0.1347, step time: 1.0322\n",
      "45/295, train_loss: 0.5634, step time: 1.0501\n",
      "46/295, train_loss: 0.1498, step time: 1.0521\n",
      "47/295, train_loss: 0.1848, step time: 1.0325\n",
      "48/295, train_loss: 0.2125, step time: 1.0527\n",
      "49/295, train_loss: 0.4996, step time: 1.0423\n",
      "50/295, train_loss: 0.4400, step time: 1.0470\n",
      "51/295, train_loss: 0.0850, step time: 1.0551\n",
      "52/295, train_loss: 0.4940, step time: 1.0348\n",
      "53/295, train_loss: 0.1465, step time: 1.0357\n",
      "54/295, train_loss: 0.2114, step time: 1.0432\n",
      "55/295, train_loss: 0.1559, step time: 1.0545\n",
      "56/295, train_loss: 0.1531, step time: 1.0335\n",
      "57/295, train_loss: 0.3208, step time: 1.0631\n",
      "58/295, train_loss: 0.1736, step time: 1.0367\n",
      "59/295, train_loss: 0.2615, step time: 1.0434\n",
      "60/295, train_loss: 0.1675, step time: 1.0569\n",
      "61/295, train_loss: 0.3753, step time: 1.0382\n",
      "62/295, train_loss: 0.4371, step time: 1.0304\n",
      "63/295, train_loss: 0.3376, step time: 1.0452\n",
      "64/295, train_loss: 0.3170, step time: 1.0547\n",
      "65/295, train_loss: 0.3256, step time: 1.0296\n",
      "66/295, train_loss: 0.0963, step time: 1.0324\n",
      "67/295, train_loss: 0.2159, step time: 1.0376\n",
      "68/295, train_loss: 0.1023, step time: 1.0404\n",
      "69/295, train_loss: 0.4996, step time: 1.0581\n",
      "70/295, train_loss: 0.3824, step time: 1.0315\n",
      "71/295, train_loss: 0.1454, step time: 1.0427\n",
      "72/295, train_loss: 0.1259, step time: 1.0447\n",
      "73/295, train_loss: 0.1947, step time: 1.0890\n",
      "74/295, train_loss: 0.1172, step time: 1.0406\n",
      "75/295, train_loss: 0.5041, step time: 1.0319\n",
      "76/295, train_loss: 0.4611, step time: 1.0397\n",
      "77/295, train_loss: 0.0976, step time: 1.0443\n",
      "78/295, train_loss: 0.1798, step time: 1.0334\n",
      "79/295, train_loss: 0.1409, step time: 1.0473\n",
      "80/295, train_loss: 0.4351, step time: 1.0510\n",
      "81/295, train_loss: 0.0769, step time: 1.0344\n",
      "82/295, train_loss: 0.1991, step time: 1.0408\n",
      "83/295, train_loss: 0.1400, step time: 1.0373\n",
      "84/295, train_loss: 0.1052, step time: 1.1038\n",
      "85/295, train_loss: 0.2827, step time: 1.0348\n",
      "86/295, train_loss: 0.4277, step time: 1.0342\n",
      "87/295, train_loss: 0.1725, step time: 1.0671\n",
      "88/295, train_loss: 0.1066, step time: 1.0576\n",
      "89/295, train_loss: 0.6241, step time: 1.0619\n",
      "90/295, train_loss: 0.1063, step time: 1.0603\n",
      "91/295, train_loss: 0.0600, step time: 1.0637\n",
      "92/295, train_loss: 0.0888, step time: 1.0390\n",
      "93/295, train_loss: 0.1329, step time: 1.0297\n",
      "94/295, train_loss: 0.4059, step time: 1.0392\n",
      "95/295, train_loss: 0.4399, step time: 1.0310\n",
      "96/295, train_loss: 0.1698, step time: 1.0320\n",
      "97/295, train_loss: 0.2577, step time: 1.0338\n",
      "98/295, train_loss: 0.2119, step time: 1.0439\n",
      "99/295, train_loss: 0.2073, step time: 1.0726\n",
      "100/295, train_loss: 0.1095, step time: 1.0382\n",
      "101/295, train_loss: 0.1965, step time: 1.0564\n",
      "102/295, train_loss: 0.1089, step time: 1.0982\n",
      "103/295, train_loss: 0.2206, step time: 1.0459\n",
      "104/295, train_loss: 0.2836, step time: 1.0608\n",
      "105/295, train_loss: 0.5022, step time: 1.0506\n",
      "106/295, train_loss: 0.0609, step time: 1.0417\n",
      "107/295, train_loss: 0.1080, step time: 1.0480\n",
      "108/295, train_loss: 0.3210, step time: 1.0573\n",
      "109/295, train_loss: 0.1438, step time: 1.0510\n",
      "110/295, train_loss: 0.1424, step time: 1.0409\n",
      "111/295, train_loss: 0.1614, step time: 1.1400\n",
      "112/295, train_loss: 0.1421, step time: 1.0412\n",
      "113/295, train_loss: 0.1268, step time: 1.0309\n",
      "114/295, train_loss: 0.1858, step time: 1.0572\n",
      "115/295, train_loss: 0.0676, step time: 1.0370\n",
      "116/295, train_loss: 0.6096, step time: 1.0368\n",
      "117/295, train_loss: 0.1260, step time: 1.0443\n",
      "118/295, train_loss: 0.1644, step time: 1.0582\n",
      "119/295, train_loss: 0.2178, step time: 1.1051\n",
      "120/295, train_loss: 0.2806, step time: 1.0561\n",
      "121/295, train_loss: 0.3913, step time: 1.0322\n",
      "122/295, train_loss: 0.1924, step time: 1.0356\n",
      "123/295, train_loss: 0.1849, step time: 1.0849\n",
      "124/295, train_loss: 0.0631, step time: 1.0302\n",
      "125/295, train_loss: 0.3187, step time: 1.0373\n",
      "126/295, train_loss: 0.0585, step time: 1.0930\n",
      "127/295, train_loss: 0.5585, step time: 1.0422\n",
      "128/295, train_loss: 0.1073, step time: 1.0508\n",
      "129/295, train_loss: 0.1502, step time: 1.0824\n",
      "130/295, train_loss: 0.3988, step time: 1.0435\n",
      "131/295, train_loss: 0.1528, step time: 1.0314\n",
      "132/295, train_loss: 0.2377, step time: 1.0444\n",
      "133/295, train_loss: 0.1390, step time: 1.0355\n",
      "134/295, train_loss: 0.4206, step time: 1.0640\n",
      "135/295, train_loss: 0.1106, step time: 1.0496\n",
      "136/295, train_loss: 0.1250, step time: 1.0319\n",
      "137/295, train_loss: 0.1666, step time: 1.0567\n",
      "138/295, train_loss: 0.1782, step time: 1.0593\n",
      "139/295, train_loss: 0.1833, step time: 1.0496\n",
      "140/295, train_loss: 0.1397, step time: 1.0814\n",
      "141/295, train_loss: 0.2296, step time: 1.0523\n",
      "142/295, train_loss: 0.1691, step time: 1.0532\n",
      "143/295, train_loss: 0.5030, step time: 1.0320\n",
      "144/295, train_loss: 0.1890, step time: 1.0368\n",
      "145/295, train_loss: 0.5977, step time: 1.0651\n",
      "146/295, train_loss: 0.1481, step time: 1.0340\n",
      "147/295, train_loss: 0.0804, step time: 1.0452\n",
      "148/295, train_loss: 0.4011, step time: 1.0304\n",
      "149/295, train_loss: 0.1701, step time: 1.0406\n",
      "150/295, train_loss: 0.1840, step time: 1.0389\n",
      "151/295, train_loss: 0.5248, step time: 1.0581\n",
      "152/295, train_loss: 0.1712, step time: 1.0696\n",
      "153/295, train_loss: 0.1098, step time: 1.0349\n",
      "154/295, train_loss: 0.1516, step time: 1.0454\n",
      "155/295, train_loss: 0.2540, step time: 1.0330\n",
      "156/295, train_loss: 0.0791, step time: 1.0414\n",
      "157/295, train_loss: 0.1753, step time: 1.0681\n",
      "158/295, train_loss: 0.2909, step time: 1.0315\n",
      "159/295, train_loss: 0.2009, step time: 1.0377\n",
      "160/295, train_loss: 0.6587, step time: 1.0322\n",
      "161/295, train_loss: 0.4160, step time: 1.0968\n",
      "162/295, train_loss: 0.1761, step time: 1.0587\n",
      "163/295, train_loss: 0.1819, step time: 1.0575\n",
      "164/295, train_loss: 0.1403, step time: 1.0334\n",
      "165/295, train_loss: 0.1660, step time: 1.0367\n",
      "166/295, train_loss: 0.2160, step time: 1.0450\n",
      "167/295, train_loss: 0.1379, step time: 1.0668\n",
      "168/295, train_loss: 0.3651, step time: 1.0844\n",
      "169/295, train_loss: 0.2696, step time: 1.0400\n",
      "170/295, train_loss: 0.1302, step time: 1.0541\n",
      "171/295, train_loss: 0.2189, step time: 1.0501\n",
      "172/295, train_loss: 0.3345, step time: 1.0611\n",
      "173/295, train_loss: 0.2280, step time: 1.0475\n",
      "174/295, train_loss: 0.4944, step time: 1.0329\n",
      "175/295, train_loss: 0.1133, step time: 1.0540\n",
      "176/295, train_loss: 0.0796, step time: 1.0336\n",
      "177/295, train_loss: 0.1759, step time: 1.0510\n",
      "178/295, train_loss: 0.4503, step time: 1.0361\n",
      "179/295, train_loss: 0.4481, step time: 1.0344\n",
      "180/295, train_loss: 0.2930, step time: 1.0614\n",
      "181/295, train_loss: 0.7998, step time: 1.0328\n",
      "182/295, train_loss: 0.1931, step time: 1.0418\n",
      "183/295, train_loss: 0.0979, step time: 1.0695\n",
      "184/295, train_loss: 0.1446, step time: 1.0334\n",
      "185/295, train_loss: 0.5428, step time: 1.0401\n",
      "186/295, train_loss: 0.4344, step time: 1.0368\n",
      "187/295, train_loss: 0.1797, step time: 1.0534\n",
      "188/295, train_loss: 0.1820, step time: 1.0494\n",
      "189/295, train_loss: 0.2967, step time: 1.0362\n",
      "190/295, train_loss: 0.2190, step time: 1.0587\n",
      "191/295, train_loss: 0.0752, step time: 1.0561\n",
      "192/295, train_loss: 0.1121, step time: 1.0531\n",
      "193/295, train_loss: 0.1183, step time: 1.0396\n",
      "194/295, train_loss: 0.1089, step time: 1.0345\n",
      "195/295, train_loss: 0.2052, step time: 1.0524\n",
      "196/295, train_loss: 0.1519, step time: 1.0475\n",
      "197/295, train_loss: 0.1103, step time: 1.0648\n",
      "198/295, train_loss: 0.1111, step time: 1.0707\n",
      "199/295, train_loss: 0.4530, step time: 1.0306\n",
      "200/295, train_loss: 0.1483, step time: 1.0350\n",
      "201/295, train_loss: 0.0882, step time: 1.0347\n",
      "202/295, train_loss: 0.0713, step time: 1.0373\n",
      "203/295, train_loss: 0.3570, step time: 1.0307\n",
      "204/295, train_loss: 0.3224, step time: 1.0421\n",
      "205/295, train_loss: 0.3855, step time: 1.0527\n",
      "206/295, train_loss: 0.1030, step time: 1.0312\n",
      "207/295, train_loss: 0.2096, step time: 1.0386\n",
      "208/295, train_loss: 0.1390, step time: 1.0321\n",
      "209/295, train_loss: 0.1349, step time: 1.0343\n",
      "210/295, train_loss: 0.2148, step time: 1.0593\n",
      "211/295, train_loss: 0.1358, step time: 1.0888\n",
      "212/295, train_loss: 0.0968, step time: 1.0508\n",
      "213/295, train_loss: 0.1126, step time: 1.0470\n",
      "214/295, train_loss: 0.2269, step time: 1.0329\n",
      "215/295, train_loss: 0.3916, step time: 1.0443\n",
      "216/295, train_loss: 0.4849, step time: 1.0735\n",
      "217/295, train_loss: 0.1370, step time: 1.0471\n",
      "218/295, train_loss: 0.1133, step time: 1.0356\n",
      "219/295, train_loss: 0.3088, step time: 1.0432\n",
      "220/295, train_loss: 0.1478, step time: 1.0386\n",
      "221/295, train_loss: 0.1408, step time: 1.0433\n",
      "222/295, train_loss: 0.2440, step time: 1.0535\n",
      "223/295, train_loss: 0.3946, step time: 1.0500\n",
      "224/295, train_loss: 0.1104, step time: 1.0344\n",
      "225/295, train_loss: 0.0780, step time: 1.0323\n",
      "226/295, train_loss: 0.5152, step time: 1.0419\n",
      "227/295, train_loss: 0.1259, step time: 1.0573\n",
      "228/295, train_loss: 0.1068, step time: 1.0477\n",
      "229/295, train_loss: 0.0840, step time: 1.0423\n",
      "230/295, train_loss: 0.1693, step time: 1.1094\n",
      "231/295, train_loss: 0.0637, step time: 1.0527\n",
      "232/295, train_loss: 0.0839, step time: 1.0418\n",
      "233/295, train_loss: 0.0868, step time: 1.0534\n",
      "234/295, train_loss: 0.1193, step time: 1.0650\n",
      "235/295, train_loss: 0.4157, step time: 1.0350\n",
      "236/295, train_loss: 0.5266, step time: 1.0376\n",
      "237/295, train_loss: 0.2481, step time: 1.0384\n",
      "238/295, train_loss: 0.2630, step time: 1.0508\n",
      "239/295, train_loss: 0.1478, step time: 1.0386\n",
      "240/295, train_loss: 0.1447, step time: 1.0515\n",
      "241/295, train_loss: 0.0906, step time: 1.0327\n",
      "242/295, train_loss: 0.1937, step time: 1.0580\n",
      "243/295, train_loss: 0.1298, step time: 1.0943\n",
      "244/295, train_loss: 0.0752, step time: 1.0694\n",
      "245/295, train_loss: 0.6083, step time: 1.0331\n",
      "246/295, train_loss: 0.1546, step time: 1.0316\n",
      "247/295, train_loss: 0.1995, step time: 1.0368\n",
      "248/295, train_loss: 0.1100, step time: 1.0354\n",
      "249/295, train_loss: 0.0620, step time: 1.0341\n",
      "250/295, train_loss: 0.2941, step time: 1.0381\n",
      "251/295, train_loss: 0.1691, step time: 1.0554\n",
      "252/295, train_loss: 0.1818, step time: 1.0375\n",
      "253/295, train_loss: 0.1331, step time: 1.0348\n",
      "254/295, train_loss: 0.1711, step time: 1.0498\n",
      "255/295, train_loss: 0.2211, step time: 1.0349\n",
      "256/295, train_loss: 0.2388, step time: 1.0336\n",
      "257/295, train_loss: 0.1340, step time: 1.0336\n",
      "258/295, train_loss: 0.2205, step time: 1.0332\n",
      "259/295, train_loss: 0.0811, step time: 1.0379\n",
      "260/295, train_loss: 0.4658, step time: 1.0383\n",
      "261/295, train_loss: 0.3759, step time: 1.0432\n",
      "262/295, train_loss: 0.1347, step time: 1.0501\n",
      "263/295, train_loss: 0.1444, step time: 1.0555\n",
      "264/295, train_loss: 0.2538, step time: 1.0413\n",
      "265/295, train_loss: 0.1016, step time: 1.0311\n",
      "266/295, train_loss: 0.6519, step time: 1.0326\n",
      "267/295, train_loss: 0.2154, step time: 1.0330\n",
      "268/295, train_loss: 0.0722, step time: 1.0453\n",
      "269/295, train_loss: 0.1212, step time: 1.0380\n",
      "270/295, train_loss: 0.1757, step time: 1.0541\n",
      "271/295, train_loss: 0.2042, step time: 1.0424\n",
      "272/295, train_loss: 0.3686, step time: 1.0371\n",
      "273/295, train_loss: 0.0854, step time: 1.0391\n",
      "274/295, train_loss: 0.1763, step time: 1.0394\n",
      "275/295, train_loss: 0.0871, step time: 1.0409\n",
      "276/295, train_loss: 0.1010, step time: 1.0554\n",
      "277/295, train_loss: 0.1528, step time: 1.0455\n",
      "278/295, train_loss: 0.2168, step time: 1.0450\n",
      "279/295, train_loss: 0.1554, step time: 1.0321\n",
      "280/295, train_loss: 0.1442, step time: 1.0804\n",
      "281/295, train_loss: 0.3380, step time: 1.0666\n",
      "282/295, train_loss: 0.2109, step time: 1.0354\n",
      "283/295, train_loss: 0.1207, step time: 1.0353\n",
      "284/295, train_loss: 0.2541, step time: 1.0344\n",
      "285/295, train_loss: 0.1126, step time: 1.0370\n",
      "286/295, train_loss: 0.5075, step time: 1.0384\n",
      "287/295, train_loss: 0.1751, step time: 1.0346\n",
      "288/295, train_loss: 0.2392, step time: 1.0336\n",
      "289/295, train_loss: 0.5178, step time: 1.0269\n",
      "290/295, train_loss: 0.2351, step time: 1.0282\n",
      "291/295, train_loss: 0.4114, step time: 1.0290\n",
      "292/295, train_loss: 0.6365, step time: 1.0283\n",
      "293/295, train_loss: 0.1569, step time: 1.0282\n",
      "294/295, train_loss: 0.1073, step time: 1.0286\n",
      "295/295, train_loss: 0.3786, step time: 1.0281\n",
      "epoch 6 average loss: 0.2343\n",
      "current epoch: 6 current mean dice: 0.7136 tc: 0.6500 wt: 0.8057 et: 0.6734\n",
      "best mean dice: 0.7230 at epoch: 4\n",
      "time consuming of epoch 6 is: 377.4424\n",
      "----------\n",
      "epoch 7/100\n",
      "1/295, train_loss: 0.1670, step time: 1.0942\n",
      "2/295, train_loss: 0.6074, step time: 1.0674\n",
      "3/295, train_loss: 0.4250, step time: 1.0449\n",
      "4/295, train_loss: 0.2149, step time: 1.0732\n",
      "5/295, train_loss: 0.1309, step time: 1.0401\n",
      "6/295, train_loss: 0.1096, step time: 1.0808\n",
      "7/295, train_loss: 0.3326, step time: 1.0542\n",
      "8/295, train_loss: 0.1777, step time: 1.0457\n",
      "9/295, train_loss: 0.0898, step time: 1.0308\n",
      "10/295, train_loss: 0.3267, step time: 1.0349\n",
      "11/295, train_loss: 0.0773, step time: 1.0623\n",
      "12/295, train_loss: 0.2374, step time: 1.0716\n",
      "13/295, train_loss: 0.1493, step time: 1.0618\n",
      "14/295, train_loss: 0.2219, step time: 1.0321\n",
      "15/295, train_loss: 0.4729, step time: 1.0371\n",
      "16/295, train_loss: 0.0728, step time: 1.0307\n",
      "17/295, train_loss: 0.1121, step time: 1.0429\n",
      "18/295, train_loss: 0.1417, step time: 1.0327\n",
      "19/295, train_loss: 0.1903, step time: 1.0358\n",
      "20/295, train_loss: 0.2508, step time: 1.0613\n",
      "21/295, train_loss: 0.0627, step time: 1.0431\n",
      "22/295, train_loss: 0.0623, step time: 1.0372\n",
      "23/295, train_loss: 0.3839, step time: 1.0521\n",
      "24/295, train_loss: 0.2035, step time: 1.0969\n",
      "25/295, train_loss: 0.0979, step time: 1.0737\n",
      "26/295, train_loss: 0.0957, step time: 1.1230\n",
      "27/295, train_loss: 0.3949, step time: 1.0568\n",
      "28/295, train_loss: 0.1056, step time: 1.0744\n",
      "29/295, train_loss: 0.0798, step time: 1.0462\n",
      "30/295, train_loss: 0.2394, step time: 1.0907\n",
      "31/295, train_loss: 0.2839, step time: 1.1041\n",
      "32/295, train_loss: 0.1093, step time: 1.0444\n",
      "33/295, train_loss: 0.1381, step time: 1.0324\n",
      "34/295, train_loss: 0.5718, step time: 1.0586\n",
      "35/295, train_loss: 0.1977, step time: 1.0423\n",
      "36/295, train_loss: 0.6875, step time: 1.0591\n",
      "37/295, train_loss: 0.1547, step time: 1.0495\n",
      "38/295, train_loss: 0.4948, step time: 1.0384\n",
      "39/295, train_loss: 0.2429, step time: 1.0580\n",
      "40/295, train_loss: 0.0969, step time: 1.0566\n",
      "41/295, train_loss: 0.4773, step time: 1.0450\n",
      "42/295, train_loss: 0.0894, step time: 1.0554\n",
      "43/295, train_loss: 0.1406, step time: 1.0383\n",
      "44/295, train_loss: 0.2031, step time: 1.0693\n",
      "45/295, train_loss: 0.3168, step time: 1.0380\n",
      "46/295, train_loss: 0.1302, step time: 1.0378\n",
      "47/295, train_loss: 0.0625, step time: 1.0620\n",
      "48/295, train_loss: 0.6160, step time: 1.0489\n",
      "49/295, train_loss: 0.2269, step time: 1.0306\n",
      "50/295, train_loss: 0.4799, step time: 1.0395\n",
      "51/295, train_loss: 0.1678, step time: 1.0531\n",
      "52/295, train_loss: 0.2933, step time: 1.0538\n",
      "53/295, train_loss: 0.1150, step time: 1.0417\n",
      "54/295, train_loss: 0.5120, step time: 1.0527\n",
      "55/295, train_loss: 0.4179, step time: 1.0513\n",
      "56/295, train_loss: 0.4788, step time: 1.0539\n",
      "57/295, train_loss: 0.1311, step time: 1.0690\n",
      "58/295, train_loss: 0.1589, step time: 1.0334\n",
      "59/295, train_loss: 0.1196, step time: 1.0435\n",
      "60/295, train_loss: 0.4218, step time: 1.0752\n",
      "61/295, train_loss: 0.2085, step time: 1.0444\n",
      "62/295, train_loss: 0.1756, step time: 1.0383\n",
      "63/295, train_loss: 0.1339, step time: 1.0403\n",
      "64/295, train_loss: 0.5549, step time: 1.0329\n",
      "65/295, train_loss: 0.2795, step time: 1.0363\n",
      "66/295, train_loss: 0.4479, step time: 1.0301\n",
      "67/295, train_loss: 0.1081, step time: 1.0670\n",
      "68/295, train_loss: 0.4417, step time: 1.0289\n",
      "69/295, train_loss: 0.1599, step time: 1.0308\n",
      "70/295, train_loss: 0.1555, step time: 1.0351\n",
      "71/295, train_loss: 0.1302, step time: 1.0393\n",
      "72/295, train_loss: 0.2089, step time: 1.0306\n",
      "73/295, train_loss: 0.1326, step time: 1.0311\n",
      "74/295, train_loss: 0.1170, step time: 1.0566\n",
      "75/295, train_loss: 0.0662, step time: 1.0351\n",
      "76/295, train_loss: 0.2491, step time: 1.0520\n",
      "77/295, train_loss: 0.1463, step time: 1.0368\n",
      "78/295, train_loss: 0.1149, step time: 1.0328\n",
      "79/295, train_loss: 0.1215, step time: 1.0989\n",
      "80/295, train_loss: 0.2012, step time: 1.0529\n",
      "81/295, train_loss: 0.1003, step time: 1.0651\n",
      "82/295, train_loss: 0.1503, step time: 1.0376\n",
      "83/295, train_loss: 0.1060, step time: 1.0646\n",
      "84/295, train_loss: 0.0999, step time: 1.0536\n",
      "85/295, train_loss: 0.1651, step time: 1.0420\n",
      "86/295, train_loss: 0.1286, step time: 1.0658\n",
      "87/295, train_loss: 0.1867, step time: 1.0392\n",
      "88/295, train_loss: 0.2148, step time: 1.0558\n",
      "89/295, train_loss: 0.1341, step time: 1.0387\n",
      "90/295, train_loss: 0.1159, step time: 1.0641\n",
      "91/295, train_loss: 0.2505, step time: 1.0496\n",
      "92/295, train_loss: 0.1393, step time: 1.0608\n",
      "93/295, train_loss: 0.1914, step time: 1.0399\n",
      "94/295, train_loss: 0.5294, step time: 1.0320\n",
      "95/295, train_loss: 0.3710, step time: 1.0649\n",
      "96/295, train_loss: 0.1799, step time: 1.0603\n",
      "97/295, train_loss: 0.0891, step time: 1.0372\n",
      "98/295, train_loss: 0.0799, step time: 1.0667\n",
      "99/295, train_loss: 0.1370, step time: 1.0634\n",
      "100/295, train_loss: 0.2220, step time: 1.0353\n",
      "101/295, train_loss: 0.0692, step time: 1.0363\n",
      "102/295, train_loss: 0.1038, step time: 1.0520\n",
      "103/295, train_loss: 0.3739, step time: 1.0384\n",
      "104/295, train_loss: 0.0699, step time: 1.0485\n",
      "105/295, train_loss: 0.4619, step time: 1.0599\n",
      "106/295, train_loss: 0.1083, step time: 1.0492\n",
      "107/295, train_loss: 0.1981, step time: 1.0498\n",
      "108/295, train_loss: 0.0663, step time: 1.0358\n",
      "109/295, train_loss: 0.1595, step time: 1.0323\n",
      "110/295, train_loss: 0.2592, step time: 1.0465\n",
      "111/295, train_loss: 0.0747, step time: 1.0311\n",
      "112/295, train_loss: 0.0989, step time: 1.0688\n",
      "113/295, train_loss: 0.1000, step time: 1.0620\n",
      "114/295, train_loss: 0.0881, step time: 1.0375\n",
      "115/295, train_loss: 0.2092, step time: 1.0717\n",
      "116/295, train_loss: 0.0776, step time: 1.0688\n",
      "117/295, train_loss: 0.1241, step time: 1.0310\n",
      "118/295, train_loss: 0.1349, step time: 1.0365\n",
      "119/295, train_loss: 0.1108, step time: 1.0322\n",
      "120/295, train_loss: 0.1257, step time: 1.0425\n",
      "121/295, train_loss: 0.1796, step time: 1.0538\n",
      "122/295, train_loss: 0.0696, step time: 1.0511\n",
      "123/295, train_loss: 0.1431, step time: 1.0588\n",
      "124/295, train_loss: 0.2167, step time: 1.0690\n",
      "125/295, train_loss: 0.0642, step time: 1.0696\n",
      "126/295, train_loss: 0.2094, step time: 1.0475\n",
      "127/295, train_loss: 0.1714, step time: 1.0795\n",
      "128/295, train_loss: 0.3387, step time: 1.0419\n",
      "129/295, train_loss: 0.2192, step time: 1.0503\n",
      "130/295, train_loss: 0.1072, step time: 1.0387\n",
      "131/295, train_loss: 0.2194, step time: 1.0535\n",
      "132/295, train_loss: 0.4819, step time: 1.0815\n",
      "133/295, train_loss: 0.1986, step time: 1.0543\n",
      "134/295, train_loss: 0.3901, step time: 1.0329\n",
      "135/295, train_loss: 0.0933, step time: 1.0433\n",
      "136/295, train_loss: 0.0868, step time: 1.0446\n",
      "137/295, train_loss: 0.1632, step time: 1.0324\n",
      "138/295, train_loss: 0.5109, step time: 1.0526\n",
      "139/295, train_loss: 0.1644, step time: 1.0474\n",
      "140/295, train_loss: 0.1242, step time: 1.0833\n",
      "141/295, train_loss: 0.2637, step time: 1.0505\n",
      "142/295, train_loss: 0.1025, step time: 1.0303\n",
      "143/295, train_loss: 0.0930, step time: 1.0771\n",
      "144/295, train_loss: 0.1200, step time: 1.0375\n",
      "145/295, train_loss: 0.0548, step time: 1.0463\n",
      "146/295, train_loss: 0.1452, step time: 1.0939\n",
      "147/295, train_loss: 0.4483, step time: 1.0375\n",
      "148/295, train_loss: 0.4704, step time: 1.0654\n",
      "149/295, train_loss: 0.0843, step time: 1.0671\n",
      "150/295, train_loss: 0.1113, step time: 1.0301\n",
      "151/295, train_loss: 0.1444, step time: 1.0319\n",
      "152/295, train_loss: 0.1340, step time: 1.0388\n",
      "153/295, train_loss: 0.1249, step time: 1.0647\n",
      "154/295, train_loss: 0.2987, step time: 1.0372\n",
      "155/295, train_loss: 0.0649, step time: 1.0628\n",
      "156/295, train_loss: 0.4037, step time: 1.0518\n",
      "157/295, train_loss: 0.1175, step time: 1.0323\n",
      "158/295, train_loss: 0.3844, step time: 1.0373\n",
      "159/295, train_loss: 0.0962, step time: 1.0781\n",
      "160/295, train_loss: 0.3174, step time: 1.0309\n",
      "161/295, train_loss: 0.2603, step time: 1.0399\n",
      "162/295, train_loss: 0.0812, step time: 1.0472\n",
      "163/295, train_loss: 0.4556, step time: 1.0547\n",
      "164/295, train_loss: 0.0596, step time: 1.0345\n",
      "165/295, train_loss: 0.2338, step time: 1.0335\n",
      "166/295, train_loss: 0.6372, step time: 1.0580\n",
      "167/295, train_loss: 0.1409, step time: 1.0301\n",
      "168/295, train_loss: 0.0784, step time: 1.0438\n",
      "169/295, train_loss: 0.6730, step time: 1.0513\n",
      "170/295, train_loss: 0.0908, step time: 1.0345\n",
      "171/295, train_loss: 0.1692, step time: 1.0431\n",
      "172/295, train_loss: 0.1602, step time: 1.0528\n",
      "173/295, train_loss: 0.1385, step time: 1.0610\n",
      "174/295, train_loss: 0.2047, step time: 1.0301\n",
      "175/295, train_loss: 0.1118, step time: 1.0354\n",
      "176/295, train_loss: 0.2764, step time: 1.0667\n",
      "177/295, train_loss: 0.4251, step time: 1.0562\n",
      "178/295, train_loss: 0.1642, step time: 1.0400\n",
      "179/295, train_loss: 0.1447, step time: 1.0329\n",
      "180/295, train_loss: 0.1955, step time: 1.0661\n",
      "181/295, train_loss: 0.1313, step time: 1.0367\n",
      "182/295, train_loss: 0.1894, step time: 1.0407\n",
      "183/295, train_loss: 0.3419, step time: 1.0326\n",
      "184/295, train_loss: 0.2131, step time: 1.0326\n",
      "185/295, train_loss: 0.1122, step time: 1.0340\n",
      "186/295, train_loss: 0.4677, step time: 1.0306\n",
      "187/295, train_loss: 0.1753, step time: 1.0311\n",
      "188/295, train_loss: 0.2635, step time: 1.0653\n",
      "189/295, train_loss: 0.4335, step time: 1.0379\n",
      "190/295, train_loss: 0.1105, step time: 1.0398\n",
      "191/295, train_loss: 0.1111, step time: 1.0560\n",
      "192/295, train_loss: 0.2717, step time: 1.0536\n",
      "193/295, train_loss: 0.0553, step time: 1.0340\n",
      "194/295, train_loss: 0.1566, step time: 1.0320\n",
      "195/295, train_loss: 0.3057, step time: 1.0326\n",
      "196/295, train_loss: 0.0675, step time: 1.0361\n",
      "197/295, train_loss: 0.1463, step time: 1.0564\n",
      "198/295, train_loss: 0.3907, step time: 1.0941\n",
      "199/295, train_loss: 0.1952, step time: 1.0483\n",
      "200/295, train_loss: 0.1057, step time: 1.0697\n",
      "201/295, train_loss: 0.4139, step time: 1.0559\n",
      "202/295, train_loss: 0.4415, step time: 1.0300\n",
      "203/295, train_loss: 0.3093, step time: 1.0530\n",
      "204/295, train_loss: 0.1271, step time: 1.0310\n",
      "205/295, train_loss: 0.0999, step time: 1.0355\n",
      "206/295, train_loss: 0.1387, step time: 1.0305\n",
      "207/295, train_loss: 0.2685, step time: 1.0454\n",
      "208/295, train_loss: 0.0618, step time: 1.0332\n",
      "209/295, train_loss: 0.2311, step time: 1.0334\n",
      "210/295, train_loss: 0.1249, step time: 1.0364\n",
      "211/295, train_loss: 0.1252, step time: 1.0325\n",
      "212/295, train_loss: 0.4810, step time: 1.0394\n",
      "213/295, train_loss: 0.1166, step time: 1.0302\n",
      "214/295, train_loss: 0.3523, step time: 1.0378\n",
      "215/295, train_loss: 0.2685, step time: 1.0792\n",
      "216/295, train_loss: 0.0991, step time: 1.0375\n",
      "217/295, train_loss: 0.0965, step time: 1.0322\n",
      "218/295, train_loss: 0.1382, step time: 1.0395\n",
      "219/295, train_loss: 0.5236, step time: 1.0407\n",
      "220/295, train_loss: 0.2382, step time: 1.0391\n",
      "221/295, train_loss: 0.3442, step time: 1.0369\n",
      "222/295, train_loss: 0.2266, step time: 1.0507\n",
      "223/295, train_loss: 0.4510, step time: 1.0435\n",
      "224/295, train_loss: 0.2001, step time: 1.0375\n",
      "225/295, train_loss: 0.1463, step time: 1.0413\n",
      "226/295, train_loss: 0.0885, step time: 1.0437\n",
      "227/295, train_loss: 0.3896, step time: 1.0689\n",
      "228/295, train_loss: 0.2700, step time: 1.0473\n",
      "229/295, train_loss: 0.1024, step time: 1.0539\n",
      "230/295, train_loss: 0.1174, step time: 1.0318\n",
      "231/295, train_loss: 0.2138, step time: 1.0543\n",
      "232/295, train_loss: 0.2247, step time: 1.0539\n",
      "233/295, train_loss: 0.0973, step time: 1.0591\n",
      "234/295, train_loss: 0.3016, step time: 1.0377\n",
      "235/295, train_loss: 0.1077, step time: 1.0487\n",
      "236/295, train_loss: 0.1219, step time: 1.0430\n",
      "237/295, train_loss: 0.5234, step time: 1.0363\n",
      "238/295, train_loss: 0.2154, step time: 1.0374\n",
      "239/295, train_loss: 0.1253, step time: 1.0413\n",
      "240/295, train_loss: 0.0778, step time: 1.0732\n",
      "241/295, train_loss: 0.0804, step time: 1.0544\n",
      "242/295, train_loss: 0.2111, step time: 1.0379\n",
      "243/295, train_loss: 0.0764, step time: 1.0470\n",
      "244/295, train_loss: 0.1477, step time: 1.0952\n",
      "245/295, train_loss: 0.1419, step time: 1.0305\n",
      "246/295, train_loss: 0.1373, step time: 1.0371\n",
      "247/295, train_loss: 0.0598, step time: 1.0411\n",
      "248/295, train_loss: 0.2026, step time: 1.0527\n",
      "249/295, train_loss: 0.1193, step time: 1.0520\n",
      "250/295, train_loss: 0.1044, step time: 1.0322\n",
      "251/295, train_loss: 0.2631, step time: 1.0381\n",
      "252/295, train_loss: 0.3975, step time: 1.0366\n",
      "253/295, train_loss: 0.1242, step time: 1.0314\n",
      "254/295, train_loss: 0.1215, step time: 1.0332\n",
      "255/295, train_loss: 0.1151, step time: 1.0642\n",
      "256/295, train_loss: 0.2910, step time: 1.1269\n",
      "257/295, train_loss: 0.2004, step time: 1.0364\n",
      "258/295, train_loss: 0.0912, step time: 1.0371\n",
      "259/295, train_loss: 0.2060, step time: 1.0460\n",
      "260/295, train_loss: 0.1231, step time: 1.0864\n",
      "261/295, train_loss: 0.3204, step time: 1.0479\n",
      "262/295, train_loss: 0.1129, step time: 1.0539\n",
      "263/295, train_loss: 0.1974, step time: 1.0308\n",
      "264/295, train_loss: 0.3337, step time: 1.0325\n",
      "265/295, train_loss: 0.4773, step time: 1.0391\n",
      "266/295, train_loss: 0.4902, step time: 1.0327\n",
      "267/295, train_loss: 0.1015, step time: 1.0372\n",
      "268/295, train_loss: 0.1986, step time: 1.0343\n",
      "269/295, train_loss: 0.1244, step time: 1.0323\n",
      "270/295, train_loss: 0.1483, step time: 1.0320\n",
      "271/295, train_loss: 0.5537, step time: 1.0331\n",
      "272/295, train_loss: 0.1355, step time: 1.0630\n",
      "273/295, train_loss: 0.3567, step time: 1.0548\n",
      "274/295, train_loss: 0.4835, step time: 1.0436\n",
      "275/295, train_loss: 0.2446, step time: 1.0703\n",
      "276/295, train_loss: 0.1692, step time: 1.0321\n",
      "277/295, train_loss: 0.2174, step time: 1.0350\n",
      "278/295, train_loss: 0.0730, step time: 1.0350\n",
      "279/295, train_loss: 0.1115, step time: 1.0401\n",
      "280/295, train_loss: 0.2179, step time: 1.0348\n",
      "281/295, train_loss: 0.1904, step time: 1.0336\n",
      "282/295, train_loss: 0.1242, step time: 1.1752\n",
      "283/295, train_loss: 0.1307, step time: 1.0390\n",
      "284/295, train_loss: 0.1434, step time: 1.0468\n",
      "285/295, train_loss: 0.1583, step time: 1.0386\n",
      "286/295, train_loss: 0.0895, step time: 1.0316\n",
      "287/295, train_loss: 0.1714, step time: 1.0325\n",
      "288/295, train_loss: 0.0876, step time: 1.0599\n",
      "289/295, train_loss: 0.2863, step time: 1.0291\n",
      "290/295, train_loss: 0.2719, step time: 1.0312\n",
      "291/295, train_loss: 0.6367, step time: 1.0290\n",
      "292/295, train_loss: 0.1502, step time: 1.0282\n",
      "293/295, train_loss: 0.1025, step time: 1.0283\n",
      "294/295, train_loss: 0.2255, step time: 1.0281\n",
      "295/295, train_loss: 0.2421, step time: 1.0286\n",
      "epoch 7 average loss: 0.2132\n",
      "saved new best metric model\n",
      "current epoch: 7 current mean dice: 0.7472 tc: 0.6966 wt: 0.8220 et: 0.7196\n",
      "best mean dice: 0.7472 at epoch: 7\n",
      "time consuming of epoch 7 is: 378.7883\n",
      "----------\n",
      "epoch 8/100\n",
      "1/295, train_loss: 0.1727, step time: 1.1143\n",
      "2/295, train_loss: 0.1074, step time: 1.0545\n",
      "3/295, train_loss: 0.1698, step time: 1.0651\n",
      "4/295, train_loss: 0.3010, step time: 1.0571\n",
      "5/295, train_loss: 0.4484, step time: 1.0540\n",
      "6/295, train_loss: 0.0731, step time: 1.0365\n",
      "7/295, train_loss: 0.1276, step time: 1.0323\n",
      "8/295, train_loss: 0.2402, step time: 1.0413\n",
      "9/295, train_loss: 0.2919, step time: 1.0724\n",
      "10/295, train_loss: 0.1692, step time: 1.0720\n",
      "11/295, train_loss: 0.4635, step time: 1.0566\n",
      "12/295, train_loss: 0.1728, step time: 1.0410\n",
      "13/295, train_loss: 0.4193, step time: 1.0336\n",
      "14/295, train_loss: 0.0671, step time: 1.0317\n",
      "15/295, train_loss: 0.0703, step time: 1.0382\n",
      "16/295, train_loss: 0.2332, step time: 1.0343\n",
      "17/295, train_loss: 0.0565, step time: 1.0319\n",
      "18/295, train_loss: 0.1282, step time: 1.0322\n",
      "19/295, train_loss: 0.2294, step time: 1.0380\n",
      "20/295, train_loss: 0.1505, step time: 1.0365\n",
      "21/295, train_loss: 0.1065, step time: 1.0586\n",
      "22/295, train_loss: 0.1116, step time: 1.0776\n",
      "23/295, train_loss: 0.0576, step time: 1.0442\n",
      "24/295, train_loss: 0.0730, step time: 1.0393\n",
      "25/295, train_loss: 0.1371, step time: 1.0405\n",
      "26/295, train_loss: 0.1897, step time: 1.0320\n",
      "27/295, train_loss: 0.1459, step time: 1.0460\n",
      "28/295, train_loss: 0.0741, step time: 1.0344\n",
      "29/295, train_loss: 0.1825, step time: 1.0323\n",
      "30/295, train_loss: 0.3138, step time: 1.0316\n",
      "31/295, train_loss: 0.3824, step time: 1.0493\n",
      "32/295, train_loss: 0.4629, step time: 1.0426\n",
      "33/295, train_loss: 0.1070, step time: 1.0826\n",
      "34/295, train_loss: 0.2358, step time: 1.0347\n",
      "35/295, train_loss: 0.3986, step time: 1.0583\n",
      "36/295, train_loss: 0.1210, step time: 1.0599\n",
      "37/295, train_loss: 0.1512, step time: 1.0585\n",
      "38/295, train_loss: 0.5532, step time: 1.0941\n",
      "39/295, train_loss: 0.0962, step time: 1.0380\n",
      "40/295, train_loss: 0.1928, step time: 1.0373\n",
      "41/295, train_loss: 0.1127, step time: 1.1391\n",
      "42/295, train_loss: 0.2573, step time: 1.0382\n",
      "43/295, train_loss: 0.4338, step time: 1.0354\n",
      "44/295, train_loss: 0.1090, step time: 1.0378\n",
      "45/295, train_loss: 0.1413, step time: 1.0341\n",
      "46/295, train_loss: 0.1265, step time: 1.0333\n",
      "47/295, train_loss: 0.1033, step time: 1.0352\n",
      "48/295, train_loss: 0.4425, step time: 1.0629\n",
      "49/295, train_loss: 0.1696, step time: 1.0374\n",
      "50/295, train_loss: 0.6652, step time: 1.0358\n",
      "51/295, train_loss: 0.1350, step time: 1.1021\n",
      "52/295, train_loss: 0.0815, step time: 1.0346\n",
      "53/295, train_loss: 0.0689, step time: 1.0400\n",
      "54/295, train_loss: 0.1415, step time: 1.0360\n",
      "55/295, train_loss: 0.1536, step time: 1.0331\n",
      "56/295, train_loss: 0.0713, step time: 1.0378\n",
      "57/295, train_loss: 0.1125, step time: 1.0590\n",
      "58/295, train_loss: 0.3747, step time: 1.0552\n",
      "59/295, train_loss: 0.0951, step time: 1.0496\n",
      "60/295, train_loss: 0.3612, step time: 1.0549\n",
      "61/295, train_loss: 0.1216, step time: 1.0618\n",
      "62/295, train_loss: 0.1673, step time: 1.0406\n",
      "63/295, train_loss: 0.4301, step time: 1.0347\n",
      "64/295, train_loss: 0.0849, step time: 1.0348\n",
      "65/295, train_loss: 0.6164, step time: 1.0515\n",
      "66/295, train_loss: 0.2449, step time: 1.0532\n",
      "67/295, train_loss: 0.1088, step time: 1.0537\n",
      "68/295, train_loss: 0.2693, step time: 1.0440\n",
      "69/295, train_loss: 0.0863, step time: 1.0347\n",
      "70/295, train_loss: 0.2287, step time: 1.0374\n",
      "71/295, train_loss: 0.4148, step time: 1.0638\n",
      "72/295, train_loss: 0.1101, step time: 1.0388\n",
      "73/295, train_loss: 0.1762, step time: 1.0417\n",
      "74/295, train_loss: 0.0964, step time: 1.0833\n",
      "75/295, train_loss: 0.2223, step time: 1.0351\n",
      "76/295, train_loss: 0.1445, step time: 1.0642\n",
      "77/295, train_loss: 0.1614, step time: 1.0607\n",
      "78/295, train_loss: 0.2104, step time: 1.0565\n",
      "79/295, train_loss: 0.0931, step time: 1.0416\n",
      "80/295, train_loss: 0.0971, step time: 1.0409\n",
      "81/295, train_loss: 0.2393, step time: 1.0444\n",
      "82/295, train_loss: 0.0756, step time: 1.0413\n",
      "83/295, train_loss: 0.2205, step time: 1.0326\n",
      "84/295, train_loss: 0.1583, step time: 1.0582\n",
      "85/295, train_loss: 0.1076, step time: 1.0322\n",
      "86/295, train_loss: 0.1152, step time: 1.0367\n",
      "87/295, train_loss: 0.4457, step time: 1.0328\n",
      "88/295, train_loss: 0.2813, step time: 1.0372\n",
      "89/295, train_loss: 0.0799, step time: 1.0332\n",
      "90/295, train_loss: 0.3383, step time: 1.0391\n",
      "91/295, train_loss: 0.2368, step time: 1.1154\n",
      "92/295, train_loss: 0.1263, step time: 1.0453\n",
      "93/295, train_loss: 0.1510, step time: 1.0392\n",
      "94/295, train_loss: 0.3626, step time: 1.0404\n",
      "95/295, train_loss: 0.5244, step time: 1.0728\n",
      "96/295, train_loss: 0.5567, step time: 1.0339\n",
      "97/295, train_loss: 0.1273, step time: 1.0393\n",
      "98/295, train_loss: 0.1049, step time: 1.0498\n",
      "99/295, train_loss: 0.2000, step time: 1.0557\n",
      "100/295, train_loss: 0.4183, step time: 1.0971\n",
      "101/295, train_loss: 0.2527, step time: 1.0383\n",
      "102/295, train_loss: 0.2222, step time: 1.0457\n",
      "103/295, train_loss: 0.1171, step time: 1.0526\n",
      "104/295, train_loss: 0.1528, step time: 1.0398\n",
      "105/295, train_loss: 0.1953, step time: 1.0408\n",
      "106/295, train_loss: 0.0885, step time: 1.0391\n",
      "107/295, train_loss: 0.1574, step time: 1.0448\n",
      "108/295, train_loss: 0.2693, step time: 1.0609\n",
      "109/295, train_loss: 0.0684, step time: 1.1005\n",
      "110/295, train_loss: 0.2334, step time: 1.0404\n",
      "111/295, train_loss: 0.4740, step time: 1.0412\n",
      "112/295, train_loss: 0.6053, step time: 1.0524\n",
      "113/295, train_loss: 0.1272, step time: 1.0595\n",
      "114/295, train_loss: 0.1070, step time: 1.0310\n",
      "115/295, train_loss: 0.4209, step time: 1.0575\n",
      "116/295, train_loss: 0.1465, step time: 1.0369\n",
      "117/295, train_loss: 0.1573, step time: 1.0342\n",
      "118/295, train_loss: 0.1283, step time: 1.0392\n",
      "119/295, train_loss: 0.3118, step time: 1.0503\n",
      "120/295, train_loss: 0.0674, step time: 1.0633\n",
      "121/295, train_loss: 0.1245, step time: 1.0398\n",
      "122/295, train_loss: 0.1247, step time: 1.0319\n",
      "123/295, train_loss: 0.1614, step time: 1.0326\n",
      "124/295, train_loss: 0.0768, step time: 1.0317\n",
      "125/295, train_loss: 0.0838, step time: 1.0566\n",
      "126/295, train_loss: 0.4118, step time: 1.0324\n",
      "127/295, train_loss: 0.2035, step time: 1.0663\n",
      "128/295, train_loss: 0.0725, step time: 1.1037\n",
      "129/295, train_loss: 0.2044, step time: 1.0342\n",
      "130/295, train_loss: 0.1086, step time: 1.0682\n",
      "131/295, train_loss: 0.3710, step time: 1.0594\n",
      "132/295, train_loss: 0.0589, step time: 1.0621\n",
      "133/295, train_loss: 0.1134, step time: 1.0526\n",
      "134/295, train_loss: 0.1737, step time: 1.0362\n",
      "135/295, train_loss: 0.2032, step time: 1.0306\n",
      "136/295, train_loss: 0.0578, step time: 1.0383\n",
      "137/295, train_loss: 0.1232, step time: 1.0403\n",
      "138/295, train_loss: 0.2689, step time: 1.0403\n",
      "139/295, train_loss: 0.2247, step time: 1.0858\n",
      "140/295, train_loss: 0.1731, step time: 1.0515\n",
      "141/295, train_loss: 0.1608, step time: 1.0492\n",
      "142/295, train_loss: 0.2157, step time: 1.0595\n",
      "143/295, train_loss: 0.0868, step time: 1.0434\n",
      "144/295, train_loss: 0.1382, step time: 1.0395\n",
      "145/295, train_loss: 0.1925, step time: 1.0551\n",
      "146/295, train_loss: 0.2381, step time: 1.0344\n",
      "147/295, train_loss: 0.1776, step time: 1.0373\n",
      "148/295, train_loss: 0.2279, step time: 1.0652\n",
      "149/295, train_loss: 0.1266, step time: 1.0370\n",
      "150/295, train_loss: 0.2112, step time: 1.0337\n",
      "151/295, train_loss: 0.1591, step time: 1.1028\n",
      "152/295, train_loss: 0.2408, step time: 1.0364\n",
      "153/295, train_loss: 0.3760, step time: 1.0310\n",
      "154/295, train_loss: 0.5059, step time: 1.0333\n",
      "155/295, train_loss: 0.1784, step time: 1.0375\n",
      "156/295, train_loss: 0.0645, step time: 1.0449\n",
      "157/295, train_loss: 0.1333, step time: 1.0615\n",
      "158/295, train_loss: 0.3165, step time: 1.0484\n",
      "159/295, train_loss: 0.1529, step time: 1.0377\n",
      "160/295, train_loss: 0.1348, step time: 1.0840\n",
      "161/295, train_loss: 0.4067, step time: 1.0350\n",
      "162/295, train_loss: 0.1230, step time: 1.0389\n",
      "163/295, train_loss: 0.1321, step time: 1.0573\n",
      "164/295, train_loss: 0.2773, step time: 1.0370\n",
      "165/295, train_loss: 0.2399, step time: 1.0349\n",
      "166/295, train_loss: 0.2778, step time: 1.0360\n",
      "167/295, train_loss: 0.0791, step time: 1.0561\n",
      "168/295, train_loss: 0.1066, step time: 1.1051\n",
      "169/295, train_loss: 0.2965, step time: 1.0318\n",
      "170/295, train_loss: 0.4680, step time: 1.0803\n",
      "171/295, train_loss: 0.2214, step time: 1.0348\n",
      "172/295, train_loss: 0.0802, step time: 1.0377\n",
      "173/295, train_loss: 0.1093, step time: 1.0516\n",
      "174/295, train_loss: 0.1181, step time: 1.0366\n",
      "175/295, train_loss: 0.0746, step time: 1.0326\n",
      "176/295, train_loss: 0.3188, step time: 1.0484\n",
      "177/295, train_loss: 0.2655, step time: 1.0514\n",
      "178/295, train_loss: 0.1304, step time: 1.0325\n",
      "179/295, train_loss: 0.2396, step time: 1.0387\n",
      "180/295, train_loss: 0.1792, step time: 1.0434\n",
      "181/295, train_loss: 0.1326, step time: 1.1010\n",
      "182/295, train_loss: 0.1391, step time: 1.0792\n",
      "183/295, train_loss: 0.4656, step time: 1.0440\n",
      "184/295, train_loss: 0.1843, step time: 1.0392\n",
      "185/295, train_loss: 0.0911, step time: 1.0368\n",
      "186/295, train_loss: 0.3925, step time: 1.1259\n",
      "187/295, train_loss: 0.2863, step time: 1.0660\n",
      "188/295, train_loss: 0.1477, step time: 1.0373\n",
      "189/295, train_loss: 0.1393, step time: 1.0959\n",
      "190/295, train_loss: 0.1884, step time: 1.0534\n",
      "191/295, train_loss: 0.0618, step time: 1.0576\n",
      "192/295, train_loss: 0.1532, step time: 1.0357\n",
      "193/295, train_loss: 0.4929, step time: 1.0542\n",
      "194/295, train_loss: 0.2008, step time: 1.0507\n",
      "195/295, train_loss: 0.3383, step time: 1.1099\n",
      "196/295, train_loss: 0.1984, step time: 1.0758\n",
      "197/295, train_loss: 0.0735, step time: 1.0323\n",
      "198/295, train_loss: 0.6116, step time: 1.1147\n",
      "199/295, train_loss: 0.1959, step time: 1.0729\n",
      "200/295, train_loss: 0.5025, step time: 1.0575\n",
      "201/295, train_loss: 0.7021, step time: 1.0506\n",
      "202/295, train_loss: 0.0903, step time: 1.0371\n",
      "203/295, train_loss: 0.1258, step time: 1.0686\n",
      "204/295, train_loss: 0.4311, step time: 1.0308\n",
      "205/295, train_loss: 0.2831, step time: 1.0321\n",
      "206/295, train_loss: 0.1102, step time: 1.0525\n",
      "207/295, train_loss: 0.3769, step time: 1.0390\n",
      "208/295, train_loss: 0.1017, step time: 1.0395\n",
      "209/295, train_loss: 0.1245, step time: 1.0369\n",
      "210/295, train_loss: 0.2312, step time: 1.0350\n",
      "211/295, train_loss: 0.0857, step time: 1.0366\n",
      "212/295, train_loss: 0.1857, step time: 1.0349\n",
      "213/295, train_loss: 0.5171, step time: 1.0305\n",
      "214/295, train_loss: 0.1031, step time: 1.0682\n",
      "215/295, train_loss: 0.0624, step time: 1.0325\n",
      "216/295, train_loss: 0.1994, step time: 1.0409\n",
      "217/295, train_loss: 0.1174, step time: 1.0352\n",
      "218/295, train_loss: 0.0815, step time: 1.0326\n",
      "219/295, train_loss: 0.0955, step time: 1.0713\n",
      "220/295, train_loss: 0.0524, step time: 1.0573\n",
      "221/295, train_loss: 0.1204, step time: 1.0396\n",
      "222/295, train_loss: 0.1136, step time: 1.0306\n",
      "223/295, train_loss: 0.1799, step time: 1.0350\n",
      "224/295, train_loss: 0.0955, step time: 1.0364\n",
      "225/295, train_loss: 0.1520, step time: 1.0373\n",
      "226/295, train_loss: 0.1016, step time: 1.0365\n",
      "227/295, train_loss: 0.0874, step time: 1.0410\n",
      "228/295, train_loss: 0.1367, step time: 1.0366\n",
      "229/295, train_loss: 0.0736, step time: 1.0418\n",
      "230/295, train_loss: 0.0582, step time: 1.0412\n",
      "231/295, train_loss: 0.2785, step time: 1.0393\n",
      "232/295, train_loss: 0.0779, step time: 1.1127\n",
      "233/295, train_loss: 0.2496, step time: 1.0664\n",
      "234/295, train_loss: 0.1686, step time: 1.0379\n",
      "235/295, train_loss: 0.2666, step time: 1.0528\n",
      "236/295, train_loss: 0.0926, step time: 1.0577\n",
      "237/295, train_loss: 0.1738, step time: 1.0308\n",
      "238/295, train_loss: 0.0921, step time: 1.0314\n",
      "239/295, train_loss: 0.4154, step time: 1.0530\n",
      "240/295, train_loss: 0.0634, step time: 1.0343\n",
      "241/295, train_loss: 0.1124, step time: 1.0387\n",
      "242/295, train_loss: 0.0834, step time: 1.0353\n",
      "243/295, train_loss: 0.5035, step time: 1.0319\n",
      "244/295, train_loss: 0.0968, step time: 1.0319\n",
      "245/295, train_loss: 0.3145, step time: 1.0352\n",
      "246/295, train_loss: 0.2126, step time: 1.0368\n",
      "247/295, train_loss: 0.1757, step time: 1.0843\n",
      "248/295, train_loss: 0.1319, step time: 1.0382\n",
      "249/295, train_loss: 0.1251, step time: 1.0550\n",
      "250/295, train_loss: 0.1122, step time: 1.0692\n",
      "251/295, train_loss: 0.1288, step time: 1.0328\n",
      "252/295, train_loss: 0.1431, step time: 1.0413\n",
      "253/295, train_loss: 0.0989, step time: 1.0633\n",
      "254/295, train_loss: 0.0638, step time: 1.0320\n",
      "255/295, train_loss: 0.0813, step time: 1.0349\n",
      "256/295, train_loss: 0.1050, step time: 1.0488\n",
      "257/295, train_loss: 0.1606, step time: 1.0333\n",
      "258/295, train_loss: 0.5084, step time: 1.0381\n",
      "259/295, train_loss: 0.0904, step time: 1.0368\n",
      "260/295, train_loss: 0.1412, step time: 1.0353\n",
      "261/295, train_loss: 0.0812, step time: 1.0459\n",
      "262/295, train_loss: 0.4279, step time: 1.0441\n",
      "263/295, train_loss: 0.0581, step time: 1.0328\n",
      "264/295, train_loss: 0.2474, step time: 1.0341\n",
      "265/295, train_loss: 0.0567, step time: 1.0431\n",
      "266/295, train_loss: 0.2319, step time: 1.0508\n",
      "267/295, train_loss: 0.1680, step time: 1.1064\n",
      "268/295, train_loss: 0.1101, step time: 1.0346\n",
      "269/295, train_loss: 0.0642, step time: 1.0634\n",
      "270/295, train_loss: 0.0867, step time: 1.0455\n",
      "271/295, train_loss: 0.1776, step time: 1.0398\n",
      "272/295, train_loss: 0.1373, step time: 1.0364\n",
      "273/295, train_loss: 0.2326, step time: 1.0403\n",
      "274/295, train_loss: 0.1231, step time: 1.0337\n",
      "275/295, train_loss: 0.6298, step time: 1.0433\n",
      "276/295, train_loss: 0.0807, step time: 1.0568\n",
      "277/295, train_loss: 0.1546, step time: 1.0334\n",
      "278/295, train_loss: 0.1970, step time: 1.0409\n",
      "279/295, train_loss: 0.2151, step time: 1.0479\n",
      "280/295, train_loss: 0.4926, step time: 1.0477\n",
      "281/295, train_loss: 0.1129, step time: 1.0495\n",
      "282/295, train_loss: 0.3523, step time: 1.0322\n",
      "283/295, train_loss: 0.3081, step time: 1.0307\n",
      "284/295, train_loss: 0.0653, step time: 1.0332\n",
      "285/295, train_loss: 0.1629, step time: 1.0426\n",
      "286/295, train_loss: 0.2075, step time: 1.0411\n",
      "287/295, train_loss: 0.1464, step time: 1.0726\n",
      "288/295, train_loss: 0.4807, step time: 1.0295\n",
      "289/295, train_loss: 0.1881, step time: 1.0286\n",
      "290/295, train_loss: 0.2748, step time: 1.0290\n",
      "291/295, train_loss: 0.0929, step time: 1.0288\n",
      "292/295, train_loss: 0.3983, step time: 1.0292\n",
      "293/295, train_loss: 0.1547, step time: 1.0295\n",
      "294/295, train_loss: 0.1556, step time: 1.0293\n",
      "295/295, train_loss: 0.3994, step time: 1.0332\n",
      "epoch 8 average loss: 0.2036\n",
      "current epoch: 8 current mean dice: 0.7341 tc: 0.6572 wt: 0.8248 et: 0.7149\n",
      "best mean dice: 0.7472 at epoch: 7\n",
      "time consuming of epoch 8 is: 381.7428\n",
      "----------\n",
      "epoch 9/100\n",
      "1/295, train_loss: 0.2414, step time: 1.1223\n",
      "2/295, train_loss: 0.6096, step time: 1.1341\n",
      "3/295, train_loss: 0.1044, step time: 1.0905\n",
      "4/295, train_loss: 0.0940, step time: 1.0658\n",
      "5/295, train_loss: 0.1121, step time: 1.0520\n",
      "6/295, train_loss: 0.0968, step time: 1.0421\n",
      "7/295, train_loss: 0.1470, step time: 1.0366\n",
      "8/295, train_loss: 0.1813, step time: 1.0339\n",
      "9/295, train_loss: 0.2820, step time: 1.0479\n",
      "10/295, train_loss: 0.6858, step time: 1.0538\n",
      "11/295, train_loss: 0.4910, step time: 1.0392\n",
      "12/295, train_loss: 0.0889, step time: 1.0377\n",
      "13/295, train_loss: 0.0618, step time: 1.0507\n",
      "14/295, train_loss: 0.2667, step time: 1.0555\n",
      "15/295, train_loss: 0.2655, step time: 1.0757\n",
      "16/295, train_loss: 0.1399, step time: 1.1019\n",
      "17/295, train_loss: 0.1542, step time: 1.0483\n",
      "18/295, train_loss: 0.3658, step time: 1.0948\n",
      "19/295, train_loss: 0.3930, step time: 1.0316\n",
      "20/295, train_loss: 0.1984, step time: 1.0702\n",
      "21/295, train_loss: 0.0857, step time: 1.0372\n",
      "22/295, train_loss: 0.1544, step time: 1.0340\n",
      "23/295, train_loss: 0.1836, step time: 1.1557\n",
      "24/295, train_loss: 0.1393, step time: 1.0395\n",
      "25/295, train_loss: 0.1463, step time: 1.0389\n",
      "26/295, train_loss: 0.0761, step time: 1.0342\n",
      "27/295, train_loss: 0.0666, step time: 1.0325\n",
      "28/295, train_loss: 0.1860, step time: 1.0553\n",
      "29/295, train_loss: 0.1364, step time: 1.0320\n",
      "30/295, train_loss: 0.1685, step time: 1.0555\n",
      "31/295, train_loss: 0.0692, step time: 1.0596\n",
      "32/295, train_loss: 0.1308, step time: 1.0481\n",
      "33/295, train_loss: 0.0919, step time: 1.0318\n",
      "34/295, train_loss: 0.1253, step time: 1.0542\n",
      "35/295, train_loss: 0.1256, step time: 1.0337\n",
      "36/295, train_loss: 0.0972, step time: 1.0317\n",
      "37/295, train_loss: 0.1682, step time: 1.0445\n",
      "38/295, train_loss: 0.3581, step time: 1.0707\n",
      "39/295, train_loss: 0.0854, step time: 1.0746\n",
      "40/295, train_loss: 0.0693, step time: 1.0409\n",
      "41/295, train_loss: 0.5417, step time: 1.0368\n",
      "42/295, train_loss: 0.1138, step time: 1.0370\n",
      "43/295, train_loss: 0.1602, step time: 1.0669\n",
      "44/295, train_loss: 0.1699, step time: 1.0397\n",
      "45/295, train_loss: 0.0653, step time: 1.0456\n",
      "46/295, train_loss: 0.1272, step time: 1.0395\n",
      "47/295, train_loss: 0.3252, step time: 1.0437\n",
      "48/295, train_loss: 0.1151, step time: 1.0406\n",
      "49/295, train_loss: 0.4442, step time: 1.0346\n",
      "50/295, train_loss: 0.1316, step time: 1.0407\n",
      "51/295, train_loss: 0.5388, step time: 1.0538\n",
      "52/295, train_loss: 0.1444, step time: 1.0323\n",
      "53/295, train_loss: 0.1042, step time: 1.0397\n",
      "54/295, train_loss: 0.3256, step time: 1.0548\n",
      "55/295, train_loss: 0.1842, step time: 1.0379\n",
      "56/295, train_loss: 0.1531, step time: 1.0741\n",
      "57/295, train_loss: 0.1370, step time: 1.0415\n",
      "58/295, train_loss: 0.1791, step time: 1.0370\n",
      "59/295, train_loss: 0.0784, step time: 1.0505\n",
      "60/295, train_loss: 0.1598, step time: 1.0555\n",
      "61/295, train_loss: 0.0628, step time: 1.0335\n",
      "62/295, train_loss: 0.1051, step time: 1.0398\n",
      "63/295, train_loss: 0.4577, step time: 1.0599\n",
      "64/295, train_loss: 0.0773, step time: 1.0327\n",
      "65/295, train_loss: 0.3491, step time: 1.0384\n",
      "66/295, train_loss: 0.0774, step time: 1.0334\n",
      "67/295, train_loss: 0.1391, step time: 1.0428\n",
      "68/295, train_loss: 0.6776, step time: 1.0465\n",
      "69/295, train_loss: 0.1214, step time: 1.0397\n",
      "70/295, train_loss: 0.1405, step time: 1.0535\n",
      "71/295, train_loss: 0.5304, step time: 1.0406\n",
      "72/295, train_loss: 0.1407, step time: 1.0662\n",
      "73/295, train_loss: 0.3968, step time: 1.0593\n",
      "74/295, train_loss: 0.0899, step time: 1.0395\n",
      "75/295, train_loss: 0.1262, step time: 1.0416\n",
      "76/295, train_loss: 0.0807, step time: 1.0629\n",
      "77/295, train_loss: 0.3657, step time: 1.0595\n",
      "78/295, train_loss: 0.2280, step time: 1.0330\n",
      "79/295, train_loss: 0.0714, step time: 1.0728\n",
      "80/295, train_loss: 0.1355, step time: 1.0361\n",
      "81/295, train_loss: 0.0912, step time: 1.0370\n",
      "82/295, train_loss: 0.3106, step time: 1.0378\n",
      "83/295, train_loss: 0.1187, step time: 1.0613\n",
      "84/295, train_loss: 0.3347, step time: 1.0723\n",
      "85/295, train_loss: 0.4146, step time: 1.0520\n",
      "86/295, train_loss: 0.0694, step time: 1.0425\n",
      "87/295, train_loss: 0.1765, step time: 1.0343\n",
      "88/295, train_loss: 0.2779, step time: 1.0859\n",
      "89/295, train_loss: 0.0802, step time: 1.1242\n",
      "90/295, train_loss: 0.1144, step time: 1.0428\n",
      "91/295, train_loss: 0.1026, step time: 1.0542\n",
      "92/295, train_loss: 0.2148, step time: 1.0347\n",
      "93/295, train_loss: 0.5145, step time: 1.0730\n",
      "94/295, train_loss: 0.2644, step time: 1.0433\n",
      "95/295, train_loss: 0.1478, step time: 1.0364\n",
      "96/295, train_loss: 0.1431, step time: 1.0303\n",
      "97/295, train_loss: 0.2007, step time: 1.0369\n",
      "98/295, train_loss: 0.4312, step time: 1.0337\n",
      "99/295, train_loss: 0.1855, step time: 1.0574\n",
      "100/295, train_loss: 0.2131, step time: 1.0365\n",
      "101/295, train_loss: 0.2754, step time: 1.0393\n",
      "102/295, train_loss: 0.1105, step time: 1.0397\n",
      "103/295, train_loss: 0.0700, step time: 1.0352\n",
      "104/295, train_loss: 0.1784, step time: 1.0300\n",
      "105/295, train_loss: 0.0670, step time: 1.0406\n",
      "106/295, train_loss: 0.1012, step time: 1.0354\n",
      "107/295, train_loss: 0.0762, step time: 1.0430\n",
      "108/295, train_loss: 0.1446, step time: 1.0537\n",
      "109/295, train_loss: 0.0587, step time: 1.0349\n",
      "110/295, train_loss: 0.0776, step time: 1.0363\n",
      "111/295, train_loss: 0.1956, step time: 1.0493\n",
      "112/295, train_loss: 0.1366, step time: 1.0334\n",
      "113/295, train_loss: 0.1959, step time: 1.0357\n",
      "114/295, train_loss: 0.2187, step time: 1.0329\n",
      "115/295, train_loss: 0.4497, step time: 1.0353\n",
      "116/295, train_loss: 0.4511, step time: 1.0619\n",
      "117/295, train_loss: 0.1015, step time: 1.0330\n",
      "118/295, train_loss: 0.1281, step time: 1.0358\n",
      "119/295, train_loss: 0.1953, step time: 1.0327\n",
      "120/295, train_loss: 0.0738, step time: 1.0325\n",
      "121/295, train_loss: 0.1072, step time: 1.0334\n",
      "122/295, train_loss: 0.0525, step time: 1.0414\n",
      "123/295, train_loss: 0.4098, step time: 1.0929\n",
      "124/295, train_loss: 0.0541, step time: 1.0329\n",
      "125/295, train_loss: 0.3651, step time: 1.0403\n",
      "126/295, train_loss: 0.2690, step time: 1.0560\n",
      "127/295, train_loss: 0.0589, step time: 1.0567\n",
      "128/295, train_loss: 0.1620, step time: 1.0790\n",
      "129/295, train_loss: 0.2071, step time: 1.0370\n",
      "130/295, train_loss: 0.4501, step time: 1.0371\n",
      "131/295, train_loss: 0.0968, step time: 1.0309\n",
      "132/295, train_loss: 0.0746, step time: 1.0348\n",
      "133/295, train_loss: 0.1022, step time: 1.0326\n",
      "134/295, train_loss: 0.1157, step time: 1.0317\n",
      "135/295, train_loss: 0.4090, step time: 1.0599\n",
      "136/295, train_loss: 0.1577, step time: 1.0660\n",
      "137/295, train_loss: 0.2103, step time: 1.0348\n",
      "138/295, train_loss: 0.1165, step time: 1.0432\n",
      "139/295, train_loss: 0.1246, step time: 1.0808\n",
      "140/295, train_loss: 0.1143, step time: 1.0341\n",
      "141/295, train_loss: 0.0915, step time: 1.0483\n",
      "142/295, train_loss: 0.2084, step time: 1.0352\n",
      "143/295, train_loss: 0.1356, step time: 1.0756\n",
      "144/295, train_loss: 0.2379, step time: 1.0391\n",
      "145/295, train_loss: 0.5896, step time: 1.0500\n",
      "146/295, train_loss: 0.0871, step time: 1.0345\n",
      "147/295, train_loss: 0.3109, step time: 1.0443\n",
      "148/295, train_loss: 0.4036, step time: 1.1152\n",
      "149/295, train_loss: 0.0759, step time: 1.0394\n",
      "150/295, train_loss: 0.4519, step time: 1.0418\n",
      "151/295, train_loss: 0.1300, step time: 1.0428\n",
      "152/295, train_loss: 0.1055, step time: 1.0725\n",
      "153/295, train_loss: 0.2110, step time: 1.0503\n",
      "154/295, train_loss: 0.0819, step time: 1.0416\n",
      "155/295, train_loss: 0.3182, step time: 1.0396\n",
      "156/295, train_loss: 0.0713, step time: 1.0697\n",
      "157/295, train_loss: 0.0706, step time: 1.0427\n",
      "158/295, train_loss: 0.2002, step time: 1.0743\n",
      "159/295, train_loss: 0.4260, step time: 1.0461\n",
      "160/295, train_loss: 0.0938, step time: 1.0353\n",
      "161/295, train_loss: 0.1226, step time: 1.0423\n",
      "162/295, train_loss: 0.1132, step time: 1.0390\n",
      "163/295, train_loss: 0.1793, step time: 1.0386\n",
      "164/295, train_loss: 0.6411, step time: 1.0436\n",
      "165/295, train_loss: 0.1380, step time: 1.0560\n",
      "166/295, train_loss: 0.2267, step time: 1.0377\n",
      "167/295, train_loss: 0.4518, step time: 1.0577\n",
      "168/295, train_loss: 0.0962, step time: 1.0335\n",
      "169/295, train_loss: 0.1008, step time: 1.0394\n",
      "170/295, train_loss: 0.1221, step time: 1.0362\n",
      "171/295, train_loss: 0.1240, step time: 1.0339\n",
      "172/295, train_loss: 0.1344, step time: 1.0581\n",
      "173/295, train_loss: 0.0938, step time: 1.0506\n",
      "174/295, train_loss: 0.1316, step time: 1.0368\n",
      "175/295, train_loss: 0.4328, step time: 1.0336\n",
      "176/295, train_loss: 0.3065, step time: 1.0362\n",
      "177/295, train_loss: 0.1309, step time: 1.0398\n",
      "178/295, train_loss: 0.0507, step time: 1.0457\n",
      "179/295, train_loss: 0.3022, step time: 1.0327\n",
      "180/295, train_loss: 0.3042, step time: 1.0353\n",
      "181/295, train_loss: 0.2299, step time: 1.0353\n",
      "182/295, train_loss: 0.0490, step time: 1.0322\n",
      "183/295, train_loss: 0.1516, step time: 1.0338\n",
      "184/295, train_loss: 0.1032, step time: 1.0483\n",
      "185/295, train_loss: 0.1105, step time: 1.0366\n",
      "186/295, train_loss: 0.6072, step time: 1.0356\n",
      "187/295, train_loss: 0.0456, step time: 1.0364\n",
      "188/295, train_loss: 0.1055, step time: 1.0325\n",
      "189/295, train_loss: 0.1023, step time: 1.0506\n",
      "190/295, train_loss: 0.1387, step time: 1.0460\n",
      "191/295, train_loss: 0.1254, step time: 1.0410\n",
      "192/295, train_loss: 0.1076, step time: 1.0383\n",
      "193/295, train_loss: 0.0845, step time: 1.0337\n",
      "194/295, train_loss: 0.1483, step time: 1.0322\n",
      "195/295, train_loss: 0.2026, step time: 1.0338\n",
      "196/295, train_loss: 0.1406, step time: 1.0382\n",
      "197/295, train_loss: 0.4523, step time: 1.0660\n",
      "198/295, train_loss: 0.2718, step time: 1.0742\n",
      "199/295, train_loss: 0.1078, step time: 1.0348\n",
      "200/295, train_loss: 0.5569, step time: 1.0348\n",
      "201/295, train_loss: 0.0902, step time: 1.0343\n",
      "202/295, train_loss: 0.1262, step time: 1.0599\n",
      "203/295, train_loss: 0.0962, step time: 1.0354\n",
      "204/295, train_loss: 0.0770, step time: 1.0693\n",
      "205/295, train_loss: 0.4586, step time: 1.0367\n",
      "206/295, train_loss: 0.3496, step time: 1.0416\n",
      "207/295, train_loss: 0.0997, step time: 1.0513\n",
      "208/295, train_loss: 0.1755, step time: 1.0406\n",
      "209/295, train_loss: 0.4970, step time: 1.0385\n",
      "210/295, train_loss: 0.1535, step time: 1.0641\n",
      "211/295, train_loss: 0.4003, step time: 1.0336\n",
      "212/295, train_loss: 0.1844, step time: 1.0426\n",
      "213/295, train_loss: 0.1276, step time: 1.0404\n",
      "214/295, train_loss: 0.2999, step time: 1.0340\n",
      "215/295, train_loss: 0.0742, step time: 1.0365\n",
      "216/295, train_loss: 0.3375, step time: 1.0331\n",
      "217/295, train_loss: 0.0927, step time: 1.0488\n",
      "218/295, train_loss: 0.0753, step time: 1.0483\n",
      "219/295, train_loss: 0.1347, step time: 1.0439\n",
      "220/295, train_loss: 0.1937, step time: 1.0347\n",
      "221/295, train_loss: 0.0779, step time: 1.0313\n",
      "222/295, train_loss: 0.0938, step time: 1.0323\n",
      "223/295, train_loss: 0.2186, step time: 1.0318\n",
      "224/295, train_loss: 0.1570, step time: 1.0337\n",
      "225/295, train_loss: 0.1512, step time: 1.0589\n",
      "226/295, train_loss: 0.0576, step time: 1.0329\n",
      "227/295, train_loss: 0.1942, step time: 1.0398\n",
      "228/295, train_loss: 0.3833, step time: 1.0521\n",
      "229/295, train_loss: 0.1212, step time: 1.0505\n",
      "230/295, train_loss: 0.2277, step time: 1.0468\n",
      "231/295, train_loss: 0.0648, step time: 1.0388\n",
      "232/295, train_loss: 0.3807, step time: 1.0482\n",
      "233/295, train_loss: 0.0927, step time: 1.0533\n",
      "234/295, train_loss: 0.2466, step time: 1.0576\n",
      "235/295, train_loss: 0.1256, step time: 1.0470\n",
      "236/295, train_loss: 0.3279, step time: 1.0331\n",
      "237/295, train_loss: 0.4374, step time: 1.0345\n",
      "238/295, train_loss: 0.0773, step time: 1.0335\n",
      "239/295, train_loss: 0.0726, step time: 1.0396\n",
      "240/295, train_loss: 0.2959, step time: 1.0437\n",
      "241/295, train_loss: 0.2743, step time: 1.0385\n",
      "242/295, train_loss: 0.1271, step time: 1.0561\n",
      "243/295, train_loss: 0.0969, step time: 1.0640\n",
      "244/295, train_loss: 0.2219, step time: 1.0979\n",
      "245/295, train_loss: 0.1399, step time: 1.0458\n",
      "246/295, train_loss: 0.1796, step time: 1.0697\n",
      "247/295, train_loss: 0.0917, step time: 1.0420\n",
      "248/295, train_loss: 0.0874, step time: 1.0381\n",
      "249/295, train_loss: 0.1196, step time: 1.0357\n",
      "250/295, train_loss: 0.2154, step time: 1.0371\n",
      "251/295, train_loss: 0.0857, step time: 1.0344\n",
      "252/295, train_loss: 0.0765, step time: 1.0358\n",
      "253/295, train_loss: 0.1083, step time: 1.0410\n",
      "254/295, train_loss: 0.1411, step time: 1.0413\n",
      "255/295, train_loss: 0.1579, step time: 1.0551\n",
      "256/295, train_loss: 0.6143, step time: 1.0351\n",
      "257/295, train_loss: 0.0748, step time: 1.0381\n",
      "258/295, train_loss: 0.1109, step time: 1.0521\n",
      "259/295, train_loss: 0.4146, step time: 1.0319\n",
      "260/295, train_loss: 0.0764, step time: 1.0359\n",
      "261/295, train_loss: 0.0592, step time: 1.0399\n",
      "262/295, train_loss: 0.1680, step time: 1.0715\n",
      "263/295, train_loss: 0.1269, step time: 1.0398\n",
      "264/295, train_loss: 0.2437, step time: 1.0375\n",
      "265/295, train_loss: 0.1824, step time: 1.0450\n",
      "266/295, train_loss: 0.2343, step time: 1.0378\n",
      "267/295, train_loss: 0.1317, step time: 1.0552\n",
      "268/295, train_loss: 0.1374, step time: 1.0385\n",
      "269/295, train_loss: 0.2421, step time: 1.0691\n",
      "270/295, train_loss: 0.2103, step time: 1.0323\n",
      "271/295, train_loss: 0.3676, step time: 1.0327\n",
      "272/295, train_loss: 0.0799, step time: 1.0377\n",
      "273/295, train_loss: 0.0771, step time: 1.0328\n",
      "274/295, train_loss: 0.0963, step time: 1.0628\n",
      "275/295, train_loss: 0.0605, step time: 1.0362\n",
      "276/295, train_loss: 0.2118, step time: 1.0439\n",
      "277/295, train_loss: 0.3593, step time: 1.0376\n",
      "278/295, train_loss: 0.0921, step time: 1.0409\n",
      "279/295, train_loss: 0.1271, step time: 1.0303\n",
      "280/295, train_loss: 0.4636, step time: 1.0444\n",
      "281/295, train_loss: 0.1115, step time: 1.0345\n",
      "282/295, train_loss: 0.1074, step time: 1.0425\n",
      "283/295, train_loss: 0.1208, step time: 1.0371\n",
      "284/295, train_loss: 0.2310, step time: 1.0394\n",
      "285/295, train_loss: 0.3127, step time: 1.0549\n",
      "286/295, train_loss: 0.0618, step time: 1.0403\n",
      "287/295, train_loss: 0.1027, step time: 1.0566\n",
      "288/295, train_loss: 0.4169, step time: 1.0324\n",
      "289/295, train_loss: 0.1412, step time: 1.0289\n",
      "290/295, train_loss: 0.1551, step time: 1.0285\n",
      "291/295, train_loss: 0.1733, step time: 1.0296\n",
      "292/295, train_loss: 0.1049, step time: 1.0292\n",
      "293/295, train_loss: 0.1728, step time: 1.0295\n",
      "294/295, train_loss: 0.2133, step time: 1.0288\n",
      "295/295, train_loss: 0.3172, step time: 1.0287\n",
      "epoch 9 average loss: 0.1953\n",
      "saved new best metric model\n",
      "current epoch: 9 current mean dice: 0.7553 tc: 0.6889 wt: 0.8276 et: 0.7463\n",
      "best mean dice: 0.7553 at epoch: 9\n",
      "time consuming of epoch 9 is: 378.0899\n",
      "----------\n",
      "epoch 10/100\n",
      "1/295, train_loss: 0.2563, step time: 1.0557\n",
      "2/295, train_loss: 0.0591, step time: 1.1148\n",
      "3/295, train_loss: 0.0780, step time: 1.0859\n",
      "4/295, train_loss: 0.1365, step time: 1.0862\n",
      "5/295, train_loss: 0.4358, step time: 1.0664\n",
      "6/295, train_loss: 0.1215, step time: 1.0372\n",
      "7/295, train_loss: 0.1055, step time: 1.0351\n",
      "8/295, train_loss: 0.0711, step time: 1.0323\n",
      "9/295, train_loss: 0.5346, step time: 1.0312\n",
      "10/295, train_loss: 0.3452, step time: 1.0304\n",
      "11/295, train_loss: 0.0571, step time: 1.0996\n",
      "12/295, train_loss: 0.3975, step time: 1.0529\n",
      "13/295, train_loss: 0.3614, step time: 1.0808\n",
      "14/295, train_loss: 0.1177, step time: 1.0380\n",
      "15/295, train_loss: 0.0681, step time: 1.0401\n",
      "16/295, train_loss: 0.0921, step time: 1.0501\n",
      "17/295, train_loss: 0.3644, step time: 1.0670\n",
      "18/295, train_loss: 0.1011, step time: 1.0722\n",
      "19/295, train_loss: 0.1524, step time: 1.0896\n",
      "20/295, train_loss: 0.2144, step time: 1.0297\n",
      "21/295, train_loss: 0.1657, step time: 1.0437\n",
      "22/295, train_loss: 0.1251, step time: 1.1049\n",
      "23/295, train_loss: 0.0812, step time: 1.0328\n",
      "24/295, train_loss: 0.0824, step time: 1.0435\n",
      "25/295, train_loss: 0.0895, step time: 1.0345\n",
      "26/295, train_loss: 0.3861, step time: 1.0357\n",
      "27/295, train_loss: 0.1289, step time: 1.0412\n",
      "28/295, train_loss: 0.0640, step time: 1.0439\n",
      "29/295, train_loss: 0.0829, step time: 1.0319\n",
      "30/295, train_loss: 0.4707, step time: 1.0432\n",
      "31/295, train_loss: 0.1283, step time: 1.0369\n",
      "32/295, train_loss: 0.2580, step time: 1.0377\n",
      "33/295, train_loss: 0.6596, step time: 1.0359\n",
      "34/295, train_loss: 0.4028, step time: 1.0335\n",
      "35/295, train_loss: 0.1162, step time: 1.0454\n",
      "36/295, train_loss: 0.1998, step time: 1.0463\n",
      "37/295, train_loss: 0.3795, step time: 1.0739\n",
      "38/295, train_loss: 0.1609, step time: 1.0572\n",
      "39/295, train_loss: 0.2627, step time: 1.0305\n",
      "40/295, train_loss: 0.1998, step time: 1.0519\n",
      "41/295, train_loss: 0.5081, step time: 1.0520\n",
      "42/295, train_loss: 0.1640, step time: 1.0415\n",
      "43/295, train_loss: 0.0997, step time: 1.0411\n",
      "44/295, train_loss: 0.1919, step time: 1.0407\n",
      "45/295, train_loss: 0.1876, step time: 1.0379\n",
      "46/295, train_loss: 0.1940, step time: 1.0467\n",
      "47/295, train_loss: 0.0554, step time: 1.0351\n",
      "48/295, train_loss: 0.0991, step time: 1.1316\n",
      "49/295, train_loss: 0.0694, step time: 1.0444\n",
      "50/295, train_loss: 0.0499, step time: 1.0447\n",
      "51/295, train_loss: 0.1164, step time: 1.0328\n",
      "52/295, train_loss: 0.0644, step time: 1.0458\n",
      "53/295, train_loss: 0.2317, step time: 1.0543\n",
      "54/295, train_loss: 0.1072, step time: 1.0438\n",
      "55/295, train_loss: 0.3468, step time: 1.0351\n",
      "56/295, train_loss: 0.2876, step time: 1.0412\n",
      "57/295, train_loss: 0.1712, step time: 1.0313\n",
      "58/295, train_loss: 0.1131, step time: 1.0371\n",
      "59/295, train_loss: 0.0746, step time: 1.0368\n",
      "60/295, train_loss: 0.4338, step time: 1.0590\n",
      "61/295, train_loss: 0.1268, step time: 1.0504\n",
      "62/295, train_loss: 0.1573, step time: 1.0343\n",
      "63/295, train_loss: 0.4553, step time: 1.0388\n",
      "64/295, train_loss: 0.0951, step time: 1.0797\n",
      "65/295, train_loss: 0.1823, step time: 1.0752\n",
      "66/295, train_loss: 0.1629, step time: 1.0513\n",
      "67/295, train_loss: 0.1176, step time: 1.0562\n",
      "68/295, train_loss: 0.0735, step time: 1.0639\n",
      "69/295, train_loss: 0.1830, step time: 1.0886\n",
      "70/295, train_loss: 0.1910, step time: 1.0414\n",
      "71/295, train_loss: 0.0914, step time: 1.0444\n",
      "72/295, train_loss: 0.0964, step time: 1.0570\n",
      "73/295, train_loss: 0.0946, step time: 1.0332\n",
      "74/295, train_loss: 0.0720, step time: 1.0355\n",
      "75/295, train_loss: 0.1496, step time: 1.0345\n",
      "76/295, train_loss: 0.1716, step time: 1.0327\n",
      "77/295, train_loss: 0.1280, step time: 1.0394\n",
      "78/295, train_loss: 0.1662, step time: 1.0707\n",
      "79/295, train_loss: 0.0985, step time: 1.0333\n",
      "80/295, train_loss: 0.3715, step time: 1.0363\n",
      "81/295, train_loss: 0.4633, step time: 1.0344\n",
      "82/295, train_loss: 0.1124, step time: 1.0308\n",
      "83/295, train_loss: 0.1071, step time: 1.0384\n",
      "84/295, train_loss: 0.1662, step time: 1.0366\n",
      "85/295, train_loss: 0.2252, step time: 1.0396\n",
      "86/295, train_loss: 0.1638, step time: 1.0495\n",
      "87/295, train_loss: 0.1020, step time: 1.0528\n",
      "88/295, train_loss: 0.0776, step time: 1.0485\n",
      "89/295, train_loss: 0.0620, step time: 1.0359\n",
      "90/295, train_loss: 0.0840, step time: 1.0356\n",
      "91/295, train_loss: 0.1000, step time: 1.0328\n",
      "92/295, train_loss: 0.3250, step time: 1.0328\n",
      "93/295, train_loss: 0.0590, step time: 1.0569\n",
      "94/295, train_loss: 0.1749, step time: 1.0668\n",
      "95/295, train_loss: 0.1622, step time: 1.0378\n",
      "96/295, train_loss: 0.0622, step time: 1.0508\n",
      "97/295, train_loss: 0.2729, step time: 1.0330\n",
      "98/295, train_loss: 0.2513, step time: 1.0349\n",
      "99/295, train_loss: 0.0886, step time: 1.0405\n",
      "100/295, train_loss: 0.5065, step time: 1.0383\n",
      "101/295, train_loss: 0.0951, step time: 1.0394\n",
      "102/295, train_loss: 0.5746, step time: 1.0547\n",
      "103/295, train_loss: 0.1952, step time: 1.0343\n",
      "104/295, train_loss: 0.1164, step time: 1.0404\n",
      "105/295, train_loss: 0.3825, step time: 1.0520\n",
      "106/295, train_loss: 0.4232, step time: 1.0341\n",
      "107/295, train_loss: 0.0970, step time: 1.0320\n",
      "108/295, train_loss: 0.1125, step time: 1.0508\n",
      "109/295, train_loss: 0.1299, step time: 1.0609\n",
      "110/295, train_loss: 0.1530, step time: 1.0379\n",
      "111/295, train_loss: 0.0905, step time: 1.0607\n",
      "112/295, train_loss: 0.0783, step time: 1.0373\n",
      "113/295, train_loss: 0.3949, step time: 1.0545\n",
      "114/295, train_loss: 0.3884, step time: 1.0389\n",
      "115/295, train_loss: 0.1700, step time: 1.0414\n",
      "116/295, train_loss: 0.1080, step time: 1.0540\n",
      "117/295, train_loss: 0.0845, step time: 1.0383\n",
      "118/295, train_loss: 0.1051, step time: 1.0355\n",
      "119/295, train_loss: 0.1784, step time: 1.0386\n",
      "120/295, train_loss: 0.1442, step time: 1.0352\n",
      "121/295, train_loss: 0.0948, step time: 1.0529\n",
      "122/295, train_loss: 0.2109, step time: 1.0367\n",
      "123/295, train_loss: 0.1645, step time: 1.0481\n",
      "124/295, train_loss: 0.1096, step time: 1.0360\n",
      "125/295, train_loss: 0.1343, step time: 1.0341\n",
      "126/295, train_loss: 0.1514, step time: 1.0332\n",
      "127/295, train_loss: 0.0692, step time: 1.0402\n",
      "128/295, train_loss: 0.0696, step time: 1.0387\n",
      "129/295, train_loss: 0.2960, step time: 1.0344\n",
      "130/295, train_loss: 0.0832, step time: 1.0429\n",
      "131/295, train_loss: 0.1125, step time: 1.0443\n",
      "132/295, train_loss: 0.1014, step time: 1.0515\n",
      "133/295, train_loss: 0.1027, step time: 1.0358\n",
      "134/295, train_loss: 0.1253, step time: 1.0580\n",
      "135/295, train_loss: 0.1441, step time: 1.0363\n",
      "136/295, train_loss: 0.1334, step time: 1.0324\n",
      "137/295, train_loss: 0.4289, step time: 1.0328\n",
      "138/295, train_loss: 0.1179, step time: 1.0366\n",
      "139/295, train_loss: 0.1482, step time: 1.0597\n",
      "140/295, train_loss: 0.0672, step time: 1.0431\n",
      "141/295, train_loss: 0.1600, step time: 1.0321\n",
      "142/295, train_loss: 0.0493, step time: 1.0397\n",
      "143/295, train_loss: 0.0603, step time: 1.0347\n",
      "144/295, train_loss: 0.0954, step time: 1.0439\n",
      "145/295, train_loss: 0.0471, step time: 1.0330\n",
      "146/295, train_loss: 0.1043, step time: 1.0628\n",
      "147/295, train_loss: 0.0744, step time: 1.0606\n",
      "148/295, train_loss: 0.4557, step time: 1.0424\n",
      "149/295, train_loss: 0.0835, step time: 1.0327\n",
      "150/295, train_loss: 0.1035, step time: 1.0973\n",
      "151/295, train_loss: 0.2208, step time: 1.0460\n",
      "152/295, train_loss: 0.1411, step time: 1.0324\n",
      "153/295, train_loss: 0.0657, step time: 1.0511\n",
      "154/295, train_loss: 0.0727, step time: 1.0409\n",
      "155/295, train_loss: 0.1225, step time: 1.0427\n",
      "156/295, train_loss: 0.2010, step time: 1.0383\n",
      "157/295, train_loss: 0.0719, step time: 1.0513\n",
      "158/295, train_loss: 0.0857, step time: 1.0332\n",
      "159/295, train_loss: 0.1242, step time: 1.0306\n",
      "160/295, train_loss: 0.2007, step time: 1.0427\n",
      "161/295, train_loss: 0.3019, step time: 1.0373\n",
      "162/295, train_loss: 0.1697, step time: 1.0708\n",
      "163/295, train_loss: 0.1119, step time: 1.0357\n",
      "164/295, train_loss: 0.1782, step time: 1.0499\n",
      "165/295, train_loss: 0.2530, step time: 1.0606\n",
      "166/295, train_loss: 0.1061, step time: 1.0539\n",
      "167/295, train_loss: 0.1117, step time: 1.0328\n",
      "168/295, train_loss: 0.4588, step time: 1.0361\n",
      "169/295, train_loss: 0.3222, step time: 1.0322\n",
      "170/295, train_loss: 0.0594, step time: 1.0355\n",
      "171/295, train_loss: 0.3965, step time: 1.0374\n",
      "172/295, train_loss: 0.1559, step time: 1.0345\n",
      "173/295, train_loss: 0.0636, step time: 1.0351\n",
      "174/295, train_loss: 0.0785, step time: 1.0328\n",
      "175/295, train_loss: 0.0575, step time: 1.0363\n",
      "176/295, train_loss: 0.1277, step time: 1.0432\n",
      "177/295, train_loss: 0.0839, step time: 1.0327\n",
      "178/295, train_loss: 0.0627, step time: 1.0311\n",
      "179/295, train_loss: 0.2190, step time: 1.0415\n",
      "180/295, train_loss: 0.1947, step time: 1.0694\n",
      "181/295, train_loss: 0.1492, step time: 1.0519\n",
      "182/295, train_loss: 0.4113, step time: 1.0447\n",
      "183/295, train_loss: 0.3856, step time: 1.0412\n",
      "184/295, train_loss: 0.0844, step time: 1.0449\n",
      "185/295, train_loss: 0.1755, step time: 1.0328\n",
      "186/295, train_loss: 0.1328, step time: 1.0330\n",
      "187/295, train_loss: 0.5240, step time: 1.0325\n",
      "188/295, train_loss: 0.0509, step time: 1.0400\n",
      "189/295, train_loss: 0.1133, step time: 1.0430\n",
      "190/295, train_loss: 0.2008, step time: 1.0397\n",
      "191/295, train_loss: 0.0865, step time: 1.0440\n",
      "192/295, train_loss: 0.1024, step time: 1.0391\n",
      "193/295, train_loss: 0.1411, step time: 1.0373\n",
      "194/295, train_loss: 0.1108, step time: 1.0570\n",
      "195/295, train_loss: 0.5368, step time: 1.0397\n",
      "196/295, train_loss: 0.1008, step time: 1.1057\n",
      "197/295, train_loss: 0.1148, step time: 1.0325\n",
      "198/295, train_loss: 0.0442, step time: 1.0367\n",
      "199/295, train_loss: 0.2000, step time: 1.0420\n",
      "200/295, train_loss: 0.1581, step time: 1.0433\n",
      "201/295, train_loss: 0.5453, step time: 1.0312\n",
      "202/295, train_loss: 0.1814, step time: 1.0356\n",
      "203/295, train_loss: 0.1541, step time: 1.0393\n",
      "204/295, train_loss: 0.0958, step time: 1.0400\n",
      "205/295, train_loss: 0.1404, step time: 1.0649\n",
      "206/295, train_loss: 0.1362, step time: 1.0567\n",
      "207/295, train_loss: 0.1147, step time: 1.0440\n",
      "208/295, train_loss: 0.2525, step time: 1.0339\n",
      "209/295, train_loss: 0.1324, step time: 1.0675\n",
      "210/295, train_loss: 0.1390, step time: 1.0517\n",
      "211/295, train_loss: 0.1817, step time: 1.0740\n",
      "212/295, train_loss: 0.2413, step time: 1.0392\n",
      "213/295, train_loss: 0.3981, step time: 1.0366\n",
      "214/295, train_loss: 0.1023, step time: 1.0360\n",
      "215/295, train_loss: 0.3712, step time: 1.0531\n",
      "216/295, train_loss: 0.1857, step time: 1.0324\n",
      "217/295, train_loss: 0.1836, step time: 1.0545\n",
      "218/295, train_loss: 0.0613, step time: 1.0392\n",
      "219/295, train_loss: 0.1747, step time: 1.0846\n",
      "220/295, train_loss: 0.2376, step time: 1.0366\n",
      "221/295, train_loss: 0.4544, step time: 1.0360\n",
      "222/295, train_loss: 0.1761, step time: 1.0341\n",
      "223/295, train_loss: 0.1204, step time: 1.0317\n",
      "224/295, train_loss: 0.4398, step time: 1.0323\n",
      "225/295, train_loss: 0.0982, step time: 1.0359\n",
      "226/295, train_loss: 0.0665, step time: 1.0429\n",
      "227/295, train_loss: 0.1064, step time: 1.0378\n",
      "228/295, train_loss: 0.2178, step time: 1.0387\n",
      "229/295, train_loss: 0.1703, step time: 1.0327\n",
      "230/295, train_loss: 0.6503, step time: 1.0365\n",
      "231/295, train_loss: 0.2040, step time: 1.0342\n",
      "232/295, train_loss: 0.3899, step time: 1.0334\n",
      "233/295, train_loss: 0.1847, step time: 1.0337\n",
      "234/295, train_loss: 0.0910, step time: 1.0348\n",
      "235/295, train_loss: 0.5494, step time: 1.0349\n",
      "236/295, train_loss: 0.0804, step time: 1.0883\n",
      "237/295, train_loss: 0.2772, step time: 1.0325\n",
      "238/295, train_loss: 0.3665, step time: 1.0328\n",
      "239/295, train_loss: 0.1068, step time: 1.0389\n",
      "240/295, train_loss: 0.3649, step time: 1.0916\n",
      "241/295, train_loss: 0.2278, step time: 1.0450\n",
      "242/295, train_loss: 0.1623, step time: 1.0367\n",
      "243/295, train_loss: 0.1400, step time: 1.0342\n",
      "244/295, train_loss: 0.0820, step time: 1.0429\n",
      "245/295, train_loss: 0.1423, step time: 1.0560\n",
      "246/295, train_loss: 0.1516, step time: 1.0590\n",
      "247/295, train_loss: 0.2292, step time: 1.0317\n",
      "248/295, train_loss: 0.2929, step time: 1.0360\n",
      "249/295, train_loss: 0.1302, step time: 1.0700\n",
      "250/295, train_loss: 0.1592, step time: 1.0711\n",
      "251/295, train_loss: 0.0972, step time: 1.0565\n",
      "252/295, train_loss: 0.0864, step time: 1.0405\n",
      "253/295, train_loss: 0.4682, step time: 1.0325\n",
      "254/295, train_loss: 0.0520, step time: 1.0362\n",
      "255/295, train_loss: 0.3560, step time: 1.0617\n",
      "256/295, train_loss: 0.4638, step time: 1.0381\n",
      "257/295, train_loss: 0.0809, step time: 1.0457\n",
      "258/295, train_loss: 0.1677, step time: 1.0547\n",
      "259/295, train_loss: 0.2579, step time: 1.0500\n",
      "260/295, train_loss: 0.0926, step time: 1.0443\n",
      "261/295, train_loss: 0.1581, step time: 1.0297\n",
      "262/295, train_loss: 0.1760, step time: 1.0404\n",
      "263/295, train_loss: 0.4371, step time: 1.0513\n",
      "264/295, train_loss: 0.1282, step time: 1.0484\n",
      "265/295, train_loss: 0.1359, step time: 1.0483\n",
      "266/295, train_loss: 0.1046, step time: 1.0345\n",
      "267/295, train_loss: 0.4976, step time: 1.0614\n",
      "268/295, train_loss: 0.1917, step time: 1.0326\n",
      "269/295, train_loss: 0.2066, step time: 1.0332\n",
      "270/295, train_loss: 0.2346, step time: 1.0678\n",
      "271/295, train_loss: 0.1273, step time: 1.0349\n",
      "272/295, train_loss: 0.0880, step time: 1.0309\n",
      "273/295, train_loss: 0.4117, step time: 1.0358\n",
      "274/295, train_loss: 0.2164, step time: 1.0362\n",
      "275/295, train_loss: 0.0863, step time: 1.0424\n",
      "276/295, train_loss: 0.2800, step time: 1.0331\n",
      "277/295, train_loss: 0.1807, step time: 1.0583\n",
      "278/295, train_loss: 0.1336, step time: 1.0438\n",
      "279/295, train_loss: 0.1308, step time: 1.0368\n",
      "280/295, train_loss: 0.0830, step time: 1.0667\n",
      "281/295, train_loss: 0.3075, step time: 1.0560\n",
      "282/295, train_loss: 0.1726, step time: 1.0371\n",
      "283/295, train_loss: 0.1499, step time: 1.0314\n",
      "284/295, train_loss: 0.0487, step time: 1.0374\n",
      "285/295, train_loss: 0.1083, step time: 1.0318\n",
      "286/295, train_loss: 0.1553, step time: 1.0331\n",
      "287/295, train_loss: 0.0674, step time: 1.0400\n",
      "288/295, train_loss: 0.5179, step time: 1.0292\n",
      "289/295, train_loss: 0.1688, step time: 1.0319\n",
      "290/295, train_loss: 0.0782, step time: 1.0301\n",
      "291/295, train_loss: 0.2256, step time: 1.0291\n",
      "292/295, train_loss: 0.1936, step time: 1.0285\n",
      "293/295, train_loss: 0.1056, step time: 1.0295\n",
      "294/295, train_loss: 0.1329, step time: 1.0328\n",
      "295/295, train_loss: 0.3841, step time: 1.0373\n",
      "epoch 10 average loss: 0.1878\n",
      "saved new best metric model\n",
      "current epoch: 10 current mean dice: 0.7618 tc: 0.7258 wt: 0.8189 et: 0.7438\n",
      "best mean dice: 0.7618 at epoch: 10\n",
      "time consuming of epoch 10 is: 378.1622\n",
      "----------\n",
      "epoch 11/100\n",
      "1/295, train_loss: 0.0904, step time: 1.0568\n",
      "2/295, train_loss: 0.2842, step time: 1.1129\n",
      "3/295, train_loss: 0.1014, step time: 1.0883\n",
      "4/295, train_loss: 0.4650, step time: 1.0709\n",
      "5/295, train_loss: 0.1076, step time: 1.0452\n",
      "6/295, train_loss: 0.1385, step time: 1.0553\n",
      "7/295, train_loss: 0.1220, step time: 1.0476\n",
      "8/295, train_loss: 0.0702, step time: 1.0591\n",
      "9/295, train_loss: 0.1391, step time: 1.0318\n",
      "10/295, train_loss: 0.1742, step time: 1.0347\n",
      "11/295, train_loss: 0.0784, step time: 1.0359\n",
      "12/295, train_loss: 0.1025, step time: 1.0303\n",
      "13/295, train_loss: 0.0884, step time: 1.0315\n",
      "14/295, train_loss: 0.0664, step time: 1.0588\n",
      "15/295, train_loss: 0.0804, step time: 1.0310\n",
      "16/295, train_loss: 0.1655, step time: 1.0355\n",
      "17/295, train_loss: 0.0891, step time: 1.0451\n",
      "18/295, train_loss: 0.0960, step time: 1.0590\n",
      "19/295, train_loss: 0.1058, step time: 1.0408\n",
      "20/295, train_loss: 0.0566, step time: 1.0507\n",
      "21/295, train_loss: 0.2004, step time: 1.0372\n",
      "22/295, train_loss: 0.4907, step time: 1.0311\n",
      "23/295, train_loss: 0.1458, step time: 1.0562\n",
      "24/295, train_loss: 0.4031, step time: 1.0484\n",
      "25/295, train_loss: 0.0833, step time: 1.0439\n",
      "26/295, train_loss: 0.2348, step time: 1.0337\n",
      "27/295, train_loss: 0.0606, step time: 1.0327\n",
      "28/295, train_loss: 0.3952, step time: 1.0331\n",
      "29/295, train_loss: 0.1059, step time: 1.0311\n",
      "30/295, train_loss: 0.1458, step time: 1.0332\n",
      "31/295, train_loss: 0.4198, step time: 1.0392\n",
      "32/295, train_loss: 0.2073, step time: 1.0327\n",
      "33/295, train_loss: 0.1012, step time: 1.0387\n",
      "34/295, train_loss: 0.1598, step time: 1.0489\n",
      "35/295, train_loss: 0.0744, step time: 1.0298\n",
      "36/295, train_loss: 0.1198, step time: 1.0323\n",
      "37/295, train_loss: 0.2050, step time: 1.0761\n",
      "38/295, train_loss: 0.0953, step time: 1.0312\n",
      "39/295, train_loss: 0.1037, step time: 1.0406\n",
      "40/295, train_loss: 0.1475, step time: 1.0390\n",
      "41/295, train_loss: 0.2606, step time: 1.0406\n",
      "42/295, train_loss: 0.1944, step time: 1.0636\n",
      "43/295, train_loss: 0.0686, step time: 1.0607\n",
      "44/295, train_loss: 0.0916, step time: 1.0358\n",
      "45/295, train_loss: 0.1080, step time: 1.0345\n",
      "46/295, train_loss: 0.1945, step time: 1.0379\n",
      "47/295, train_loss: 0.1297, step time: 1.0577\n",
      "48/295, train_loss: 0.2045, step time: 1.0422\n",
      "49/295, train_loss: 0.1003, step time: 1.0402\n",
      "50/295, train_loss: 0.1863, step time: 1.0646\n",
      "51/295, train_loss: 0.1037, step time: 1.0361\n",
      "52/295, train_loss: 0.0594, step time: 1.0304\n",
      "53/295, train_loss: 0.1497, step time: 1.0296\n",
      "54/295, train_loss: 0.1436, step time: 1.0363\n",
      "55/295, train_loss: 0.0972, step time: 1.0387\n",
      "56/295, train_loss: 0.0702, step time: 1.0470\n",
      "57/295, train_loss: 0.0991, step time: 1.0362\n",
      "58/295, train_loss: 0.1513, step time: 1.0327\n",
      "59/295, train_loss: 0.1270, step time: 1.0324\n",
      "60/295, train_loss: 0.0744, step time: 1.0389\n",
      "61/295, train_loss: 0.1341, step time: 1.0380\n",
      "62/295, train_loss: 0.0832, step time: 1.0533\n",
      "63/295, train_loss: 0.1023, step time: 1.0335\n",
      "64/295, train_loss: 0.2154, step time: 1.0360\n",
      "65/295, train_loss: 0.1697, step time: 1.0329\n",
      "66/295, train_loss: 0.0906, step time: 1.0370\n",
      "67/295, train_loss: 0.4875, step time: 1.0323\n",
      "68/295, train_loss: 0.1289, step time: 1.0330\n",
      "69/295, train_loss: 0.2900, step time: 1.0320\n",
      "70/295, train_loss: 0.4767, step time: 1.0332\n",
      "71/295, train_loss: 0.2248, step time: 1.0370\n",
      "72/295, train_loss: 0.0685, step time: 1.0370\n",
      "73/295, train_loss: 0.0870, step time: 1.0339\n",
      "74/295, train_loss: 0.1024, step time: 1.0392\n",
      "75/295, train_loss: 0.1797, step time: 1.0381\n",
      "76/295, train_loss: 0.2737, step time: 1.0539\n",
      "77/295, train_loss: 0.1092, step time: 1.0452\n",
      "78/295, train_loss: 0.4404, step time: 1.0403\n",
      "79/295, train_loss: 0.1169, step time: 1.0677\n",
      "80/295, train_loss: 0.1980, step time: 1.0495\n",
      "81/295, train_loss: 0.1913, step time: 1.0572\n",
      "82/295, train_loss: 0.0565, step time: 1.0378\n",
      "83/295, train_loss: 0.1262, step time: 1.0533\n",
      "84/295, train_loss: 0.1130, step time: 1.0380\n",
      "85/295, train_loss: 0.1397, step time: 1.0578\n",
      "86/295, train_loss: 0.1875, step time: 1.0381\n",
      "87/295, train_loss: 0.3813, step time: 1.0361\n",
      "88/295, train_loss: 0.0842, step time: 1.0361\n",
      "89/295, train_loss: 0.1440, step time: 1.0403\n",
      "90/295, train_loss: 0.0799, step time: 1.0373\n",
      "91/295, train_loss: 0.3815, step time: 1.0482\n",
      "92/295, train_loss: 0.2410, step time: 1.0325\n",
      "93/295, train_loss: 0.4790, step time: 1.0526\n",
      "94/295, train_loss: 0.0820, step time: 1.0351\n",
      "95/295, train_loss: 0.0626, step time: 1.0632\n",
      "96/295, train_loss: 0.0829, step time: 1.0573\n",
      "97/295, train_loss: 0.0775, step time: 1.0597\n",
      "98/295, train_loss: 0.6075, step time: 1.0616\n",
      "99/295, train_loss: 0.5163, step time: 1.0421\n",
      "100/295, train_loss: 0.1063, step time: 1.0333\n",
      "101/295, train_loss: 0.3294, step time: 1.0326\n",
      "102/295, train_loss: 0.2077, step time: 1.0345\n",
      "103/295, train_loss: 0.1693, step time: 1.0606\n",
      "104/295, train_loss: 0.4299, step time: 1.0781\n",
      "105/295, train_loss: 0.1361, step time: 1.0608\n",
      "106/295, train_loss: 0.1057, step time: 1.0512\n",
      "107/295, train_loss: 0.1045, step time: 1.0354\n",
      "108/295, train_loss: 0.1309, step time: 1.0358\n",
      "109/295, train_loss: 0.6050, step time: 1.0345\n",
      "110/295, train_loss: 0.1240, step time: 1.0338\n",
      "111/295, train_loss: 0.1284, step time: 1.0418\n",
      "112/295, train_loss: 0.4137, step time: 1.0633\n",
      "113/295, train_loss: 0.2184, step time: 1.0606\n",
      "114/295, train_loss: 0.0737, step time: 1.0424\n",
      "115/295, train_loss: 0.1230, step time: 1.0354\n",
      "116/295, train_loss: 0.1750, step time: 1.0375\n",
      "117/295, train_loss: 0.0465, step time: 1.0354\n",
      "118/295, train_loss: 0.1075, step time: 1.0541\n",
      "119/295, train_loss: 0.0783, step time: 1.0768\n",
      "120/295, train_loss: 0.0922, step time: 1.0518\n",
      "121/295, train_loss: 0.0653, step time: 1.0708\n",
      "122/295, train_loss: 0.4732, step time: 1.0330\n",
      "123/295, train_loss: 0.1987, step time: 1.0361\n",
      "124/295, train_loss: 0.1299, step time: 1.0374\n",
      "125/295, train_loss: 0.3852, step time: 1.0363\n",
      "126/295, train_loss: 0.3234, step time: 1.0302\n",
      "127/295, train_loss: 0.0853, step time: 1.0331\n",
      "128/295, train_loss: 0.2265, step time: 1.0314\n",
      "129/295, train_loss: 0.2817, step time: 1.1165\n",
      "130/295, train_loss: 0.3291, step time: 1.0387\n",
      "131/295, train_loss: 0.0649, step time: 1.0394\n",
      "132/295, train_loss: 0.1825, step time: 1.0938\n",
      "133/295, train_loss: 0.3681, step time: 1.0428\n",
      "134/295, train_loss: 0.0557, step time: 1.0357\n",
      "135/295, train_loss: 0.1101, step time: 1.0318\n",
      "136/295, train_loss: 0.1952, step time: 1.0724\n",
      "137/295, train_loss: 0.0774, step time: 1.0639\n",
      "138/295, train_loss: 0.0620, step time: 1.0514\n",
      "139/295, train_loss: 0.4513, step time: 1.0748\n",
      "140/295, train_loss: 0.3768, step time: 1.0846\n",
      "141/295, train_loss: 0.0965, step time: 1.0589\n",
      "142/295, train_loss: 0.1604, step time: 1.0335\n",
      "143/295, train_loss: 0.1497, step time: 1.0439\n",
      "144/295, train_loss: 0.1760, step time: 1.0375\n",
      "145/295, train_loss: 0.1692, step time: 1.0325\n",
      "146/295, train_loss: 0.2830, step time: 1.0381\n",
      "147/295, train_loss: 0.4286, step time: 1.0387\n",
      "148/295, train_loss: 0.0545, step time: 1.0493\n",
      "149/295, train_loss: 0.0944, step time: 1.0349\n",
      "150/295, train_loss: 0.0708, step time: 1.0341\n",
      "151/295, train_loss: 0.0688, step time: 1.0330\n",
      "152/295, train_loss: 0.0688, step time: 1.0397\n",
      "153/295, train_loss: 0.0586, step time: 1.0631\n",
      "154/295, train_loss: 0.6563, step time: 1.0396\n",
      "155/295, train_loss: 0.1626, step time: 1.0627\n",
      "156/295, train_loss: 0.1352, step time: 1.0365\n",
      "157/295, train_loss: 0.0937, step time: 1.1089\n",
      "158/295, train_loss: 0.1117, step time: 1.0368\n",
      "159/295, train_loss: 0.1180, step time: 1.0330\n",
      "160/295, train_loss: 0.0825, step time: 1.0604\n",
      "161/295, train_loss: 0.1519, step time: 1.0373\n",
      "162/295, train_loss: 0.1432, step time: 1.0451\n",
      "163/295, train_loss: 0.4892, step time: 1.0442\n",
      "164/295, train_loss: 0.1265, step time: 1.0332\n",
      "165/295, train_loss: 0.0815, step time: 1.0331\n",
      "166/295, train_loss: 0.1570, step time: 1.0328\n",
      "167/295, train_loss: 0.2403, step time: 1.0418\n",
      "168/295, train_loss: 0.2667, step time: 1.0570\n",
      "169/295, train_loss: 0.1671, step time: 1.0413\n",
      "170/295, train_loss: 0.0949, step time: 1.0378\n",
      "171/295, train_loss: 0.1530, step time: 1.0357\n",
      "172/295, train_loss: 0.0680, step time: 1.0351\n",
      "173/295, train_loss: 0.0875, step time: 1.0319\n",
      "174/295, train_loss: 0.1225, step time: 1.0728\n",
      "175/295, train_loss: 0.2568, step time: 1.0623\n",
      "176/295, train_loss: 0.1182, step time: 1.0610\n",
      "177/295, train_loss: 0.2013, step time: 1.0442\n",
      "178/295, train_loss: 0.3718, step time: 1.0309\n",
      "179/295, train_loss: 0.4024, step time: 1.0350\n",
      "180/295, train_loss: 0.1885, step time: 1.0485\n",
      "181/295, train_loss: 0.2142, step time: 1.1024\n",
      "182/295, train_loss: 0.4906, step time: 1.0350\n",
      "183/295, train_loss: 0.2015, step time: 1.0624\n",
      "184/295, train_loss: 0.1996, step time: 1.1108\n",
      "185/295, train_loss: 0.1979, step time: 1.0415\n",
      "186/295, train_loss: 0.0986, step time: 1.0342\n",
      "187/295, train_loss: 0.3993, step time: 1.0534\n",
      "188/295, train_loss: 0.1966, step time: 1.0433\n",
      "189/295, train_loss: 0.1416, step time: 1.1072\n",
      "190/295, train_loss: 0.1543, step time: 1.0343\n",
      "191/295, train_loss: 0.2291, step time: 1.0390\n",
      "192/295, train_loss: 0.1510, step time: 1.1005\n",
      "193/295, train_loss: 0.0916, step time: 1.0361\n",
      "194/295, train_loss: 0.1111, step time: 1.0553\n",
      "195/295, train_loss: 0.0806, step time: 1.0348\n",
      "196/295, train_loss: 0.1055, step time: 1.0362\n",
      "197/295, train_loss: 0.1550, step time: 1.0916\n",
      "198/295, train_loss: 0.1687, step time: 1.0413\n",
      "199/295, train_loss: 0.2182, step time: 1.0633\n",
      "200/295, train_loss: 0.0779, step time: 1.0393\n",
      "201/295, train_loss: 0.0961, step time: 1.0325\n",
      "202/295, train_loss: 0.0971, step time: 1.0313\n",
      "203/295, train_loss: 0.1231, step time: 1.0496\n",
      "204/295, train_loss: 0.5054, step time: 1.0418\n",
      "205/295, train_loss: 0.0656, step time: 1.0686\n",
      "206/295, train_loss: 0.0825, step time: 1.0409\n",
      "207/295, train_loss: 0.1970, step time: 1.0657\n",
      "208/295, train_loss: 0.0977, step time: 1.0307\n",
      "209/295, train_loss: 0.1049, step time: 1.0425\n",
      "210/295, train_loss: 0.0937, step time: 1.0967\n",
      "211/295, train_loss: 0.4193, step time: 1.0323\n",
      "212/295, train_loss: 0.0617, step time: 1.1035\n",
      "213/295, train_loss: 0.0812, step time: 1.0341\n",
      "214/295, train_loss: 0.3961, step time: 1.0418\n",
      "215/295, train_loss: 0.0512, step time: 1.0425\n",
      "216/295, train_loss: 0.0810, step time: 1.0726\n",
      "217/295, train_loss: 0.0953, step time: 1.0411\n",
      "218/295, train_loss: 0.0661, step time: 1.0323\n",
      "219/295, train_loss: 0.0645, step time: 1.0324\n",
      "220/295, train_loss: 0.1359, step time: 1.0403\n",
      "221/295, train_loss: 0.1625, step time: 1.0560\n",
      "222/295, train_loss: 0.0681, step time: 1.0332\n",
      "223/295, train_loss: 0.1036, step time: 1.0350\n",
      "224/295, train_loss: 0.1491, step time: 1.0512\n",
      "225/295, train_loss: 0.1809, step time: 1.0641\n",
      "226/295, train_loss: 0.4519, step time: 1.0510\n",
      "227/295, train_loss: 0.1733, step time: 1.0355\n",
      "228/295, train_loss: 0.1218, step time: 1.0318\n",
      "229/295, train_loss: 0.1282, step time: 1.0339\n",
      "230/295, train_loss: 0.0509, step time: 1.0628\n",
      "231/295, train_loss: 0.0529, step time: 1.0592\n",
      "232/295, train_loss: 0.0877, step time: 1.0343\n",
      "233/295, train_loss: 0.0610, step time: 1.0369\n",
      "234/295, train_loss: 0.0778, step time: 1.0333\n",
      "235/295, train_loss: 0.2045, step time: 1.0330\n",
      "236/295, train_loss: 0.0508, step time: 1.0301\n",
      "237/295, train_loss: 0.0750, step time: 1.0424\n",
      "238/295, train_loss: 0.1348, step time: 1.0471\n",
      "239/295, train_loss: 0.0528, step time: 1.0403\n",
      "240/295, train_loss: 0.1310, step time: 1.0811\n",
      "241/295, train_loss: 0.3157, step time: 1.0310\n",
      "242/295, train_loss: 0.0921, step time: 1.0374\n",
      "243/295, train_loss: 0.3542, step time: 1.0411\n",
      "244/295, train_loss: 0.1330, step time: 1.0408\n",
      "245/295, train_loss: 0.0704, step time: 1.0428\n",
      "246/295, train_loss: 0.0982, step time: 1.0916\n",
      "247/295, train_loss: 0.1167, step time: 1.0466\n",
      "248/295, train_loss: 0.1662, step time: 1.0882\n",
      "249/295, train_loss: 0.4358, step time: 1.0333\n",
      "250/295, train_loss: 0.1868, step time: 1.0444\n",
      "251/295, train_loss: 0.0965, step time: 1.0530\n",
      "252/295, train_loss: 0.0940, step time: 1.0418\n",
      "253/295, train_loss: 0.2167, step time: 1.0431\n",
      "254/295, train_loss: 0.0746, step time: 1.0400\n",
      "255/295, train_loss: 0.1094, step time: 1.0432\n",
      "256/295, train_loss: 0.1874, step time: 1.1628\n",
      "257/295, train_loss: 0.1858, step time: 1.0322\n",
      "258/295, train_loss: 0.0999, step time: 1.0402\n",
      "259/295, train_loss: 0.1307, step time: 1.0425\n",
      "260/295, train_loss: 0.6175, step time: 1.0511\n",
      "261/295, train_loss: 0.0631, step time: 1.0996\n",
      "262/295, train_loss: 0.1243, step time: 1.0519\n",
      "263/295, train_loss: 0.1448, step time: 1.0495\n",
      "264/295, train_loss: 0.2117, step time: 1.0596\n",
      "265/295, train_loss: 0.0520, step time: 1.0371\n",
      "266/295, train_loss: 0.1849, step time: 1.0739\n",
      "267/295, train_loss: 0.1991, step time: 1.0354\n",
      "268/295, train_loss: 0.5692, step time: 1.0371\n",
      "269/295, train_loss: 0.1164, step time: 1.0437\n",
      "270/295, train_loss: 0.2165, step time: 1.0373\n",
      "271/295, train_loss: 0.0864, step time: 1.0454\n",
      "272/295, train_loss: 0.0600, step time: 1.0437\n",
      "273/295, train_loss: 0.3849, step time: 1.0684\n",
      "274/295, train_loss: 0.1382, step time: 1.0459\n",
      "275/295, train_loss: 0.0486, step time: 1.0354\n",
      "276/295, train_loss: 0.1345, step time: 1.0356\n",
      "277/295, train_loss: 0.1018, step time: 1.0427\n",
      "278/295, train_loss: 0.2842, step time: 1.0412\n",
      "279/295, train_loss: 0.2537, step time: 1.0351\n",
      "280/295, train_loss: 0.4372, step time: 1.0450\n",
      "281/295, train_loss: 0.0844, step time: 1.0442\n",
      "282/295, train_loss: 0.1129, step time: 1.0603\n",
      "283/295, train_loss: 0.2301, step time: 1.0304\n",
      "284/295, train_loss: 0.0786, step time: 1.0320\n",
      "285/295, train_loss: 0.1904, step time: 1.0798\n",
      "286/295, train_loss: 0.3104, step time: 1.0391\n",
      "287/295, train_loss: 0.5046, step time: 1.0316\n",
      "288/295, train_loss: 0.3077, step time: 1.0297\n",
      "289/295, train_loss: 0.1399, step time: 1.0289\n",
      "290/295, train_loss: 0.1478, step time: 1.0282\n",
      "291/295, train_loss: 0.1986, step time: 1.0291\n",
      "292/295, train_loss: 0.2484, step time: 1.0291\n",
      "293/295, train_loss: 0.3731, step time: 1.0295\n",
      "294/295, train_loss: 0.0558, step time: 1.0290\n",
      "295/295, train_loss: 0.2898, step time: 1.0290\n",
      "epoch 11 average loss: 0.1783\n",
      "saved new best metric model\n",
      "current epoch: 11 current mean dice: 0.7798 tc: 0.7341 wt: 0.8443 et: 0.7717\n",
      "best mean dice: 0.7798 at epoch: 11\n",
      "time consuming of epoch 11 is: 380.7357\n",
      "----------\n",
      "epoch 12/100\n",
      "1/295, train_loss: 0.5553, step time: 1.1250\n",
      "2/295, train_loss: 0.1883, step time: 1.2264\n",
      "3/295, train_loss: 0.0941, step time: 1.0446\n",
      "4/295, train_loss: 0.0967, step time: 1.1087\n",
      "5/295, train_loss: 0.2286, step time: 1.0693\n",
      "6/295, train_loss: 0.2250, step time: 1.0410\n",
      "7/295, train_loss: 0.1577, step time: 1.0523\n",
      "8/295, train_loss: 0.1109, step time: 1.0917\n",
      "9/295, train_loss: 0.1294, step time: 1.0361\n",
      "10/295, train_loss: 0.1053, step time: 1.0637\n",
      "11/295, train_loss: 0.0872, step time: 1.0792\n",
      "12/295, train_loss: 0.1062, step time: 1.0326\n",
      "13/295, train_loss: 0.1296, step time: 1.0846\n",
      "14/295, train_loss: 0.3057, step time: 1.0521\n",
      "15/295, train_loss: 0.0680, step time: 1.0371\n",
      "16/295, train_loss: 0.2450, step time: 1.0348\n",
      "17/295, train_loss: 0.1284, step time: 1.0641\n",
      "18/295, train_loss: 0.1556, step time: 1.0403\n",
      "19/295, train_loss: 0.0790, step time: 1.0406\n",
      "20/295, train_loss: 0.1053, step time: 1.0382\n",
      "21/295, train_loss: 0.1302, step time: 1.0324\n",
      "22/295, train_loss: 0.1768, step time: 1.0400\n",
      "23/295, train_loss: 0.3693, step time: 1.0421\n",
      "24/295, train_loss: 0.0705, step time: 1.0784\n",
      "25/295, train_loss: 0.4522, step time: 1.0571\n",
      "26/295, train_loss: 0.1566, step time: 1.0768\n",
      "27/295, train_loss: 0.1729, step time: 1.0693\n",
      "28/295, train_loss: 0.1500, step time: 1.0564\n",
      "29/295, train_loss: 0.0815, step time: 1.0297\n",
      "30/295, train_loss: 0.0966, step time: 1.0330\n",
      "31/295, train_loss: 0.0899, step time: 1.0954\n",
      "32/295, train_loss: 0.0662, step time: 1.0392\n",
      "33/295, train_loss: 0.0846, step time: 1.0341\n",
      "34/295, train_loss: 0.0625, step time: 1.0334\n",
      "35/295, train_loss: 0.3397, step time: 1.0844\n",
      "36/295, train_loss: 0.1434, step time: 1.0356\n",
      "37/295, train_loss: 0.0873, step time: 1.0318\n",
      "38/295, train_loss: 0.0917, step time: 1.0578\n",
      "39/295, train_loss: 0.0808, step time: 1.0800\n",
      "40/295, train_loss: 0.0794, step time: 1.0444\n",
      "41/295, train_loss: 0.2064, step time: 1.0449\n",
      "42/295, train_loss: 0.0703, step time: 1.0912\n",
      "43/295, train_loss: 0.2040, step time: 1.0329\n",
      "44/295, train_loss: 0.1466, step time: 1.0494\n",
      "45/295, train_loss: 0.3922, step time: 1.0652\n",
      "46/295, train_loss: 0.1533, step time: 1.0485\n",
      "47/295, train_loss: 0.2328, step time: 1.0696\n",
      "48/295, train_loss: 0.1783, step time: 1.0368\n",
      "49/295, train_loss: 0.3981, step time: 1.0507\n",
      "50/295, train_loss: 0.1527, step time: 1.0513\n",
      "51/295, train_loss: 0.3700, step time: 1.0339\n",
      "52/295, train_loss: 0.0728, step time: 1.0489\n",
      "53/295, train_loss: 0.4202, step time: 1.0474\n",
      "54/295, train_loss: 0.0959, step time: 1.0596\n",
      "55/295, train_loss: 0.4753, step time: 1.0478\n",
      "56/295, train_loss: 0.2435, step time: 1.0310\n",
      "57/295, train_loss: 0.1313, step time: 1.0374\n",
      "58/295, train_loss: 0.1480, step time: 1.0351\n",
      "59/295, train_loss: 0.1570, step time: 1.0521\n",
      "60/295, train_loss: 0.0939, step time: 1.0811\n",
      "61/295, train_loss: 0.0774, step time: 1.0420\n",
      "62/295, train_loss: 0.1336, step time: 1.0417\n",
      "63/295, train_loss: 0.0659, step time: 1.0476\n",
      "64/295, train_loss: 0.4362, step time: 1.0895\n",
      "65/295, train_loss: 0.0750, step time: 1.0316\n",
      "66/295, train_loss: 0.4149, step time: 1.0654\n",
      "67/295, train_loss: 0.0979, step time: 1.0392\n",
      "68/295, train_loss: 0.0964, step time: 1.0372\n",
      "69/295, train_loss: 0.0896, step time: 1.0370\n",
      "70/295, train_loss: 0.0482, step time: 1.0335\n",
      "71/295, train_loss: 0.1132, step time: 1.0691\n",
      "72/295, train_loss: 0.1046, step time: 1.0331\n",
      "73/295, train_loss: 0.0920, step time: 1.0430\n",
      "74/295, train_loss: 0.1185, step time: 1.0578\n",
      "75/295, train_loss: 0.1699, step time: 1.0359\n",
      "76/295, train_loss: 0.1813, step time: 1.0699\n",
      "77/295, train_loss: 0.1891, step time: 1.0591\n",
      "78/295, train_loss: 0.0912, step time: 1.0395\n",
      "79/295, train_loss: 0.1858, step time: 1.0381\n",
      "80/295, train_loss: 0.1785, step time: 1.0524\n",
      "81/295, train_loss: 0.1050, step time: 1.0770\n",
      "82/295, train_loss: 0.0513, step time: 1.0598\n",
      "83/295, train_loss: 0.1167, step time: 1.0360\n",
      "84/295, train_loss: 0.2070, step time: 1.0370\n",
      "85/295, train_loss: 0.0842, step time: 1.0373\n",
      "86/295, train_loss: 0.3657, step time: 1.1071\n",
      "87/295, train_loss: 0.1235, step time: 1.0649\n",
      "88/295, train_loss: 0.3344, step time: 1.0385\n",
      "89/295, train_loss: 0.4047, step time: 1.0352\n",
      "90/295, train_loss: 0.3908, step time: 1.0436\n",
      "91/295, train_loss: 0.4167, step time: 1.0827\n",
      "92/295, train_loss: 0.1096, step time: 1.0429\n",
      "93/295, train_loss: 0.3419, step time: 1.0351\n",
      "94/295, train_loss: 0.0704, step time: 1.0306\n",
      "95/295, train_loss: 0.0672, step time: 1.0333\n",
      "96/295, train_loss: 0.0586, step time: 1.0522\n",
      "97/295, train_loss: 0.2841, step time: 1.0536\n",
      "98/295, train_loss: 0.2054, step time: 1.0445\n",
      "99/295, train_loss: 0.3730, step time: 1.0553\n",
      "100/295, train_loss: 0.0776, step time: 1.0328\n",
      "101/295, train_loss: 0.1457, step time: 1.0429\n",
      "102/295, train_loss: 0.3842, step time: 1.0459\n",
      "103/295, train_loss: 0.1036, step time: 1.0519\n",
      "104/295, train_loss: 0.4296, step time: 1.0580\n",
      "105/295, train_loss: 0.2599, step time: 1.0475\n",
      "106/295, train_loss: 0.2560, step time: 1.0485\n",
      "107/295, train_loss: 0.1030, step time: 1.0432\n",
      "108/295, train_loss: 0.3481, step time: 1.0559\n",
      "109/295, train_loss: 0.5191, step time: 1.0704\n",
      "110/295, train_loss: 0.4166, step time: 1.0373\n",
      "111/295, train_loss: 0.1146, step time: 1.0427\n",
      "112/295, train_loss: 0.1243, step time: 1.0332\n",
      "113/295, train_loss: 0.1797, step time: 1.0485\n",
      "114/295, train_loss: 0.1986, step time: 1.0469\n",
      "115/295, train_loss: 0.6009, step time: 1.0721\n",
      "116/295, train_loss: 0.0783, step time: 1.0376\n",
      "117/295, train_loss: 0.3800, step time: 1.0435\n",
      "118/295, train_loss: 0.1008, step time: 1.0740\n",
      "119/295, train_loss: 0.0852, step time: 1.0394\n",
      "120/295, train_loss: 0.0602, step time: 1.0441\n",
      "121/295, train_loss: 0.1729, step time: 1.0519\n",
      "122/295, train_loss: 0.1860, step time: 1.0465\n",
      "123/295, train_loss: 0.2665, step time: 1.0320\n",
      "124/295, train_loss: 0.0664, step time: 1.0312\n",
      "125/295, train_loss: 0.1123, step time: 1.0542\n",
      "126/295, train_loss: 0.0538, step time: 1.0344\n",
      "127/295, train_loss: 0.3989, step time: 1.0446\n",
      "128/295, train_loss: 0.1186, step time: 1.0680\n",
      "129/295, train_loss: 0.2074, step time: 1.0320\n",
      "130/295, train_loss: 0.0649, step time: 1.0863\n",
      "131/295, train_loss: 0.1231, step time: 1.0488\n",
      "132/295, train_loss: 0.1875, step time: 1.0529\n",
      "133/295, train_loss: 0.0930, step time: 1.0559\n",
      "134/295, train_loss: 0.0691, step time: 1.0618\n",
      "135/295, train_loss: 0.1444, step time: 1.0602\n",
      "136/295, train_loss: 0.0679, step time: 1.0410\n",
      "137/295, train_loss: 0.0726, step time: 1.0732\n",
      "138/295, train_loss: 0.2005, step time: 1.0476\n",
      "139/295, train_loss: 0.1891, step time: 1.0315\n",
      "140/295, train_loss: 0.1960, step time: 1.0606\n",
      "141/295, train_loss: 0.4524, step time: 1.0324\n",
      "142/295, train_loss: 0.0510, step time: 1.0361\n",
      "143/295, train_loss: 0.0623, step time: 1.0395\n",
      "144/295, train_loss: 0.1296, step time: 1.0496\n",
      "145/295, train_loss: 0.1474, step time: 1.0653\n",
      "146/295, train_loss: 0.5067, step time: 1.0403\n",
      "147/295, train_loss: 0.1639, step time: 1.0351\n",
      "148/295, train_loss: 0.0616, step time: 1.0532\n",
      "149/295, train_loss: 0.2950, step time: 1.0978\n",
      "150/295, train_loss: 0.4889, step time: 1.0313\n",
      "151/295, train_loss: 0.0571, step time: 1.0401\n",
      "152/295, train_loss: 0.4732, step time: 1.0865\n",
      "153/295, train_loss: 0.1862, step time: 1.0353\n",
      "154/295, train_loss: 0.1342, step time: 1.0326\n",
      "155/295, train_loss: 0.1495, step time: 1.0370\n",
      "156/295, train_loss: 0.1177, step time: 1.0505\n",
      "157/295, train_loss: 0.0527, step time: 1.0478\n",
      "158/295, train_loss: 0.1553, step time: 1.0722\n",
      "159/295, train_loss: 0.0852, step time: 1.0793\n",
      "160/295, train_loss: 0.1093, step time: 1.0610\n",
      "161/295, train_loss: 0.1739, step time: 1.0375\n",
      "162/295, train_loss: 0.0855, step time: 1.0447\n",
      "163/295, train_loss: 0.0799, step time: 1.0564\n",
      "164/295, train_loss: 0.2880, step time: 1.0611\n",
      "165/295, train_loss: 0.1657, step time: 1.0364\n",
      "166/295, train_loss: 0.3229, step time: 1.0346\n",
      "167/295, train_loss: 0.1857, step time: 1.0379\n",
      "168/295, train_loss: 0.1200, step time: 1.0370\n",
      "169/295, train_loss: 0.0442, step time: 1.0572\n",
      "170/295, train_loss: 0.0940, step time: 1.0378\n",
      "171/295, train_loss: 0.0849, step time: 1.0330\n",
      "172/295, train_loss: 0.1561, step time: 1.0402\n",
      "173/295, train_loss: 0.2017, step time: 1.0480\n",
      "174/295, train_loss: 0.0967, step time: 1.0361\n",
      "175/295, train_loss: 0.2740, step time: 1.0610\n",
      "176/295, train_loss: 0.5116, step time: 1.0415\n",
      "177/295, train_loss: 0.0635, step time: 1.0326\n",
      "178/295, train_loss: 0.2526, step time: 1.0363\n",
      "179/295, train_loss: 0.0718, step time: 1.0394\n",
      "180/295, train_loss: 0.0627, step time: 1.0391\n",
      "181/295, train_loss: 0.0531, step time: 1.0794\n",
      "182/295, train_loss: 0.0654, step time: 1.0338\n",
      "183/295, train_loss: 0.2926, step time: 1.0429\n",
      "184/295, train_loss: 0.5110, step time: 1.0581\n",
      "185/295, train_loss: 0.1910, step time: 1.0570\n",
      "186/295, train_loss: 0.2241, step time: 1.0338\n",
      "187/295, train_loss: 0.1202, step time: 1.0353\n",
      "188/295, train_loss: 0.0423, step time: 1.0439\n",
      "189/295, train_loss: 0.1763, step time: 1.0509\n",
      "190/295, train_loss: 0.2423, step time: 1.0606\n",
      "191/295, train_loss: 0.0678, step time: 1.0424\n",
      "192/295, train_loss: 0.0905, step time: 1.0432\n",
      "193/295, train_loss: 0.2594, step time: 1.0578\n",
      "194/295, train_loss: 0.2023, step time: 1.0385\n",
      "195/295, train_loss: 0.0862, step time: 1.0517\n",
      "196/295, train_loss: 0.0764, step time: 1.0465\n",
      "197/295, train_loss: 0.0649, step time: 1.0431\n",
      "198/295, train_loss: 0.0840, step time: 1.0396\n",
      "199/295, train_loss: 0.0691, step time: 1.0387\n",
      "200/295, train_loss: 0.0982, step time: 1.0442\n",
      "201/295, train_loss: 0.1211, step time: 1.0686\n",
      "202/295, train_loss: 0.1443, step time: 1.0473\n",
      "203/295, train_loss: 0.0669, step time: 1.0359\n",
      "204/295, train_loss: 0.2588, step time: 1.0360\n",
      "205/295, train_loss: 0.1109, step time: 1.0346\n",
      "206/295, train_loss: 0.0956, step time: 1.0555\n",
      "207/295, train_loss: 0.1524, step time: 1.0622\n",
      "208/295, train_loss: 0.6668, step time: 1.0811\n",
      "209/295, train_loss: 0.1036, step time: 1.0318\n",
      "210/295, train_loss: 0.1169, step time: 1.0641\n",
      "211/295, train_loss: 0.1332, step time: 1.0575\n",
      "212/295, train_loss: 0.1609, step time: 1.0411\n",
      "213/295, train_loss: 0.2298, step time: 1.0325\n",
      "214/295, train_loss: 0.0814, step time: 1.0356\n",
      "215/295, train_loss: 0.2257, step time: 1.0387\n",
      "216/295, train_loss: 0.1554, step time: 1.0414\n",
      "217/295, train_loss: 0.1292, step time: 1.0370\n",
      "218/295, train_loss: 0.1527, step time: 1.0386\n",
      "219/295, train_loss: 0.1410, step time: 1.0627\n",
      "220/295, train_loss: 0.0975, step time: 1.0683\n",
      "221/295, train_loss: 0.0882, step time: 1.0410\n",
      "222/295, train_loss: 0.1012, step time: 1.0370\n",
      "223/295, train_loss: 0.0560, step time: 1.0639\n",
      "224/295, train_loss: 0.1201, step time: 1.0342\n",
      "225/295, train_loss: 0.1483, step time: 1.0335\n",
      "226/295, train_loss: 0.4657, step time: 1.0528\n",
      "227/295, train_loss: 0.0752, step time: 1.0341\n",
      "228/295, train_loss: 0.1172, step time: 1.0345\n",
      "229/295, train_loss: 0.1187, step time: 1.0601\n",
      "230/295, train_loss: 0.0741, step time: 1.0315\n",
      "231/295, train_loss: 0.0519, step time: 1.0358\n",
      "232/295, train_loss: 0.1059, step time: 1.0341\n",
      "233/295, train_loss: 0.3892, step time: 1.1075\n",
      "234/295, train_loss: 0.0993, step time: 1.0348\n",
      "235/295, train_loss: 0.1528, step time: 1.0343\n",
      "236/295, train_loss: 0.0472, step time: 1.0786\n",
      "237/295, train_loss: 0.0437, step time: 1.0358\n",
      "238/295, train_loss: 0.4328, step time: 1.0573\n",
      "239/295, train_loss: 0.1080, step time: 1.0416\n",
      "240/295, train_loss: 0.1959, step time: 1.0364\n",
      "241/295, train_loss: 0.1205, step time: 1.0668\n",
      "242/295, train_loss: 0.1167, step time: 1.0342\n",
      "243/295, train_loss: 0.0816, step time: 1.0328\n",
      "244/295, train_loss: 0.0591, step time: 1.0447\n",
      "245/295, train_loss: 0.0984, step time: 1.0344\n",
      "246/295, train_loss: 0.1709, step time: 1.0350\n",
      "247/295, train_loss: 0.0926, step time: 1.0453\n",
      "248/295, train_loss: 0.4289, step time: 1.0567\n",
      "249/295, train_loss: 0.0854, step time: 1.0364\n",
      "250/295, train_loss: 0.1004, step time: 1.0395\n",
      "251/295, train_loss: 0.1547, step time: 1.0358\n",
      "252/295, train_loss: 0.2196, step time: 1.0363\n",
      "253/295, train_loss: 0.2245, step time: 1.0399\n",
      "254/295, train_loss: 0.1163, step time: 1.0573\n",
      "255/295, train_loss: 0.6491, step time: 1.0367\n",
      "256/295, train_loss: 0.2719, step time: 1.0018\n",
      "257/295, train_loss: 0.0611, step time: 1.0350\n",
      "258/295, train_loss: 0.1559, step time: 1.0358\n",
      "259/295, train_loss: 0.1523, step time: 1.0395\n",
      "260/295, train_loss: 0.1133, step time: 1.0350\n",
      "261/295, train_loss: 0.4656, step time: 1.0333\n",
      "262/295, train_loss: 0.2280, step time: 1.0407\n",
      "263/295, train_loss: 0.0681, step time: 1.0639\n",
      "264/295, train_loss: 0.1677, step time: 1.0366\n",
      "265/295, train_loss: 0.0656, step time: 1.0407\n",
      "266/295, train_loss: 0.0532, step time: 1.0375\n",
      "267/295, train_loss: 0.4151, step time: 1.0586\n",
      "268/295, train_loss: 0.1782, step time: 1.0349\n",
      "269/295, train_loss: 0.2883, step time: 1.0336\n",
      "270/295, train_loss: 0.0535, step time: 1.0331\n",
      "271/295, train_loss: 0.4599, step time: 1.0395\n",
      "272/295, train_loss: 0.1372, step time: 1.0392\n",
      "273/295, train_loss: 0.0906, step time: 1.0384\n",
      "274/295, train_loss: 0.3836, step time: 1.0385\n",
      "275/295, train_loss: 0.4030, step time: 1.0524\n",
      "276/295, train_loss: 0.0618, step time: 1.0309\n",
      "277/295, train_loss: 0.0966, step time: 1.0368\n",
      "278/295, train_loss: 0.0942, step time: 1.0335\n",
      "279/295, train_loss: 0.1071, step time: 1.0421\n",
      "280/295, train_loss: 0.0978, step time: 1.0395\n",
      "281/295, train_loss: 0.0876, step time: 1.0386\n",
      "282/295, train_loss: 0.1448, step time: 1.0388\n",
      "283/295, train_loss: 0.1664, step time: 1.0607\n",
      "284/295, train_loss: 0.0539, step time: 1.0561\n",
      "285/295, train_loss: 0.1736, step time: 1.0317\n",
      "286/295, train_loss: 0.1485, step time: 1.0527\n",
      "287/295, train_loss: 0.1165, step time: 1.0711\n",
      "288/295, train_loss: 0.0747, step time: 1.0293\n",
      "289/295, train_loss: 0.1225, step time: 1.0286\n",
      "290/295, train_loss: 0.0670, step time: 1.0292\n",
      "291/295, train_loss: 0.0896, step time: 1.0291\n",
      "292/295, train_loss: 0.1381, step time: 1.0279\n",
      "293/295, train_loss: 0.1109, step time: 1.0297\n",
      "294/295, train_loss: 0.4189, step time: 1.0287\n",
      "295/295, train_loss: 0.1281, step time: 1.0287\n",
      "epoch 12 average loss: 0.1751\n",
      "current epoch: 12 current mean dice: 0.7733 tc: 0.7306 wt: 0.8397 et: 0.7546\n",
      "best mean dice: 0.7798 at epoch: 11\n",
      "time consuming of epoch 12 is: 381.4653\n",
      "----------\n",
      "epoch 13/100\n",
      "1/295, train_loss: 0.0545, step time: 1.0816\n",
      "2/295, train_loss: 0.0547, step time: 1.2071\n",
      "3/295, train_loss: 0.2028, step time: 1.0495\n",
      "4/295, train_loss: 0.2081, step time: 1.0563\n",
      "5/295, train_loss: 0.0579, step time: 1.0386\n",
      "6/295, train_loss: 0.0795, step time: 1.0355\n",
      "7/295, train_loss: 0.0741, step time: 1.0610\n",
      "8/295, train_loss: 0.0924, step time: 1.0615\n",
      "9/295, train_loss: 0.0514, step time: 1.0353\n",
      "10/295, train_loss: 0.5064, step time: 1.0416\n",
      "11/295, train_loss: 0.1381, step time: 1.0704\n",
      "12/295, train_loss: 0.1776, step time: 1.0466\n",
      "13/295, train_loss: 0.0956, step time: 1.0713\n",
      "14/295, train_loss: 0.0908, step time: 1.0674\n",
      "15/295, train_loss: 0.4111, step time: 1.0438\n",
      "16/295, train_loss: 0.0797, step time: 1.0351\n",
      "17/295, train_loss: 0.2375, step time: 1.0484\n",
      "18/295, train_loss: 0.1788, step time: 1.0337\n",
      "19/295, train_loss: 0.0570, step time: 1.0412\n",
      "20/295, train_loss: 0.2569, step time: 1.0445\n",
      "21/295, train_loss: 0.1050, step time: 1.0866\n",
      "22/295, train_loss: 0.0922, step time: 1.0443\n",
      "23/295, train_loss: 0.3140, step time: 1.0345\n",
      "24/295, train_loss: 0.1471, step time: 1.0391\n",
      "25/295, train_loss: 0.0726, step time: 1.0358\n",
      "26/295, train_loss: 0.4777, step time: 1.0318\n",
      "27/295, train_loss: 0.0795, step time: 1.0404\n",
      "28/295, train_loss: 0.3893, step time: 1.0401\n",
      "29/295, train_loss: 0.1900, step time: 1.0638\n",
      "30/295, train_loss: 0.2261, step time: 1.0397\n",
      "31/295, train_loss: 0.2949, step time: 1.0314\n",
      "32/295, train_loss: 0.1533, step time: 1.0624\n",
      "33/295, train_loss: 0.1970, step time: 1.0361\n",
      "34/295, train_loss: 0.1570, step time: 1.0967\n",
      "35/295, train_loss: 0.0567, step time: 1.0331\n",
      "36/295, train_loss: 0.0796, step time: 1.0502\n",
      "37/295, train_loss: 0.1317, step time: 1.0329\n",
      "38/295, train_loss: 0.0714, step time: 1.0658\n",
      "39/295, train_loss: 0.1683, step time: 1.0320\n",
      "40/295, train_loss: 0.3167, step time: 1.0450\n",
      "41/295, train_loss: 0.0834, step time: 1.0375\n",
      "42/295, train_loss: 0.0533, step time: 1.0497\n",
      "43/295, train_loss: 0.2260, step time: 1.0473\n",
      "44/295, train_loss: 0.4550, step time: 1.0619\n",
      "45/295, train_loss: 0.1098, step time: 1.0322\n",
      "46/295, train_loss: 0.1701, step time: 1.0409\n",
      "47/295, train_loss: 0.1058, step time: 1.0765\n",
      "48/295, train_loss: 0.0807, step time: 1.0439\n",
      "49/295, train_loss: 0.0468, step time: 1.0606\n",
      "50/295, train_loss: 0.1717, step time: 1.0317\n",
      "51/295, train_loss: 0.2059, step time: 1.0483\n",
      "52/295, train_loss: 0.1117, step time: 1.0653\n",
      "53/295, train_loss: 0.0662, step time: 1.0369\n",
      "54/295, train_loss: 0.1515, step time: 1.0537\n",
      "55/295, train_loss: 0.2561, step time: 1.0593\n",
      "56/295, train_loss: 0.1444, step time: 1.0523\n",
      "57/295, train_loss: 0.1383, step time: 1.0389\n",
      "58/295, train_loss: 0.0935, step time: 1.0348\n",
      "59/295, train_loss: 0.0727, step time: 1.0397\n",
      "60/295, train_loss: 0.0742, step time: 1.0671\n",
      "61/295, train_loss: 0.0897, step time: 1.0415\n",
      "62/295, train_loss: 0.1476, step time: 1.0394\n",
      "63/295, train_loss: 0.1159, step time: 1.0365\n",
      "64/295, train_loss: 0.1549, step time: 1.0335\n",
      "65/295, train_loss: 0.1859, step time: 1.0445\n",
      "66/295, train_loss: 0.1265, step time: 1.0336\n",
      "67/295, train_loss: 0.1535, step time: 1.0459\n",
      "68/295, train_loss: 0.4048, step time: 1.0447\n",
      "69/295, train_loss: 0.1102, step time: 1.0354\n",
      "70/295, train_loss: 0.4428, step time: 1.0493\n",
      "71/295, train_loss: 0.2001, step time: 1.0612\n",
      "72/295, train_loss: 0.1351, step time: 1.0368\n",
      "73/295, train_loss: 0.5549, step time: 1.0387\n",
      "74/295, train_loss: 0.1184, step time: 1.0361\n",
      "75/295, train_loss: 0.0711, step time: 1.0557\n",
      "76/295, train_loss: 0.1028, step time: 1.0379\n",
      "77/295, train_loss: 0.0671, step time: 1.0398\n",
      "78/295, train_loss: 0.1840, step time: 1.0877\n",
      "79/295, train_loss: 0.1447, step time: 1.0492\n",
      "80/295, train_loss: 0.1057, step time: 1.0384\n",
      "81/295, train_loss: 0.1280, step time: 1.0665\n",
      "82/295, train_loss: 0.1442, step time: 1.0477\n",
      "83/295, train_loss: 0.5005, step time: 1.0742\n",
      "84/295, train_loss: 0.1042, step time: 1.0600\n",
      "85/295, train_loss: 0.4158, step time: 1.0384\n",
      "86/295, train_loss: 0.0792, step time: 1.0398\n",
      "87/295, train_loss: 0.1410, step time: 1.0346\n",
      "88/295, train_loss: 0.1103, step time: 1.0663\n",
      "89/295, train_loss: 0.0845, step time: 1.0408\n",
      "90/295, train_loss: 0.0723, step time: 1.0476\n",
      "91/295, train_loss: 0.0949, step time: 1.0410\n",
      "92/295, train_loss: 0.0774, step time: 1.0439\n",
      "93/295, train_loss: 0.2307, step time: 1.0390\n",
      "94/295, train_loss: 0.1177, step time: 1.0398\n",
      "95/295, train_loss: 0.0747, step time: 1.0323\n",
      "96/295, train_loss: 0.1734, step time: 1.0328\n",
      "97/295, train_loss: 0.0695, step time: 1.0556\n",
      "98/295, train_loss: 0.0992, step time: 1.0368\n",
      "99/295, train_loss: 0.0592, step time: 1.0377\n",
      "100/295, train_loss: 0.4559, step time: 1.0364\n",
      "101/295, train_loss: 0.1284, step time: 1.0388\n",
      "102/295, train_loss: 0.1347, step time: 1.0994\n",
      "103/295, train_loss: 0.4311, step time: 1.0938\n",
      "104/295, train_loss: 0.1858, step time: 1.0435\n",
      "105/295, train_loss: 0.1197, step time: 1.0605\n",
      "106/295, train_loss: 0.1255, step time: 1.0746\n",
      "107/295, train_loss: 0.0599, step time: 1.0407\n",
      "108/295, train_loss: 0.0663, step time: 1.0450\n",
      "109/295, train_loss: 0.1875, step time: 1.0445\n",
      "110/295, train_loss: 0.1598, step time: 1.0458\n",
      "111/295, train_loss: 0.0772, step time: 1.0541\n",
      "112/295, train_loss: 0.0831, step time: 1.0315\n",
      "113/295, train_loss: 0.1236, step time: 1.0505\n",
      "114/295, train_loss: 0.0787, step time: 1.0437\n",
      "115/295, train_loss: 0.0808, step time: 1.0466\n",
      "116/295, train_loss: 0.1041, step time: 1.1080\n",
      "117/295, train_loss: 0.1253, step time: 1.0613\n",
      "118/295, train_loss: 0.1803, step time: 1.0343\n",
      "119/295, train_loss: 0.0913, step time: 1.0372\n",
      "120/295, train_loss: 0.0922, step time: 1.0360\n",
      "121/295, train_loss: 0.0871, step time: 1.0331\n",
      "122/295, train_loss: 0.3889, step time: 1.0323\n",
      "123/295, train_loss: 0.1091, step time: 1.0365\n",
      "124/295, train_loss: 0.1725, step time: 1.0341\n",
      "125/295, train_loss: 0.4456, step time: 1.0615\n",
      "126/295, train_loss: 0.0522, step time: 1.0518\n",
      "127/295, train_loss: 0.1567, step time: 1.0583\n",
      "128/295, train_loss: 0.1190, step time: 1.0358\n",
      "129/295, train_loss: 0.1845, step time: 1.0387\n",
      "130/295, train_loss: 0.4234, step time: 1.0614\n",
      "131/295, train_loss: 0.1385, step time: 1.0407\n",
      "132/295, train_loss: 0.5084, step time: 1.0317\n",
      "133/295, train_loss: 0.3173, step time: 1.0550\n",
      "134/295, train_loss: 0.2007, step time: 1.0405\n",
      "135/295, train_loss: 0.1633, step time: 1.0435\n",
      "136/295, train_loss: 0.0737, step time: 1.0352\n",
      "137/295, train_loss: 0.0940, step time: 1.0356\n",
      "138/295, train_loss: 0.1375, step time: 1.0345\n",
      "139/295, train_loss: 0.0991, step time: 1.0380\n",
      "140/295, train_loss: 0.6386, step time: 1.0693\n",
      "141/295, train_loss: 0.0811, step time: 1.0380\n",
      "142/295, train_loss: 0.0627, step time: 1.0487\n",
      "143/295, train_loss: 0.1346, step time: 1.0958\n",
      "144/295, train_loss: 0.2411, step time: 1.0370\n",
      "145/295, train_loss: 0.0728, step time: 1.0352\n",
      "146/295, train_loss: 0.0537, step time: 1.0523\n",
      "147/295, train_loss: 0.5040, step time: 1.0641\n",
      "148/295, train_loss: 0.1372, step time: 1.0334\n",
      "149/295, train_loss: 0.3529, step time: 1.0371\n",
      "150/295, train_loss: 0.1001, step time: 1.0343\n",
      "151/295, train_loss: 0.0423, step time: 1.0479\n",
      "152/295, train_loss: 0.0859, step time: 1.0393\n",
      "153/295, train_loss: 0.4253, step time: 1.0661\n",
      "154/295, train_loss: 0.1307, step time: 1.0411\n",
      "155/295, train_loss: 0.2135, step time: 1.0547\n",
      "156/295, train_loss: 0.0654, step time: 1.0396\n",
      "157/295, train_loss: 0.1733, step time: 1.0402\n",
      "158/295, train_loss: 0.1003, step time: 1.0412\n",
      "159/295, train_loss: 0.0898, step time: 1.0381\n",
      "160/295, train_loss: 0.2394, step time: 1.0569\n",
      "161/295, train_loss: 0.0954, step time: 1.0365\n",
      "162/295, train_loss: 0.1094, step time: 1.0436\n",
      "163/295, train_loss: 0.1591, step time: 1.0565\n",
      "164/295, train_loss: 0.1317, step time: 1.1026\n",
      "165/295, train_loss: 0.1504, step time: 1.0608\n",
      "166/295, train_loss: 0.4196, step time: 1.0401\n",
      "167/295, train_loss: 0.0958, step time: 1.0597\n",
      "168/295, train_loss: 0.0812, step time: 1.0412\n",
      "169/295, train_loss: 0.1426, step time: 1.0353\n",
      "170/295, train_loss: 0.0818, step time: 1.0375\n",
      "171/295, train_loss: 0.2465, step time: 1.0326\n",
      "172/295, train_loss: 0.3434, step time: 1.0352\n",
      "173/295, train_loss: 0.1654, step time: 1.0328\n",
      "174/295, train_loss: 0.1238, step time: 1.0384\n",
      "175/295, train_loss: 0.1569, step time: 1.0510\n",
      "176/295, train_loss: 0.0655, step time: 1.0424\n",
      "177/295, train_loss: 0.4262, step time: 1.0545\n",
      "178/295, train_loss: 0.1060, step time: 1.0374\n",
      "179/295, train_loss: 0.3939, step time: 1.0374\n",
      "180/295, train_loss: 0.1988, step time: 1.0382\n",
      "181/295, train_loss: 0.1225, step time: 1.0409\n",
      "182/295, train_loss: 0.1047, step time: 1.0386\n",
      "183/295, train_loss: 0.1485, step time: 1.0511\n",
      "184/295, train_loss: 0.1052, step time: 1.0413\n",
      "185/295, train_loss: 0.4458, step time: 1.0430\n",
      "186/295, train_loss: 0.3915, step time: 1.0422\n",
      "187/295, train_loss: 0.5527, step time: 1.0350\n",
      "188/295, train_loss: 0.0752, step time: 1.0328\n",
      "189/295, train_loss: 0.1161, step time: 1.0356\n",
      "190/295, train_loss: 0.1337, step time: 1.0362\n",
      "191/295, train_loss: 0.0757, step time: 1.0621\n",
      "192/295, train_loss: 0.1442, step time: 1.0320\n",
      "193/295, train_loss: 0.1842, step time: 1.0547\n",
      "194/295, train_loss: 0.6263, step time: 1.0337\n",
      "195/295, train_loss: 0.1588, step time: 1.0356\n",
      "196/295, train_loss: 0.0878, step time: 1.0427\n",
      "197/295, train_loss: 0.0845, step time: 1.0350\n",
      "198/295, train_loss: 0.1702, step time: 1.0799\n",
      "199/295, train_loss: 0.1604, step time: 1.0437\n",
      "200/295, train_loss: 0.2885, step time: 1.0556\n",
      "201/295, train_loss: 0.0462, step time: 1.0741\n",
      "202/295, train_loss: 0.3107, step time: 1.1050\n",
      "203/295, train_loss: 0.1644, step time: 1.0356\n",
      "204/295, train_loss: 0.1526, step time: 1.0689\n",
      "205/295, train_loss: 0.0742, step time: 1.0357\n",
      "206/295, train_loss: 0.0737, step time: 1.0340\n",
      "207/295, train_loss: 0.0602, step time: 1.0595\n",
      "208/295, train_loss: 0.1117, step time: 1.0385\n",
      "209/295, train_loss: 0.1748, step time: 1.0944\n",
      "210/295, train_loss: 0.0755, step time: 1.0389\n",
      "211/295, train_loss: 0.2881, step time: 1.0425\n",
      "212/295, train_loss: 0.2312, step time: 1.0485\n",
      "213/295, train_loss: 0.0907, step time: 1.0342\n",
      "214/295, train_loss: 0.0510, step time: 1.0339\n",
      "215/295, train_loss: 0.2158, step time: 1.0543\n",
      "216/295, train_loss: 0.0762, step time: 1.0363\n",
      "217/295, train_loss: 0.0591, step time: 1.0355\n",
      "218/295, train_loss: 0.1246, step time: 1.0363\n",
      "219/295, train_loss: 0.3789, step time: 1.0482\n",
      "220/295, train_loss: 0.1508, step time: 1.0447\n",
      "221/295, train_loss: 0.2620, step time: 1.0403\n",
      "222/295, train_loss: 0.0859, step time: 1.0376\n",
      "223/295, train_loss: 0.1197, step time: 1.0320\n",
      "224/295, train_loss: 0.1080, step time: 1.0543\n",
      "225/295, train_loss: 0.0805, step time: 1.0318\n",
      "226/295, train_loss: 0.1026, step time: 1.0328\n",
      "227/295, train_loss: 0.1109, step time: 1.0392\n",
      "228/295, train_loss: 0.1217, step time: 1.0381\n",
      "229/295, train_loss: 0.0714, step time: 1.0355\n",
      "230/295, train_loss: 0.0754, step time: 1.0630\n",
      "231/295, train_loss: 0.0805, step time: 1.0404\n",
      "232/295, train_loss: 0.0555, step time: 1.0329\n",
      "233/295, train_loss: 0.3957, step time: 1.0348\n",
      "234/295, train_loss: 0.0781, step time: 1.0346\n",
      "235/295, train_loss: 0.2651, step time: 1.0841\n",
      "236/295, train_loss: 0.3132, step time: 1.0492\n",
      "237/295, train_loss: 0.0474, step time: 1.0345\n",
      "238/295, train_loss: 0.1841, step time: 1.0558\n",
      "239/295, train_loss: 0.1357, step time: 1.0343\n",
      "240/295, train_loss: 0.0797, step time: 1.0400\n",
      "241/295, train_loss: 0.0617, step time: 1.0370\n",
      "242/295, train_loss: 0.0718, step time: 1.0354\n",
      "243/295, train_loss: 0.1355, step time: 1.0361\n",
      "244/295, train_loss: 0.1999, step time: 1.0330\n",
      "245/295, train_loss: 0.1467, step time: 1.0385\n",
      "246/295, train_loss: 0.1648, step time: 1.0369\n",
      "247/295, train_loss: 0.1369, step time: 1.0373\n",
      "248/295, train_loss: 0.0665, step time: 1.0564\n",
      "249/295, train_loss: 0.2894, step time: 1.0363\n",
      "250/295, train_loss: 0.1315, step time: 1.0352\n",
      "251/295, train_loss: 0.4824, step time: 1.0473\n",
      "252/295, train_loss: 0.1296, step time: 1.0553\n",
      "253/295, train_loss: 0.3544, step time: 1.0333\n",
      "254/295, train_loss: 0.1130, step time: 1.0303\n",
      "255/295, train_loss: 0.0486, step time: 1.0350\n",
      "256/295, train_loss: 0.3937, step time: 1.0350\n",
      "257/295, train_loss: 0.2568, step time: 1.0331\n",
      "258/295, train_loss: 0.0533, step time: 1.0615\n",
      "259/295, train_loss: 0.0866, step time: 1.0379\n",
      "260/295, train_loss: 0.2742, step time: 1.0348\n",
      "261/295, train_loss: 0.2432, step time: 1.0379\n",
      "262/295, train_loss: 0.1199, step time: 1.0381\n",
      "263/295, train_loss: 0.0697, step time: 1.0448\n",
      "264/295, train_loss: 0.1801, step time: 1.0353\n",
      "265/295, train_loss: 0.0667, step time: 1.0432\n",
      "266/295, train_loss: 0.0509, step time: 1.0361\n",
      "267/295, train_loss: 0.1081, step time: 1.0411\n",
      "268/295, train_loss: 0.2437, step time: 1.0440\n",
      "269/295, train_loss: 0.4019, step time: 1.0489\n",
      "270/295, train_loss: 0.0421, step time: 1.0386\n",
      "271/295, train_loss: 0.0504, step time: 1.0397\n",
      "272/295, train_loss: 0.1097, step time: 1.0444\n",
      "273/295, train_loss: 0.5148, step time: 1.0431\n",
      "274/295, train_loss: 0.0514, step time: 1.0613\n",
      "275/295, train_loss: 0.1061, step time: 1.0426\n",
      "276/295, train_loss: 0.1883, step time: 1.0492\n",
      "277/295, train_loss: 0.1158, step time: 1.0656\n",
      "278/295, train_loss: 0.0965, step time: 1.0597\n",
      "279/295, train_loss: 0.0490, step time: 1.0407\n",
      "280/295, train_loss: 0.6039, step time: 1.0373\n",
      "281/295, train_loss: 0.1649, step time: 1.0335\n",
      "282/295, train_loss: 0.1307, step time: 1.0352\n",
      "283/295, train_loss: 0.0832, step time: 1.0368\n",
      "284/295, train_loss: 0.1215, step time: 1.0380\n",
      "285/295, train_loss: 0.2603, step time: 1.0476\n",
      "286/295, train_loss: 0.1404, step time: 1.0366\n",
      "287/295, train_loss: 0.0850, step time: 1.0317\n",
      "288/295, train_loss: 0.0618, step time: 1.0300\n",
      "289/295, train_loss: 0.1763, step time: 1.0289\n",
      "290/295, train_loss: 0.1521, step time: 1.0284\n",
      "291/295, train_loss: 0.0852, step time: 1.0295\n",
      "292/295, train_loss: 0.2988, step time: 1.0291\n",
      "293/295, train_loss: 0.1336, step time: 1.0295\n",
      "294/295, train_loss: 0.5208, step time: 1.0293\n",
      "295/295, train_loss: 0.1335, step time: 1.0282\n",
      "epoch 13 average loss: 0.1697\n",
      "current epoch: 13 current mean dice: 0.7627 tc: 0.7051 wt: 0.8512 et: 0.7393\n",
      "best mean dice: 0.7798 at epoch: 11\n",
      "time consuming of epoch 13 is: 379.5162\n",
      "----------\n",
      "epoch 14/100\n",
      "1/295, train_loss: 0.3878, step time: 1.1249\n",
      "2/295, train_loss: 0.1048, step time: 1.0896\n",
      "3/295, train_loss: 0.1025, step time: 1.0665\n",
      "4/295, train_loss: 0.1745, step time: 1.0737\n",
      "5/295, train_loss: 0.1943, step time: 1.0966\n",
      "6/295, train_loss: 0.2436, step time: 1.0451\n",
      "7/295, train_loss: 0.1032, step time: 1.0356\n",
      "8/295, train_loss: 0.2394, step time: 1.0625\n",
      "9/295, train_loss: 0.1111, step time: 1.0384\n",
      "10/295, train_loss: 0.1187, step time: 1.0512\n",
      "11/295, train_loss: 0.1465, step time: 1.0308\n",
      "12/295, train_loss: 0.1090, step time: 1.0320\n",
      "13/295, train_loss: 0.1241, step time: 1.0492\n",
      "14/295, train_loss: 0.1333, step time: 1.0336\n",
      "15/295, train_loss: 0.1132, step time: 1.0391\n",
      "16/295, train_loss: 0.1830, step time: 1.0659\n",
      "17/295, train_loss: 0.1513, step time: 1.1105\n",
      "18/295, train_loss: 0.3680, step time: 1.0565\n",
      "19/295, train_loss: 0.0619, step time: 1.0439\n",
      "20/295, train_loss: 0.0984, step time: 1.0381\n",
      "21/295, train_loss: 0.0623, step time: 1.0371\n",
      "22/295, train_loss: 0.0506, step time: 1.0620\n",
      "23/295, train_loss: 0.1043, step time: 1.0603\n",
      "24/295, train_loss: 0.0949, step time: 1.0361\n",
      "25/295, train_loss: 0.1336, step time: 1.0313\n",
      "26/295, train_loss: 0.1321, step time: 1.0400\n",
      "27/295, train_loss: 0.1178, step time: 1.0582\n",
      "28/295, train_loss: 0.1039, step time: 1.0782\n",
      "29/295, train_loss: 0.0955, step time: 1.0367\n",
      "30/295, train_loss: 0.0631, step time: 1.0375\n",
      "31/295, train_loss: 0.0861, step time: 1.0567\n",
      "32/295, train_loss: 0.0645, step time: 1.0385\n",
      "33/295, train_loss: 0.1157, step time: 1.0473\n",
      "34/295, train_loss: 0.4510, step time: 1.0343\n",
      "35/295, train_loss: 0.0872, step time: 1.0331\n",
      "36/295, train_loss: 0.1448, step time: 1.0310\n",
      "37/295, train_loss: 0.0686, step time: 1.0346\n",
      "38/295, train_loss: 0.1774, step time: 1.0437\n",
      "39/295, train_loss: 0.0981, step time: 1.0446\n",
      "40/295, train_loss: 0.0592, step time: 1.0418\n",
      "41/295, train_loss: 0.1325, step time: 1.0720\n",
      "42/295, train_loss: 0.2517, step time: 1.0421\n",
      "43/295, train_loss: 0.0538, step time: 1.0780\n",
      "44/295, train_loss: 0.1150, step time: 1.0703\n",
      "45/295, train_loss: 0.1170, step time: 1.0750\n",
      "46/295, train_loss: 0.0550, step time: 1.0376\n",
      "47/295, train_loss: 0.2533, step time: 1.0336\n",
      "48/295, train_loss: 0.1084, step time: 1.0381\n",
      "49/295, train_loss: 0.2136, step time: 1.0470\n",
      "50/295, train_loss: 0.0926, step time: 1.0642\n",
      "51/295, train_loss: 0.1829, step time: 1.0387\n",
      "52/295, train_loss: 0.4516, step time: 1.0692\n",
      "53/295, train_loss: 0.0970, step time: 1.0443\n",
      "54/295, train_loss: 0.1301, step time: 1.0385\n",
      "55/295, train_loss: 0.1506, step time: 1.0612\n",
      "56/295, train_loss: 0.0825, step time: 1.0590\n",
      "57/295, train_loss: 0.4875, step time: 1.0521\n",
      "58/295, train_loss: 0.0867, step time: 1.0598\n",
      "59/295, train_loss: 0.1109, step time: 1.0373\n",
      "60/295, train_loss: 0.0410, step time: 1.0388\n",
      "61/295, train_loss: 0.3837, step time: 1.0343\n",
      "62/295, train_loss: 0.1508, step time: 1.0814\n",
      "63/295, train_loss: 0.1269, step time: 1.0361\n",
      "64/295, train_loss: 0.1689, step time: 1.0397\n",
      "65/295, train_loss: 0.0644, step time: 1.0482\n",
      "66/295, train_loss: 0.1862, step time: 1.0336\n",
      "67/295, train_loss: 0.1365, step time: 1.0729\n",
      "68/295, train_loss: 0.0656, step time: 1.0398\n",
      "69/295, train_loss: 0.1263, step time: 1.0734\n",
      "70/295, train_loss: 0.0621, step time: 1.0336\n",
      "71/295, train_loss: 0.0692, step time: 1.0360\n",
      "72/295, train_loss: 0.4718, step time: 1.0374\n",
      "73/295, train_loss: 0.4467, step time: 1.0431\n",
      "74/295, train_loss: 0.1203, step time: 1.0746\n",
      "75/295, train_loss: 0.2285, step time: 1.0401\n",
      "76/295, train_loss: 0.0749, step time: 1.0381\n",
      "77/295, train_loss: 0.1054, step time: 1.0598\n",
      "78/295, train_loss: 0.1272, step time: 1.0446\n",
      "79/295, train_loss: 0.0514, step time: 1.0378\n",
      "80/295, train_loss: 0.0720, step time: 1.0398\n",
      "81/295, train_loss: 0.0779, step time: 1.0397\n",
      "82/295, train_loss: 0.1631, step time: 1.0853\n",
      "83/295, train_loss: 0.4861, step time: 1.0347\n",
      "84/295, train_loss: 0.1460, step time: 1.0346\n",
      "85/295, train_loss: 0.0494, step time: 1.1164\n",
      "86/295, train_loss: 0.3548, step time: 1.1065\n",
      "87/295, train_loss: 0.1530, step time: 1.0545\n",
      "88/295, train_loss: 0.0715, step time: 1.0375\n",
      "89/295, train_loss: 0.2344, step time: 1.0315\n",
      "90/295, train_loss: 0.1158, step time: 1.0440\n",
      "91/295, train_loss: 0.6661, step time: 1.0332\n",
      "92/295, train_loss: 0.1488, step time: 1.0384\n",
      "93/295, train_loss: 0.2002, step time: 1.0437\n",
      "94/295, train_loss: 0.0889, step time: 1.0625\n",
      "95/295, train_loss: 0.1149, step time: 1.0320\n",
      "96/295, train_loss: 0.1009, step time: 1.0320\n",
      "97/295, train_loss: 0.1208, step time: 1.0365\n",
      "98/295, train_loss: 0.1430, step time: 1.0341\n",
      "99/295, train_loss: 0.1110, step time: 1.0620\n",
      "100/295, train_loss: 0.5475, step time: 1.0561\n",
      "101/295, train_loss: 0.0995, step time: 1.0533\n",
      "102/295, train_loss: 0.1349, step time: 1.0324\n",
      "103/295, train_loss: 0.0695, step time: 1.0508\n",
      "104/295, train_loss: 0.0881, step time: 1.0614\n",
      "105/295, train_loss: 0.2553, step time: 1.0370\n",
      "106/295, train_loss: 0.2001, step time: 1.0331\n",
      "107/295, train_loss: 0.0959, step time: 1.0410\n",
      "108/295, train_loss: 0.0994, step time: 1.0391\n",
      "109/295, train_loss: 0.2979, step time: 1.0515\n",
      "110/295, train_loss: 0.6570, step time: 1.0402\n",
      "111/295, train_loss: 0.4531, step time: 1.0343\n",
      "112/295, train_loss: 0.1119, step time: 1.0740\n",
      "113/295, train_loss: 0.1031, step time: 1.0359\n",
      "114/295, train_loss: 0.1101, step time: 1.0615\n",
      "115/295, train_loss: 0.2123, step time: 1.0377\n",
      "116/295, train_loss: 0.0949, step time: 1.0396\n",
      "117/295, train_loss: 0.0992, step time: 1.0426\n",
      "118/295, train_loss: 0.1057, step time: 1.0357\n",
      "119/295, train_loss: 0.4039, step time: 1.0331\n",
      "120/295, train_loss: 0.0769, step time: 1.0369\n",
      "121/295, train_loss: 0.0770, step time: 1.0393\n",
      "122/295, train_loss: 0.0500, step time: 1.0390\n",
      "123/295, train_loss: 0.4493, step time: 1.0358\n",
      "124/295, train_loss: 0.0939, step time: 1.0323\n",
      "125/295, train_loss: 0.0898, step time: 1.0377\n",
      "126/295, train_loss: 0.1130, step time: 1.0539\n",
      "127/295, train_loss: 0.0598, step time: 1.0432\n",
      "128/295, train_loss: 0.2016, step time: 1.0361\n",
      "129/295, train_loss: 0.1220, step time: 1.0382\n",
      "130/295, train_loss: 0.1039, step time: 1.0322\n",
      "131/295, train_loss: 0.1243, step time: 1.0426\n",
      "132/295, train_loss: 0.1224, step time: 1.0410\n",
      "133/295, train_loss: 0.1069, step time: 1.0407\n",
      "134/295, train_loss: 0.0838, step time: 1.0365\n",
      "135/295, train_loss: 0.0756, step time: 1.0602\n",
      "136/295, train_loss: 0.1268, step time: 1.0805\n",
      "137/295, train_loss: 0.3162, step time: 1.0396\n",
      "138/295, train_loss: 0.0996, step time: 1.0565\n",
      "139/295, train_loss: 0.0655, step time: 1.0360\n",
      "140/295, train_loss: 0.2463, step time: 1.0358\n",
      "141/295, train_loss: 0.4380, step time: 1.0397\n",
      "142/295, train_loss: 0.1691, step time: 1.0385\n",
      "143/295, train_loss: 0.1323, step time: 1.0326\n",
      "144/295, train_loss: 0.0971, step time: 1.0366\n",
      "145/295, train_loss: 0.0552, step time: 1.0641\n",
      "146/295, train_loss: 0.0915, step time: 1.0384\n",
      "147/295, train_loss: 0.1292, step time: 1.0326\n",
      "148/295, train_loss: 0.0829, step time: 1.0382\n",
      "149/295, train_loss: 0.1793, step time: 1.0333\n",
      "150/295, train_loss: 0.0345, step time: 1.0331\n",
      "151/295, train_loss: 0.0568, step time: 1.0587\n",
      "152/295, train_loss: 0.4561, step time: 1.0356\n",
      "153/295, train_loss: 0.0698, step time: 1.0705\n",
      "154/295, train_loss: 0.0613, step time: 1.0372\n",
      "155/295, train_loss: 0.1045, step time: 1.0789\n",
      "156/295, train_loss: 0.0713, step time: 1.0399\n",
      "157/295, train_loss: 0.1203, step time: 1.0362\n",
      "158/295, train_loss: 0.0843, step time: 1.0468\n",
      "159/295, train_loss: 0.1038, step time: 1.0352\n",
      "160/295, train_loss: 0.2036, step time: 1.0484\n",
      "161/295, train_loss: 0.1118, step time: 1.0361\n",
      "162/295, train_loss: 0.0646, step time: 1.0382\n",
      "163/295, train_loss: 0.1525, step time: 1.0376\n",
      "164/295, train_loss: 0.3457, step time: 1.0354\n",
      "165/295, train_loss: 0.1316, step time: 1.0422\n",
      "166/295, train_loss: 0.3634, step time: 1.0490\n",
      "167/295, train_loss: 0.1706, step time: 1.0359\n",
      "168/295, train_loss: 0.3393, step time: 1.0363\n",
      "169/295, train_loss: 0.2111, step time: 1.0356\n",
      "170/295, train_loss: 0.4109, step time: 1.0347\n",
      "171/295, train_loss: 0.2240, step time: 1.0373\n",
      "172/295, train_loss: 0.1088, step time: 1.1166\n",
      "173/295, train_loss: 0.1915, step time: 1.0523\n",
      "174/295, train_loss: 0.3863, step time: 1.0597\n",
      "175/295, train_loss: 0.2311, step time: 1.0381\n",
      "176/295, train_loss: 0.0482, step time: 1.0400\n",
      "177/295, train_loss: 0.0781, step time: 1.0339\n",
      "178/295, train_loss: 0.1584, step time: 1.0381\n",
      "179/295, train_loss: 0.2378, step time: 1.0360\n",
      "180/295, train_loss: 0.0913, step time: 1.0540\n",
      "181/295, train_loss: 0.0545, step time: 1.0334\n",
      "182/295, train_loss: 0.0724, step time: 1.0392\n",
      "183/295, train_loss: 0.1152, step time: 1.0542\n",
      "184/295, train_loss: 0.1691, step time: 1.0917\n",
      "185/295, train_loss: 0.1428, step time: 1.0373\n",
      "186/295, train_loss: 0.1041, step time: 1.1222\n",
      "187/295, train_loss: 0.4873, step time: 1.0579\n",
      "188/295, train_loss: 0.2148, step time: 1.0411\n",
      "189/295, train_loss: 0.1357, step time: 1.0382\n",
      "190/295, train_loss: 0.0686, step time: 1.0446\n",
      "191/295, train_loss: 0.1772, step time: 1.0322\n",
      "192/295, train_loss: 0.1542, step time: 1.0643\n",
      "193/295, train_loss: 0.4744, step time: 1.0412\n",
      "194/295, train_loss: 0.1266, step time: 1.0412\n",
      "195/295, train_loss: 0.0498, step time: 1.0354\n",
      "196/295, train_loss: 0.2073, step time: 1.0670\n",
      "197/295, train_loss: 0.0493, step time: 1.0423\n",
      "198/295, train_loss: 0.1159, step time: 1.0692\n",
      "199/295, train_loss: 0.1241, step time: 1.0342\n",
      "200/295, train_loss: 0.0551, step time: 1.0369\n",
      "201/295, train_loss: 0.5392, step time: 1.0761\n",
      "202/295, train_loss: 0.0703, step time: 1.0873\n",
      "203/295, train_loss: 0.0771, step time: 1.0499\n",
      "204/295, train_loss: 0.0648, step time: 1.0681\n",
      "205/295, train_loss: 0.1334, step time: 1.0372\n",
      "206/295, train_loss: 0.0704, step time: 1.0401\n",
      "207/295, train_loss: 0.1069, step time: 1.0369\n",
      "208/295, train_loss: 0.0578, step time: 1.0373\n",
      "209/295, train_loss: 0.1504, step time: 1.0594\n",
      "210/295, train_loss: 0.0982, step time: 1.0602\n",
      "211/295, train_loss: 0.1117, step time: 1.0343\n",
      "212/295, train_loss: 0.1616, step time: 1.0522\n",
      "213/295, train_loss: 0.3024, step time: 1.0474\n",
      "214/295, train_loss: 0.1002, step time: 1.0639\n",
      "215/295, train_loss: 0.0581, step time: 1.0721\n",
      "216/295, train_loss: 0.0620, step time: 1.0415\n",
      "217/295, train_loss: 0.4041, step time: 1.0557\n",
      "218/295, train_loss: 0.0707, step time: 1.0400\n",
      "219/295, train_loss: 0.4312, step time: 1.0503\n",
      "220/295, train_loss: 0.4281, step time: 1.0576\n",
      "221/295, train_loss: 0.1731, step time: 1.0615\n",
      "222/295, train_loss: 0.2287, step time: 1.0532\n",
      "223/295, train_loss: 0.2017, step time: 1.0410\n",
      "224/295, train_loss: 0.0463, step time: 1.0390\n",
      "225/295, train_loss: 0.0731, step time: 1.0397\n",
      "226/295, train_loss: 0.2302, step time: 1.0438\n",
      "227/295, train_loss: 0.4276, step time: 1.0334\n",
      "228/295, train_loss: 0.1295, step time: 1.0352\n",
      "229/295, train_loss: 0.1331, step time: 1.0372\n",
      "230/295, train_loss: 0.0592, step time: 1.0725\n",
      "231/295, train_loss: 0.5374, step time: 1.0347\n",
      "232/295, train_loss: 0.1263, step time: 1.0381\n",
      "233/295, train_loss: 0.1633, step time: 1.0430\n",
      "234/295, train_loss: 0.3822, step time: 1.0352\n",
      "235/295, train_loss: 0.1719, step time: 1.0392\n",
      "236/295, train_loss: 0.1204, step time: 1.0454\n",
      "237/295, train_loss: 0.0744, step time: 1.0700\n",
      "238/295, train_loss: 0.2793, step time: 1.0639\n",
      "239/295, train_loss: 0.0541, step time: 1.0386\n",
      "240/295, train_loss: 0.1652, step time: 1.0411\n",
      "241/295, train_loss: 0.0871, step time: 1.0460\n",
      "242/295, train_loss: 0.1812, step time: 1.0458\n",
      "243/295, train_loss: 0.1138, step time: 1.0413\n",
      "244/295, train_loss: 0.1406, step time: 1.0510\n",
      "245/295, train_loss: 0.1228, step time: 1.0489\n",
      "246/295, train_loss: 0.2502, step time: 1.0408\n",
      "247/295, train_loss: 0.1576, step time: 1.0495\n",
      "248/295, train_loss: 0.0884, step time: 1.0625\n",
      "249/295, train_loss: 0.0721, step time: 1.0368\n",
      "250/295, train_loss: 0.1143, step time: 1.0456\n",
      "251/295, train_loss: 0.1314, step time: 1.1188\n",
      "252/295, train_loss: 0.1047, step time: 1.0532\n",
      "253/295, train_loss: 0.0689, step time: 1.0347\n",
      "254/295, train_loss: 0.0954, step time: 1.0348\n",
      "255/295, train_loss: 0.0780, step time: 1.0492\n",
      "256/295, train_loss: 0.0632, step time: 1.0356\n",
      "257/295, train_loss: 0.6085, step time: 1.0408\n",
      "258/295, train_loss: 0.1286, step time: 1.0417\n",
      "259/295, train_loss: 0.1987, step time: 1.0507\n",
      "260/295, train_loss: 0.1321, step time: 1.0387\n",
      "261/295, train_loss: 0.0533, step time: 1.0471\n",
      "262/295, train_loss: 0.0858, step time: 1.0421\n",
      "263/295, train_loss: 0.1379, step time: 1.0403\n",
      "264/295, train_loss: 0.4352, step time: 1.0404\n",
      "265/295, train_loss: 0.2449, step time: 1.0344\n",
      "266/295, train_loss: 0.2350, step time: 1.0394\n",
      "267/295, train_loss: 0.3991, step time: 1.0515\n",
      "268/295, train_loss: 0.1662, step time: 1.0705\n",
      "269/295, train_loss: 0.4300, step time: 1.0512\n",
      "270/295, train_loss: 0.1039, step time: 1.0388\n",
      "271/295, train_loss: 0.0741, step time: 1.0384\n",
      "272/295, train_loss: 0.0458, step time: 1.0344\n",
      "273/295, train_loss: 0.0787, step time: 1.0447\n",
      "274/295, train_loss: 0.0681, step time: 1.0452\n",
      "275/295, train_loss: 0.1121, step time: 1.0326\n",
      "276/295, train_loss: 0.1317, step time: 1.0422\n",
      "277/295, train_loss: 0.0941, step time: 1.0657\n",
      "278/295, train_loss: 0.1855, step time: 1.0425\n",
      "279/295, train_loss: 0.1940, step time: 1.0520\n",
      "280/295, train_loss: 0.0972, step time: 1.0373\n",
      "281/295, train_loss: 0.1384, step time: 1.0545\n",
      "282/295, train_loss: 0.3969, step time: 1.0348\n",
      "283/295, train_loss: 0.3999, step time: 1.0366\n",
      "284/295, train_loss: 0.0977, step time: 1.0370\n",
      "285/295, train_loss: 0.1733, step time: 1.0395\n",
      "286/295, train_loss: 0.1104, step time: 1.0354\n",
      "287/295, train_loss: 0.1267, step time: 1.0353\n",
      "288/295, train_loss: 0.0523, step time: 1.0610\n",
      "289/295, train_loss: 0.2755, step time: 1.0309\n",
      "290/295, train_loss: 0.2048, step time: 1.0292\n",
      "291/295, train_loss: 0.6040, step time: 1.0300\n",
      "292/295, train_loss: 0.1736, step time: 1.0297\n",
      "293/295, train_loss: 0.2882, step time: 1.0292\n",
      "294/295, train_loss: 0.2089, step time: 1.0292\n",
      "295/295, train_loss: 0.4366, step time: 1.0284\n",
      "epoch 14 average loss: 0.1688\n",
      "saved new best metric model\n",
      "current epoch: 14 current mean dice: 0.7804 tc: 0.7404 wt: 0.8476 et: 0.7636\n",
      "best mean dice: 0.7804 at epoch: 14\n",
      "time consuming of epoch 14 is: 379.3891\n",
      "----------\n",
      "epoch 15/100\n",
      "1/295, train_loss: 0.0971, step time: 1.0762\n",
      "2/295, train_loss: 0.0667, step time: 1.1591\n",
      "3/295, train_loss: 0.1548, step time: 1.0403\n",
      "4/295, train_loss: 0.4377, step time: 1.0458\n",
      "5/295, train_loss: 0.1289, step time: 1.0465\n",
      "6/295, train_loss: 0.0528, step time: 1.0614\n",
      "7/295, train_loss: 0.1863, step time: 1.0353\n",
      "8/295, train_loss: 0.1001, step time: 1.0398\n",
      "9/295, train_loss: 0.0823, step time: 1.0635\n",
      "10/295, train_loss: 0.0886, step time: 1.0396\n",
      "11/295, train_loss: 0.0765, step time: 1.0496\n",
      "12/295, train_loss: 0.0841, step time: 1.0449\n",
      "13/295, train_loss: 0.1955, step time: 1.0360\n",
      "14/295, train_loss: 0.4192, step time: 1.0390\n",
      "15/295, train_loss: 0.1290, step time: 1.0410\n",
      "16/295, train_loss: 0.0434, step time: 1.0397\n",
      "17/295, train_loss: 0.1221, step time: 1.0618\n",
      "18/295, train_loss: 0.0638, step time: 1.0665\n",
      "19/295, train_loss: 0.0676, step time: 1.0449\n",
      "20/295, train_loss: 0.1562, step time: 1.0364\n",
      "21/295, train_loss: 0.2617, step time: 1.0494\n",
      "22/295, train_loss: 0.3493, step time: 1.0584\n",
      "23/295, train_loss: 0.0537, step time: 1.0653\n",
      "24/295, train_loss: 0.4063, step time: 1.0560\n",
      "25/295, train_loss: 0.4186, step time: 1.0437\n",
      "26/295, train_loss: 0.0822, step time: 1.0460\n",
      "27/295, train_loss: 0.0771, step time: 1.0341\n",
      "28/295, train_loss: 0.1916, step time: 1.0889\n",
      "29/295, train_loss: 0.4868, step time: 1.0338\n",
      "30/295, train_loss: 0.2318, step time: 1.0939\n",
      "31/295, train_loss: 0.0952, step time: 1.0314\n",
      "32/295, train_loss: 0.0634, step time: 1.0531\n",
      "33/295, train_loss: 0.2239, step time: 1.0337\n",
      "34/295, train_loss: 0.1184, step time: 1.0485\n",
      "35/295, train_loss: 0.1382, step time: 1.0378\n",
      "36/295, train_loss: 0.1694, step time: 1.0477\n",
      "37/295, train_loss: 0.2746, step time: 1.0800\n",
      "38/295, train_loss: 0.0421, step time: 1.0622\n",
      "39/295, train_loss: 0.2843, step time: 1.0398\n",
      "40/295, train_loss: 0.1084, step time: 1.0462\n",
      "41/295, train_loss: 0.0591, step time: 1.0339\n",
      "42/295, train_loss: 0.1958, step time: 1.0544\n",
      "43/295, train_loss: 0.3974, step time: 1.0355\n",
      "44/295, train_loss: 0.0712, step time: 1.0433\n",
      "45/295, train_loss: 0.1357, step time: 1.0329\n",
      "46/295, train_loss: 0.1600, step time: 1.0353\n",
      "47/295, train_loss: 0.1151, step time: 1.0471\n",
      "48/295, train_loss: 0.0832, step time: 1.0293\n",
      "49/295, train_loss: 0.1376, step time: 1.0296\n",
      "50/295, train_loss: 0.1006, step time: 1.0312\n",
      "51/295, train_loss: 0.1075, step time: 1.0348\n",
      "52/295, train_loss: 0.0467, step time: 1.1051\n",
      "53/295, train_loss: 0.0512, step time: 1.0627\n",
      "54/295, train_loss: 0.1023, step time: 1.0440\n",
      "55/295, train_loss: 0.4071, step time: 1.0930\n",
      "56/295, train_loss: 0.0903, step time: 1.0407\n",
      "57/295, train_loss: 0.2425, step time: 1.0352\n",
      "58/295, train_loss: 0.1142, step time: 1.0639\n",
      "59/295, train_loss: 0.1273, step time: 1.0454\n",
      "60/295, train_loss: 0.2448, step time: 1.0376\n",
      "61/295, train_loss: 0.0863, step time: 1.0455\n",
      "62/295, train_loss: 0.0865, step time: 1.0427\n",
      "63/295, train_loss: 0.0563, step time: 1.0329\n",
      "64/295, train_loss: 0.1592, step time: 1.0441\n",
      "65/295, train_loss: 0.0542, step time: 1.0659\n",
      "66/295, train_loss: 0.1243, step time: 1.0547\n",
      "67/295, train_loss: 0.1198, step time: 1.0504\n",
      "68/295, train_loss: 0.0988, step time: 1.0396\n",
      "69/295, train_loss: 0.0826, step time: 1.0371\n",
      "70/295, train_loss: 0.0559, step time: 1.0342\n",
      "71/295, train_loss: 0.0970, step time: 1.0432\n",
      "72/295, train_loss: 0.0630, step time: 1.0340\n",
      "73/295, train_loss: 0.1772, step time: 1.0755\n",
      "74/295, train_loss: 0.3697, step time: 1.0654\n",
      "75/295, train_loss: 0.0787, step time: 1.0465\n",
      "76/295, train_loss: 0.1663, step time: 1.0483\n",
      "77/295, train_loss: 0.0692, step time: 1.0313\n",
      "78/295, train_loss: 0.1170, step time: 1.0416\n",
      "79/295, train_loss: 0.0433, step time: 1.0526\n",
      "80/295, train_loss: 0.1216, step time: 1.1427\n",
      "81/295, train_loss: 0.1088, step time: 1.0449\n",
      "82/295, train_loss: 0.2644, step time: 1.0698\n",
      "83/295, train_loss: 0.1348, step time: 1.0324\n",
      "84/295, train_loss: 0.0985, step time: 1.0378\n",
      "85/295, train_loss: 0.0897, step time: 1.0604\n",
      "86/295, train_loss: 0.1399, step time: 1.0398\n",
      "87/295, train_loss: 0.0833, step time: 1.0356\n",
      "88/295, train_loss: 0.1669, step time: 1.0568\n",
      "89/295, train_loss: 0.1553, step time: 1.0537\n",
      "90/295, train_loss: 0.0740, step time: 1.1329\n",
      "91/295, train_loss: 0.1313, step time: 1.0543\n",
      "92/295, train_loss: 0.4588, step time: 1.0377\n",
      "93/295, train_loss: 0.0667, step time: 1.0376\n",
      "94/295, train_loss: 0.0828, step time: 1.0465\n",
      "95/295, train_loss: 0.0501, step time: 1.0373\n",
      "96/295, train_loss: 0.1013, step time: 1.0431\n",
      "97/295, train_loss: 0.0525, step time: 1.0358\n",
      "98/295, train_loss: 0.2739, step time: 1.0460\n",
      "99/295, train_loss: 0.0546, step time: 1.0343\n",
      "100/295, train_loss: 0.1349, step time: 1.0357\n",
      "101/295, train_loss: 0.1263, step time: 1.0376\n",
      "102/295, train_loss: 0.1343, step time: 1.0348\n",
      "103/295, train_loss: 0.1543, step time: 1.0329\n",
      "104/295, train_loss: 0.1320, step time: 1.0349\n",
      "105/295, train_loss: 0.0823, step time: 1.0379\n",
      "106/295, train_loss: 0.0566, step time: 1.0361\n",
      "107/295, train_loss: 0.1250, step time: 1.0438\n",
      "108/295, train_loss: 0.2614, step time: 1.0392\n",
      "109/295, train_loss: 0.4263, step time: 1.0306\n",
      "110/295, train_loss: 0.3746, step time: 1.0376\n",
      "111/295, train_loss: 0.0707, step time: 1.0458\n",
      "112/295, train_loss: 0.0979, step time: 1.0392\n",
      "113/295, train_loss: 0.3969, step time: 1.0433\n",
      "114/295, train_loss: 0.0689, step time: 1.0655\n",
      "115/295, train_loss: 0.0727, step time: 1.0346\n",
      "116/295, train_loss: 0.2114, step time: 1.0537\n",
      "117/295, train_loss: 0.0501, step time: 1.0568\n",
      "118/295, train_loss: 0.0617, step time: 1.0491\n",
      "119/295, train_loss: 0.2520, step time: 1.0406\n",
      "120/295, train_loss: 0.1867, step time: 1.0683\n",
      "121/295, train_loss: 0.0550, step time: 1.0356\n",
      "122/295, train_loss: 0.1015, step time: 1.0330\n",
      "123/295, train_loss: 0.1551, step time: 1.0412\n",
      "124/295, train_loss: 0.0962, step time: 1.0556\n",
      "125/295, train_loss: 0.0918, step time: 1.0340\n",
      "126/295, train_loss: 0.1037, step time: 1.0396\n",
      "127/295, train_loss: 0.1463, step time: 1.0351\n",
      "128/295, train_loss: 0.1464, step time: 1.0432\n",
      "129/295, train_loss: 0.0894, step time: 1.0612\n",
      "130/295, train_loss: 0.4601, step time: 1.0484\n",
      "131/295, train_loss: 0.3481, step time: 1.0365\n",
      "132/295, train_loss: 0.0466, step time: 1.0853\n",
      "133/295, train_loss: 0.0853, step time: 1.0341\n",
      "134/295, train_loss: 0.1030, step time: 1.0410\n",
      "135/295, train_loss: 0.0511, step time: 1.0417\n",
      "136/295, train_loss: 0.1969, step time: 1.0706\n",
      "137/295, train_loss: 0.1902, step time: 1.0407\n",
      "138/295, train_loss: 0.4505, step time: 1.0360\n",
      "139/295, train_loss: 0.1050, step time: 1.0476\n",
      "140/295, train_loss: 0.0844, step time: 1.0408\n",
      "141/295, train_loss: 0.1261, step time: 1.0638\n",
      "142/295, train_loss: 0.0543, step time: 1.0352\n",
      "143/295, train_loss: 0.0817, step time: 1.0331\n",
      "144/295, train_loss: 0.0800, step time: 1.0338\n",
      "145/295, train_loss: 0.0431, step time: 1.0399\n",
      "146/295, train_loss: 0.1064, step time: 1.0377\n",
      "147/295, train_loss: 0.3812, step time: 1.0381\n",
      "148/295, train_loss: 0.1350, step time: 1.0388\n",
      "149/295, train_loss: 0.0755, step time: 1.0386\n",
      "150/295, train_loss: 0.1610, step time: 1.0414\n",
      "151/295, train_loss: 0.1566, step time: 1.0373\n",
      "152/295, train_loss: 0.2006, step time: 1.0453\n",
      "153/295, train_loss: 0.0742, step time: 1.0373\n",
      "154/295, train_loss: 0.1687, step time: 1.0324\n",
      "155/295, train_loss: 0.1109, step time: 1.0451\n",
      "156/295, train_loss: 0.4812, step time: 1.0335\n",
      "157/295, train_loss: 0.2035, step time: 1.0461\n",
      "158/295, train_loss: 0.1326, step time: 1.0491\n",
      "159/295, train_loss: 0.0734, step time: 1.0751\n",
      "160/295, train_loss: 0.0789, step time: 1.0450\n",
      "161/295, train_loss: 0.0865, step time: 1.0581\n",
      "162/295, train_loss: 0.1232, step time: 1.0393\n",
      "163/295, train_loss: 0.1053, step time: 1.0420\n",
      "164/295, train_loss: 0.4914, step time: 1.0370\n",
      "165/295, train_loss: 0.1174, step time: 1.0319\n",
      "166/295, train_loss: 0.0463, step time: 1.0616\n",
      "167/295, train_loss: 0.4054, step time: 1.0389\n",
      "168/295, train_loss: 0.3660, step time: 1.0530\n",
      "169/295, train_loss: 0.1622, step time: 1.0697\n",
      "170/295, train_loss: 0.1380, step time: 1.0326\n",
      "171/295, train_loss: 0.2156, step time: 1.0416\n",
      "172/295, train_loss: 0.1283, step time: 1.0393\n",
      "173/295, train_loss: 0.0786, step time: 1.0435\n",
      "174/295, train_loss: 0.0610, step time: 1.0410\n",
      "175/295, train_loss: 0.1275, step time: 1.0769\n",
      "176/295, train_loss: 0.0851, step time: 1.0604\n",
      "177/295, train_loss: 0.1047, step time: 1.0333\n",
      "178/295, train_loss: 0.0645, step time: 1.0313\n",
      "179/295, train_loss: 0.0538, step time: 1.0321\n",
      "180/295, train_loss: 0.0737, step time: 1.0388\n",
      "181/295, train_loss: 0.2569, step time: 1.0368\n",
      "182/295, train_loss: 0.0559, step time: 1.0457\n",
      "183/295, train_loss: 0.4206, step time: 1.0386\n",
      "184/295, train_loss: 0.1146, step time: 1.0340\n",
      "185/295, train_loss: 0.1453, step time: 1.0469\n",
      "186/295, train_loss: 0.1270, step time: 1.0319\n",
      "187/295, train_loss: 0.0713, step time: 1.0892\n",
      "188/295, train_loss: 0.0984, step time: 1.0384\n",
      "189/295, train_loss: 0.1863, step time: 1.0630\n",
      "190/295, train_loss: 0.1168, step time: 1.0376\n",
      "191/295, train_loss: 0.3069, step time: 1.0684\n",
      "192/295, train_loss: 0.1469, step time: 1.0461\n",
      "193/295, train_loss: 0.1019, step time: 1.0860\n",
      "194/295, train_loss: 0.0766, step time: 1.0370\n",
      "195/295, train_loss: 0.4976, step time: 1.0477\n",
      "196/295, train_loss: 0.1810, step time: 1.0401\n",
      "197/295, train_loss: 0.2733, step time: 1.0626\n",
      "198/295, train_loss: 0.3955, step time: 1.0453\n",
      "199/295, train_loss: 0.0744, step time: 1.1057\n",
      "200/295, train_loss: 0.0828, step time: 1.0677\n",
      "201/295, train_loss: 0.0567, step time: 1.0559\n",
      "202/295, train_loss: 0.0729, step time: 1.0348\n",
      "203/295, train_loss: 0.1713, step time: 1.0366\n",
      "204/295, train_loss: 0.1346, step time: 1.0355\n",
      "205/295, train_loss: 0.0751, step time: 1.0365\n",
      "206/295, train_loss: 0.1945, step time: 1.0321\n",
      "207/295, train_loss: 0.4379, step time: 1.0341\n",
      "208/295, train_loss: 0.1854, step time: 1.0570\n",
      "209/295, train_loss: 0.0940, step time: 1.0534\n",
      "210/295, train_loss: 0.1023, step time: 1.0558\n",
      "211/295, train_loss: 0.1318, step time: 1.0493\n",
      "212/295, train_loss: 0.1628, step time: 1.0414\n",
      "213/295, train_loss: 0.1640, step time: 1.0428\n",
      "214/295, train_loss: 0.1077, step time: 1.0398\n",
      "215/295, train_loss: 0.3057, step time: 1.0414\n",
      "216/295, train_loss: 0.1364, step time: 1.0402\n",
      "217/295, train_loss: 0.0777, step time: 1.0325\n",
      "218/295, train_loss: 0.6538, step time: 1.0391\n",
      "219/295, train_loss: 0.0788, step time: 1.0420\n",
      "220/295, train_loss: 0.1103, step time: 1.0479\n",
      "221/295, train_loss: 0.0493, step time: 1.0420\n",
      "222/295, train_loss: 0.1163, step time: 1.0406\n",
      "223/295, train_loss: 0.0942, step time: 1.0740\n",
      "224/295, train_loss: 0.3922, step time: 1.0662\n",
      "225/295, train_loss: 0.1896, step time: 1.0762\n",
      "226/295, train_loss: 0.0865, step time: 1.0373\n",
      "227/295, train_loss: 0.1600, step time: 1.0586\n",
      "228/295, train_loss: 0.2638, step time: 1.0603\n",
      "229/295, train_loss: 0.1563, step time: 1.0411\n",
      "230/295, train_loss: 0.3523, step time: 1.0466\n",
      "231/295, train_loss: 0.0586, step time: 1.0910\n",
      "232/295, train_loss: 0.3381, step time: 1.0377\n",
      "233/295, train_loss: 0.3790, step time: 1.0433\n",
      "234/295, train_loss: 0.4014, step time: 1.0372\n",
      "235/295, train_loss: 0.1052, step time: 1.0594\n",
      "236/295, train_loss: 0.1915, step time: 1.0366\n",
      "237/295, train_loss: 0.0742, step time: 1.0461\n",
      "238/295, train_loss: 0.4169, step time: 1.0529\n",
      "239/295, train_loss: 0.2991, step time: 1.0369\n",
      "240/295, train_loss: 0.0685, step time: 1.0843\n",
      "241/295, train_loss: 0.0766, step time: 1.0412\n",
      "242/295, train_loss: 0.0945, step time: 1.0729\n",
      "243/295, train_loss: 0.1097, step time: 1.0421\n",
      "244/295, train_loss: 0.1106, step time: 1.0333\n",
      "245/295, train_loss: 0.0704, step time: 1.0587\n",
      "246/295, train_loss: 0.0963, step time: 1.0484\n",
      "247/295, train_loss: 0.1651, step time: 1.0369\n",
      "248/295, train_loss: 0.1209, step time: 1.0362\n",
      "249/295, train_loss: 0.0463, step time: 1.0522\n",
      "250/295, train_loss: 0.4377, step time: 1.0416\n",
      "251/295, train_loss: 0.1231, step time: 1.0447\n",
      "252/295, train_loss: 0.1483, step time: 1.0448\n",
      "253/295, train_loss: 0.0746, step time: 1.0473\n",
      "254/295, train_loss: 0.1361, step time: 1.0483\n",
      "255/295, train_loss: 0.0461, step time: 1.0395\n",
      "256/295, train_loss: 0.1788, step time: 1.0362\n",
      "257/295, train_loss: 0.5049, step time: 1.0376\n",
      "258/295, train_loss: 0.0914, step time: 1.0401\n",
      "259/295, train_loss: 0.0797, step time: 1.0393\n",
      "260/295, train_loss: 0.4307, step time: 1.0421\n",
      "261/295, train_loss: 0.1693, step time: 1.0363\n",
      "262/295, train_loss: 0.0936, step time: 1.0434\n",
      "263/295, train_loss: 0.0508, step time: 1.0705\n",
      "264/295, train_loss: 0.1330, step time: 1.0385\n",
      "265/295, train_loss: 0.5548, step time: 1.0448\n",
      "266/295, train_loss: 0.0541, step time: 1.0392\n",
      "267/295, train_loss: 0.0671, step time: 1.0416\n",
      "268/295, train_loss: 0.1412, step time: 1.0558\n",
      "269/295, train_loss: 0.0829, step time: 1.0593\n",
      "270/295, train_loss: 0.4729, step time: 1.0647\n",
      "271/295, train_loss: 0.1423, step time: 1.1128\n",
      "272/295, train_loss: 0.1205, step time: 1.0343\n",
      "273/295, train_loss: 0.1860, step time: 1.0681\n",
      "274/295, train_loss: 0.2440, step time: 1.0510\n",
      "275/295, train_loss: 0.1182, step time: 1.1103\n",
      "276/295, train_loss: 0.4272, step time: 1.0391\n",
      "277/295, train_loss: 0.1097, step time: 1.0568\n",
      "278/295, train_loss: 0.1137, step time: 1.0398\n",
      "279/295, train_loss: 0.1886, step time: 1.0349\n",
      "280/295, train_loss: 0.0997, step time: 1.0418\n",
      "281/295, train_loss: 0.1233, step time: 1.0514\n",
      "282/295, train_loss: 0.2098, step time: 1.0370\n",
      "283/295, train_loss: 0.0628, step time: 1.0636\n",
      "284/295, train_loss: 0.0647, step time: 1.0624\n",
      "285/295, train_loss: 0.1088, step time: 1.0665\n",
      "286/295, train_loss: 0.0621, step time: 1.0488\n",
      "287/295, train_loss: 0.0757, step time: 1.0376\n",
      "288/295, train_loss: 0.4140, step time: 1.0341\n",
      "289/295, train_loss: 0.3844, step time: 1.0285\n",
      "290/295, train_loss: 0.0966, step time: 1.0294\n",
      "291/295, train_loss: 0.3138, step time: 1.0286\n",
      "292/295, train_loss: 0.1273, step time: 1.0300\n",
      "293/295, train_loss: 0.1316, step time: 1.0293\n",
      "294/295, train_loss: 0.2624, step time: 1.0283\n",
      "295/295, train_loss: 0.0507, step time: 1.0290\n",
      "epoch 15 average loss: 0.1619\n",
      "current epoch: 15 current mean dice: 0.7403 tc: 0.6816 wt: 0.8164 et: 0.7213\n",
      "best mean dice: 0.7804 at epoch: 14\n",
      "time consuming of epoch 15 is: 376.9453\n",
      "----------\n",
      "epoch 16/100\n",
      "1/295, train_loss: 0.4196, step time: 1.0722\n",
      "2/295, train_loss: 0.3751, step time: 1.0816\n",
      "3/295, train_loss: 0.0700, step time: 1.0684\n",
      "4/295, train_loss: 0.1976, step time: 1.0744\n",
      "5/295, train_loss: 0.4581, step time: 1.0490\n",
      "6/295, train_loss: 0.0577, step time: 1.0347\n",
      "7/295, train_loss: 0.1528, step time: 1.0997\n",
      "8/295, train_loss: 0.0724, step time: 1.1257\n",
      "9/295, train_loss: 0.5590, step time: 1.0415\n",
      "10/295, train_loss: 0.1519, step time: 1.0867\n",
      "11/295, train_loss: 0.1444, step time: 1.0332\n",
      "12/295, train_loss: 0.4300, step time: 1.0373\n",
      "13/295, train_loss: 0.1378, step time: 1.0533\n",
      "14/295, train_loss: 0.1959, step time: 1.0649\n",
      "15/295, train_loss: 0.1309, step time: 1.0349\n",
      "16/295, train_loss: 0.4117, step time: 1.0314\n",
      "17/295, train_loss: 0.0540, step time: 1.0341\n",
      "18/295, train_loss: 0.0761, step time: 1.0639\n",
      "19/295, train_loss: 0.1071, step time: 1.0451\n",
      "20/295, train_loss: 0.6359, step time: 1.1032\n",
      "21/295, train_loss: 0.1485, step time: 1.0589\n",
      "22/295, train_loss: 0.0732, step time: 1.0368\n",
      "23/295, train_loss: 0.0961, step time: 1.0684\n",
      "24/295, train_loss: 0.1288, step time: 1.0340\n",
      "25/295, train_loss: 0.0455, step time: 1.0527\n",
      "26/295, train_loss: 0.0515, step time: 1.0308\n",
      "27/295, train_loss: 0.1571, step time: 1.0639\n",
      "28/295, train_loss: 0.1376, step time: 1.0355\n",
      "29/295, train_loss: 0.3097, step time: 1.0396\n",
      "30/295, train_loss: 0.1730, step time: 1.0767\n",
      "31/295, train_loss: 0.0919, step time: 1.0306\n",
      "32/295, train_loss: 0.0736, step time: 1.0355\n",
      "33/295, train_loss: 0.1332, step time: 1.0769\n",
      "34/295, train_loss: 0.1108, step time: 1.0468\n",
      "35/295, train_loss: 0.0880, step time: 1.0348\n",
      "36/295, train_loss: 0.0521, step time: 1.0415\n",
      "37/295, train_loss: 0.4490, step time: 1.1066\n",
      "38/295, train_loss: 0.1435, step time: 1.1154\n",
      "39/295, train_loss: 0.2912, step time: 1.0335\n",
      "40/295, train_loss: 0.0488, step time: 1.0376\n",
      "41/295, train_loss: 0.1056, step time: 1.0338\n",
      "42/295, train_loss: 0.0595, step time: 1.0634\n",
      "43/295, train_loss: 0.3412, step time: 1.0388\n",
      "44/295, train_loss: 0.1714, step time: 1.0589\n",
      "45/295, train_loss: 0.1256, step time: 1.1052\n",
      "46/295, train_loss: 0.1291, step time: 1.0369\n",
      "47/295, train_loss: 0.3862, step time: 1.0399\n",
      "48/295, train_loss: 0.1127, step time: 1.0588\n",
      "49/295, train_loss: 0.5250, step time: 1.0471\n",
      "50/295, train_loss: 0.1309, step time: 1.0353\n",
      "51/295, train_loss: 0.0558, step time: 1.0451\n",
      "52/295, train_loss: 0.0904, step time: 1.0908\n",
      "53/295, train_loss: 0.1897, step time: 1.0475\n",
      "54/295, train_loss: 0.1479, step time: 1.0567\n",
      "55/295, train_loss: 0.0750, step time: 1.0364\n",
      "56/295, train_loss: 0.2396, step time: 1.0611\n",
      "57/295, train_loss: 0.3699, step time: 1.0383\n",
      "58/295, train_loss: 0.4535, step time: 1.0337\n",
      "59/295, train_loss: 0.0899, step time: 1.0483\n",
      "60/295, train_loss: 0.0390, step time: 1.0362\n",
      "61/295, train_loss: 0.1249, step time: 1.0977\n",
      "62/295, train_loss: 0.1167, step time: 1.1085\n",
      "63/295, train_loss: 0.1123, step time: 1.0401\n",
      "64/295, train_loss: 0.0506, step time: 1.0587\n",
      "65/295, train_loss: 0.2239, step time: 1.0605\n",
      "66/295, train_loss: 0.1162, step time: 1.0445\n",
      "67/295, train_loss: 0.1952, step time: 1.0349\n",
      "68/295, train_loss: 0.0569, step time: 1.0369\n",
      "69/295, train_loss: 0.0458, step time: 1.0550\n",
      "70/295, train_loss: 0.3956, step time: 1.0430\n",
      "71/295, train_loss: 0.0490, step time: 1.0358\n",
      "72/295, train_loss: 0.1593, step time: 1.0376\n",
      "73/295, train_loss: 0.1089, step time: 1.0369\n",
      "74/295, train_loss: 0.3101, step time: 1.0416\n",
      "75/295, train_loss: 0.1098, step time: 1.0303\n",
      "76/295, train_loss: 0.1370, step time: 1.0365\n",
      "77/295, train_loss: 0.1350, step time: 1.0406\n",
      "78/295, train_loss: 0.0671, step time: 1.0417\n",
      "79/295, train_loss: 0.1400, step time: 1.0530\n",
      "80/295, train_loss: 0.1149, step time: 1.0382\n",
      "81/295, train_loss: 0.1169, step time: 1.0539\n",
      "82/295, train_loss: 0.0925, step time: 1.0600\n",
      "83/295, train_loss: 0.0579, step time: 1.0326\n",
      "84/295, train_loss: 0.0503, step time: 1.0342\n",
      "85/295, train_loss: 0.3788, step time: 1.0404\n",
      "86/295, train_loss: 0.1733, step time: 1.0608\n",
      "87/295, train_loss: 0.2625, step time: 1.0544\n",
      "88/295, train_loss: 0.0499, step time: 1.0548\n",
      "89/295, train_loss: 0.1311, step time: 1.0396\n",
      "90/295, train_loss: 0.1305, step time: 1.0500\n",
      "91/295, train_loss: 0.4452, step time: 1.0464\n",
      "92/295, train_loss: 0.0757, step time: 1.0387\n",
      "93/295, train_loss: 0.0664, step time: 1.0396\n",
      "94/295, train_loss: 0.3375, step time: 1.0521\n",
      "95/295, train_loss: 0.3214, step time: 1.0678\n",
      "96/295, train_loss: 0.4185, step time: 1.0368\n",
      "97/295, train_loss: 0.1446, step time: 1.0324\n",
      "98/295, train_loss: 0.1105, step time: 1.0390\n",
      "99/295, train_loss: 0.0512, step time: 1.0399\n",
      "100/295, train_loss: 0.4714, step time: 1.0786\n",
      "101/295, train_loss: 0.0790, step time: 1.0352\n",
      "102/295, train_loss: 0.1758, step time: 1.0576\n",
      "103/295, train_loss: 0.0588, step time: 1.0614\n",
      "104/295, train_loss: 0.2462, step time: 1.0443\n",
      "105/295, train_loss: 0.0941, step time: 1.0517\n",
      "106/295, train_loss: 0.1145, step time: 1.0360\n",
      "107/295, train_loss: 0.4479, step time: 1.0313\n",
      "108/295, train_loss: 0.0801, step time: 1.0315\n",
      "109/295, train_loss: 0.1356, step time: 1.0413\n",
      "110/295, train_loss: 0.2302, step time: 1.0879\n",
      "111/295, train_loss: 0.1267, step time: 1.0330\n",
      "112/295, train_loss: 0.1043, step time: 1.0468\n",
      "113/295, train_loss: 0.4908, step time: 1.0374\n",
      "114/295, train_loss: 0.1734, step time: 1.0343\n",
      "115/295, train_loss: 0.1457, step time: 1.0358\n",
      "116/295, train_loss: 0.0897, step time: 1.0572\n",
      "117/295, train_loss: 0.1721, step time: 1.0630\n",
      "118/295, train_loss: 0.0567, step time: 1.0376\n",
      "119/295, train_loss: 0.4974, step time: 1.0643\n",
      "120/295, train_loss: 0.1136, step time: 1.0451\n",
      "121/295, train_loss: 0.1037, step time: 1.0570\n",
      "122/295, train_loss: 0.1198, step time: 1.0545\n",
      "123/295, train_loss: 0.1112, step time: 1.0426\n",
      "124/295, train_loss: 0.0786, step time: 1.0419\n",
      "125/295, train_loss: 0.0773, step time: 1.0398\n",
      "126/295, train_loss: 0.1582, step time: 1.0495\n",
      "127/295, train_loss: 0.1097, step time: 1.0508\n",
      "128/295, train_loss: 0.1150, step time: 1.0767\n",
      "129/295, train_loss: 0.2073, step time: 1.0402\n",
      "130/295, train_loss: 0.4429, step time: 1.0900\n",
      "131/295, train_loss: 0.2471, step time: 1.0314\n",
      "132/295, train_loss: 0.1312, step time: 1.0308\n",
      "133/295, train_loss: 0.1476, step time: 1.0777\n",
      "134/295, train_loss: 0.0702, step time: 1.0585\n",
      "135/295, train_loss: 0.3227, step time: 1.0369\n",
      "136/295, train_loss: 0.1008, step time: 1.0395\n",
      "137/295, train_loss: 0.1177, step time: 1.0647\n",
      "138/295, train_loss: 0.1421, step time: 1.0592\n",
      "139/295, train_loss: 0.1920, step time: 1.0736\n",
      "140/295, train_loss: 0.0991, step time: 1.0323\n",
      "141/295, train_loss: 0.1548, step time: 1.0380\n",
      "142/295, train_loss: 0.0551, step time: 1.0528\n",
      "143/295, train_loss: 0.0726, step time: 1.0389\n",
      "144/295, train_loss: 0.2356, step time: 1.0328\n",
      "145/295, train_loss: 0.0536, step time: 1.0479\n",
      "146/295, train_loss: 0.0781, step time: 1.0565\n",
      "147/295, train_loss: 0.1304, step time: 1.0392\n",
      "148/295, train_loss: 0.0798, step time: 1.0341\n",
      "149/295, train_loss: 0.0671, step time: 1.0560\n",
      "150/295, train_loss: 0.2018, step time: 1.0526\n",
      "151/295, train_loss: 0.1002, step time: 1.0485\n",
      "152/295, train_loss: 0.4369, step time: 1.0896\n",
      "153/295, train_loss: 0.2504, step time: 1.0386\n",
      "154/295, train_loss: 0.0736, step time: 1.0563\n",
      "155/295, train_loss: 0.3991, step time: 1.0424\n",
      "156/295, train_loss: 0.1912, step time: 1.0546\n",
      "157/295, train_loss: 0.1285, step time: 1.0362\n",
      "158/295, train_loss: 0.1020, step time: 1.0395\n",
      "159/295, train_loss: 0.0794, step time: 1.0591\n",
      "160/295, train_loss: 0.4023, step time: 1.1114\n",
      "161/295, train_loss: 0.0770, step time: 1.0350\n",
      "162/295, train_loss: 0.1506, step time: 1.0401\n",
      "163/295, train_loss: 0.2716, step time: 1.0328\n",
      "164/295, train_loss: 0.1207, step time: 1.0401\n",
      "165/295, train_loss: 0.1339, step time: 1.0368\n",
      "166/295, train_loss: 0.0604, step time: 1.0688\n",
      "167/295, train_loss: 0.1039, step time: 1.0363\n",
      "168/295, train_loss: 0.1504, step time: 1.0455\n",
      "169/295, train_loss: 0.0807, step time: 1.0459\n",
      "170/295, train_loss: 0.0684, step time: 1.0389\n",
      "171/295, train_loss: 0.1275, step time: 1.0379\n",
      "172/295, train_loss: 0.1057, step time: 1.0946\n",
      "173/295, train_loss: 0.1704, step time: 1.0323\n",
      "174/295, train_loss: 0.1036, step time: 1.0397\n",
      "175/295, train_loss: 0.1734, step time: 1.0375\n",
      "176/295, train_loss: 0.0818, step time: 1.0791\n",
      "177/295, train_loss: 0.1821, step time: 1.0433\n",
      "178/295, train_loss: 0.0967, step time: 1.0454\n",
      "179/295, train_loss: 0.1947, step time: 1.0366\n",
      "180/295, train_loss: 0.2991, step time: 1.0322\n",
      "181/295, train_loss: 0.0968, step time: 1.0393\n",
      "182/295, train_loss: 0.1586, step time: 1.0353\n",
      "183/295, train_loss: 0.1799, step time: 1.0544\n",
      "184/295, train_loss: 0.1321, step time: 1.0352\n",
      "185/295, train_loss: 0.1079, step time: 1.0436\n",
      "186/295, train_loss: 0.0528, step time: 1.0384\n",
      "187/295, train_loss: 0.1279, step time: 1.0370\n",
      "188/295, train_loss: 0.0623, step time: 1.0489\n",
      "189/295, train_loss: 0.0458, step time: 1.0466\n",
      "190/295, train_loss: 0.2322, step time: 1.0378\n",
      "191/295, train_loss: 0.2235, step time: 1.0401\n",
      "192/295, train_loss: 0.2931, step time: 1.0364\n",
      "193/295, train_loss: 0.1594, step time: 1.0674\n",
      "194/295, train_loss: 0.0545, step time: 1.0640\n",
      "195/295, train_loss: 0.0596, step time: 1.0458\n",
      "196/295, train_loss: 0.1416, step time: 1.0400\n",
      "197/295, train_loss: 0.0724, step time: 1.0409\n",
      "198/295, train_loss: 0.0888, step time: 1.0312\n",
      "199/295, train_loss: 0.0958, step time: 1.0417\n",
      "200/295, train_loss: 0.1514, step time: 1.0349\n",
      "201/295, train_loss: 0.2008, step time: 1.0426\n",
      "202/295, train_loss: 0.0823, step time: 1.0395\n",
      "203/295, train_loss: 0.1525, step time: 1.0531\n",
      "204/295, train_loss: 0.1643, step time: 1.0512\n",
      "205/295, train_loss: 0.0639, step time: 1.0427\n",
      "206/295, train_loss: 0.1158, step time: 1.0500\n",
      "207/295, train_loss: 0.0838, step time: 1.0390\n",
      "208/295, train_loss: 0.0852, step time: 1.0370\n",
      "209/295, train_loss: 0.0996, step time: 1.0430\n",
      "210/295, train_loss: 0.4398, step time: 1.0369\n",
      "211/295, train_loss: 0.1754, step time: 1.0362\n",
      "212/295, train_loss: 0.1900, step time: 1.0360\n",
      "213/295, train_loss: 0.1616, step time: 1.0675\n",
      "214/295, train_loss: 0.4863, step time: 1.0522\n",
      "215/295, train_loss: 0.1217, step time: 1.0386\n",
      "216/295, train_loss: 0.2134, step time: 1.0329\n",
      "217/295, train_loss: 0.1261, step time: 1.0594\n",
      "218/295, train_loss: 0.1135, step time: 1.0747\n",
      "219/295, train_loss: 0.0678, step time: 1.0607\n",
      "220/295, train_loss: 0.0618, step time: 1.0335\n",
      "221/295, train_loss: 0.0768, step time: 1.0448\n",
      "222/295, train_loss: 0.1363, step time: 1.0382\n",
      "223/295, train_loss: 0.0807, step time: 1.0450\n",
      "224/295, train_loss: 0.1495, step time: 1.0510\n",
      "225/295, train_loss: 0.0556, step time: 1.0648\n",
      "226/295, train_loss: 0.3808, step time: 1.0376\n",
      "227/295, train_loss: 0.0977, step time: 1.0648\n",
      "228/295, train_loss: 0.0889, step time: 1.0426\n",
      "229/295, train_loss: 0.0700, step time: 1.0476\n",
      "230/295, train_loss: 0.0514, step time: 1.0500\n",
      "231/295, train_loss: 0.1135, step time: 1.0321\n",
      "232/295, train_loss: 0.0373, step time: 1.0324\n",
      "233/295, train_loss: 0.1434, step time: 1.0531\n",
      "234/295, train_loss: 0.4143, step time: 1.0322\n",
      "235/295, train_loss: 0.0866, step time: 1.0588\n",
      "236/295, train_loss: 0.1303, step time: 1.0424\n",
      "237/295, train_loss: 0.2010, step time: 1.0815\n",
      "238/295, train_loss: 0.1768, step time: 1.0449\n",
      "239/295, train_loss: 0.3453, step time: 1.0328\n",
      "240/295, train_loss: 0.1254, step time: 1.0424\n",
      "241/295, train_loss: 0.0677, step time: 1.0615\n",
      "242/295, train_loss: 0.0536, step time: 1.0361\n",
      "243/295, train_loss: 0.3385, step time: 1.0389\n",
      "244/295, train_loss: 0.1672, step time: 1.0436\n",
      "245/295, train_loss: 0.0997, step time: 1.0385\n",
      "246/295, train_loss: 0.0680, step time: 1.0351\n",
      "247/295, train_loss: 0.0804, step time: 1.0377\n",
      "248/295, train_loss: 0.0676, step time: 1.0317\n",
      "249/295, train_loss: 0.0526, step time: 1.0347\n",
      "250/295, train_loss: 0.1277, step time: 1.0976\n",
      "251/295, train_loss: 0.1020, step time: 1.0419\n",
      "252/295, train_loss: 0.1059, step time: 1.0350\n",
      "253/295, train_loss: 0.3643, step time: 1.0367\n",
      "254/295, train_loss: 0.1619, step time: 1.0436\n",
      "255/295, train_loss: 0.0767, step time: 1.0363\n",
      "256/295, train_loss: 0.0608, step time: 1.0492\n",
      "257/295, train_loss: 0.1052, step time: 1.0664\n",
      "258/295, train_loss: 0.1290, step time: 1.0583\n",
      "259/295, train_loss: 0.0737, step time: 1.0372\n",
      "260/295, train_loss: 0.1101, step time: 1.0475\n",
      "261/295, train_loss: 0.0924, step time: 1.0626\n",
      "262/295, train_loss: 0.0723, step time: 1.0647\n",
      "263/295, train_loss: 0.0864, step time: 1.0365\n",
      "264/295, train_loss: 0.1360, step time: 1.0438\n",
      "265/295, train_loss: 0.1793, step time: 1.0955\n",
      "266/295, train_loss: 0.0727, step time: 1.0653\n",
      "267/295, train_loss: 0.0506, step time: 1.0590\n",
      "268/295, train_loss: 0.3844, step time: 1.0762\n",
      "269/295, train_loss: 0.2605, step time: 1.0430\n",
      "270/295, train_loss: 0.0624, step time: 1.0407\n",
      "271/295, train_loss: 0.0910, step time: 1.1266\n",
      "272/295, train_loss: 0.0540, step time: 1.0424\n",
      "273/295, train_loss: 0.0893, step time: 1.0347\n",
      "274/295, train_loss: 0.1059, step time: 1.0395\n",
      "275/295, train_loss: 0.3082, step time: 1.0313\n",
      "276/295, train_loss: 0.1168, step time: 1.0367\n",
      "277/295, train_loss: 0.1067, step time: 1.0318\n",
      "278/295, train_loss: 0.0712, step time: 1.0333\n",
      "279/295, train_loss: 0.0476, step time: 1.0394\n",
      "280/295, train_loss: 0.0610, step time: 1.0388\n",
      "281/295, train_loss: 0.1205, step time: 1.0363\n",
      "282/295, train_loss: 0.3278, step time: 1.0375\n",
      "283/295, train_loss: 0.0816, step time: 1.0474\n",
      "284/295, train_loss: 0.2402, step time: 1.0689\n",
      "285/295, train_loss: 0.2479, step time: 1.0659\n",
      "286/295, train_loss: 0.1316, step time: 1.0354\n",
      "287/295, train_loss: 0.0884, step time: 1.0346\n",
      "288/295, train_loss: 0.1484, step time: 1.0291\n",
      "289/295, train_loss: 0.0612, step time: 1.0292\n",
      "290/295, train_loss: 0.5326, step time: 1.0291\n",
      "291/295, train_loss: 0.1919, step time: 1.0297\n",
      "292/295, train_loss: 0.0936, step time: 1.0298\n",
      "293/295, train_loss: 0.0836, step time: 1.0286\n",
      "294/295, train_loss: 0.7172, step time: 1.0291\n",
      "295/295, train_loss: 0.0733, step time: 1.0284\n",
      "epoch 16 average loss: 0.1614\n",
      "current epoch: 16 current mean dice: 0.7481 tc: 0.6918 wt: 0.8233 et: 0.7409\n",
      "best mean dice: 0.7804 at epoch: 14\n",
      "time consuming of epoch 16 is: 377.7895\n",
      "----------\n",
      "epoch 17/100\n",
      "1/295, train_loss: 0.0588, step time: 1.1641\n",
      "2/295, train_loss: 0.1025, step time: 1.0780\n",
      "3/295, train_loss: 0.1024, step time: 1.0650\n",
      "4/295, train_loss: 0.0890, step time: 1.0329\n",
      "5/295, train_loss: 0.3825, step time: 1.0309\n",
      "6/295, train_loss: 0.1899, step time: 1.0352\n",
      "7/295, train_loss: 0.3151, step time: 1.0449\n",
      "8/295, train_loss: 0.1261, step time: 1.0889\n",
      "9/295, train_loss: 0.1599, step time: 1.0389\n",
      "10/295, train_loss: 0.2006, step time: 1.0410\n",
      "11/295, train_loss: 0.3100, step time: 1.0395\n",
      "12/295, train_loss: 0.0754, step time: 1.0953\n",
      "13/295, train_loss: 0.1246, step time: 1.0775\n",
      "14/295, train_loss: 0.0876, step time: 1.0664\n",
      "15/295, train_loss: 0.0871, step time: 1.0435\n",
      "16/295, train_loss: 0.1576, step time: 1.0364\n",
      "17/295, train_loss: 0.0566, step time: 1.0573\n",
      "18/295, train_loss: 0.1170, step time: 1.0493\n",
      "19/295, train_loss: 0.0480, step time: 1.0608\n",
      "20/295, train_loss: 0.0591, step time: 1.0423\n",
      "21/295, train_loss: 0.1934, step time: 1.0536\n",
      "22/295, train_loss: 0.1285, step time: 1.0369\n",
      "23/295, train_loss: 0.1224, step time: 1.1074\n",
      "24/295, train_loss: 0.1378, step time: 1.0416\n",
      "25/295, train_loss: 0.0825, step time: 1.0331\n",
      "26/295, train_loss: 0.1919, step time: 1.0455\n",
      "27/295, train_loss: 0.1327, step time: 1.0706\n",
      "28/295, train_loss: 0.0570, step time: 1.0985\n",
      "29/295, train_loss: 0.0439, step time: 1.0698\n",
      "30/295, train_loss: 0.0886, step time: 1.0312\n",
      "31/295, train_loss: 0.0884, step time: 1.0511\n",
      "32/295, train_loss: 0.1184, step time: 1.0998\n",
      "33/295, train_loss: 0.1077, step time: 1.0474\n",
      "34/295, train_loss: 0.0508, step time: 1.0612\n",
      "35/295, train_loss: 0.1459, step time: 1.1027\n",
      "36/295, train_loss: 0.0971, step time: 1.0405\n",
      "37/295, train_loss: 0.1640, step time: 1.0368\n",
      "38/295, train_loss: 0.1747, step time: 1.0680\n",
      "39/295, train_loss: 0.1006, step time: 1.0539\n",
      "40/295, train_loss: 0.0594, step time: 1.0436\n",
      "41/295, train_loss: 0.0923, step time: 1.0505\n",
      "42/295, train_loss: 0.1684, step time: 1.1100\n",
      "43/295, train_loss: 0.1934, step time: 1.0321\n",
      "44/295, train_loss: 0.6248, step time: 1.0317\n",
      "45/295, train_loss: 0.0820, step time: 1.0392\n",
      "46/295, train_loss: 0.1946, step time: 1.0363\n",
      "47/295, train_loss: 0.1555, step time: 1.0670\n",
      "48/295, train_loss: 0.1041, step time: 1.0836\n",
      "49/295, train_loss: 0.0892, step time: 1.0425\n",
      "50/295, train_loss: 0.1331, step time: 1.0406\n",
      "51/295, train_loss: 0.4321, step time: 1.0446\n",
      "52/295, train_loss: 0.1239, step time: 1.0435\n",
      "53/295, train_loss: 0.0837, step time: 1.0486\n",
      "54/295, train_loss: 0.0631, step time: 1.0332\n",
      "55/295, train_loss: 0.2312, step time: 1.0404\n",
      "56/295, train_loss: 0.0718, step time: 1.0438\n",
      "57/295, train_loss: 0.1002, step time: 1.0654\n",
      "58/295, train_loss: 0.0861, step time: 1.0333\n",
      "59/295, train_loss: 0.4684, step time: 1.0411\n",
      "60/295, train_loss: 0.1060, step time: 1.0425\n",
      "61/295, train_loss: 0.1658, step time: 1.0613\n",
      "62/295, train_loss: 0.5065, step time: 1.0406\n",
      "63/295, train_loss: 0.0656, step time: 1.0377\n",
      "64/295, train_loss: 0.1474, step time: 1.0394\n",
      "65/295, train_loss: 0.0707, step time: 1.0632\n",
      "66/295, train_loss: 0.0601, step time: 1.0699\n",
      "67/295, train_loss: 0.1089, step time: 1.0442\n",
      "68/295, train_loss: 0.1615, step time: 1.0317\n",
      "69/295, train_loss: 0.4074, step time: 1.0306\n",
      "70/295, train_loss: 0.1841, step time: 1.0651\n",
      "71/295, train_loss: 0.0474, step time: 1.0369\n",
      "72/295, train_loss: 0.0859, step time: 1.0563\n",
      "73/295, train_loss: 0.0819, step time: 1.0369\n",
      "74/295, train_loss: 0.5303, step time: 1.0545\n",
      "75/295, train_loss: 0.0911, step time: 1.0634\n",
      "76/295, train_loss: 0.1394, step time: 1.0409\n",
      "77/295, train_loss: 0.0693, step time: 1.0597\n",
      "78/295, train_loss: 0.3294, step time: 1.0324\n",
      "79/295, train_loss: 0.1057, step time: 1.0315\n",
      "80/295, train_loss: 0.0904, step time: 1.0413\n",
      "81/295, train_loss: 0.1586, step time: 1.1167\n",
      "82/295, train_loss: 0.1542, step time: 1.0505\n",
      "83/295, train_loss: 0.1404, step time: 1.0448\n",
      "84/295, train_loss: 0.0831, step time: 1.0309\n",
      "85/295, train_loss: 0.0954, step time: 1.0407\n",
      "86/295, train_loss: 0.0587, step time: 1.0413\n",
      "87/295, train_loss: 0.4001, step time: 1.0804\n",
      "88/295, train_loss: 0.2121, step time: 1.0390\n",
      "89/295, train_loss: 0.0830, step time: 1.0475\n",
      "90/295, train_loss: 0.0596, step time: 1.0487\n",
      "91/295, train_loss: 0.3812, step time: 1.0387\n",
      "92/295, train_loss: 0.2309, step time: 1.0533\n",
      "93/295, train_loss: 0.0929, step time: 1.0423\n",
      "94/295, train_loss: 0.2528, step time: 1.1037\n",
      "95/295, train_loss: 0.4526, step time: 1.0804\n",
      "96/295, train_loss: 0.1190, step time: 1.0396\n",
      "97/295, train_loss: 0.3975, step time: 1.0675\n",
      "98/295, train_loss: 0.0633, step time: 1.0609\n",
      "99/295, train_loss: 0.0625, step time: 1.0424\n",
      "100/295, train_loss: 0.3006, step time: 1.0721\n",
      "101/295, train_loss: 0.0634, step time: 1.0720\n",
      "102/295, train_loss: 0.1339, step time: 1.0370\n",
      "103/295, train_loss: 0.1266, step time: 1.0303\n",
      "104/295, train_loss: 0.1107, step time: 1.0707\n",
      "105/295, train_loss: 0.0349, step time: 1.0519\n",
      "106/295, train_loss: 0.0537, step time: 1.0336\n",
      "107/295, train_loss: 0.3660, step time: 1.0376\n",
      "108/295, train_loss: 0.0852, step time: 1.0345\n",
      "109/295, train_loss: 0.0817, step time: 1.0393\n",
      "110/295, train_loss: 0.0917, step time: 1.0644\n",
      "111/295, train_loss: 0.1085, step time: 1.0539\n",
      "112/295, train_loss: 0.0676, step time: 1.0748\n",
      "113/295, train_loss: 0.0528, step time: 1.0360\n",
      "114/295, train_loss: 0.1599, step time: 1.0598\n",
      "115/295, train_loss: 0.0852, step time: 1.0563\n",
      "116/295, train_loss: 0.1627, step time: 1.1521\n",
      "117/295, train_loss: 0.0896, step time: 1.1341\n",
      "118/295, train_loss: 0.1124, step time: 1.0423\n",
      "119/295, train_loss: 0.1215, step time: 1.0400\n",
      "120/295, train_loss: 0.1199, step time: 1.0648\n",
      "121/295, train_loss: 0.0692, step time: 1.0335\n",
      "122/295, train_loss: 0.4666, step time: 1.0375\n",
      "123/295, train_loss: 0.4949, step time: 1.0366\n",
      "124/295, train_loss: 0.0771, step time: 1.0418\n",
      "125/295, train_loss: 0.0701, step time: 1.0446\n",
      "126/295, train_loss: 0.0658, step time: 1.0360\n",
      "127/295, train_loss: 0.3768, step time: 1.0385\n",
      "128/295, train_loss: 0.3467, step time: 1.0497\n",
      "129/295, train_loss: 0.3734, step time: 1.0365\n",
      "130/295, train_loss: 0.1185, step time: 1.0307\n",
      "131/295, train_loss: 0.0576, step time: 1.0566\n",
      "132/295, train_loss: 0.1190, step time: 1.0351\n",
      "133/295, train_loss: 0.1040, step time: 1.0397\n",
      "134/295, train_loss: 0.0761, step time: 1.0525\n",
      "135/295, train_loss: 0.1745, step time: 1.0576\n",
      "136/295, train_loss: 0.1541, step time: 1.0319\n",
      "137/295, train_loss: 0.1070, step time: 1.0341\n",
      "138/295, train_loss: 0.1141, step time: 1.0547\n",
      "139/295, train_loss: 0.0973, step time: 1.0524\n",
      "140/295, train_loss: 0.1257, step time: 1.0372\n",
      "141/295, train_loss: 0.0446, step time: 1.0575\n",
      "142/295, train_loss: 0.0964, step time: 1.0494\n",
      "143/295, train_loss: 0.1134, step time: 1.0339\n",
      "144/295, train_loss: 0.1183, step time: 1.0396\n",
      "145/295, train_loss: 0.0644, step time: 1.0520\n",
      "146/295, train_loss: 0.4661, step time: 1.0411\n",
      "147/295, train_loss: 0.0828, step time: 1.0499\n",
      "148/295, train_loss: 0.1200, step time: 1.0616\n",
      "149/295, train_loss: 0.2310, step time: 1.1159\n",
      "150/295, train_loss: 0.0547, step time: 1.0642\n",
      "151/295, train_loss: 0.0575, step time: 1.0346\n",
      "152/295, train_loss: 0.0932, step time: 1.0601\n",
      "153/295, train_loss: 0.3153, step time: 1.0382\n",
      "154/295, train_loss: 0.0762, step time: 1.0364\n",
      "155/295, train_loss: 0.0575, step time: 1.0438\n",
      "156/295, train_loss: 0.0804, step time: 1.0408\n",
      "157/295, train_loss: 0.1734, step time: 1.0385\n",
      "158/295, train_loss: 0.0476, step time: 1.1039\n",
      "159/295, train_loss: 0.0681, step time: 1.0374\n",
      "160/295, train_loss: 0.2383, step time: 1.0309\n",
      "161/295, train_loss: 0.0859, step time: 1.0321\n",
      "162/295, train_loss: 0.4417, step time: 1.0387\n",
      "163/295, train_loss: 0.0937, step time: 1.0340\n",
      "164/295, train_loss: 0.1487, step time: 1.1005\n",
      "165/295, train_loss: 0.0909, step time: 1.0338\n",
      "166/295, train_loss: 0.1491, step time: 1.0326\n",
      "167/295, train_loss: 0.0948, step time: 1.0697\n",
      "168/295, train_loss: 0.1161, step time: 1.0343\n",
      "169/295, train_loss: 0.0773, step time: 1.0408\n",
      "170/295, train_loss: 0.1432, step time: 1.0462\n",
      "171/295, train_loss: 0.2184, step time: 1.0475\n",
      "172/295, train_loss: 0.0559, step time: 1.0414\n",
      "173/295, train_loss: 0.1152, step time: 1.0341\n",
      "174/295, train_loss: 0.0761, step time: 1.0327\n",
      "175/295, train_loss: 0.3364, step time: 1.0398\n",
      "176/295, train_loss: 0.1519, step time: 1.0662\n",
      "177/295, train_loss: 0.4940, step time: 1.0789\n",
      "178/295, train_loss: 0.1074, step time: 1.0431\n",
      "179/295, train_loss: 0.2110, step time: 1.0397\n",
      "180/295, train_loss: 0.1264, step time: 1.0369\n",
      "181/295, train_loss: 0.1979, step time: 1.0558\n",
      "182/295, train_loss: 0.0783, step time: 1.0375\n",
      "183/295, train_loss: 0.0910, step time: 1.0774\n",
      "184/295, train_loss: 0.0988, step time: 1.0431\n",
      "185/295, train_loss: 0.1241, step time: 1.0637\n",
      "186/295, train_loss: 0.1973, step time: 1.0359\n",
      "187/295, train_loss: 0.0805, step time: 1.0415\n",
      "188/295, train_loss: 0.1246, step time: 1.0746\n",
      "189/295, train_loss: 0.1128, step time: 1.0454\n",
      "190/295, train_loss: 0.0600, step time: 1.0401\n",
      "191/295, train_loss: 0.1633, step time: 1.0719\n",
      "192/295, train_loss: 0.0786, step time: 1.0369\n",
      "193/295, train_loss: 0.1552, step time: 1.0435\n",
      "194/295, train_loss: 0.0489, step time: 1.0635\n",
      "195/295, train_loss: 0.0455, step time: 1.0557\n",
      "196/295, train_loss: 0.1049, step time: 1.0395\n",
      "197/295, train_loss: 0.0548, step time: 1.0391\n",
      "198/295, train_loss: 0.1484, step time: 1.0944\n",
      "199/295, train_loss: 0.0781, step time: 1.0610\n",
      "200/295, train_loss: 0.0505, step time: 1.0375\n",
      "201/295, train_loss: 0.0533, step time: 1.0441\n",
      "202/295, train_loss: 0.1127, step time: 1.0369\n",
      "203/295, train_loss: 0.1167, step time: 1.0402\n",
      "204/295, train_loss: 0.1053, step time: 1.0376\n",
      "205/295, train_loss: 0.1944, step time: 1.0590\n",
      "206/295, train_loss: 0.2121, step time: 1.0491\n",
      "207/295, train_loss: 0.3894, step time: 1.0491\n",
      "208/295, train_loss: 0.0742, step time: 1.0406\n",
      "209/295, train_loss: 0.0928, step time: 1.0468\n",
      "210/295, train_loss: 0.1129, step time: 1.0381\n",
      "211/295, train_loss: 0.1004, step time: 1.0336\n",
      "212/295, train_loss: 0.3853, step time: 1.0403\n",
      "213/295, train_loss: 0.0694, step time: 1.0551\n",
      "214/295, train_loss: 0.3921, step time: 1.0366\n",
      "215/295, train_loss: 0.0894, step time: 1.0316\n",
      "216/295, train_loss: 0.0707, step time: 1.0356\n",
      "217/295, train_loss: 0.0821, step time: 1.0353\n",
      "218/295, train_loss: 0.4234, step time: 1.0318\n",
      "219/295, train_loss: 0.1799, step time: 1.0358\n",
      "220/295, train_loss: 0.1359, step time: 1.0445\n",
      "221/295, train_loss: 0.2945, step time: 1.0522\n",
      "222/295, train_loss: 0.4157, step time: 1.0607\n",
      "223/295, train_loss: 0.4116, step time: 1.0406\n",
      "224/295, train_loss: 0.2162, step time: 1.0346\n",
      "225/295, train_loss: 0.0487, step time: 1.0449\n",
      "226/295, train_loss: 0.1347, step time: 1.0428\n",
      "227/295, train_loss: 0.1069, step time: 1.0685\n",
      "228/295, train_loss: 0.0467, step time: 1.0462\n",
      "229/295, train_loss: 0.1054, step time: 1.0426\n",
      "230/295, train_loss: 0.0760, step time: 1.0538\n",
      "231/295, train_loss: 0.1282, step time: 1.0399\n",
      "232/295, train_loss: 0.2425, step time: 1.0431\n",
      "233/295, train_loss: 0.1709, step time: 1.0535\n",
      "234/295, train_loss: 0.1570, step time: 1.0426\n",
      "235/295, train_loss: 0.0514, step time: 1.0391\n",
      "236/295, train_loss: 0.2505, step time: 1.0470\n",
      "237/295, train_loss: 0.0974, step time: 1.0347\n",
      "238/295, train_loss: 0.0557, step time: 1.0373\n",
      "239/295, train_loss: 0.1266, step time: 1.0370\n",
      "240/295, train_loss: 0.0786, step time: 1.0428\n",
      "241/295, train_loss: 0.2177, step time: 1.0417\n",
      "242/295, train_loss: 0.3286, step time: 1.0579\n",
      "243/295, train_loss: 0.1211, step time: 1.0697\n",
      "244/295, train_loss: 0.2099, step time: 1.0788\n",
      "245/295, train_loss: 0.4501, step time: 1.0395\n",
      "246/295, train_loss: 0.1043, step time: 1.0357\n",
      "247/295, train_loss: 0.1261, step time: 1.0399\n",
      "248/295, train_loss: 0.3001, step time: 1.0673\n",
      "249/295, train_loss: 0.0853, step time: 1.0349\n",
      "250/295, train_loss: 0.0631, step time: 1.0605\n",
      "251/295, train_loss: 0.1577, step time: 1.0476\n",
      "252/295, train_loss: 0.0544, step time: 1.0697\n",
      "253/295, train_loss: 0.0708, step time: 1.0679\n",
      "254/295, train_loss: 0.2206, step time: 1.0413\n",
      "255/295, train_loss: 0.2499, step time: 1.0348\n",
      "256/295, train_loss: 0.0475, step time: 1.0504\n",
      "257/295, train_loss: 0.0605, step time: 1.0444\n",
      "258/295, train_loss: 0.0813, step time: 1.0367\n",
      "259/295, train_loss: 0.0496, step time: 1.0405\n",
      "260/295, train_loss: 0.1937, step time: 1.0681\n",
      "261/295, train_loss: 0.1025, step time: 1.0830\n",
      "262/295, train_loss: 0.4007, step time: 1.1619\n",
      "263/295, train_loss: 0.4205, step time: 1.0344\n",
      "264/295, train_loss: 0.4128, step time: 1.0326\n",
      "265/295, train_loss: 0.0698, step time: 1.0409\n",
      "266/295, train_loss: 0.0509, step time: 1.0558\n",
      "267/295, train_loss: 0.0926, step time: 1.0471\n",
      "268/295, train_loss: 0.0882, step time: 1.0647\n",
      "269/295, train_loss: 0.3784, step time: 1.0818\n",
      "270/295, train_loss: 0.1189, step time: 1.0322\n",
      "271/295, train_loss: 0.1249, step time: 1.0633\n",
      "272/295, train_loss: 0.0565, step time: 1.0598\n",
      "273/295, train_loss: 0.4133, step time: 1.0358\n",
      "274/295, train_loss: 0.1221, step time: 1.0382\n",
      "275/295, train_loss: 0.0577, step time: 1.0382\n",
      "276/295, train_loss: 0.1764, step time: 1.0437\n",
      "277/295, train_loss: 0.0884, step time: 1.0639\n",
      "278/295, train_loss: 0.0625, step time: 1.0324\n",
      "279/295, train_loss: 0.1789, step time: 1.0545\n",
      "280/295, train_loss: 0.1044, step time: 1.0540\n",
      "281/295, train_loss: 0.0418, step time: 1.0437\n",
      "282/295, train_loss: 0.0659, step time: 1.0400\n",
      "283/295, train_loss: 0.1400, step time: 1.0345\n",
      "284/295, train_loss: 0.1815, step time: 1.0410\n",
      "285/295, train_loss: 0.1201, step time: 1.0570\n",
      "286/295, train_loss: 0.2131, step time: 1.0349\n",
      "287/295, train_loss: 0.0662, step time: 1.0310\n",
      "288/295, train_loss: 0.1342, step time: 1.0374\n",
      "289/295, train_loss: 0.0495, step time: 1.0295\n",
      "290/295, train_loss: 0.0444, step time: 1.0287\n",
      "291/295, train_loss: 0.0808, step time: 1.0284\n",
      "292/295, train_loss: 0.0863, step time: 1.0289\n",
      "293/295, train_loss: 0.0428, step time: 1.0317\n",
      "294/295, train_loss: 0.0912, step time: 1.0300\n",
      "295/295, train_loss: 0.1134, step time: 1.0293\n",
      "epoch 17 average loss: 0.1513\n",
      "saved new best metric model\n",
      "current epoch: 17 current mean dice: 0.7821 tc: 0.7363 wt: 0.8489 et: 0.7687\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 17 is: 386.9514\n",
      "----------\n",
      "epoch 18/100\n",
      "1/295, train_loss: 0.1264, step time: 1.0984\n",
      "2/295, train_loss: 0.3813, step time: 1.0422\n",
      "3/295, train_loss: 0.1391, step time: 1.0669\n",
      "4/295, train_loss: 0.0887, step time: 1.1869\n",
      "5/295, train_loss: 0.2314, step time: 1.0331\n",
      "6/295, train_loss: 0.1194, step time: 1.0557\n",
      "7/295, train_loss: 0.4253, step time: 1.0367\n",
      "8/295, train_loss: 0.3944, step time: 1.0734\n",
      "9/295, train_loss: 0.0965, step time: 1.0394\n",
      "10/295, train_loss: 0.5348, step time: 1.0379\n",
      "11/295, train_loss: 0.0754, step time: 1.0601\n",
      "12/295, train_loss: 0.1144, step time: 1.0463\n",
      "13/295, train_loss: 0.0777, step time: 1.0402\n",
      "14/295, train_loss: 0.0722, step time: 1.0455\n",
      "15/295, train_loss: 0.0822, step time: 1.0552\n",
      "16/295, train_loss: 0.0708, step time: 1.0352\n",
      "17/295, train_loss: 0.0919, step time: 1.0320\n",
      "18/295, train_loss: 0.1930, step time: 1.0558\n",
      "19/295, train_loss: 0.0830, step time: 1.0376\n",
      "20/295, train_loss: 0.1988, step time: 1.0585\n",
      "21/295, train_loss: 0.1871, step time: 1.0371\n",
      "22/295, train_loss: 0.4307, step time: 1.0382\n",
      "23/295, train_loss: 0.1112, step time: 1.0379\n",
      "24/295, train_loss: 0.0623, step time: 1.0567\n",
      "25/295, train_loss: 0.0953, step time: 1.0420\n",
      "26/295, train_loss: 0.0539, step time: 1.1277\n",
      "27/295, train_loss: 0.0739, step time: 1.0331\n",
      "28/295, train_loss: 0.0558, step time: 1.0432\n",
      "29/295, train_loss: 0.1813, step time: 1.0374\n",
      "30/295, train_loss: 0.1462, step time: 1.0452\n",
      "31/295, train_loss: 0.3876, step time: 1.0771\n",
      "32/295, train_loss: 0.1813, step time: 1.0366\n",
      "33/295, train_loss: 0.0665, step time: 1.0902\n",
      "34/295, train_loss: 0.1474, step time: 1.0702\n",
      "35/295, train_loss: 0.0468, step time: 1.0315\n",
      "36/295, train_loss: 0.2085, step time: 1.0664\n",
      "37/295, train_loss: 0.1123, step time: 1.1588\n",
      "38/295, train_loss: 0.1301, step time: 1.0323\n",
      "39/295, train_loss: 0.0752, step time: 1.0346\n",
      "40/295, train_loss: 0.0836, step time: 1.0394\n",
      "41/295, train_loss: 0.1055, step time: 1.0620\n",
      "42/295, train_loss: 0.1299, step time: 1.1253\n",
      "43/295, train_loss: 0.0594, step time: 1.0497\n",
      "44/295, train_loss: 0.1764, step time: 1.0403\n",
      "45/295, train_loss: 0.4476, step time: 1.1013\n",
      "46/295, train_loss: 0.0518, step time: 1.0355\n",
      "47/295, train_loss: 0.1761, step time: 1.0302\n",
      "48/295, train_loss: 0.0461, step time: 1.0488\n",
      "49/295, train_loss: 0.1219, step time: 1.0410\n",
      "50/295, train_loss: 0.0771, step time: 1.0387\n",
      "51/295, train_loss: 0.1924, step time: 1.0377\n",
      "52/295, train_loss: 0.0588, step time: 1.0623\n",
      "53/295, train_loss: 0.3588, step time: 1.0889\n",
      "54/295, train_loss: 0.1235, step time: 1.0359\n",
      "55/295, train_loss: 0.1833, step time: 1.0396\n",
      "56/295, train_loss: 0.4507, step time: 1.0594\n",
      "57/295, train_loss: 0.1089, step time: 1.0384\n",
      "58/295, train_loss: 0.0960, step time: 1.0377\n",
      "59/295, train_loss: 0.4775, step time: 1.0566\n",
      "60/295, train_loss: 0.1415, step time: 1.1379\n",
      "61/295, train_loss: 0.1010, step time: 1.0322\n",
      "62/295, train_loss: 0.0721, step time: 1.0662\n",
      "63/295, train_loss: 0.0954, step time: 1.0325\n",
      "64/295, train_loss: 0.1435, step time: 1.0628\n",
      "65/295, train_loss: 0.3737, step time: 1.0508\n",
      "66/295, train_loss: 0.0533, step time: 1.0424\n",
      "67/295, train_loss: 0.1052, step time: 1.0633\n",
      "68/295, train_loss: 0.0856, step time: 1.0526\n",
      "69/295, train_loss: 0.0662, step time: 1.0456\n",
      "70/295, train_loss: 0.1976, step time: 1.0464\n",
      "71/295, train_loss: 0.0670, step time: 1.0361\n",
      "72/295, train_loss: 0.1738, step time: 1.0470\n",
      "73/295, train_loss: 0.1343, step time: 1.0476\n",
      "74/295, train_loss: 0.1203, step time: 1.0743\n",
      "75/295, train_loss: 0.0857, step time: 1.0371\n",
      "76/295, train_loss: 0.0613, step time: 1.0541\n",
      "77/295, train_loss: 0.1006, step time: 1.0910\n",
      "78/295, train_loss: 0.0480, step time: 1.0325\n",
      "79/295, train_loss: 0.0624, step time: 1.0359\n",
      "80/295, train_loss: 0.0867, step time: 1.0501\n",
      "81/295, train_loss: 0.0928, step time: 1.0530\n",
      "82/295, train_loss: 0.0658, step time: 1.0535\n",
      "83/295, train_loss: 0.2242, step time: 1.0523\n",
      "84/295, train_loss: 0.4160, step time: 1.0335\n",
      "85/295, train_loss: 0.2415, step time: 1.0358\n",
      "86/295, train_loss: 0.0648, step time: 1.0480\n",
      "87/295, train_loss: 0.0575, step time: 1.0372\n",
      "88/295, train_loss: 0.0560, step time: 1.0563\n",
      "89/295, train_loss: 0.6463, step time: 1.0370\n",
      "90/295, train_loss: 0.3036, step time: 1.0321\n",
      "91/295, train_loss: 0.2391, step time: 1.0406\n",
      "92/295, train_loss: 0.2906, step time: 1.0586\n",
      "93/295, train_loss: 0.0686, step time: 1.0535\n",
      "94/295, train_loss: 0.0751, step time: 1.0412\n",
      "95/295, train_loss: 0.2135, step time: 1.0414\n",
      "96/295, train_loss: 0.0414, step time: 1.0463\n",
      "97/295, train_loss: 0.1032, step time: 1.0383\n",
      "98/295, train_loss: 0.1394, step time: 1.0519\n",
      "99/295, train_loss: 0.1878, step time: 1.0313\n",
      "100/295, train_loss: 0.1222, step time: 1.0572\n",
      "101/295, train_loss: 0.2517, step time: 1.0424\n",
      "102/295, train_loss: 0.1500, step time: 1.0892\n",
      "103/295, train_loss: 0.1943, step time: 1.1442\n",
      "104/295, train_loss: 0.3870, step time: 1.0342\n",
      "105/295, train_loss: 0.1558, step time: 1.0647\n",
      "106/295, train_loss: 0.0798, step time: 1.0371\n",
      "107/295, train_loss: 0.0605, step time: 1.0554\n",
      "108/295, train_loss: 0.1830, step time: 1.0989\n",
      "109/295, train_loss: 0.4457, step time: 1.0322\n",
      "110/295, train_loss: 0.1658, step time: 1.0437\n",
      "111/295, train_loss: 0.1225, step time: 1.0376\n",
      "112/295, train_loss: 0.1401, step time: 1.0401\n",
      "113/295, train_loss: 0.0390, step time: 1.0579\n",
      "114/295, train_loss: 0.1068, step time: 1.0541\n",
      "115/295, train_loss: 0.2200, step time: 1.0934\n",
      "116/295, train_loss: 0.1633, step time: 1.0639\n",
      "117/295, train_loss: 0.4025, step time: 1.0458\n",
      "118/295, train_loss: 0.0951, step time: 1.0601\n",
      "119/295, train_loss: 0.1603, step time: 1.0772\n",
      "120/295, train_loss: 0.1595, step time: 1.0544\n",
      "121/295, train_loss: 0.4147, step time: 1.0638\n",
      "122/295, train_loss: 0.0586, step time: 1.0780\n",
      "123/295, train_loss: 0.4021, step time: 1.0392\n",
      "124/295, train_loss: 0.0742, step time: 1.0503\n",
      "125/295, train_loss: 0.0915, step time: 1.0508\n",
      "126/295, train_loss: 0.1192, step time: 1.0387\n",
      "127/295, train_loss: 0.0886, step time: 1.0346\n",
      "128/295, train_loss: 0.1038, step time: 1.1768\n",
      "129/295, train_loss: 0.0843, step time: 1.0524\n",
      "130/295, train_loss: 0.0742, step time: 1.0447\n",
      "131/295, train_loss: 0.0989, step time: 1.1156\n",
      "132/295, train_loss: 0.1258, step time: 1.0412\n",
      "133/295, train_loss: 0.1689, step time: 1.0549\n",
      "134/295, train_loss: 0.4445, step time: 1.0351\n",
      "135/295, train_loss: 0.0847, step time: 1.0348\n",
      "136/295, train_loss: 0.0526, step time: 1.0406\n",
      "137/295, train_loss: 0.1925, step time: 1.0321\n",
      "138/295, train_loss: 0.0642, step time: 1.0356\n",
      "139/295, train_loss: 0.1097, step time: 1.0497\n",
      "140/295, train_loss: 0.3288, step time: 1.0376\n",
      "141/295, train_loss: 0.0555, step time: 1.0446\n",
      "142/295, train_loss: 0.0574, step time: 1.0437\n",
      "143/295, train_loss: 0.0840, step time: 1.0399\n",
      "144/295, train_loss: 0.4693, step time: 1.0322\n",
      "145/295, train_loss: 0.1254, step time: 1.0570\n",
      "146/295, train_loss: 0.4058, step time: 1.1438\n",
      "147/295, train_loss: 0.0855, step time: 1.1118\n",
      "148/295, train_loss: 0.0852, step time: 1.0353\n",
      "149/295, train_loss: 0.0907, step time: 1.0556\n",
      "150/295, train_loss: 0.3755, step time: 1.0506\n",
      "151/295, train_loss: 0.0746, step time: 1.0369\n",
      "152/295, train_loss: 0.0971, step time: 1.0386\n",
      "153/295, train_loss: 0.0785, step time: 1.0452\n",
      "154/295, train_loss: 0.2377, step time: 1.1440\n",
      "155/295, train_loss: 0.0575, step time: 1.0386\n",
      "156/295, train_loss: 0.0952, step time: 1.0350\n",
      "157/295, train_loss: 0.1683, step time: 1.0547\n",
      "158/295, train_loss: 0.2672, step time: 1.1202\n",
      "159/295, train_loss: 0.0923, step time: 1.0554\n",
      "160/295, train_loss: 0.0799, step time: 1.0595\n",
      "161/295, train_loss: 0.1363, step time: 1.0368\n",
      "162/295, train_loss: 0.1007, step time: 1.0405\n",
      "163/295, train_loss: 0.4203, step time: 1.0432\n",
      "164/295, train_loss: 0.0859, step time: 1.0392\n",
      "165/295, train_loss: 0.1481, step time: 1.0389\n",
      "166/295, train_loss: 0.1122, step time: 1.0331\n",
      "167/295, train_loss: 0.0592, step time: 1.0359\n",
      "168/295, train_loss: 0.1356, step time: 1.0343\n",
      "169/295, train_loss: 0.0645, step time: 1.0879\n",
      "170/295, train_loss: 0.0980, step time: 1.0489\n",
      "171/295, train_loss: 0.1619, step time: 1.1032\n",
      "172/295, train_loss: 0.0980, step time: 1.0361\n",
      "173/295, train_loss: 0.0939, step time: 1.0514\n",
      "174/295, train_loss: 0.0924, step time: 1.0439\n",
      "175/295, train_loss: 0.1699, step time: 1.0348\n",
      "176/295, train_loss: 0.0860, step time: 1.0991\n",
      "177/295, train_loss: 0.2389, step time: 1.0599\n",
      "178/295, train_loss: 0.2153, step time: 1.0433\n",
      "179/295, train_loss: 0.1252, step time: 1.0420\n",
      "180/295, train_loss: 0.0792, step time: 1.1005\n",
      "181/295, train_loss: 0.0937, step time: 1.0543\n",
      "182/295, train_loss: 0.0693, step time: 1.0405\n",
      "183/295, train_loss: 0.1342, step time: 1.0789\n",
      "184/295, train_loss: 0.0798, step time: 1.0451\n",
      "185/295, train_loss: 0.0425, step time: 1.0413\n",
      "186/295, train_loss: 0.1126, step time: 1.0529\n",
      "187/295, train_loss: 0.1495, step time: 1.0498\n",
      "188/295, train_loss: 0.0593, step time: 1.0560\n",
      "189/295, train_loss: 0.0386, step time: 1.0416\n",
      "190/295, train_loss: 0.0551, step time: 1.0853\n",
      "191/295, train_loss: 0.0802, step time: 1.0330\n",
      "192/295, train_loss: 0.1194, step time: 1.0510\n",
      "193/295, train_loss: 0.5575, step time: 1.0362\n",
      "194/295, train_loss: 0.1476, step time: 1.0572\n",
      "195/295, train_loss: 0.0810, step time: 1.0534\n",
      "196/295, train_loss: 0.0661, step time: 1.1548\n",
      "197/295, train_loss: 0.0854, step time: 1.0450\n",
      "198/295, train_loss: 0.1157, step time: 1.0421\n",
      "199/295, train_loss: 0.0429, step time: 1.0454\n",
      "200/295, train_loss: 0.0582, step time: 1.0434\n",
      "201/295, train_loss: 0.1345, step time: 1.0357\n",
      "202/295, train_loss: 0.1142, step time: 1.0458\n",
      "203/295, train_loss: 0.1109, step time: 1.0495\n",
      "204/295, train_loss: 0.1545, step time: 1.0513\n",
      "205/295, train_loss: 0.3753, step time: 1.0363\n",
      "206/295, train_loss: 0.1322, step time: 1.1060\n",
      "207/295, train_loss: 0.0978, step time: 1.0416\n",
      "208/295, train_loss: 0.0863, step time: 1.0695\n",
      "209/295, train_loss: 0.1231, step time: 1.0957\n",
      "210/295, train_loss: 0.3410, step time: 1.0414\n",
      "211/295, train_loss: 0.0411, step time: 1.0353\n",
      "212/295, train_loss: 0.0666, step time: 1.0345\n",
      "213/295, train_loss: 0.0589, step time: 1.0453\n",
      "214/295, train_loss: 0.1315, step time: 1.0364\n",
      "215/295, train_loss: 0.0991, step time: 1.0691\n",
      "216/295, train_loss: 0.0773, step time: 1.0365\n",
      "217/295, train_loss: 0.0701, step time: 1.0324\n",
      "218/295, train_loss: 0.2150, step time: 1.0341\n",
      "219/295, train_loss: 0.2603, step time: 1.0351\n",
      "220/295, train_loss: 0.0516, step time: 1.1015\n",
      "221/295, train_loss: 0.3391, step time: 1.0414\n",
      "222/295, train_loss: 0.0569, step time: 1.0889\n",
      "223/295, train_loss: 0.3723, step time: 1.0373\n",
      "224/295, train_loss: 0.0860, step time: 1.0360\n",
      "225/295, train_loss: 0.1274, step time: 1.0402\n",
      "226/295, train_loss: 0.0642, step time: 1.0682\n",
      "227/295, train_loss: 0.0479, step time: 1.0847\n",
      "228/295, train_loss: 0.2220, step time: 1.0448\n",
      "229/295, train_loss: 0.0952, step time: 1.0731\n",
      "230/295, train_loss: 0.0482, step time: 1.0340\n",
      "231/295, train_loss: 0.3347, step time: 1.0353\n",
      "232/295, train_loss: 0.1123, step time: 1.1127\n",
      "233/295, train_loss: 0.0523, step time: 1.0481\n",
      "234/295, train_loss: 0.1078, step time: 1.0463\n",
      "235/295, train_loss: 0.3711, step time: 1.0335\n",
      "236/295, train_loss: 0.2960, step time: 1.0576\n",
      "237/295, train_loss: 0.0463, step time: 1.0386\n",
      "238/295, train_loss: 0.0569, step time: 1.0373\n",
      "239/295, train_loss: 0.4137, step time: 1.0410\n",
      "240/295, train_loss: 0.1331, step time: 1.0574\n",
      "241/295, train_loss: 0.0953, step time: 1.0415\n",
      "242/295, train_loss: 0.0944, step time: 1.0354\n",
      "243/295, train_loss: 0.0562, step time: 1.0433\n",
      "244/295, train_loss: 0.1804, step time: 1.0393\n",
      "245/295, train_loss: 0.1293, step time: 1.0421\n",
      "246/295, train_loss: 0.1148, step time: 1.0431\n",
      "247/295, train_loss: 0.0987, step time: 1.0372\n",
      "248/295, train_loss: 0.1050, step time: 1.0360\n",
      "249/295, train_loss: 0.1473, step time: 1.1356\n",
      "250/295, train_loss: 0.1203, step time: 1.0434\n",
      "251/295, train_loss: 0.1486, step time: 1.0432\n",
      "252/295, train_loss: 0.2099, step time: 1.0652\n",
      "253/295, train_loss: 0.1090, step time: 1.0472\n",
      "254/295, train_loss: 0.1534, step time: 1.0871\n",
      "255/295, train_loss: 0.0882, step time: 1.0836\n",
      "256/295, train_loss: 0.1883, step time: 1.0707\n",
      "257/295, train_loss: 0.1644, step time: 1.0838\n",
      "258/295, train_loss: 0.0787, step time: 1.0484\n",
      "259/295, train_loss: 0.2210, step time: 1.0678\n",
      "260/295, train_loss: 0.0537, step time: 1.0380\n",
      "261/295, train_loss: 0.1652, step time: 1.0314\n",
      "262/295, train_loss: 0.0821, step time: 1.0314\n",
      "263/295, train_loss: 0.0785, step time: 1.0324\n",
      "264/295, train_loss: 0.1266, step time: 1.0445\n",
      "265/295, train_loss: 0.0777, step time: 1.1019\n",
      "266/295, train_loss: 0.0528, step time: 1.0674\n",
      "267/295, train_loss: 0.1109, step time: 1.0412\n",
      "268/295, train_loss: 0.0709, step time: 1.0695\n",
      "269/295, train_loss: 0.4152, step time: 1.0328\n",
      "270/295, train_loss: 0.1187, step time: 1.0666\n",
      "271/295, train_loss: 0.3992, step time: 1.0442\n",
      "272/295, train_loss: 0.4616, step time: 1.0587\n",
      "273/295, train_loss: 0.0728, step time: 1.0539\n",
      "274/295, train_loss: 0.0983, step time: 1.0400\n",
      "275/295, train_loss: 0.1314, step time: 1.0390\n",
      "276/295, train_loss: 0.1935, step time: 1.0414\n",
      "277/295, train_loss: 0.0933, step time: 1.0423\n",
      "278/295, train_loss: 0.1127, step time: 1.0491\n",
      "279/295, train_loss: 0.1296, step time: 1.0420\n",
      "280/295, train_loss: 0.1655, step time: 1.0852\n",
      "281/295, train_loss: 0.0399, step time: 1.0319\n",
      "282/295, train_loss: 0.0985, step time: 1.0365\n",
      "283/295, train_loss: 0.1274, step time: 1.0549\n",
      "284/295, train_loss: 0.1564, step time: 1.0441\n",
      "285/295, train_loss: 0.0747, step time: 1.0378\n",
      "286/295, train_loss: 0.0543, step time: 1.0617\n",
      "287/295, train_loss: 0.0836, step time: 1.0422\n",
      "288/295, train_loss: 0.0739, step time: 1.0551\n",
      "289/295, train_loss: 0.2856, step time: 1.0298\n",
      "290/295, train_loss: 0.1244, step time: 1.0299\n",
      "291/295, train_loss: 0.2075, step time: 1.0337\n",
      "292/295, train_loss: 0.0960, step time: 1.0294\n",
      "293/295, train_loss: 0.1194, step time: 1.0294\n",
      "294/295, train_loss: 0.0792, step time: 1.0290\n",
      "295/295, train_loss: 0.5283, step time: 1.0281\n",
      "epoch 18 average loss: 0.1517\n",
      "current epoch: 18 current mean dice: 0.7719 tc: 0.7118 wt: 0.8532 et: 0.7672\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 18 is: 393.2668\n",
      "----------\n",
      "epoch 19/100\n",
      "1/295, train_loss: 0.3108, step time: 1.1839\n",
      "2/295, train_loss: 0.1124, step time: 1.1466\n",
      "3/295, train_loss: 0.0612, step time: 1.0910\n",
      "4/295, train_loss: 0.0948, step time: 1.1321\n",
      "5/295, train_loss: 0.1255, step time: 1.0462\n",
      "6/295, train_loss: 0.1223, step time: 1.0444\n",
      "7/295, train_loss: 0.0910, step time: 1.0326\n",
      "8/295, train_loss: 0.4362, step time: 1.0319\n",
      "9/295, train_loss: 0.1761, step time: 1.0332\n",
      "10/295, train_loss: 0.0468, step time: 1.0329\n",
      "11/295, train_loss: 0.1327, step time: 1.0321\n",
      "12/295, train_loss: 0.1640, step time: 1.0349\n",
      "13/295, train_loss: 0.1065, step time: 1.0358\n",
      "14/295, train_loss: 0.2254, step time: 1.0468\n",
      "15/295, train_loss: 0.2850, step time: 1.0380\n",
      "16/295, train_loss: 0.6162, step time: 1.0369\n",
      "17/295, train_loss: 0.0896, step time: 1.1026\n",
      "18/295, train_loss: 0.0993, step time: 1.0797\n",
      "19/295, train_loss: 0.1461, step time: 1.0346\n",
      "20/295, train_loss: 0.0753, step time: 1.0556\n",
      "21/295, train_loss: 0.0790, step time: 1.0665\n",
      "22/295, train_loss: 0.0923, step time: 1.0594\n",
      "23/295, train_loss: 0.3874, step time: 1.0834\n",
      "24/295, train_loss: 0.2120, step time: 1.1016\n",
      "25/295, train_loss: 0.3101, step time: 1.0427\n",
      "26/295, train_loss: 0.4166, step time: 1.0525\n",
      "27/295, train_loss: 0.1631, step time: 1.0335\n",
      "28/295, train_loss: 0.0592, step time: 1.0427\n",
      "29/295, train_loss: 0.2607, step time: 1.0777\n",
      "30/295, train_loss: 0.1732, step time: 1.0553\n",
      "31/295, train_loss: 0.1187, step time: 1.0492\n",
      "32/295, train_loss: 0.1099, step time: 1.0468\n",
      "33/295, train_loss: 0.0612, step time: 1.0501\n",
      "34/295, train_loss: 0.1624, step time: 1.0754\n",
      "35/295, train_loss: 0.0851, step time: 1.0566\n",
      "36/295, train_loss: 0.1167, step time: 1.0318\n",
      "37/295, train_loss: 0.1915, step time: 1.0348\n",
      "38/295, train_loss: 0.0560, step time: 1.0564\n",
      "39/295, train_loss: 0.1797, step time: 1.0397\n",
      "40/295, train_loss: 0.3784, step time: 1.0392\n",
      "41/295, train_loss: 0.1228, step time: 1.0394\n",
      "42/295, train_loss: 0.0590, step time: 1.0436\n",
      "43/295, train_loss: 0.0840, step time: 1.0577\n",
      "44/295, train_loss: 0.1333, step time: 1.0306\n",
      "45/295, train_loss: 0.0937, step time: 1.0507\n",
      "46/295, train_loss: 0.5587, step time: 1.0581\n",
      "47/295, train_loss: 0.0927, step time: 1.0323\n",
      "48/295, train_loss: 0.1905, step time: 1.0436\n",
      "49/295, train_loss: 0.1525, step time: 1.0349\n",
      "50/295, train_loss: 0.0801, step time: 1.0504\n",
      "51/295, train_loss: 0.1388, step time: 1.0568\n",
      "52/295, train_loss: 0.0795, step time: 1.0383\n",
      "53/295, train_loss: 0.1152, step time: 1.0400\n",
      "54/295, train_loss: 0.0820, step time: 1.0369\n",
      "55/295, train_loss: 0.1685, step time: 1.0671\n",
      "56/295, train_loss: 0.1315, step time: 1.1066\n",
      "57/295, train_loss: 0.3278, step time: 1.0489\n",
      "58/295, train_loss: 0.1412, step time: 1.0389\n",
      "59/295, train_loss: 0.0588, step time: 1.0353\n",
      "60/295, train_loss: 0.1693, step time: 1.0400\n",
      "61/295, train_loss: 0.1089, step time: 1.0986\n",
      "62/295, train_loss: 0.1361, step time: 1.0653\n",
      "63/295, train_loss: 0.2650, step time: 1.0750\n",
      "64/295, train_loss: 0.3679, step time: 1.0416\n",
      "65/295, train_loss: 0.1037, step time: 1.0334\n",
      "66/295, train_loss: 0.0576, step time: 1.0496\n",
      "67/295, train_loss: 0.0581, step time: 1.0595\n",
      "68/295, train_loss: 0.1798, step time: 1.0509\n",
      "69/295, train_loss: 0.0455, step time: 1.0610\n",
      "70/295, train_loss: 0.0779, step time: 1.0472\n",
      "71/295, train_loss: 0.1151, step time: 1.0526\n",
      "72/295, train_loss: 0.5281, step time: 1.0346\n",
      "73/295, train_loss: 0.1181, step time: 1.0448\n",
      "74/295, train_loss: 0.1242, step time: 1.0369\n",
      "75/295, train_loss: 0.1562, step time: 1.0578\n",
      "76/295, train_loss: 0.0623, step time: 1.0761\n",
      "77/295, train_loss: 0.0552, step time: 1.0669\n",
      "78/295, train_loss: 0.0852, step time: 1.0858\n",
      "79/295, train_loss: 0.0808, step time: 1.0397\n",
      "80/295, train_loss: 0.2082, step time: 1.0356\n",
      "81/295, train_loss: 0.1335, step time: 1.0442\n",
      "82/295, train_loss: 0.3724, step time: 1.1041\n",
      "83/295, train_loss: 0.0623, step time: 1.0388\n",
      "84/295, train_loss: 0.2393, step time: 1.0505\n",
      "85/295, train_loss: 0.1307, step time: 1.0442\n",
      "86/295, train_loss: 0.0559, step time: 1.0398\n",
      "87/295, train_loss: 0.2125, step time: 1.0792\n",
      "88/295, train_loss: 0.0540, step time: 1.0433\n",
      "89/295, train_loss: 0.1222, step time: 1.0461\n",
      "90/295, train_loss: 0.0720, step time: 1.0451\n",
      "91/295, train_loss: 0.1493, step time: 1.0546\n",
      "92/295, train_loss: 0.0964, step time: 1.0397\n",
      "93/295, train_loss: 0.3957, step time: 1.0327\n",
      "94/295, train_loss: 0.3860, step time: 1.0445\n",
      "95/295, train_loss: 0.0450, step time: 1.0948\n",
      "96/295, train_loss: 0.2095, step time: 1.0428\n",
      "97/295, train_loss: 0.0478, step time: 1.0452\n",
      "98/295, train_loss: 0.1530, step time: 1.0552\n",
      "99/295, train_loss: 0.2175, step time: 1.0415\n",
      "100/295, train_loss: 0.1358, step time: 1.0414\n",
      "101/295, train_loss: 0.0547, step time: 1.0390\n",
      "102/295, train_loss: 0.2581, step time: 1.0509\n",
      "103/295, train_loss: 0.0748, step time: 1.0376\n",
      "104/295, train_loss: 0.4701, step time: 1.0577\n",
      "105/295, train_loss: 0.0719, step time: 1.0356\n",
      "106/295, train_loss: 0.0826, step time: 1.0406\n",
      "107/295, train_loss: 0.4411, step time: 1.0330\n",
      "108/295, train_loss: 0.4407, step time: 1.1018\n",
      "109/295, train_loss: 0.3581, step time: 1.0322\n",
      "110/295, train_loss: 0.0818, step time: 1.0627\n",
      "111/295, train_loss: 0.2997, step time: 1.0624\n",
      "112/295, train_loss: 0.4125, step time: 1.0494\n",
      "113/295, train_loss: 0.1078, step time: 1.0540\n",
      "114/295, train_loss: 0.0711, step time: 1.0479\n",
      "115/295, train_loss: 0.1029, step time: 1.0313\n",
      "116/295, train_loss: 0.1921, step time: 1.0439\n",
      "117/295, train_loss: 0.4120, step time: 1.0396\n",
      "118/295, train_loss: 0.3439, step time: 1.0370\n",
      "119/295, train_loss: 0.0844, step time: 1.0393\n",
      "120/295, train_loss: 0.1061, step time: 1.0515\n",
      "121/295, train_loss: 0.5048, step time: 1.0637\n",
      "122/295, train_loss: 0.3940, step time: 1.0345\n",
      "123/295, train_loss: 0.0913, step time: 1.0330\n",
      "124/295, train_loss: 0.1616, step time: 1.0371\n",
      "125/295, train_loss: 0.0633, step time: 1.0389\n",
      "126/295, train_loss: 0.1037, step time: 1.0385\n",
      "127/295, train_loss: 0.1260, step time: 1.0542\n",
      "128/295, train_loss: 0.4411, step time: 1.0422\n",
      "129/295, train_loss: 0.0438, step time: 1.0392\n",
      "130/295, train_loss: 0.1210, step time: 1.0369\n",
      "131/295, train_loss: 0.1073, step time: 1.0711\n",
      "132/295, train_loss: 0.0728, step time: 1.0532\n",
      "133/295, train_loss: 0.1225, step time: 1.0394\n",
      "134/295, train_loss: 0.1339, step time: 1.0341\n",
      "135/295, train_loss: 0.4760, step time: 1.0382\n",
      "136/295, train_loss: 0.1054, step time: 1.0365\n",
      "137/295, train_loss: 0.0560, step time: 1.0462\n",
      "138/295, train_loss: 0.1059, step time: 1.0352\n",
      "139/295, train_loss: 0.1418, step time: 1.0442\n",
      "140/295, train_loss: 0.1180, step time: 1.0428\n",
      "141/295, train_loss: 0.0455, step time: 1.0601\n",
      "142/295, train_loss: 0.0637, step time: 1.0389\n",
      "143/295, train_loss: 0.0440, step time: 1.0441\n",
      "144/295, train_loss: 0.1529, step time: 1.0345\n",
      "145/295, train_loss: 0.0616, step time: 1.0346\n",
      "146/295, train_loss: 0.0839, step time: 1.0373\n",
      "147/295, train_loss: 0.0675, step time: 1.0400\n",
      "148/295, train_loss: 0.0494, step time: 1.0363\n",
      "149/295, train_loss: 0.0972, step time: 1.0374\n",
      "150/295, train_loss: 0.0648, step time: 1.0361\n",
      "151/295, train_loss: 0.1910, step time: 1.0770\n",
      "152/295, train_loss: 0.0636, step time: 1.0688\n",
      "153/295, train_loss: 0.0620, step time: 1.0495\n",
      "154/295, train_loss: 0.1473, step time: 1.0498\n",
      "155/295, train_loss: 0.0717, step time: 1.0378\n",
      "156/295, train_loss: 0.3041, step time: 1.0431\n",
      "157/295, train_loss: 0.0856, step time: 1.0705\n",
      "158/295, train_loss: 0.0677, step time: 1.0800\n",
      "159/295, train_loss: 0.0441, step time: 1.0349\n",
      "160/295, train_loss: 0.1256, step time: 1.0704\n",
      "161/295, train_loss: 0.0905, step time: 1.0328\n",
      "162/295, train_loss: 0.2915, step time: 1.0336\n",
      "163/295, train_loss: 0.0415, step time: 1.0398\n",
      "164/295, train_loss: 0.0457, step time: 1.0370\n",
      "165/295, train_loss: 0.1281, step time: 1.0605\n",
      "166/295, train_loss: 0.0491, step time: 1.0327\n",
      "167/295, train_loss: 0.2000, step time: 1.0364\n",
      "168/295, train_loss: 0.1203, step time: 1.0397\n",
      "169/295, train_loss: 0.1552, step time: 1.0353\n",
      "170/295, train_loss: 0.1211, step time: 1.0377\n",
      "171/295, train_loss: 0.0816, step time: 1.0457\n",
      "172/295, train_loss: 0.4380, step time: 1.0404\n",
      "173/295, train_loss: 0.2988, step time: 1.0448\n",
      "174/295, train_loss: 0.0890, step time: 1.0388\n",
      "175/295, train_loss: 0.1160, step time: 1.0444\n",
      "176/295, train_loss: 0.1792, step time: 1.0716\n",
      "177/295, train_loss: 0.1183, step time: 1.0611\n",
      "178/295, train_loss: 0.0722, step time: 1.0484\n",
      "179/295, train_loss: 0.1878, step time: 1.0354\n",
      "180/295, train_loss: 0.1827, step time: 1.0612\n",
      "181/295, train_loss: 0.0585, step time: 1.0379\n",
      "182/295, train_loss: 0.1116, step time: 1.0461\n",
      "183/295, train_loss: 0.2023, step time: 1.0434\n",
      "184/295, train_loss: 0.0833, step time: 1.0432\n",
      "185/295, train_loss: 0.1466, step time: 1.0411\n",
      "186/295, train_loss: 0.1023, step time: 1.0389\n",
      "187/295, train_loss: 0.1140, step time: 1.0332\n",
      "188/295, train_loss: 0.1071, step time: 1.0401\n",
      "189/295, train_loss: 0.1096, step time: 1.0390\n",
      "190/295, train_loss: 0.0655, step time: 1.0420\n",
      "191/295, train_loss: 0.0604, step time: 1.0531\n",
      "192/295, train_loss: 0.1902, step time: 1.0439\n",
      "193/295, train_loss: 0.1240, step time: 1.0358\n",
      "194/295, train_loss: 0.0721, step time: 1.0333\n",
      "195/295, train_loss: 0.0459, step time: 1.0391\n",
      "196/295, train_loss: 0.0489, step time: 1.0681\n",
      "197/295, train_loss: 0.1238, step time: 1.0486\n",
      "198/295, train_loss: 0.0775, step time: 1.0331\n",
      "199/295, train_loss: 0.1243, step time: 1.0371\n",
      "200/295, train_loss: 0.0973, step time: 1.0347\n",
      "201/295, train_loss: 0.4101, step time: 1.0375\n",
      "202/295, train_loss: 0.0597, step time: 1.0449\n",
      "203/295, train_loss: 0.0475, step time: 1.0645\n",
      "204/295, train_loss: 0.1071, step time: 1.0591\n",
      "205/295, train_loss: 0.0743, step time: 1.0478\n",
      "206/295, train_loss: 0.5267, step time: 1.0326\n",
      "207/295, train_loss: 0.0682, step time: 1.0314\n",
      "208/295, train_loss: 0.0461, step time: 1.0494\n",
      "209/295, train_loss: 0.1966, step time: 1.0333\n",
      "210/295, train_loss: 0.0890, step time: 1.0380\n",
      "211/295, train_loss: 0.0777, step time: 1.0792\n",
      "212/295, train_loss: 0.0789, step time: 1.0327\n",
      "213/295, train_loss: 0.0540, step time: 1.0373\n",
      "214/295, train_loss: 0.1075, step time: 1.0341\n",
      "215/295, train_loss: 0.1099, step time: 1.0364\n",
      "216/295, train_loss: 0.1958, step time: 1.0408\n",
      "217/295, train_loss: 0.2856, step time: 1.0658\n",
      "218/295, train_loss: 0.0779, step time: 1.0428\n",
      "219/295, train_loss: 0.3797, step time: 1.0389\n",
      "220/295, train_loss: 0.0649, step time: 1.0385\n",
      "221/295, train_loss: 0.0615, step time: 1.0296\n",
      "222/295, train_loss: 0.0870, step time: 1.0397\n",
      "223/295, train_loss: 0.1836, step time: 1.0473\n",
      "224/295, train_loss: 0.4049, step time: 1.0364\n",
      "225/295, train_loss: 0.1062, step time: 1.0643\n",
      "226/295, train_loss: 0.0408, step time: 1.0375\n",
      "227/295, train_loss: 0.0768, step time: 1.0423\n",
      "228/295, train_loss: 0.0851, step time: 1.0357\n",
      "229/295, train_loss: 0.1228, step time: 1.0371\n",
      "230/295, train_loss: 0.0589, step time: 1.0414\n",
      "231/295, train_loss: 0.1191, step time: 1.0523\n",
      "232/295, train_loss: 0.0649, step time: 1.0590\n",
      "233/295, train_loss: 0.1791, step time: 1.0381\n",
      "234/295, train_loss: 0.1144, step time: 1.0628\n",
      "235/295, train_loss: 0.2766, step time: 1.0531\n",
      "236/295, train_loss: 0.3795, step time: 1.0481\n",
      "237/295, train_loss: 0.5061, step time: 1.0333\n",
      "238/295, train_loss: 0.0867, step time: 1.0429\n",
      "239/295, train_loss: 0.0859, step time: 1.0398\n",
      "240/295, train_loss: 0.1221, step time: 1.0395\n",
      "241/295, train_loss: 0.1600, step time: 1.0914\n",
      "242/295, train_loss: 0.0424, step time: 1.0847\n",
      "243/295, train_loss: 0.0799, step time: 1.0365\n",
      "244/295, train_loss: 0.0492, step time: 1.0458\n",
      "245/295, train_loss: 0.1036, step time: 1.0404\n",
      "246/295, train_loss: 0.0876, step time: 1.0393\n",
      "247/295, train_loss: 0.1364, step time: 1.0461\n",
      "248/295, train_loss: 0.0473, step time: 1.0575\n",
      "249/295, train_loss: 0.1316, step time: 1.0414\n",
      "250/295, train_loss: 0.0519, step time: 1.0426\n",
      "251/295, train_loss: 0.1042, step time: 1.0580\n",
      "252/295, train_loss: 0.0881, step time: 1.0342\n",
      "253/295, train_loss: 0.1351, step time: 1.0342\n",
      "254/295, train_loss: 0.0844, step time: 1.0306\n",
      "255/295, train_loss: 0.4683, step time: 1.0504\n",
      "256/295, train_loss: 0.1860, step time: 1.0405\n",
      "257/295, train_loss: 0.4141, step time: 1.0593\n",
      "258/295, train_loss: 0.0663, step time: 1.0494\n",
      "259/295, train_loss: 0.0570, step time: 1.0370\n",
      "260/295, train_loss: 0.1639, step time: 1.0526\n",
      "261/295, train_loss: 0.1286, step time: 1.0464\n",
      "262/295, train_loss: 0.3586, step time: 1.0379\n",
      "263/295, train_loss: 0.2166, step time: 1.0343\n",
      "264/295, train_loss: 0.1362, step time: 1.0471\n",
      "265/295, train_loss: 0.1231, step time: 1.0673\n",
      "266/295, train_loss: 0.0791, step time: 1.0433\n",
      "267/295, train_loss: 0.1665, step time: 1.0369\n",
      "268/295, train_loss: 0.0511, step time: 1.0536\n",
      "269/295, train_loss: 0.0638, step time: 1.0675\n",
      "270/295, train_loss: 0.1052, step time: 1.0393\n",
      "271/295, train_loss: 0.0843, step time: 1.0369\n",
      "272/295, train_loss: 0.0696, step time: 1.0382\n",
      "273/295, train_loss: 0.0986, step time: 1.0367\n",
      "274/295, train_loss: 0.0807, step time: 1.0321\n",
      "275/295, train_loss: 0.1378, step time: 1.0489\n",
      "276/295, train_loss: 0.1032, step time: 1.0428\n",
      "277/295, train_loss: 0.0463, step time: 1.0374\n",
      "278/295, train_loss: 0.0931, step time: 1.0399\n",
      "279/295, train_loss: 0.1552, step time: 1.0508\n",
      "280/295, train_loss: 0.0769, step time: 1.0392\n",
      "281/295, train_loss: 0.1630, step time: 1.0389\n",
      "282/295, train_loss: 0.0416, step time: 1.0391\n",
      "283/295, train_loss: 0.1186, step time: 1.0358\n",
      "284/295, train_loss: 0.1370, step time: 1.0344\n",
      "285/295, train_loss: 0.0741, step time: 1.0390\n",
      "286/295, train_loss: 0.0799, step time: 1.0332\n",
      "287/295, train_loss: 0.0826, step time: 1.0361\n",
      "288/295, train_loss: 0.1404, step time: 1.0431\n",
      "289/295, train_loss: 0.0966, step time: 1.0291\n",
      "290/295, train_loss: 0.0831, step time: 1.0295\n",
      "291/295, train_loss: 0.1154, step time: 1.0318\n",
      "292/295, train_loss: 0.0760, step time: 1.0295\n",
      "293/295, train_loss: 0.1470, step time: 1.0291\n",
      "294/295, train_loss: 0.2226, step time: 1.0282\n",
      "295/295, train_loss: 0.1549, step time: 1.0275\n",
      "epoch 19 average loss: 0.1500\n",
      "current epoch: 19 current mean dice: 0.7747 tc: 0.7202 wt: 0.8527 et: 0.7587\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 19 is: 378.9964\n",
      "----------\n",
      "epoch 20/100\n",
      "1/295, train_loss: 0.0552, step time: 1.0804\n",
      "2/295, train_loss: 0.0416, step time: 1.1116\n",
      "3/295, train_loss: 0.0828, step time: 1.0478\n",
      "4/295, train_loss: 0.4502, step time: 1.0801\n",
      "5/295, train_loss: 0.4557, step time: 1.0374\n",
      "6/295, train_loss: 0.1869, step time: 1.0553\n",
      "7/295, train_loss: 0.1292, step time: 1.0383\n",
      "8/295, train_loss: 0.0886, step time: 1.0373\n",
      "9/295, train_loss: 0.1165, step time: 1.0302\n",
      "10/295, train_loss: 0.0618, step time: 1.0544\n",
      "11/295, train_loss: 0.1329, step time: 1.0620\n",
      "12/295, train_loss: 0.0516, step time: 1.0627\n",
      "13/295, train_loss: 0.0545, step time: 1.0442\n",
      "14/295, train_loss: 0.0909, step time: 1.0457\n",
      "15/295, train_loss: 0.3706, step time: 1.0815\n",
      "16/295, train_loss: 0.1705, step time: 1.0390\n",
      "17/295, train_loss: 0.0491, step time: 1.0343\n",
      "18/295, train_loss: 0.1429, step time: 1.0304\n",
      "19/295, train_loss: 0.0614, step time: 1.0470\n",
      "20/295, train_loss: 0.0818, step time: 1.0318\n",
      "21/295, train_loss: 0.1218, step time: 1.0309\n",
      "22/295, train_loss: 0.0544, step time: 1.0335\n",
      "23/295, train_loss: 0.0746, step time: 1.0376\n",
      "24/295, train_loss: 0.1036, step time: 1.0685\n",
      "25/295, train_loss: 0.1234, step time: 1.0449\n",
      "26/295, train_loss: 0.1365, step time: 1.0516\n",
      "27/295, train_loss: 0.1371, step time: 1.0367\n",
      "28/295, train_loss: 0.0509, step time: 1.0352\n",
      "29/295, train_loss: 0.0456, step time: 1.0526\n",
      "30/295, train_loss: 0.0689, step time: 1.0458\n",
      "31/295, train_loss: 0.4311, step time: 1.0679\n",
      "32/295, train_loss: 0.1003, step time: 1.0342\n",
      "33/295, train_loss: 0.0435, step time: 1.0460\n",
      "34/295, train_loss: 0.1083, step time: 1.0418\n",
      "35/295, train_loss: 0.0472, step time: 1.0454\n",
      "36/295, train_loss: 0.0554, step time: 1.0473\n",
      "37/295, train_loss: 0.4536, step time: 1.0337\n",
      "38/295, train_loss: 0.0700, step time: 1.0453\n",
      "39/295, train_loss: 0.0714, step time: 1.0801\n",
      "40/295, train_loss: 0.0469, step time: 1.0647\n",
      "41/295, train_loss: 0.0720, step time: 1.0578\n",
      "42/295, train_loss: 0.0518, step time: 1.0346\n",
      "43/295, train_loss: 0.0422, step time: 1.0347\n",
      "44/295, train_loss: 0.0899, step time: 1.0364\n",
      "45/295, train_loss: 0.0874, step time: 1.0506\n",
      "46/295, train_loss: 0.5283, step time: 1.0432\n",
      "47/295, train_loss: 0.0920, step time: 1.0706\n",
      "48/295, train_loss: 0.4096, step time: 1.0718\n",
      "49/295, train_loss: 0.1170, step time: 1.0366\n",
      "50/295, train_loss: 0.6192, step time: 1.0520\n",
      "51/295, train_loss: 0.0732, step time: 1.0530\n",
      "52/295, train_loss: 0.0846, step time: 1.0348\n",
      "53/295, train_loss: 0.4509, step time: 1.0438\n",
      "54/295, train_loss: 0.1671, step time: 1.0318\n",
      "55/295, train_loss: 0.0710, step time: 1.1165\n",
      "56/295, train_loss: 0.0807, step time: 1.0375\n",
      "57/295, train_loss: 0.0306, step time: 1.0571\n",
      "58/295, train_loss: 0.0842, step time: 1.0528\n",
      "59/295, train_loss: 0.0945, step time: 1.0309\n",
      "60/295, train_loss: 0.1656, step time: 1.0393\n",
      "61/295, train_loss: 0.2252, step time: 1.0334\n",
      "62/295, train_loss: 0.1176, step time: 1.0382\n",
      "63/295, train_loss: 0.2403, step time: 1.0365\n",
      "64/295, train_loss: 0.0771, step time: 1.0395\n",
      "65/295, train_loss: 0.0503, step time: 1.0732\n",
      "66/295, train_loss: 0.0771, step time: 1.0349\n",
      "67/295, train_loss: 0.0631, step time: 1.0353\n",
      "68/295, train_loss: 0.0848, step time: 1.0386\n",
      "69/295, train_loss: 0.0506, step time: 1.0560\n",
      "70/295, train_loss: 0.0480, step time: 1.1145\n",
      "71/295, train_loss: 0.1099, step time: 1.0310\n",
      "72/295, train_loss: 0.0525, step time: 1.1107\n",
      "73/295, train_loss: 0.1140, step time: 1.0480\n",
      "74/295, train_loss: 0.3831, step time: 1.0636\n",
      "75/295, train_loss: 0.0818, step time: 1.0327\n",
      "76/295, train_loss: 0.0551, step time: 1.0418\n",
      "77/295, train_loss: 0.0880, step time: 1.0958\n",
      "78/295, train_loss: 0.0869, step time: 1.0379\n",
      "79/295, train_loss: 0.1631, step time: 1.0528\n",
      "80/295, train_loss: 0.1220, step time: 1.1106\n",
      "81/295, train_loss: 0.0905, step time: 1.0336\n",
      "82/295, train_loss: 0.1011, step time: 1.0483\n",
      "83/295, train_loss: 0.0747, step time: 1.0721\n",
      "84/295, train_loss: 0.1182, step time: 1.1385\n",
      "85/295, train_loss: 0.0792, step time: 1.0387\n",
      "86/295, train_loss: 0.1076, step time: 1.0387\n",
      "87/295, train_loss: 0.1324, step time: 1.0458\n",
      "88/295, train_loss: 0.1248, step time: 1.0542\n",
      "89/295, train_loss: 0.1177, step time: 1.0876\n",
      "90/295, train_loss: 0.4305, step time: 1.0379\n",
      "91/295, train_loss: 0.1710, step time: 1.0629\n",
      "92/295, train_loss: 0.1663, step time: 1.0640\n",
      "93/295, train_loss: 0.4853, step time: 1.0336\n",
      "94/295, train_loss: 0.0505, step time: 1.0352\n",
      "95/295, train_loss: 0.0884, step time: 1.0317\n",
      "96/295, train_loss: 0.3477, step time: 1.1015\n",
      "97/295, train_loss: 0.0550, step time: 1.0720\n",
      "98/295, train_loss: 0.2152, step time: 1.0489\n",
      "99/295, train_loss: 0.1097, step time: 1.0378\n",
      "100/295, train_loss: 0.2102, step time: 1.0581\n",
      "101/295, train_loss: 0.2514, step time: 1.0414\n",
      "102/295, train_loss: 0.0845, step time: 1.0384\n",
      "103/295, train_loss: 0.1069, step time: 1.0374\n",
      "104/295, train_loss: 0.0868, step time: 1.0654\n",
      "105/295, train_loss: 0.1157, step time: 1.0722\n",
      "106/295, train_loss: 0.1651, step time: 1.0464\n",
      "107/295, train_loss: 0.0605, step time: 1.0508\n",
      "108/295, train_loss: 0.1173, step time: 1.0344\n",
      "109/295, train_loss: 0.2600, step time: 1.0326\n",
      "110/295, train_loss: 0.2158, step time: 1.0377\n",
      "111/295, train_loss: 0.5417, step time: 1.0464\n",
      "112/295, train_loss: 0.1697, step time: 1.0330\n",
      "113/295, train_loss: 0.2087, step time: 1.0603\n",
      "114/295, train_loss: 0.1626, step time: 1.0406\n",
      "115/295, train_loss: 0.0396, step time: 1.0628\n",
      "116/295, train_loss: 0.4123, step time: 1.0679\n",
      "117/295, train_loss: 0.2375, step time: 1.0599\n",
      "118/295, train_loss: 0.0586, step time: 1.0485\n",
      "119/295, train_loss: 0.1158, step time: 1.0773\n",
      "120/295, train_loss: 0.0906, step time: 1.0598\n",
      "121/295, train_loss: 0.2905, step time: 1.0351\n",
      "122/295, train_loss: 0.2501, step time: 1.0438\n",
      "123/295, train_loss: 0.0403, step time: 1.0395\n",
      "124/295, train_loss: 0.0761, step time: 1.0779\n",
      "125/295, train_loss: 0.2619, step time: 1.0339\n",
      "126/295, train_loss: 0.1991, step time: 1.0371\n",
      "127/295, train_loss: 0.1446, step time: 1.0621\n",
      "128/295, train_loss: 0.0432, step time: 1.0395\n",
      "129/295, train_loss: 0.0438, step time: 1.0393\n",
      "130/295, train_loss: 0.2818, step time: 1.1055\n",
      "131/295, train_loss: 0.0991, step time: 1.1052\n",
      "132/295, train_loss: 0.2520, step time: 1.0555\n",
      "133/295, train_loss: 0.0553, step time: 1.0350\n",
      "134/295, train_loss: 0.2361, step time: 1.0450\n",
      "135/295, train_loss: 0.0644, step time: 1.0531\n",
      "136/295, train_loss: 0.0984, step time: 1.0392\n",
      "137/295, train_loss: 0.1229, step time: 1.0377\n",
      "138/295, train_loss: 0.0839, step time: 1.0364\n",
      "139/295, train_loss: 0.1077, step time: 1.0352\n",
      "140/295, train_loss: 0.1874, step time: 1.0374\n",
      "141/295, train_loss: 0.1071, step time: 1.0498\n",
      "142/295, train_loss: 0.0619, step time: 1.0937\n",
      "143/295, train_loss: 0.2713, step time: 1.0630\n",
      "144/295, train_loss: 0.1487, step time: 1.0374\n",
      "145/295, train_loss: 0.0958, step time: 1.0547\n",
      "146/295, train_loss: 0.0941, step time: 1.0336\n",
      "147/295, train_loss: 0.1143, step time: 1.0476\n",
      "148/295, train_loss: 0.0721, step time: 1.0513\n",
      "149/295, train_loss: 0.0693, step time: 1.0359\n",
      "150/295, train_loss: 0.4481, step time: 1.0372\n",
      "151/295, train_loss: 0.0843, step time: 1.1168\n",
      "152/295, train_loss: 0.0762, step time: 1.0353\n",
      "153/295, train_loss: 0.1600, step time: 1.0375\n",
      "154/295, train_loss: 0.0745, step time: 1.0517\n",
      "155/295, train_loss: 0.1071, step time: 1.0416\n",
      "156/295, train_loss: 0.0527, step time: 1.0472\n",
      "157/295, train_loss: 0.0820, step time: 1.0654\n",
      "158/295, train_loss: 0.1088, step time: 1.0328\n",
      "159/295, train_loss: 0.1398, step time: 1.0329\n",
      "160/295, train_loss: 0.1299, step time: 1.0617\n",
      "161/295, train_loss: 0.1309, step time: 1.0512\n",
      "162/295, train_loss: 0.0701, step time: 1.0504\n",
      "163/295, train_loss: 0.0588, step time: 1.0325\n",
      "164/295, train_loss: 0.1207, step time: 1.0369\n",
      "165/295, train_loss: 0.0671, step time: 1.0393\n",
      "166/295, train_loss: 0.4050, step time: 1.0353\n",
      "167/295, train_loss: 0.5045, step time: 1.0349\n",
      "168/295, train_loss: 0.0505, step time: 1.0416\n",
      "169/295, train_loss: 0.1395, step time: 1.0682\n",
      "170/295, train_loss: 0.0447, step time: 1.1146\n",
      "171/295, train_loss: 0.1403, step time: 1.0632\n",
      "172/295, train_loss: 0.4152, step time: 1.0475\n",
      "173/295, train_loss: 0.0830, step time: 1.0378\n",
      "174/295, train_loss: 0.4614, step time: 1.0442\n",
      "175/295, train_loss: 0.0528, step time: 1.0573\n",
      "176/295, train_loss: 0.1239, step time: 1.0964\n",
      "177/295, train_loss: 0.0979, step time: 1.0336\n",
      "178/295, train_loss: 0.1089, step time: 1.0609\n",
      "179/295, train_loss: 0.1633, step time: 1.0318\n",
      "180/295, train_loss: 0.0879, step time: 1.0433\n",
      "181/295, train_loss: 0.0429, step time: 1.0628\n",
      "182/295, train_loss: 0.0635, step time: 1.0529\n",
      "183/295, train_loss: 0.0687, step time: 1.0390\n",
      "184/295, train_loss: 0.1257, step time: 1.0423\n",
      "185/295, train_loss: 0.0946, step time: 1.0708\n",
      "186/295, train_loss: 0.0728, step time: 1.0381\n",
      "187/295, train_loss: 0.1625, step time: 1.1024\n",
      "188/295, train_loss: 0.1965, step time: 1.0370\n",
      "189/295, train_loss: 0.0713, step time: 1.0422\n",
      "190/295, train_loss: 0.1178, step time: 1.0329\n",
      "191/295, train_loss: 0.0533, step time: 1.0459\n",
      "192/295, train_loss: 0.0896, step time: 1.0407\n",
      "193/295, train_loss: 0.1180, step time: 1.0331\n",
      "194/295, train_loss: 0.2914, step time: 1.0651\n",
      "195/295, train_loss: 0.0765, step time: 1.0349\n",
      "196/295, train_loss: 0.0971, step time: 1.0401\n",
      "197/295, train_loss: 0.3816, step time: 1.0377\n",
      "198/295, train_loss: 0.4076, step time: 1.0838\n",
      "199/295, train_loss: 0.1452, step time: 1.0414\n",
      "200/295, train_loss: 0.1237, step time: 1.0365\n",
      "201/295, train_loss: 0.0615, step time: 1.0365\n",
      "202/295, train_loss: 0.1356, step time: 1.0358\n",
      "203/295, train_loss: 0.1301, step time: 1.0439\n",
      "204/295, train_loss: 0.0553, step time: 1.0357\n",
      "205/295, train_loss: 0.1096, step time: 1.0378\n",
      "206/295, train_loss: 0.1839, step time: 1.0318\n",
      "207/295, train_loss: 0.0927, step time: 1.0324\n",
      "208/295, train_loss: 0.0708, step time: 1.0325\n",
      "209/295, train_loss: 0.1791, step time: 1.0580\n",
      "210/295, train_loss: 0.1794, step time: 1.0313\n",
      "211/295, train_loss: 0.0677, step time: 1.0549\n",
      "212/295, train_loss: 0.1602, step time: 1.0343\n",
      "213/295, train_loss: 0.4618, step time: 1.0453\n",
      "214/295, train_loss: 0.1378, step time: 1.0418\n",
      "215/295, train_loss: 0.0411, step time: 1.0604\n",
      "216/295, train_loss: 0.2057, step time: 1.0431\n",
      "217/295, train_loss: 0.2261, step time: 1.0408\n",
      "218/295, train_loss: 0.0785, step time: 1.0450\n",
      "219/295, train_loss: 0.1008, step time: 1.0506\n",
      "220/295, train_loss: 0.4363, step time: 1.0554\n",
      "221/295, train_loss: 0.1363, step time: 1.0492\n",
      "222/295, train_loss: 0.1811, step time: 1.0374\n",
      "223/295, train_loss: 0.2068, step time: 1.0453\n",
      "224/295, train_loss: 0.3164, step time: 1.1099\n",
      "225/295, train_loss: 0.1525, step time: 1.0362\n",
      "226/295, train_loss: 0.4109, step time: 1.0354\n",
      "227/295, train_loss: 0.2822, step time: 1.0579\n",
      "228/295, train_loss: 0.0816, step time: 1.1027\n",
      "229/295, train_loss: 0.0792, step time: 1.0480\n",
      "230/295, train_loss: 0.0817, step time: 1.0592\n",
      "231/295, train_loss: 0.3223, step time: 1.0463\n",
      "232/295, train_loss: 0.0747, step time: 1.0569\n",
      "233/295, train_loss: 0.0980, step time: 1.0391\n",
      "234/295, train_loss: 0.0702, step time: 1.0417\n",
      "235/295, train_loss: 0.0559, step time: 1.0448\n",
      "236/295, train_loss: 0.4268, step time: 1.0922\n",
      "237/295, train_loss: 0.1676, step time: 1.0395\n",
      "238/295, train_loss: 0.1012, step time: 1.0349\n",
      "239/295, train_loss: 0.0942, step time: 1.0359\n",
      "240/295, train_loss: 0.3897, step time: 1.0691\n",
      "241/295, train_loss: 0.1403, step time: 1.1522\n",
      "242/295, train_loss: 0.1125, step time: 1.0571\n",
      "243/295, train_loss: 0.3436, step time: 1.0433\n",
      "244/295, train_loss: 0.1985, step time: 1.0429\n",
      "245/295, train_loss: 0.0658, step time: 1.0433\n",
      "246/295, train_loss: 0.0632, step time: 1.0418\n",
      "247/295, train_loss: 0.1123, step time: 1.0612\n",
      "248/295, train_loss: 0.1437, step time: 1.0356\n",
      "249/295, train_loss: 0.0828, step time: 1.0752\n",
      "250/295, train_loss: 0.0732, step time: 1.0387\n",
      "251/295, train_loss: 0.0725, step time: 1.0396\n",
      "252/295, train_loss: 0.1029, step time: 1.0443\n",
      "253/295, train_loss: 0.1007, step time: 1.0336\n",
      "254/295, train_loss: 0.1150, step time: 1.0628\n",
      "255/295, train_loss: 0.0949, step time: 1.0908\n",
      "256/295, train_loss: 0.0555, step time: 1.0366\n",
      "257/295, train_loss: 0.0777, step time: 1.0680\n",
      "258/295, train_loss: 0.2138, step time: 1.0599\n",
      "259/295, train_loss: 0.0737, step time: 1.0363\n",
      "260/295, train_loss: 0.0593, step time: 1.0376\n",
      "261/295, train_loss: 0.1834, step time: 1.0365\n",
      "262/295, train_loss: 0.1748, step time: 1.0368\n",
      "263/295, train_loss: 0.0897, step time: 1.0640\n",
      "264/295, train_loss: 0.1115, step time: 1.0351\n",
      "265/295, train_loss: 0.3962, step time: 1.0371\n",
      "266/295, train_loss: 0.0986, step time: 1.0337\n",
      "267/295, train_loss: 0.0651, step time: 1.0347\n",
      "268/295, train_loss: 0.1068, step time: 1.0413\n",
      "269/295, train_loss: 0.1172, step time: 1.0396\n",
      "270/295, train_loss: 0.1677, step time: 1.1350\n",
      "271/295, train_loss: 0.1315, step time: 1.0776\n",
      "272/295, train_loss: 0.0702, step time: 1.0340\n",
      "273/295, train_loss: 0.1057, step time: 1.0468\n",
      "274/295, train_loss: 0.3828, step time: 1.0370\n",
      "275/295, train_loss: 0.0669, step time: 1.0404\n",
      "276/295, train_loss: 0.0676, step time: 1.0366\n",
      "277/295, train_loss: 0.0695, step time: 1.0487\n",
      "278/295, train_loss: 0.2051, step time: 1.0413\n",
      "279/295, train_loss: 0.1016, step time: 1.0432\n",
      "280/295, train_loss: 0.0619, step time: 1.0530\n",
      "281/295, train_loss: 0.1372, step time: 1.0591\n",
      "282/295, train_loss: 0.1077, step time: 1.0898\n",
      "283/295, train_loss: 0.1887, step time: 1.0619\n",
      "284/295, train_loss: 0.0504, step time: 1.0593\n",
      "285/295, train_loss: 0.0485, step time: 1.0555\n",
      "286/295, train_loss: 0.4371, step time: 1.0417\n",
      "287/295, train_loss: 0.0966, step time: 1.0315\n",
      "288/295, train_loss: 0.1057, step time: 1.0297\n",
      "289/295, train_loss: 0.0882, step time: 1.0289\n",
      "290/295, train_loss: 0.1015, step time: 1.0303\n",
      "291/295, train_loss: 0.0712, step time: 1.0295\n",
      "292/295, train_loss: 0.1000, step time: 1.0291\n",
      "293/295, train_loss: 0.0682, step time: 1.0297\n",
      "294/295, train_loss: 0.1513, step time: 1.0283\n",
      "295/295, train_loss: 0.0865, step time: 1.0285\n",
      "epoch 20 average loss: 0.1458\n",
      "current epoch: 20 current mean dice: 0.7600 tc: 0.6997 wt: 0.8343 et: 0.7505\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 20 is: 383.6668\n",
      "----------\n",
      "epoch 21/100\n",
      "1/295, train_loss: 0.0907, step time: 1.0917\n",
      "2/295, train_loss: 0.4922, step time: 1.1158\n",
      "3/295, train_loss: 0.1030, step time: 1.0625\n",
      "4/295, train_loss: 0.0789, step time: 1.0678\n",
      "5/295, train_loss: 0.6081, step time: 1.0611\n",
      "6/295, train_loss: 0.0398, step time: 1.0373\n",
      "7/295, train_loss: 0.1169, step time: 1.1181\n",
      "8/295, train_loss: 0.1147, step time: 1.0395\n",
      "9/295, train_loss: 0.1322, step time: 1.0958\n",
      "10/295, train_loss: 0.3912, step time: 1.0374\n",
      "11/295, train_loss: 0.1095, step time: 1.0380\n",
      "12/295, train_loss: 0.1513, step time: 1.0509\n",
      "13/295, train_loss: 0.5031, step time: 1.0010\n",
      "14/295, train_loss: 0.1433, step time: 1.0388\n",
      "15/295, train_loss: 0.0502, step time: 1.0489\n",
      "16/295, train_loss: 0.0852, step time: 1.0668\n",
      "17/295, train_loss: 0.0942, step time: 1.0454\n",
      "18/295, train_loss: 0.0754, step time: 1.0598\n",
      "19/295, train_loss: 0.4395, step time: 1.0410\n",
      "20/295, train_loss: 0.1169, step time: 1.0401\n",
      "21/295, train_loss: 0.1105, step time: 1.0505\n",
      "22/295, train_loss: 0.3930, step time: 1.1133\n",
      "23/295, train_loss: 0.0817, step time: 1.0358\n",
      "24/295, train_loss: 0.1232, step time: 1.0428\n",
      "25/295, train_loss: 0.0787, step time: 1.0440\n",
      "26/295, train_loss: 0.1405, step time: 1.0597\n",
      "27/295, train_loss: 0.3763, step time: 1.0314\n",
      "28/295, train_loss: 0.0736, step time: 1.0712\n",
      "29/295, train_loss: 0.1214, step time: 1.0375\n",
      "30/295, train_loss: 0.2660, step time: 1.0399\n",
      "31/295, train_loss: 0.0433, step time: 1.0949\n",
      "32/295, train_loss: 0.0797, step time: 1.0881\n",
      "33/295, train_loss: 0.0495, step time: 1.0469\n",
      "34/295, train_loss: 0.0915, step time: 1.0556\n",
      "35/295, train_loss: 0.2273, step time: 1.0343\n",
      "36/295, train_loss: 0.3742, step time: 1.0658\n",
      "37/295, train_loss: 0.0581, step time: 1.0478\n",
      "38/295, train_loss: 0.4336, step time: 1.0492\n",
      "39/295, train_loss: 0.1132, step time: 1.0405\n",
      "40/295, train_loss: 0.3178, step time: 1.0391\n",
      "41/295, train_loss: 0.3198, step time: 1.0334\n",
      "42/295, train_loss: 0.1983, step time: 1.0413\n",
      "43/295, train_loss: 0.0588, step time: 1.0343\n",
      "44/295, train_loss: 0.1078, step time: 1.0523\n",
      "45/295, train_loss: 0.1532, step time: 1.0491\n",
      "46/295, train_loss: 0.1076, step time: 1.0534\n",
      "47/295, train_loss: 0.1683, step time: 1.0371\n",
      "48/295, train_loss: 0.1014, step time: 1.0358\n",
      "49/295, train_loss: 0.0473, step time: 1.0456\n",
      "50/295, train_loss: 0.1328, step time: 1.0345\n",
      "51/295, train_loss: 0.1037, step time: 1.0468\n",
      "52/295, train_loss: 0.0691, step time: 1.0679\n",
      "53/295, train_loss: 0.0612, step time: 1.0507\n",
      "54/295, train_loss: 0.1003, step time: 1.0605\n",
      "55/295, train_loss: 0.0782, step time: 1.0354\n",
      "56/295, train_loss: 0.0709, step time: 1.0542\n",
      "57/295, train_loss: 0.3304, step time: 1.0493\n",
      "58/295, train_loss: 0.1411, step time: 1.0307\n",
      "59/295, train_loss: 0.1170, step time: 1.0593\n",
      "60/295, train_loss: 0.3326, step time: 1.0762\n",
      "61/295, train_loss: 0.1283, step time: 1.0598\n",
      "62/295, train_loss: 0.0794, step time: 1.0479\n",
      "63/295, train_loss: 0.0945, step time: 1.0379\n",
      "64/295, train_loss: 0.1355, step time: 1.0558\n",
      "65/295, train_loss: 0.4107, step time: 1.0416\n",
      "66/295, train_loss: 0.0939, step time: 1.0376\n",
      "67/295, train_loss: 0.0381, step time: 1.0521\n",
      "68/295, train_loss: 0.0956, step time: 1.0401\n",
      "69/295, train_loss: 0.1707, step time: 1.0399\n",
      "70/295, train_loss: 0.1205, step time: 1.0751\n",
      "71/295, train_loss: 0.1582, step time: 1.0324\n",
      "72/295, train_loss: 0.1532, step time: 1.0349\n",
      "73/295, train_loss: 0.0614, step time: 1.0389\n",
      "74/295, train_loss: 0.1451, step time: 1.0341\n",
      "75/295, train_loss: 0.3195, step time: 1.0526\n",
      "76/295, train_loss: 0.0893, step time: 1.0356\n",
      "77/295, train_loss: 0.0484, step time: 1.0486\n",
      "78/295, train_loss: 0.4388, step time: 1.0558\n",
      "79/295, train_loss: 0.1274, step time: 1.0753\n",
      "80/295, train_loss: 0.1240, step time: 1.0646\n",
      "81/295, train_loss: 0.0842, step time: 1.0348\n",
      "82/295, train_loss: 0.1464, step time: 1.1164\n",
      "83/295, train_loss: 0.0767, step time: 1.0364\n",
      "84/295, train_loss: 0.0877, step time: 1.0336\n",
      "85/295, train_loss: 0.0611, step time: 1.0709\n",
      "86/295, train_loss: 0.0481, step time: 1.0623\n",
      "87/295, train_loss: 0.2066, step time: 1.0592\n",
      "88/295, train_loss: 0.1093, step time: 1.1343\n",
      "89/295, train_loss: 0.0600, step time: 1.0393\n",
      "90/295, train_loss: 0.1474, step time: 1.0596\n",
      "91/295, train_loss: 0.1335, step time: 1.0450\n",
      "92/295, train_loss: 0.1569, step time: 1.0385\n",
      "93/295, train_loss: 0.0936, step time: 1.0343\n",
      "94/295, train_loss: 0.1067, step time: 1.0401\n",
      "95/295, train_loss: 0.0621, step time: 1.0483\n",
      "96/295, train_loss: 0.0679, step time: 1.0810\n",
      "97/295, train_loss: 0.0690, step time: 1.0366\n",
      "98/295, train_loss: 0.0643, step time: 1.0619\n",
      "99/295, train_loss: 0.0737, step time: 1.0789\n",
      "100/295, train_loss: 0.0797, step time: 1.0382\n",
      "101/295, train_loss: 0.0793, step time: 1.0385\n",
      "102/295, train_loss: 0.0688, step time: 1.0507\n",
      "103/295, train_loss: 0.0554, step time: 1.0511\n",
      "104/295, train_loss: 0.1461, step time: 1.0387\n",
      "105/295, train_loss: 0.2156, step time: 1.0364\n",
      "106/295, train_loss: 0.0834, step time: 1.0443\n",
      "107/295, train_loss: 0.1453, step time: 1.0491\n",
      "108/295, train_loss: 0.0671, step time: 1.0362\n",
      "109/295, train_loss: 0.0954, step time: 1.0420\n",
      "110/295, train_loss: 0.3307, step time: 1.0704\n",
      "111/295, train_loss: 0.1486, step time: 1.0360\n",
      "112/295, train_loss: 0.3706, step time: 1.1039\n",
      "113/295, train_loss: 0.1217, step time: 1.0353\n",
      "114/295, train_loss: 0.1302, step time: 1.0373\n",
      "115/295, train_loss: 0.1628, step time: 1.0443\n",
      "116/295, train_loss: 0.0529, step time: 1.0343\n",
      "117/295, train_loss: 0.0963, step time: 1.0365\n",
      "118/295, train_loss: 0.0498, step time: 1.0690\n",
      "119/295, train_loss: 0.0881, step time: 1.0438\n",
      "120/295, train_loss: 0.0570, step time: 1.0346\n",
      "121/295, train_loss: 0.2383, step time: 1.0345\n",
      "122/295, train_loss: 0.4366, step time: 1.0377\n",
      "123/295, train_loss: 0.1035, step time: 1.0617\n",
      "124/295, train_loss: 0.1042, step time: 1.0604\n",
      "125/295, train_loss: 0.0543, step time: 1.0442\n",
      "126/295, train_loss: 0.0766, step time: 1.1136\n",
      "127/295, train_loss: 0.0825, step time: 1.0404\n",
      "128/295, train_loss: 0.0674, step time: 1.0493\n",
      "129/295, train_loss: 0.0464, step time: 1.0611\n",
      "130/295, train_loss: 0.2159, step time: 1.0748\n",
      "131/295, train_loss: 0.1236, step time: 1.0526\n",
      "132/295, train_loss: 0.0996, step time: 1.0343\n",
      "133/295, train_loss: 0.0641, step time: 1.0424\n",
      "134/295, train_loss: 0.4329, step time: 1.0614\n",
      "135/295, train_loss: 0.0565, step time: 1.0338\n",
      "136/295, train_loss: 0.0648, step time: 1.0393\n",
      "137/295, train_loss: 0.1318, step time: 1.0517\n",
      "138/295, train_loss: 0.4413, step time: 1.0362\n",
      "139/295, train_loss: 0.0607, step time: 1.0355\n",
      "140/295, train_loss: 0.1125, step time: 1.0375\n",
      "141/295, train_loss: 0.1209, step time: 1.0336\n",
      "142/295, train_loss: 0.1360, step time: 1.0651\n",
      "143/295, train_loss: 0.4166, step time: 1.0430\n",
      "144/295, train_loss: 0.1191, step time: 1.0471\n",
      "145/295, train_loss: 0.0558, step time: 1.0576\n",
      "146/295, train_loss: 0.5348, step time: 1.0398\n",
      "147/295, train_loss: 0.1451, step time: 1.0541\n",
      "148/295, train_loss: 0.1173, step time: 1.0404\n",
      "149/295, train_loss: 0.1268, step time: 1.0537\n",
      "150/295, train_loss: 0.0726, step time: 1.0512\n",
      "151/295, train_loss: 0.2211, step time: 1.0452\n",
      "152/295, train_loss: 0.1341, step time: 1.0335\n",
      "153/295, train_loss: 0.0518, step time: 1.0964\n",
      "154/295, train_loss: 0.0852, step time: 1.0765\n",
      "155/295, train_loss: 0.2190, step time: 1.0780\n",
      "156/295, train_loss: 0.0628, step time: 1.0520\n",
      "157/295, train_loss: 0.0853, step time: 1.0471\n",
      "158/295, train_loss: 0.0580, step time: 1.1019\n",
      "159/295, train_loss: 0.4063, step time: 1.0417\n",
      "160/295, train_loss: 0.0497, step time: 1.1402\n",
      "161/295, train_loss: 0.1643, step time: 1.0348\n",
      "162/295, train_loss: 0.0902, step time: 1.0528\n",
      "163/295, train_loss: 0.0670, step time: 1.0427\n",
      "164/295, train_loss: 0.1750, step time: 1.0386\n",
      "165/295, train_loss: 0.0915, step time: 1.0416\n",
      "166/295, train_loss: 0.4700, step time: 1.0573\n",
      "167/295, train_loss: 0.0939, step time: 1.0388\n",
      "168/295, train_loss: 0.1103, step time: 1.0350\n",
      "169/295, train_loss: 0.1004, step time: 1.0637\n",
      "170/295, train_loss: 0.1301, step time: 1.0521\n",
      "171/295, train_loss: 0.1691, step time: 1.0622\n",
      "172/295, train_loss: 0.0402, step time: 1.0489\n",
      "173/295, train_loss: 0.1125, step time: 1.0358\n",
      "174/295, train_loss: 0.0584, step time: 1.0348\n",
      "175/295, train_loss: 0.0765, step time: 1.0518\n",
      "176/295, train_loss: 0.0860, step time: 1.0451\n",
      "177/295, train_loss: 0.0686, step time: 1.0451\n",
      "178/295, train_loss: 0.0635, step time: 1.0403\n",
      "179/295, train_loss: 0.1738, step time: 1.0369\n",
      "180/295, train_loss: 0.0390, step time: 1.0619\n",
      "181/295, train_loss: 0.0919, step time: 1.0475\n",
      "182/295, train_loss: 0.1573, step time: 1.0380\n",
      "183/295, train_loss: 0.0574, step time: 1.0494\n",
      "184/295, train_loss: 0.0340, step time: 1.0320\n",
      "185/295, train_loss: 0.0547, step time: 1.1127\n",
      "186/295, train_loss: 0.1016, step time: 1.0320\n",
      "187/295, train_loss: 0.0801, step time: 1.0322\n",
      "188/295, train_loss: 0.1588, step time: 1.0380\n",
      "189/295, train_loss: 0.0765, step time: 1.0398\n",
      "190/295, train_loss: 0.0856, step time: 1.0528\n",
      "191/295, train_loss: 0.4512, step time: 1.0581\n",
      "192/295, train_loss: 0.4461, step time: 1.0587\n",
      "193/295, train_loss: 0.0534, step time: 1.0374\n",
      "194/295, train_loss: 0.0581, step time: 1.0386\n",
      "195/295, train_loss: 0.0900, step time: 1.0607\n",
      "196/295, train_loss: 0.1169, step time: 1.0352\n",
      "197/295, train_loss: 0.1655, step time: 1.0371\n",
      "198/295, train_loss: 0.1167, step time: 1.0574\n",
      "199/295, train_loss: 0.0624, step time: 1.0453\n",
      "200/295, train_loss: 0.1667, step time: 1.0782\n",
      "201/295, train_loss: 0.4051, step time: 1.0361\n",
      "202/295, train_loss: 0.1393, step time: 1.0502\n",
      "203/295, train_loss: 0.0884, step time: 1.0333\n",
      "204/295, train_loss: 0.0541, step time: 1.0347\n",
      "205/295, train_loss: 0.0775, step time: 1.0514\n",
      "206/295, train_loss: 0.2758, step time: 1.0722\n",
      "207/295, train_loss: 0.0839, step time: 1.0368\n",
      "208/295, train_loss: 0.0587, step time: 1.0516\n",
      "209/295, train_loss: 0.4075, step time: 1.0773\n",
      "210/295, train_loss: 0.2803, step time: 1.0447\n",
      "211/295, train_loss: 0.0650, step time: 1.0376\n",
      "212/295, train_loss: 0.0688, step time: 1.0812\n",
      "213/295, train_loss: 0.5269, step time: 1.0358\n",
      "214/295, train_loss: 0.1135, step time: 1.0394\n",
      "215/295, train_loss: 0.0589, step time: 1.1109\n",
      "216/295, train_loss: 0.1550, step time: 1.0684\n",
      "217/295, train_loss: 0.1612, step time: 1.1076\n",
      "218/295, train_loss: 0.1291, step time: 1.0711\n",
      "219/295, train_loss: 0.1309, step time: 1.0382\n",
      "220/295, train_loss: 0.0943, step time: 1.0532\n",
      "221/295, train_loss: 0.1546, step time: 1.0367\n",
      "222/295, train_loss: 0.0920, step time: 1.0336\n",
      "223/295, train_loss: 0.2207, step time: 1.0512\n",
      "224/295, train_loss: 0.1832, step time: 1.0346\n",
      "225/295, train_loss: 0.1820, step time: 1.0758\n",
      "226/295, train_loss: 0.1041, step time: 1.0628\n",
      "227/295, train_loss: 0.0461, step time: 1.0541\n",
      "228/295, train_loss: 0.0815, step time: 1.0354\n",
      "229/295, train_loss: 0.1172, step time: 1.0329\n",
      "230/295, train_loss: 0.0530, step time: 1.0362\n",
      "231/295, train_loss: 0.0831, step time: 1.0414\n",
      "232/295, train_loss: 0.0960, step time: 1.0470\n",
      "233/295, train_loss: 0.1245, step time: 1.0435\n",
      "234/295, train_loss: 0.0861, step time: 1.0366\n",
      "235/295, train_loss: 0.1157, step time: 1.0436\n",
      "236/295, train_loss: 0.0576, step time: 1.0408\n",
      "237/295, train_loss: 0.0444, step time: 1.0364\n",
      "238/295, train_loss: 0.1332, step time: 1.0522\n",
      "239/295, train_loss: 0.1197, step time: 1.0364\n",
      "240/295, train_loss: 0.0810, step time: 1.0436\n",
      "241/295, train_loss: 0.1973, step time: 1.0421\n",
      "242/295, train_loss: 0.0787, step time: 1.0443\n",
      "243/295, train_loss: 0.0615, step time: 1.0309\n",
      "244/295, train_loss: 0.0561, step time: 1.0347\n",
      "245/295, train_loss: 0.0708, step time: 1.0415\n",
      "246/295, train_loss: 0.1316, step time: 1.0891\n",
      "247/295, train_loss: 0.0753, step time: 1.0766\n",
      "248/295, train_loss: 0.1547, step time: 1.0805\n",
      "249/295, train_loss: 0.4412, step time: 1.0374\n",
      "250/295, train_loss: 0.0413, step time: 1.1629\n",
      "251/295, train_loss: 0.0883, step time: 1.0551\n",
      "252/295, train_loss: 0.1755, step time: 1.0378\n",
      "253/295, train_loss: 0.0484, step time: 1.0776\n",
      "254/295, train_loss: 0.0491, step time: 1.1618\n",
      "255/295, train_loss: 0.0551, step time: 1.0329\n",
      "256/295, train_loss: 0.0463, step time: 1.0460\n",
      "257/295, train_loss: 0.1727, step time: 1.0886\n",
      "258/295, train_loss: 0.3844, step time: 1.0979\n",
      "259/295, train_loss: 0.2107, step time: 1.0339\n",
      "260/295, train_loss: 0.0643, step time: 1.0530\n",
      "261/295, train_loss: 0.1057, step time: 1.0311\n",
      "262/295, train_loss: 0.0574, step time: 1.0623\n",
      "263/295, train_loss: 0.0855, step time: 1.0521\n",
      "264/295, train_loss: 0.4210, step time: 1.0362\n",
      "265/295, train_loss: 0.0953, step time: 1.0574\n",
      "266/295, train_loss: 0.3792, step time: 1.0395\n",
      "267/295, train_loss: 0.0724, step time: 1.0513\n",
      "268/295, train_loss: 0.0762, step time: 1.0624\n",
      "269/295, train_loss: 0.1334, step time: 1.0526\n",
      "270/295, train_loss: 0.0776, step time: 1.0348\n",
      "271/295, train_loss: 0.1686, step time: 1.0356\n",
      "272/295, train_loss: 0.2097, step time: 1.0845\n",
      "273/295, train_loss: 0.0552, step time: 1.0533\n",
      "274/295, train_loss: 0.6758, step time: 1.0457\n",
      "275/295, train_loss: 0.0654, step time: 1.0407\n",
      "276/295, train_loss: 0.0403, step time: 1.0369\n",
      "277/295, train_loss: 0.0714, step time: 1.0440\n",
      "278/295, train_loss: 0.0483, step time: 1.0437\n",
      "279/295, train_loss: 0.4885, step time: 1.0377\n",
      "280/295, train_loss: 0.0472, step time: 1.0383\n",
      "281/295, train_loss: 0.0930, step time: 1.0529\n",
      "282/295, train_loss: 0.0569, step time: 1.0585\n",
      "283/295, train_loss: 0.0470, step time: 1.0433\n",
      "284/295, train_loss: 0.1524, step time: 1.0357\n",
      "285/295, train_loss: 0.0794, step time: 1.0445\n",
      "286/295, train_loss: 0.0888, step time: 1.0521\n",
      "287/295, train_loss: 0.0845, step time: 1.0378\n",
      "288/295, train_loss: 0.2437, step time: 1.0300\n",
      "289/295, train_loss: 0.0708, step time: 1.0296\n",
      "290/295, train_loss: 0.1809, step time: 1.0295\n",
      "291/295, train_loss: 0.1940, step time: 1.0293\n",
      "292/295, train_loss: 0.0957, step time: 1.0295\n",
      "293/295, train_loss: 0.0726, step time: 1.0300\n",
      "294/295, train_loss: 0.1278, step time: 1.0279\n",
      "295/295, train_loss: 0.0739, step time: 1.0303\n",
      "epoch 21 average loss: 0.1432\n",
      "current epoch: 21 current mean dice: 0.7756 tc: 0.7196 wt: 0.8456 et: 0.7713\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 21 is: 386.1752\n",
      "----------\n",
      "epoch 22/100\n",
      "1/295, train_loss: 0.1462, step time: 1.0696\n",
      "2/295, train_loss: 0.0702, step time: 1.0814\n",
      "3/295, train_loss: 0.0887, step time: 1.0786\n",
      "4/295, train_loss: 0.1526, step time: 1.0810\n",
      "5/295, train_loss: 0.0520, step time: 1.0586\n",
      "6/295, train_loss: 0.0843, step time: 1.0409\n",
      "7/295, train_loss: 0.1246, step time: 1.0392\n",
      "8/295, train_loss: 0.0559, step time: 1.0378\n",
      "9/295, train_loss: 0.0435, step time: 1.0416\n",
      "10/295, train_loss: 0.0452, step time: 1.0434\n",
      "11/295, train_loss: 0.0631, step time: 1.0959\n",
      "12/295, train_loss: 0.1441, step time: 1.0680\n",
      "13/295, train_loss: 0.1376, step time: 1.0693\n",
      "14/295, train_loss: 0.0541, step time: 1.0460\n",
      "15/295, train_loss: 0.0755, step time: 1.0372\n",
      "16/295, train_loss: 0.0531, step time: 1.0493\n",
      "17/295, train_loss: 0.0539, step time: 1.0863\n",
      "18/295, train_loss: 0.0886, step time: 1.0453\n",
      "19/295, train_loss: 0.1166, step time: 1.0438\n",
      "20/295, train_loss: 0.1131, step time: 1.0373\n",
      "21/295, train_loss: 0.0460, step time: 1.0378\n",
      "22/295, train_loss: 0.0591, step time: 1.0374\n",
      "23/295, train_loss: 0.3294, step time: 1.0347\n",
      "24/295, train_loss: 0.3821, step time: 1.0869\n",
      "25/295, train_loss: 0.0569, step time: 1.0620\n",
      "26/295, train_loss: 0.1052, step time: 1.0774\n",
      "27/295, train_loss: 0.0686, step time: 1.0445\n",
      "28/295, train_loss: 0.1369, step time: 1.0486\n",
      "29/295, train_loss: 0.3659, step time: 1.0309\n",
      "30/295, train_loss: 0.2523, step time: 1.0394\n",
      "31/295, train_loss: 0.0985, step time: 1.1143\n",
      "32/295, train_loss: 0.0928, step time: 1.0366\n",
      "33/295, train_loss: 0.0391, step time: 1.0354\n",
      "34/295, train_loss: 0.0891, step time: 1.0311\n",
      "35/295, train_loss: 0.0686, step time: 1.0338\n",
      "36/295, train_loss: 0.3308, step time: 1.0822\n",
      "37/295, train_loss: 0.1065, step time: 1.0857\n",
      "38/295, train_loss: 0.0466, step time: 1.0356\n",
      "39/295, train_loss: 0.0725, step time: 1.0338\n",
      "40/295, train_loss: 0.0828, step time: 1.0664\n",
      "41/295, train_loss: 0.0709, step time: 1.0687\n",
      "42/295, train_loss: 0.0710, step time: 1.0366\n",
      "43/295, train_loss: 0.1342, step time: 1.0361\n",
      "44/295, train_loss: 0.0936, step time: 1.1100\n",
      "45/295, train_loss: 0.0476, step time: 1.0338\n",
      "46/295, train_loss: 0.0638, step time: 1.0398\n",
      "47/295, train_loss: 0.1147, step time: 1.0512\n",
      "48/295, train_loss: 0.1173, step time: 1.0354\n",
      "49/295, train_loss: 0.0922, step time: 1.0301\n",
      "50/295, train_loss: 0.1188, step time: 1.0651\n",
      "51/295, train_loss: 0.1149, step time: 1.0457\n",
      "52/295, train_loss: 0.0919, step time: 1.0574\n",
      "53/295, train_loss: 0.3574, step time: 1.0596\n",
      "54/295, train_loss: 0.0610, step time: 1.0572\n",
      "55/295, train_loss: 0.1038, step time: 1.0317\n",
      "56/295, train_loss: 0.0953, step time: 1.0370\n",
      "57/295, train_loss: 0.0670, step time: 1.0570\n",
      "58/295, train_loss: 0.1828, step time: 1.0693\n",
      "59/295, train_loss: 0.3127, step time: 1.0838\n",
      "60/295, train_loss: 0.0924, step time: 1.0380\n",
      "61/295, train_loss: 0.0640, step time: 1.0365\n",
      "62/295, train_loss: 0.4057, step time: 1.0325\n",
      "63/295, train_loss: 0.0427, step time: 1.0376\n",
      "64/295, train_loss: 0.0861, step time: 1.0555\n",
      "65/295, train_loss: 0.1234, step time: 1.0634\n",
      "66/295, train_loss: 0.1068, step time: 1.0315\n",
      "67/295, train_loss: 0.0459, step time: 1.0641\n",
      "68/295, train_loss: 0.1937, step time: 1.0435\n",
      "69/295, train_loss: 0.1548, step time: 1.0365\n",
      "70/295, train_loss: 0.0402, step time: 1.0527\n",
      "71/295, train_loss: 0.1173, step time: 1.0566\n",
      "72/295, train_loss: 0.1527, step time: 1.0611\n",
      "73/295, train_loss: 0.4587, step time: 1.0370\n",
      "74/295, train_loss: 0.1496, step time: 1.0353\n",
      "75/295, train_loss: 0.3976, step time: 1.0650\n",
      "76/295, train_loss: 0.0816, step time: 1.0378\n",
      "77/295, train_loss: 0.0688, step time: 1.0459\n",
      "78/295, train_loss: 0.3922, step time: 1.0571\n",
      "79/295, train_loss: 0.0498, step time: 1.1129\n",
      "80/295, train_loss: 0.1223, step time: 1.0425\n",
      "81/295, train_loss: 0.1763, step time: 1.0517\n",
      "82/295, train_loss: 0.0492, step time: 1.0418\n",
      "83/295, train_loss: 0.0611, step time: 1.0433\n",
      "84/295, train_loss: 0.0665, step time: 1.0647\n",
      "85/295, train_loss: 0.0760, step time: 1.0478\n",
      "86/295, train_loss: 0.0530, step time: 1.0348\n",
      "87/295, train_loss: 0.1792, step time: 1.0328\n",
      "88/295, train_loss: 0.1106, step time: 1.0800\n",
      "89/295, train_loss: 0.1490, step time: 1.0807\n",
      "90/295, train_loss: 0.0488, step time: 1.0399\n",
      "91/295, train_loss: 0.0654, step time: 1.0339\n",
      "92/295, train_loss: 0.0418, step time: 1.0649\n",
      "93/295, train_loss: 0.1332, step time: 1.1033\n",
      "94/295, train_loss: 0.1101, step time: 1.0404\n",
      "95/295, train_loss: 0.3790, step time: 1.1569\n",
      "96/295, train_loss: 0.1817, step time: 1.0320\n",
      "97/295, train_loss: 0.0820, step time: 1.0537\n",
      "98/295, train_loss: 0.1447, step time: 1.0327\n",
      "99/295, train_loss: 0.0978, step time: 1.0341\n",
      "100/295, train_loss: 0.0648, step time: 1.0467\n",
      "101/295, train_loss: 0.3993, step time: 1.0385\n",
      "102/295, train_loss: 0.0579, step time: 1.0581\n",
      "103/295, train_loss: 0.1047, step time: 1.0438\n",
      "104/295, train_loss: 0.0757, step time: 1.0769\n",
      "105/295, train_loss: 0.0643, step time: 1.0321\n",
      "106/295, train_loss: 0.0890, step time: 1.0439\n",
      "107/295, train_loss: 0.1736, step time: 1.0781\n",
      "108/295, train_loss: 0.1073, step time: 1.0392\n",
      "109/295, train_loss: 0.1516, step time: 1.0321\n",
      "110/295, train_loss: 0.1311, step time: 1.0433\n",
      "111/295, train_loss: 0.1305, step time: 1.0330\n",
      "112/295, train_loss: 0.1606, step time: 1.0565\n",
      "113/295, train_loss: 0.0403, step time: 1.0421\n",
      "114/295, train_loss: 0.0764, step time: 1.0893\n",
      "115/295, train_loss: 0.0665, step time: 1.1036\n",
      "116/295, train_loss: 0.4091, step time: 1.0345\n",
      "117/295, train_loss: 0.0375, step time: 1.0364\n",
      "118/295, train_loss: 0.1706, step time: 1.0436\n",
      "119/295, train_loss: 0.1215, step time: 1.0368\n",
      "120/295, train_loss: 0.0898, step time: 1.0625\n",
      "121/295, train_loss: 0.0905, step time: 1.0398\n",
      "122/295, train_loss: 0.0609, step time: 1.0562\n",
      "123/295, train_loss: 0.0562, step time: 1.0318\n",
      "124/295, train_loss: 0.1342, step time: 1.0388\n",
      "125/295, train_loss: 0.0370, step time: 1.0525\n",
      "126/295, train_loss: 0.1173, step time: 1.0345\n",
      "127/295, train_loss: 0.1370, step time: 1.0372\n",
      "128/295, train_loss: 0.0659, step time: 1.0653\n",
      "129/295, train_loss: 0.3930, step time: 1.0760\n",
      "130/295, train_loss: 0.0327, step time: 1.0478\n",
      "131/295, train_loss: 0.0886, step time: 1.0379\n",
      "132/295, train_loss: 0.0752, step time: 1.0516\n",
      "133/295, train_loss: 0.1141, step time: 1.0347\n",
      "134/295, train_loss: 0.1525, step time: 1.0431\n",
      "135/295, train_loss: 0.1744, step time: 1.0494\n",
      "136/295, train_loss: 0.1469, step time: 1.1200\n",
      "137/295, train_loss: 0.0820, step time: 1.0469\n",
      "138/295, train_loss: 0.0711, step time: 1.0336\n",
      "139/295, train_loss: 0.0810, step time: 1.0318\n",
      "140/295, train_loss: 0.1556, step time: 1.0335\n",
      "141/295, train_loss: 0.1322, step time: 1.0462\n",
      "142/295, train_loss: 0.0724, step time: 1.1675\n",
      "143/295, train_loss: 0.1804, step time: 1.0359\n",
      "144/295, train_loss: 0.4697, step time: 1.0391\n",
      "145/295, train_loss: 0.0933, step time: 1.0968\n",
      "146/295, train_loss: 0.1598, step time: 1.0962\n",
      "147/295, train_loss: 0.0976, step time: 1.0328\n",
      "148/295, train_loss: 0.1006, step time: 1.0444\n",
      "149/295, train_loss: 0.1024, step time: 1.0501\n",
      "150/295, train_loss: 0.0722, step time: 1.0379\n",
      "151/295, train_loss: 0.4412, step time: 1.0418\n",
      "152/295, train_loss: 0.0468, step time: 1.0576\n",
      "153/295, train_loss: 0.0709, step time: 1.0356\n",
      "154/295, train_loss: 0.0410, step time: 1.0355\n",
      "155/295, train_loss: 0.0559, step time: 1.0519\n",
      "156/295, train_loss: 0.0574, step time: 1.0660\n",
      "157/295, train_loss: 0.1641, step time: 1.0359\n",
      "158/295, train_loss: 0.0710, step time: 1.0425\n",
      "159/295, train_loss: 0.0841, step time: 1.0474\n",
      "160/295, train_loss: 0.1199, step time: 1.0663\n",
      "161/295, train_loss: 0.0722, step time: 1.0415\n",
      "162/295, train_loss: 0.4251, step time: 1.0484\n",
      "163/295, train_loss: 0.0369, step time: 1.0620\n",
      "164/295, train_loss: 0.1280, step time: 1.0399\n",
      "165/295, train_loss: 0.0749, step time: 1.0448\n",
      "166/295, train_loss: 0.4251, step time: 1.0424\n",
      "167/295, train_loss: 0.1090, step time: 1.0563\n",
      "168/295, train_loss: 0.0571, step time: 1.0311\n",
      "169/295, train_loss: 0.0643, step time: 1.1814\n",
      "170/295, train_loss: 0.4249, step time: 1.0377\n",
      "171/295, train_loss: 0.3311, step time: 1.0362\n",
      "172/295, train_loss: 0.0615, step time: 1.0398\n",
      "173/295, train_loss: 0.1024, step time: 1.0441\n",
      "174/295, train_loss: 0.1091, step time: 1.0544\n",
      "175/295, train_loss: 0.0545, step time: 1.0428\n",
      "176/295, train_loss: 0.1215, step time: 1.0373\n",
      "177/295, train_loss: 0.0451, step time: 1.0351\n",
      "178/295, train_loss: 0.1132, step time: 1.0373\n",
      "179/295, train_loss: 0.0759, step time: 1.0474\n",
      "180/295, train_loss: 0.0654, step time: 1.0356\n",
      "181/295, train_loss: 0.0953, step time: 1.0322\n",
      "182/295, train_loss: 0.2191, step time: 1.0684\n",
      "183/295, train_loss: 0.0395, step time: 1.0603\n",
      "184/295, train_loss: 0.1896, step time: 1.1162\n",
      "185/295, train_loss: 0.0443, step time: 1.0394\n",
      "186/295, train_loss: 0.4630, step time: 1.0539\n",
      "187/295, train_loss: 0.0598, step time: 1.0338\n",
      "188/295, train_loss: 0.0793, step time: 1.0634\n",
      "189/295, train_loss: 0.0594, step time: 1.0775\n",
      "190/295, train_loss: 0.2859, step time: 1.0424\n",
      "191/295, train_loss: 0.0884, step time: 1.0423\n",
      "192/295, train_loss: 0.1219, step time: 1.0370\n",
      "193/295, train_loss: 0.1310, step time: 1.0420\n",
      "194/295, train_loss: 0.2075, step time: 1.0425\n",
      "195/295, train_loss: 0.1469, step time: 1.0549\n",
      "196/295, train_loss: 0.3739, step time: 1.0378\n",
      "197/295, train_loss: 0.0925, step time: 1.0439\n",
      "198/295, train_loss: 0.3850, step time: 1.0389\n",
      "199/295, train_loss: 0.2054, step time: 1.0364\n",
      "200/295, train_loss: 0.2046, step time: 1.0437\n",
      "201/295, train_loss: 0.0531, step time: 1.0582\n",
      "202/295, train_loss: 0.0440, step time: 1.0349\n",
      "203/295, train_loss: 0.0771, step time: 1.0402\n",
      "204/295, train_loss: 0.0848, step time: 1.0355\n",
      "205/295, train_loss: 0.1889, step time: 1.0459\n",
      "206/295, train_loss: 0.1246, step time: 1.1182\n",
      "207/295, train_loss: 0.0889, step time: 1.0661\n",
      "208/295, train_loss: 0.0989, step time: 1.0902\n",
      "209/295, train_loss: 0.0921, step time: 1.0415\n",
      "210/295, train_loss: 0.1094, step time: 1.0514\n",
      "211/295, train_loss: 0.0946, step time: 1.0511\n",
      "212/295, train_loss: 0.1332, step time: 1.0375\n",
      "213/295, train_loss: 0.2435, step time: 1.0391\n",
      "214/295, train_loss: 0.1671, step time: 1.0416\n",
      "215/295, train_loss: 0.1657, step time: 1.0565\n",
      "216/295, train_loss: 0.0638, step time: 1.0442\n",
      "217/295, train_loss: 0.2144, step time: 1.0597\n",
      "218/295, train_loss: 0.0752, step time: 1.0606\n",
      "219/295, train_loss: 0.0804, step time: 1.0364\n",
      "220/295, train_loss: 0.2091, step time: 1.0368\n",
      "221/295, train_loss: 0.1166, step time: 1.0378\n",
      "222/295, train_loss: 0.0915, step time: 1.0476\n",
      "223/295, train_loss: 0.0542, step time: 1.0342\n",
      "224/295, train_loss: 0.0540, step time: 1.0430\n",
      "225/295, train_loss: 0.0789, step time: 1.0414\n",
      "226/295, train_loss: 0.0866, step time: 1.0511\n",
      "227/295, train_loss: 0.4293, step time: 1.0418\n",
      "228/295, train_loss: 0.0521, step time: 1.0410\n",
      "229/295, train_loss: 0.1044, step time: 1.0542\n",
      "230/295, train_loss: 0.0751, step time: 1.0333\n",
      "231/295, train_loss: 0.0842, step time: 1.0382\n",
      "232/295, train_loss: 0.1297, step time: 1.0408\n",
      "233/295, train_loss: 0.3182, step time: 1.0429\n",
      "234/295, train_loss: 0.5518, step time: 1.0501\n",
      "235/295, train_loss: 0.2506, step time: 1.0542\n",
      "236/295, train_loss: 0.0932, step time: 1.0572\n",
      "237/295, train_loss: 0.4591, step time: 1.0411\n",
      "238/295, train_loss: 0.0893, step time: 1.0369\n",
      "239/295, train_loss: 0.2413, step time: 1.0449\n",
      "240/295, train_loss: 0.3781, step time: 1.0710\n",
      "241/295, train_loss: 0.0702, step time: 1.0981\n",
      "242/295, train_loss: 0.4533, step time: 1.0303\n",
      "243/295, train_loss: 0.0562, step time: 1.0547\n",
      "244/295, train_loss: 0.0978, step time: 1.0405\n",
      "245/295, train_loss: 0.1252, step time: 1.0374\n",
      "246/295, train_loss: 0.4355, step time: 1.0661\n",
      "247/295, train_loss: 0.0777, step time: 1.0443\n",
      "248/295, train_loss: 0.0662, step time: 1.0370\n",
      "249/295, train_loss: 0.4115, step time: 1.0696\n",
      "250/295, train_loss: 0.1681, step time: 1.0743\n",
      "251/295, train_loss: 0.1705, step time: 1.0492\n",
      "252/295, train_loss: 0.0811, step time: 1.0353\n",
      "253/295, train_loss: 0.3162, step time: 1.0678\n",
      "254/295, train_loss: 0.1094, step time: 1.0652\n",
      "255/295, train_loss: 0.6301, step time: 1.0333\n",
      "256/295, train_loss: 0.0526, step time: 1.0358\n",
      "257/295, train_loss: 0.1027, step time: 1.0476\n",
      "258/295, train_loss: 0.3600, step time: 1.0522\n",
      "259/295, train_loss: 0.1148, step time: 1.0461\n",
      "260/295, train_loss: 0.1440, step time: 1.0547\n",
      "261/295, train_loss: 0.0784, step time: 1.0577\n",
      "262/295, train_loss: 0.0763, step time: 1.0580\n",
      "263/295, train_loss: 0.0723, step time: 1.0327\n",
      "264/295, train_loss: 0.4349, step time: 1.0324\n",
      "265/295, train_loss: 0.1100, step time: 1.0640\n",
      "266/295, train_loss: 0.0921, step time: 1.0382\n",
      "267/295, train_loss: 0.1867, step time: 1.0516\n",
      "268/295, train_loss: 0.1157, step time: 1.0385\n",
      "269/295, train_loss: 0.0829, step time: 1.0374\n",
      "270/295, train_loss: 0.1633, step time: 1.0357\n",
      "271/295, train_loss: 0.1238, step time: 1.0384\n",
      "272/295, train_loss: 0.0578, step time: 1.0315\n",
      "273/295, train_loss: 0.1525, step time: 1.0394\n",
      "274/295, train_loss: 0.1874, step time: 1.0399\n",
      "275/295, train_loss: 0.1323, step time: 1.0343\n",
      "276/295, train_loss: 0.1048, step time: 1.0371\n",
      "277/295, train_loss: 0.0676, step time: 1.0387\n",
      "278/295, train_loss: 0.4221, step time: 1.0396\n",
      "279/295, train_loss: 0.1737, step time: 1.0332\n",
      "280/295, train_loss: 0.2220, step time: 1.0383\n",
      "281/295, train_loss: 0.1588, step time: 1.0375\n",
      "282/295, train_loss: 0.0847, step time: 1.0586\n",
      "283/295, train_loss: 0.1075, step time: 1.0603\n",
      "284/295, train_loss: 0.0978, step time: 1.0448\n",
      "285/295, train_loss: 0.0648, step time: 1.0510\n",
      "286/295, train_loss: 0.1230, step time: 1.0431\n",
      "287/295, train_loss: 0.1039, step time: 1.0459\n",
      "288/295, train_loss: 0.1052, step time: 1.0453\n",
      "289/295, train_loss: 0.0685, step time: 1.0528\n",
      "290/295, train_loss: 0.2814, step time: 1.0285\n",
      "291/295, train_loss: 0.1740, step time: 1.0285\n",
      "292/295, train_loss: 0.0626, step time: 1.0292\n",
      "293/295, train_loss: 0.0512, step time: 1.0293\n",
      "294/295, train_loss: 0.0904, step time: 1.0299\n",
      "295/295, train_loss: 0.1695, step time: 1.0292\n",
      "epoch 22 average loss: 0.1405\n",
      "current epoch: 22 current mean dice: 0.7528 tc: 0.6994 wt: 0.8228 et: 0.7417\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 22 is: 381.8716\n",
      "----------\n",
      "epoch 23/100\n",
      "1/295, train_loss: 0.5665, step time: 1.1162\n",
      "2/295, train_loss: 0.3162, step time: 1.0442\n",
      "3/295, train_loss: 0.1945, step time: 1.0682\n",
      "4/295, train_loss: 0.0477, step time: 1.1348\n",
      "5/295, train_loss: 0.1280, step time: 1.1118\n",
      "6/295, train_loss: 0.0903, step time: 1.0446\n",
      "7/295, train_loss: 0.0599, step time: 1.0356\n",
      "8/295, train_loss: 0.1258, step time: 1.0335\n",
      "9/295, train_loss: 0.0693, step time: 1.0582\n",
      "10/295, train_loss: 0.1812, step time: 1.0325\n",
      "11/295, train_loss: 0.0790, step time: 1.0394\n",
      "12/295, train_loss: 0.0505, step time: 1.0389\n",
      "13/295, train_loss: 0.0695, step time: 1.0395\n",
      "14/295, train_loss: 0.1394, step time: 1.1556\n",
      "15/295, train_loss: 0.0638, step time: 1.0368\n",
      "16/295, train_loss: 0.0705, step time: 1.0457\n",
      "17/295, train_loss: 0.0440, step time: 1.0870\n",
      "18/295, train_loss: 0.0580, step time: 1.0340\n",
      "19/295, train_loss: 0.0803, step time: 1.0368\n",
      "20/295, train_loss: 0.0765, step time: 1.0577\n",
      "21/295, train_loss: 0.0964, step time: 1.0418\n",
      "22/295, train_loss: 0.0532, step time: 1.0358\n",
      "23/295, train_loss: 0.1280, step time: 1.0395\n",
      "24/295, train_loss: 0.0861, step time: 1.0510\n",
      "25/295, train_loss: 0.0723, step time: 1.0420\n",
      "26/295, train_loss: 0.0472, step time: 1.0514\n",
      "27/295, train_loss: 0.0583, step time: 1.0633\n",
      "28/295, train_loss: 0.1579, step time: 1.0405\n",
      "29/295, train_loss: 0.0546, step time: 1.0338\n",
      "30/295, train_loss: 0.2224, step time: 1.0328\n",
      "31/295, train_loss: 0.0666, step time: 1.0350\n",
      "32/295, train_loss: 0.0488, step time: 1.0362\n",
      "33/295, train_loss: 0.1159, step time: 1.0467\n",
      "34/295, train_loss: 0.0561, step time: 1.0826\n",
      "35/295, train_loss: 0.0810, step time: 1.0543\n",
      "36/295, train_loss: 0.1340, step time: 1.0421\n",
      "37/295, train_loss: 0.0431, step time: 1.0343\n",
      "38/295, train_loss: 0.0683, step time: 1.0380\n",
      "39/295, train_loss: 0.1698, step time: 1.0407\n",
      "40/295, train_loss: 0.0603, step time: 1.0495\n",
      "41/295, train_loss: 0.0499, step time: 1.0408\n",
      "42/295, train_loss: 0.4265, step time: 1.0617\n",
      "43/295, train_loss: 0.4391, step time: 1.0423\n",
      "44/295, train_loss: 0.1918, step time: 1.0316\n",
      "45/295, train_loss: 0.1209, step time: 1.0626\n",
      "46/295, train_loss: 0.0714, step time: 1.0313\n",
      "47/295, train_loss: 0.2471, step time: 1.0456\n",
      "48/295, train_loss: 0.1562, step time: 1.0378\n",
      "49/295, train_loss: 0.2023, step time: 1.0598\n",
      "50/295, train_loss: 0.1038, step time: 1.0381\n",
      "51/295, train_loss: 0.1282, step time: 1.0328\n",
      "52/295, train_loss: 0.1193, step time: 1.0554\n",
      "53/295, train_loss: 0.1651, step time: 1.0338\n",
      "54/295, train_loss: 0.1584, step time: 1.0602\n",
      "55/295, train_loss: 0.1092, step time: 1.0380\n",
      "56/295, train_loss: 0.4303, step time: 1.1089\n",
      "57/295, train_loss: 0.1622, step time: 1.0504\n",
      "58/295, train_loss: 0.1261, step time: 1.0321\n",
      "59/295, train_loss: 0.3693, step time: 1.0360\n",
      "60/295, train_loss: 0.1470, step time: 1.0561\n",
      "61/295, train_loss: 0.1569, step time: 1.1024\n",
      "62/295, train_loss: 0.0820, step time: 1.0360\n",
      "63/295, train_loss: 0.2573, step time: 1.0439\n",
      "64/295, train_loss: 0.0670, step time: 1.0762\n",
      "65/295, train_loss: 0.1475, step time: 1.0385\n",
      "66/295, train_loss: 0.1436, step time: 1.0651\n",
      "67/295, train_loss: 0.4034, step time: 1.0377\n",
      "68/295, train_loss: 0.1486, step time: 1.0492\n",
      "69/295, train_loss: 0.0759, step time: 1.0500\n",
      "70/295, train_loss: 0.1043, step time: 1.0388\n",
      "71/295, train_loss: 0.0931, step time: 1.0342\n",
      "72/295, train_loss: 0.0425, step time: 1.1086\n",
      "73/295, train_loss: 0.0505, step time: 1.0316\n",
      "74/295, train_loss: 0.0504, step time: 1.0318\n",
      "75/295, train_loss: 0.0576, step time: 1.0693\n",
      "76/295, train_loss: 0.0784, step time: 1.0383\n",
      "77/295, train_loss: 0.0467, step time: 1.0446\n",
      "78/295, train_loss: 0.0937, step time: 1.0394\n",
      "79/295, train_loss: 0.0805, step time: 1.0746\n",
      "80/295, train_loss: 0.1643, step time: 1.0565\n",
      "81/295, train_loss: 0.0753, step time: 1.0369\n",
      "82/295, train_loss: 0.0714, step time: 1.0490\n",
      "83/295, train_loss: 0.0877, step time: 1.0370\n",
      "84/295, train_loss: 0.0888, step time: 1.0382\n",
      "85/295, train_loss: 0.1292, step time: 1.0806\n",
      "86/295, train_loss: 0.1496, step time: 1.0317\n",
      "87/295, train_loss: 0.1535, step time: 1.0471\n",
      "88/295, train_loss: 0.0433, step time: 1.0556\n",
      "89/295, train_loss: 0.0718, step time: 1.0359\n",
      "90/295, train_loss: 0.1064, step time: 1.0550\n",
      "91/295, train_loss: 0.0519, step time: 1.0410\n",
      "92/295, train_loss: 0.0531, step time: 1.0380\n",
      "93/295, train_loss: 0.1159, step time: 1.0800\n",
      "94/295, train_loss: 0.0824, step time: 1.0606\n",
      "95/295, train_loss: 0.1319, step time: 1.0372\n",
      "96/295, train_loss: 0.0815, step time: 1.0362\n",
      "97/295, train_loss: 0.1022, step time: 1.0899\n",
      "98/295, train_loss: 0.0539, step time: 1.0709\n",
      "99/295, train_loss: 0.0470, step time: 1.0485\n",
      "100/295, train_loss: 0.1334, step time: 1.0572\n",
      "101/295, train_loss: 0.0660, step time: 1.0370\n",
      "102/295, train_loss: 0.1746, step time: 1.0382\n",
      "103/295, train_loss: 0.0846, step time: 1.0977\n",
      "104/295, train_loss: 0.0547, step time: 1.0587\n",
      "105/295, train_loss: 0.0695, step time: 1.0457\n",
      "106/295, train_loss: 0.0840, step time: 1.0601\n",
      "107/295, train_loss: 0.1236, step time: 1.0343\n",
      "108/295, train_loss: 0.0852, step time: 1.0498\n",
      "109/295, train_loss: 0.0530, step time: 1.0633\n",
      "110/295, train_loss: 0.1165, step time: 1.1401\n",
      "111/295, train_loss: 0.1464, step time: 1.0593\n",
      "112/295, train_loss: 0.4040, step time: 1.0532\n",
      "113/295, train_loss: 0.0754, step time: 1.0467\n",
      "114/295, train_loss: 0.4554, step time: 1.0393\n",
      "115/295, train_loss: 0.0391, step time: 1.0504\n",
      "116/295, train_loss: 0.4208, step time: 1.0441\n",
      "117/295, train_loss: 0.1135, step time: 1.0634\n",
      "118/295, train_loss: 0.0949, step time: 1.0342\n",
      "119/295, train_loss: 0.3758, step time: 1.0361\n",
      "120/295, train_loss: 0.0530, step time: 1.0331\n",
      "121/295, train_loss: 0.0907, step time: 1.0452\n",
      "122/295, train_loss: 0.0576, step time: 1.1026\n",
      "123/295, train_loss: 0.0743, step time: 1.0316\n",
      "124/295, train_loss: 0.0955, step time: 1.0583\n",
      "125/295, train_loss: 0.3778, step time: 1.0392\n",
      "126/295, train_loss: 0.0525, step time: 1.0727\n",
      "127/295, train_loss: 0.0384, step time: 1.0342\n",
      "128/295, train_loss: 0.2299, step time: 1.1282\n",
      "129/295, train_loss: 0.0968, step time: 1.0600\n",
      "130/295, train_loss: 0.0856, step time: 1.0454\n",
      "131/295, train_loss: 0.1532, step time: 1.0448\n",
      "132/295, train_loss: 0.1097, step time: 1.0417\n",
      "133/295, train_loss: 0.2922, step time: 1.0414\n",
      "134/295, train_loss: 0.2046, step time: 1.0402\n",
      "135/295, train_loss: 0.0555, step time: 1.0745\n",
      "136/295, train_loss: 0.1095, step time: 1.0353\n",
      "137/295, train_loss: 0.0426, step time: 1.0493\n",
      "138/295, train_loss: 0.4372, step time: 1.0563\n",
      "139/295, train_loss: 0.1482, step time: 1.0761\n",
      "140/295, train_loss: 0.1671, step time: 1.0470\n",
      "141/295, train_loss: 0.2217, step time: 1.0346\n",
      "142/295, train_loss: 0.0936, step time: 1.0355\n",
      "143/295, train_loss: 0.3739, step time: 1.0375\n",
      "144/295, train_loss: 0.0561, step time: 1.0398\n",
      "145/295, train_loss: 0.1339, step time: 1.0343\n",
      "146/295, train_loss: 0.1578, step time: 1.0378\n",
      "147/295, train_loss: 0.0753, step time: 1.0462\n",
      "148/295, train_loss: 0.0913, step time: 1.0667\n",
      "149/295, train_loss: 0.1274, step time: 1.0610\n",
      "150/295, train_loss: 0.1140, step time: 1.0666\n",
      "151/295, train_loss: 0.0590, step time: 1.0363\n",
      "152/295, train_loss: 0.0724, step time: 1.0392\n",
      "153/295, train_loss: 0.0914, step time: 1.0446\n",
      "154/295, train_loss: 0.1198, step time: 1.0507\n",
      "155/295, train_loss: 0.1552, step time: 1.0355\n",
      "156/295, train_loss: 0.4479, step time: 1.0347\n",
      "157/295, train_loss: 0.0736, step time: 1.0649\n",
      "158/295, train_loss: 0.0759, step time: 1.0449\n",
      "159/295, train_loss: 0.0547, step time: 1.0666\n",
      "160/295, train_loss: 0.0972, step time: 1.0359\n",
      "161/295, train_loss: 0.0760, step time: 1.1597\n",
      "162/295, train_loss: 0.3808, step time: 1.0366\n",
      "163/295, train_loss: 0.0395, step time: 1.0363\n",
      "164/295, train_loss: 0.1900, step time: 1.0361\n",
      "165/295, train_loss: 0.0711, step time: 1.0455\n",
      "166/295, train_loss: 0.0652, step time: 1.0327\n",
      "167/295, train_loss: 0.0668, step time: 1.0354\n",
      "168/295, train_loss: 0.0691, step time: 1.0339\n",
      "169/295, train_loss: 0.3261, step time: 1.0442\n",
      "170/295, train_loss: 0.1353, step time: 1.0944\n",
      "171/295, train_loss: 0.1132, step time: 1.0681\n",
      "172/295, train_loss: 0.0550, step time: 1.0464\n",
      "173/295, train_loss: 0.0680, step time: 1.0413\n",
      "174/295, train_loss: 0.1198, step time: 1.0451\n",
      "175/295, train_loss: 0.3573, step time: 1.0365\n",
      "176/295, train_loss: 0.1026, step time: 1.0371\n",
      "177/295, train_loss: 0.4715, step time: 1.0362\n",
      "178/295, train_loss: 0.1125, step time: 1.0666\n",
      "179/295, train_loss: 0.0836, step time: 1.0406\n",
      "180/295, train_loss: 0.1444, step time: 1.0726\n",
      "181/295, train_loss: 0.0820, step time: 1.0336\n",
      "182/295, train_loss: 0.0740, step time: 1.0335\n",
      "183/295, train_loss: 0.0900, step time: 1.0370\n",
      "184/295, train_loss: 0.5207, step time: 1.0392\n",
      "185/295, train_loss: 0.0991, step time: 1.0327\n",
      "186/295, train_loss: 0.1451, step time: 1.0331\n",
      "187/295, train_loss: 0.0509, step time: 1.0441\n",
      "188/295, train_loss: 0.1244, step time: 1.0567\n",
      "189/295, train_loss: 0.0625, step time: 1.0387\n",
      "190/295, train_loss: 0.0429, step time: 1.0622\n",
      "191/295, train_loss: 0.0820, step time: 1.0587\n",
      "192/295, train_loss: 0.1019, step time: 1.0343\n",
      "193/295, train_loss: 0.2256, step time: 1.0320\n",
      "194/295, train_loss: 0.0540, step time: 1.0489\n",
      "195/295, train_loss: 0.1258, step time: 1.0334\n",
      "196/295, train_loss: 0.2000, step time: 1.0418\n",
      "197/295, train_loss: 0.1372, step time: 1.0657\n",
      "198/295, train_loss: 0.0731, step time: 1.0602\n",
      "199/295, train_loss: 0.1038, step time: 1.0559\n",
      "200/295, train_loss: 0.4836, step time: 1.0468\n",
      "201/295, train_loss: 0.0407, step time: 1.0378\n",
      "202/295, train_loss: 0.0729, step time: 1.0652\n",
      "203/295, train_loss: 0.1231, step time: 1.0553\n",
      "204/295, train_loss: 0.0988, step time: 1.0655\n",
      "205/295, train_loss: 0.1401, step time: 1.0384\n",
      "206/295, train_loss: 0.1811, step time: 1.0392\n",
      "207/295, train_loss: 0.1045, step time: 1.0465\n",
      "208/295, train_loss: 0.3295, step time: 1.0611\n",
      "209/295, train_loss: 0.5047, step time: 1.0432\n",
      "210/295, train_loss: 0.1716, step time: 1.0374\n",
      "211/295, train_loss: 0.0520, step time: 1.0370\n",
      "212/295, train_loss: 0.0876, step time: 1.0327\n",
      "213/295, train_loss: 0.0886, step time: 1.0460\n",
      "214/295, train_loss: 0.1306, step time: 1.0361\n",
      "215/295, train_loss: 0.1451, step time: 1.0332\n",
      "216/295, train_loss: 0.1116, step time: 1.0406\n",
      "217/295, train_loss: 0.0813, step time: 1.0531\n",
      "218/295, train_loss: 0.0620, step time: 1.0366\n",
      "219/295, train_loss: 0.0873, step time: 1.0340\n",
      "220/295, train_loss: 0.0825, step time: 1.0329\n",
      "221/295, train_loss: 0.4463, step time: 1.0454\n",
      "222/295, train_loss: 0.1961, step time: 1.0413\n",
      "223/295, train_loss: 0.0632, step time: 1.0462\n",
      "224/295, train_loss: 0.1079, step time: 1.0528\n",
      "225/295, train_loss: 0.1110, step time: 1.0338\n",
      "226/295, train_loss: 0.1490, step time: 1.0558\n",
      "227/295, train_loss: 0.0604, step time: 1.1049\n",
      "228/295, train_loss: 0.0589, step time: 1.0394\n",
      "229/295, train_loss: 0.1260, step time: 1.0331\n",
      "230/295, train_loss: 0.1187, step time: 1.0378\n",
      "231/295, train_loss: 0.0601, step time: 1.0453\n",
      "232/295, train_loss: 0.1501, step time: 1.0346\n",
      "233/295, train_loss: 0.1121, step time: 1.0341\n",
      "234/295, train_loss: 0.0641, step time: 1.0518\n",
      "235/295, train_loss: 0.1375, step time: 1.0385\n",
      "236/295, train_loss: 0.3828, step time: 1.0407\n",
      "237/295, train_loss: 0.0672, step time: 1.0306\n",
      "238/295, train_loss: 0.3864, step time: 1.0354\n",
      "239/295, train_loss: 0.0992, step time: 1.0568\n",
      "240/295, train_loss: 0.0886, step time: 1.0591\n",
      "241/295, train_loss: 0.1481, step time: 1.0667\n",
      "242/295, train_loss: 0.0826, step time: 1.0548\n",
      "243/295, train_loss: 0.0806, step time: 1.0377\n",
      "244/295, train_loss: 0.0940, step time: 1.0329\n",
      "245/295, train_loss: 0.3956, step time: 1.0360\n",
      "246/295, train_loss: 0.1243, step time: 1.0490\n",
      "247/295, train_loss: 0.0910, step time: 1.1015\n",
      "248/295, train_loss: 0.0799, step time: 1.0489\n",
      "249/295, train_loss: 0.1064, step time: 1.0452\n",
      "250/295, train_loss: 0.1221, step time: 1.0369\n",
      "251/295, train_loss: 0.1326, step time: 1.0455\n",
      "252/295, train_loss: 0.2260, step time: 1.0373\n",
      "253/295, train_loss: 0.3768, step time: 1.0647\n",
      "254/295, train_loss: 0.1631, step time: 1.0511\n",
      "255/295, train_loss: 0.0594, step time: 1.0338\n",
      "256/295, train_loss: 0.0643, step time: 1.0450\n",
      "257/295, train_loss: 0.1343, step time: 1.0660\n",
      "258/295, train_loss: 0.0893, step time: 1.0379\n",
      "259/295, train_loss: 0.0565, step time: 1.0422\n",
      "260/295, train_loss: 0.0980, step time: 1.0676\n",
      "261/295, train_loss: 0.2171, step time: 1.0448\n",
      "262/295, train_loss: 0.3935, step time: 1.0359\n",
      "263/295, train_loss: 0.0790, step time: 1.0644\n",
      "264/295, train_loss: 0.1485, step time: 1.0373\n",
      "265/295, train_loss: 0.1527, step time: 1.0339\n",
      "266/295, train_loss: 0.0489, step time: 1.0404\n",
      "267/295, train_loss: 0.0821, step time: 1.0513\n",
      "268/295, train_loss: 0.1056, step time: 1.0381\n",
      "269/295, train_loss: 0.0986, step time: 1.0534\n",
      "270/295, train_loss: 0.0482, step time: 1.0340\n",
      "271/295, train_loss: 0.0416, step time: 1.0594\n",
      "272/295, train_loss: 0.0989, step time: 1.0665\n",
      "273/295, train_loss: 0.5998, step time: 1.0537\n",
      "274/295, train_loss: 0.4132, step time: 1.1229\n",
      "275/295, train_loss: 0.0733, step time: 1.0356\n",
      "276/295, train_loss: 0.2852, step time: 1.0424\n",
      "277/295, train_loss: 0.0485, step time: 1.0444\n",
      "278/295, train_loss: 0.2805, step time: 1.0431\n",
      "279/295, train_loss: 0.1149, step time: 1.0570\n",
      "280/295, train_loss: 0.0714, step time: 1.0401\n",
      "281/295, train_loss: 0.2561, step time: 1.0448\n",
      "282/295, train_loss: 0.3597, step time: 1.0787\n",
      "283/295, train_loss: 0.0617, step time: 1.0582\n",
      "284/295, train_loss: 0.5062, step time: 1.0452\n",
      "285/295, train_loss: 0.1138, step time: 1.0404\n",
      "286/295, train_loss: 0.1113, step time: 1.0351\n",
      "287/295, train_loss: 0.1286, step time: 1.0382\n",
      "288/295, train_loss: 0.3061, step time: 1.0364\n",
      "289/295, train_loss: 0.1277, step time: 1.0313\n",
      "290/295, train_loss: 0.0677, step time: 1.0293\n",
      "291/295, train_loss: 0.0883, step time: 1.0285\n",
      "292/295, train_loss: 0.4588, step time: 1.0294\n",
      "293/295, train_loss: 0.1628, step time: 1.0297\n",
      "294/295, train_loss: 0.0486, step time: 1.0285\n",
      "295/295, train_loss: 0.2502, step time: 1.0310\n",
      "epoch 23 average loss: 0.1421\n",
      "current epoch: 23 current mean dice: 0.7732 tc: 0.7123 wt: 0.8508 et: 0.7603\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 23 is: 384.2241\n",
      "----------\n",
      "epoch 24/100\n",
      "1/295, train_loss: 0.1432, step time: 1.0765\n",
      "2/295, train_loss: 0.2205, step time: 1.0553\n",
      "3/295, train_loss: 0.0706, step time: 1.0559\n",
      "4/295, train_loss: 0.0402, step time: 1.1038\n",
      "5/295, train_loss: 0.1666, step time: 1.0709\n",
      "6/295, train_loss: 0.1154, step time: 1.0584\n",
      "7/295, train_loss: 0.2369, step time: 1.0483\n",
      "8/295, train_loss: 0.1052, step time: 1.0538\n",
      "9/295, train_loss: 0.0569, step time: 1.0464\n",
      "10/295, train_loss: 0.0658, step time: 1.0436\n",
      "11/295, train_loss: 0.0920, step time: 1.0711\n",
      "12/295, train_loss: 0.3100, step time: 1.0477\n",
      "13/295, train_loss: 0.1028, step time: 1.0468\n",
      "14/295, train_loss: 0.1130, step time: 1.0400\n",
      "15/295, train_loss: 0.0736, step time: 1.0494\n",
      "16/295, train_loss: 0.0640, step time: 1.0441\n",
      "17/295, train_loss: 0.0455, step time: 1.0323\n",
      "18/295, train_loss: 0.4444, step time: 1.0578\n",
      "19/295, train_loss: 0.1347, step time: 1.0412\n",
      "20/295, train_loss: 0.0812, step time: 1.0392\n",
      "21/295, train_loss: 0.1342, step time: 1.0445\n",
      "22/295, train_loss: 0.4639, step time: 1.0458\n",
      "23/295, train_loss: 0.0382, step time: 1.0636\n",
      "24/295, train_loss: 0.4010, step time: 1.0384\n",
      "25/295, train_loss: 0.4081, step time: 1.0460\n",
      "26/295, train_loss: 0.4492, step time: 1.0808\n",
      "27/295, train_loss: 0.0683, step time: 1.0430\n",
      "28/295, train_loss: 0.0478, step time: 1.0503\n",
      "29/295, train_loss: 0.1777, step time: 1.0360\n",
      "30/295, train_loss: 0.1142, step time: 1.0453\n",
      "31/295, train_loss: 0.0706, step time: 1.0395\n",
      "32/295, train_loss: 0.1904, step time: 1.0394\n",
      "33/295, train_loss: 0.0670, step time: 1.0394\n",
      "34/295, train_loss: 0.2568, step time: 1.0817\n",
      "35/295, train_loss: 0.5814, step time: 1.0373\n",
      "36/295, train_loss: 0.1271, step time: 1.1018\n",
      "37/295, train_loss: 0.3789, step time: 1.0374\n",
      "38/295, train_loss: 0.3144, step time: 1.0614\n",
      "39/295, train_loss: 0.5032, step time: 1.1695\n",
      "40/295, train_loss: 0.1818, step time: 1.0427\n",
      "41/295, train_loss: 0.1174, step time: 1.0589\n",
      "42/295, train_loss: 0.3749, step time: 1.0483\n",
      "43/295, train_loss: 0.4224, step time: 1.0394\n",
      "44/295, train_loss: 0.0954, step time: 1.0345\n",
      "45/295, train_loss: 0.0731, step time: 1.0333\n",
      "46/295, train_loss: 0.0841, step time: 1.0435\n",
      "47/295, train_loss: 0.0659, step time: 1.0680\n",
      "48/295, train_loss: 0.2372, step time: 1.0323\n",
      "49/295, train_loss: 0.1773, step time: 1.0349\n",
      "50/295, train_loss: 0.1364, step time: 1.0841\n",
      "51/295, train_loss: 0.1280, step time: 1.0524\n",
      "52/295, train_loss: 0.1361, step time: 1.1072\n",
      "53/295, train_loss: 0.2564, step time: 1.0643\n",
      "54/295, train_loss: 0.1260, step time: 1.0389\n",
      "55/295, train_loss: 0.0394, step time: 1.0578\n",
      "56/295, train_loss: 0.1069, step time: 1.0388\n",
      "57/295, train_loss: 0.3598, step time: 1.0386\n",
      "58/295, train_loss: 0.0939, step time: 1.0409\n",
      "59/295, train_loss: 0.0632, step time: 1.0391\n",
      "60/295, train_loss: 0.1726, step time: 1.0538\n",
      "61/295, train_loss: 0.0800, step time: 1.0950\n",
      "62/295, train_loss: 0.1737, step time: 1.0423\n",
      "63/295, train_loss: 0.0778, step time: 1.0631\n",
      "64/295, train_loss: 0.0789, step time: 1.0356\n",
      "65/295, train_loss: 0.0887, step time: 1.1179\n",
      "66/295, train_loss: 0.1094, step time: 1.0379\n",
      "67/295, train_loss: 0.0972, step time: 1.0378\n",
      "68/295, train_loss: 0.2242, step time: 1.0445\n",
      "69/295, train_loss: 0.0685, step time: 1.0426\n",
      "70/295, train_loss: 0.0588, step time: 1.0461\n",
      "71/295, train_loss: 0.3599, step time: 1.0465\n",
      "72/295, train_loss: 0.1200, step time: 1.0519\n",
      "73/295, train_loss: 0.1264, step time: 1.0338\n",
      "74/295, train_loss: 0.0686, step time: 1.1181\n",
      "75/295, train_loss: 0.0669, step time: 1.0519\n",
      "76/295, train_loss: 0.0788, step time: 1.0637\n",
      "77/295, train_loss: 0.4402, step time: 1.0328\n",
      "78/295, train_loss: 0.3267, step time: 1.0345\n",
      "79/295, train_loss: 0.1164, step time: 1.0599\n",
      "80/295, train_loss: 0.0733, step time: 1.0662\n",
      "81/295, train_loss: 0.0744, step time: 1.0389\n",
      "82/295, train_loss: 0.0976, step time: 1.1167\n",
      "83/295, train_loss: 0.4839, step time: 1.0992\n",
      "84/295, train_loss: 0.4195, step time: 1.0513\n",
      "85/295, train_loss: 0.0626, step time: 1.0421\n",
      "86/295, train_loss: 0.1029, step time: 1.0436\n",
      "87/295, train_loss: 0.1167, step time: 1.1172\n",
      "88/295, train_loss: 0.1118, step time: 1.0401\n",
      "89/295, train_loss: 0.0760, step time: 1.0550\n",
      "90/295, train_loss: 0.0440, step time: 1.0340\n",
      "91/295, train_loss: 0.1049, step time: 1.0302\n",
      "92/295, train_loss: 0.0584, step time: 1.0405\n",
      "93/295, train_loss: 0.0458, step time: 1.0522\n",
      "94/295, train_loss: 0.0973, step time: 1.0552\n",
      "95/295, train_loss: 0.4425, step time: 1.0513\n",
      "96/295, train_loss: 0.1031, step time: 1.1102\n",
      "97/295, train_loss: 0.2692, step time: 1.0642\n",
      "98/295, train_loss: 0.0911, step time: 1.0365\n",
      "99/295, train_loss: 0.0700, step time: 1.0726\n",
      "100/295, train_loss: 0.1206, step time: 1.0339\n",
      "101/295, train_loss: 0.0469, step time: 1.0391\n",
      "102/295, train_loss: 0.4534, step time: 1.0573\n",
      "103/295, train_loss: 0.0814, step time: 1.0457\n",
      "104/295, train_loss: 0.3263, step time: 1.0721\n",
      "105/295, train_loss: 0.2974, step time: 1.0540\n",
      "106/295, train_loss: 0.0881, step time: 1.0542\n",
      "107/295, train_loss: 0.1578, step time: 1.0450\n",
      "108/295, train_loss: 0.0483, step time: 1.0612\n",
      "109/295, train_loss: 0.0856, step time: 1.0460\n",
      "110/295, train_loss: 0.0485, step time: 1.0387\n",
      "111/295, train_loss: 0.0491, step time: 1.0502\n",
      "112/295, train_loss: 0.0502, step time: 1.0358\n",
      "113/295, train_loss: 0.0942, step time: 1.0975\n",
      "114/295, train_loss: 0.0495, step time: 1.0572\n",
      "115/295, train_loss: 0.4553, step time: 1.0344\n",
      "116/295, train_loss: 0.3236, step time: 1.0441\n",
      "117/295, train_loss: 0.0936, step time: 1.0363\n",
      "118/295, train_loss: 0.1002, step time: 1.0389\n",
      "119/295, train_loss: 0.0370, step time: 1.0443\n",
      "120/295, train_loss: 0.1119, step time: 1.0695\n",
      "121/295, train_loss: 0.0763, step time: 1.0455\n",
      "122/295, train_loss: 0.1732, step time: 1.0352\n",
      "123/295, train_loss: 0.1081, step time: 1.0348\n",
      "124/295, train_loss: 0.1415, step time: 1.0351\n",
      "125/295, train_loss: 0.0661, step time: 1.0544\n",
      "126/295, train_loss: 0.0528, step time: 1.0363\n",
      "127/295, train_loss: 0.1181, step time: 1.0639\n",
      "128/295, train_loss: 0.1116, step time: 1.0368\n",
      "129/295, train_loss: 0.0396, step time: 1.0333\n",
      "130/295, train_loss: 0.0667, step time: 1.0344\n",
      "131/295, train_loss: 0.1145, step time: 1.0405\n",
      "132/295, train_loss: 0.0491, step time: 1.0426\n",
      "133/295, train_loss: 0.0957, step time: 1.0314\n",
      "134/295, train_loss: 0.0923, step time: 1.0344\n",
      "135/295, train_loss: 0.0634, step time: 1.0428\n",
      "136/295, train_loss: 0.0579, step time: 1.0473\n",
      "137/295, train_loss: 0.1136, step time: 1.0366\n",
      "138/295, train_loss: 0.0753, step time: 1.0653\n",
      "139/295, train_loss: 0.1735, step time: 1.1417\n",
      "140/295, train_loss: 0.0953, step time: 1.0332\n",
      "141/295, train_loss: 0.0719, step time: 1.0338\n",
      "142/295, train_loss: 0.0873, step time: 1.0593\n",
      "143/295, train_loss: 0.0692, step time: 1.0810\n",
      "144/295, train_loss: 0.0458, step time: 1.0383\n",
      "145/295, train_loss: 0.1457, step time: 1.0319\n",
      "146/295, train_loss: 0.0680, step time: 1.0351\n",
      "147/295, train_loss: 0.0794, step time: 1.0455\n",
      "148/295, train_loss: 0.2003, step time: 1.0534\n",
      "149/295, train_loss: 0.4011, step time: 1.0411\n",
      "150/295, train_loss: 0.1448, step time: 1.0351\n",
      "151/295, train_loss: 0.0454, step time: 1.0539\n",
      "152/295, train_loss: 0.1734, step time: 1.0936\n",
      "153/295, train_loss: 0.0428, step time: 1.0393\n",
      "154/295, train_loss: 0.0823, step time: 1.0688\n",
      "155/295, train_loss: 0.4766, step time: 1.1200\n",
      "156/295, train_loss: 0.1497, step time: 1.0539\n",
      "157/295, train_loss: 0.0569, step time: 1.0414\n",
      "158/295, train_loss: 0.1203, step time: 1.0550\n",
      "159/295, train_loss: 0.1356, step time: 1.0332\n",
      "160/295, train_loss: 0.1087, step time: 1.0324\n",
      "161/295, train_loss: 0.1029, step time: 1.0471\n",
      "162/295, train_loss: 0.0696, step time: 1.0382\n",
      "163/295, train_loss: 0.0580, step time: 1.0377\n",
      "164/295, train_loss: 0.0665, step time: 1.0660\n",
      "165/295, train_loss: 0.1526, step time: 1.0374\n",
      "166/295, train_loss: 0.2196, step time: 1.0352\n",
      "167/295, train_loss: 0.0929, step time: 1.0307\n",
      "168/295, train_loss: 0.1637, step time: 1.0822\n",
      "169/295, train_loss: 0.3325, step time: 1.0489\n",
      "170/295, train_loss: 0.0506, step time: 1.0370\n",
      "171/295, train_loss: 0.0820, step time: 1.0376\n",
      "172/295, train_loss: 0.1560, step time: 1.0422\n",
      "173/295, train_loss: 0.4201, step time: 1.0868\n",
      "174/295, train_loss: 0.1777, step time: 1.0521\n",
      "175/295, train_loss: 0.0866, step time: 1.0357\n",
      "176/295, train_loss: 0.0735, step time: 1.0417\n",
      "177/295, train_loss: 0.0572, step time: 1.0378\n",
      "178/295, train_loss: 0.4203, step time: 1.0643\n",
      "179/295, train_loss: 0.2142, step time: 1.0433\n",
      "180/295, train_loss: 0.3814, step time: 1.0356\n",
      "181/295, train_loss: 0.3823, step time: 1.0348\n",
      "182/295, train_loss: 0.0508, step time: 1.0400\n",
      "183/295, train_loss: 0.0951, step time: 1.0411\n",
      "184/295, train_loss: 0.1311, step time: 1.0318\n",
      "185/295, train_loss: 0.0535, step time: 1.0381\n",
      "186/295, train_loss: 0.0980, step time: 1.0484\n",
      "187/295, train_loss: 0.0904, step time: 1.0624\n",
      "188/295, train_loss: 0.0737, step time: 1.0465\n",
      "189/295, train_loss: 0.1013, step time: 1.0458\n",
      "190/295, train_loss: 0.0438, step time: 1.0392\n",
      "191/295, train_loss: 0.1477, step time: 1.0712\n",
      "192/295, train_loss: 0.0426, step time: 1.0334\n",
      "193/295, train_loss: 0.0566, step time: 1.0452\n",
      "194/295, train_loss: 0.0807, step time: 1.0339\n",
      "195/295, train_loss: 0.0422, step time: 1.0605\n",
      "196/295, train_loss: 0.1109, step time: 1.0352\n",
      "197/295, train_loss: 0.1328, step time: 1.0409\n",
      "198/295, train_loss: 0.0398, step time: 1.0553\n",
      "199/295, train_loss: 0.1209, step time: 1.0557\n",
      "200/295, train_loss: 0.0840, step time: 1.0329\n",
      "201/295, train_loss: 0.0891, step time: 1.0408\n",
      "202/295, train_loss: 0.4228, step time: 1.0436\n",
      "203/295, train_loss: 0.1420, step time: 1.0366\n",
      "204/295, train_loss: 0.1301, step time: 1.0599\n",
      "205/295, train_loss: 0.0890, step time: 1.0627\n",
      "206/295, train_loss: 0.3904, step time: 1.0605\n",
      "207/295, train_loss: 0.1434, step time: 1.0387\n",
      "208/295, train_loss: 0.1579, step time: 1.0426\n",
      "209/295, train_loss: 0.1083, step time: 1.0410\n",
      "210/295, train_loss: 0.0664, step time: 1.0581\n",
      "211/295, train_loss: 0.0663, step time: 1.0459\n",
      "212/295, train_loss: 0.0567, step time: 1.0383\n",
      "213/295, train_loss: 0.0795, step time: 1.0559\n",
      "214/295, train_loss: 0.1254, step time: 1.0637\n",
      "215/295, train_loss: 0.0543, step time: 1.0398\n",
      "216/295, train_loss: 0.1283, step time: 1.0321\n",
      "217/295, train_loss: 0.0711, step time: 1.0676\n",
      "218/295, train_loss: 0.0737, step time: 1.0422\n",
      "219/295, train_loss: 0.1543, step time: 1.0393\n",
      "220/295, train_loss: 0.1336, step time: 1.0310\n",
      "221/295, train_loss: 0.0982, step time: 1.0819\n",
      "222/295, train_loss: 0.1741, step time: 1.0360\n",
      "223/295, train_loss: 0.1427, step time: 1.0339\n",
      "224/295, train_loss: 0.1109, step time: 1.0452\n",
      "225/295, train_loss: 0.0938, step time: 1.0366\n",
      "226/295, train_loss: 0.1250, step time: 1.0487\n",
      "227/295, train_loss: 0.1096, step time: 1.0408\n",
      "228/295, train_loss: 0.0432, step time: 1.0322\n",
      "229/295, train_loss: 0.0564, step time: 1.0402\n",
      "230/295, train_loss: 0.0711, step time: 1.0421\n",
      "231/295, train_loss: 0.0806, step time: 1.0643\n",
      "232/295, train_loss: 0.0753, step time: 1.0396\n",
      "233/295, train_loss: 0.1223, step time: 1.0395\n",
      "234/295, train_loss: 0.0958, step time: 1.0908\n",
      "235/295, train_loss: 0.2098, step time: 1.0406\n",
      "236/295, train_loss: 0.0481, step time: 1.0339\n",
      "237/295, train_loss: 0.1512, step time: 1.0440\n",
      "238/295, train_loss: 0.1302, step time: 1.0676\n",
      "239/295, train_loss: 0.0512, step time: 1.0436\n",
      "240/295, train_loss: 0.1593, step time: 1.0973\n",
      "241/295, train_loss: 0.1634, step time: 1.0382\n",
      "242/295, train_loss: 0.0773, step time: 1.0609\n",
      "243/295, train_loss: 0.0759, step time: 1.0429\n",
      "244/295, train_loss: 0.1091, step time: 1.0609\n",
      "245/295, train_loss: 0.1151, step time: 1.0381\n",
      "246/295, train_loss: 0.0612, step time: 1.0311\n",
      "247/295, train_loss: 0.0940, step time: 1.0372\n",
      "248/295, train_loss: 0.0688, step time: 1.0364\n",
      "249/295, train_loss: 0.0721, step time: 1.0350\n",
      "250/295, train_loss: 0.0535, step time: 1.0667\n",
      "251/295, train_loss: 0.1328, step time: 1.0451\n",
      "252/295, train_loss: 0.0797, step time: 1.1151\n",
      "253/295, train_loss: 0.1229, step time: 1.0392\n",
      "254/295, train_loss: 0.0662, step time: 1.0453\n",
      "255/295, train_loss: 0.0963, step time: 1.0389\n",
      "256/295, train_loss: 0.1600, step time: 1.0613\n",
      "257/295, train_loss: 0.1136, step time: 1.0647\n",
      "258/295, train_loss: 0.0931, step time: 1.0561\n",
      "259/295, train_loss: 0.1688, step time: 1.0471\n",
      "260/295, train_loss: 0.0625, step time: 1.0353\n",
      "261/295, train_loss: 0.1275, step time: 1.0407\n",
      "262/295, train_loss: 0.0456, step time: 1.0348\n",
      "263/295, train_loss: 0.0849, step time: 1.0419\n",
      "264/295, train_loss: 0.0480, step time: 1.0369\n",
      "265/295, train_loss: 0.0771, step time: 1.0360\n",
      "266/295, train_loss: 0.0529, step time: 1.0399\n",
      "267/295, train_loss: 0.0476, step time: 1.0434\n",
      "268/295, train_loss: 0.0687, step time: 1.0376\n",
      "269/295, train_loss: 0.1127, step time: 1.0333\n",
      "270/295, train_loss: 0.1391, step time: 1.0538\n",
      "271/295, train_loss: 0.0514, step time: 1.0336\n",
      "272/295, train_loss: 0.0538, step time: 1.0323\n",
      "273/295, train_loss: 0.1029, step time: 1.0346\n",
      "274/295, train_loss: 0.0810, step time: 1.0355\n",
      "275/295, train_loss: 0.0793, step time: 1.0446\n",
      "276/295, train_loss: 0.0461, step time: 1.0350\n",
      "277/295, train_loss: 0.1124, step time: 1.0368\n",
      "278/295, train_loss: 0.1760, step time: 1.0384\n",
      "279/295, train_loss: 0.0466, step time: 1.0472\n",
      "280/295, train_loss: 0.0548, step time: 1.1298\n",
      "281/295, train_loss: 0.0461, step time: 1.0435\n",
      "282/295, train_loss: 0.0900, step time: 1.0368\n",
      "283/295, train_loss: 0.0944, step time: 1.0432\n",
      "284/295, train_loss: 0.1006, step time: 1.0944\n",
      "285/295, train_loss: 0.1094, step time: 1.0573\n",
      "286/295, train_loss: 0.0586, step time: 1.0425\n",
      "287/295, train_loss: 0.1463, step time: 1.0490\n",
      "288/295, train_loss: 0.0810, step time: 1.0291\n",
      "289/295, train_loss: 0.1023, step time: 1.0286\n",
      "290/295, train_loss: 0.3666, step time: 1.0281\n",
      "291/295, train_loss: 0.0857, step time: 1.0291\n",
      "292/295, train_loss: 0.0892, step time: 1.0313\n",
      "293/295, train_loss: 0.0780, step time: 1.0305\n",
      "294/295, train_loss: 0.3909, step time: 1.0295\n",
      "295/295, train_loss: 0.1396, step time: 1.0296\n",
      "epoch 24 average loss: 0.1359\n",
      "current epoch: 24 current mean dice: 0.7793 tc: 0.7300 wt: 0.8498 et: 0.7650\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 24 is: 385.8251\n",
      "----------\n",
      "epoch 25/100\n",
      "1/295, train_loss: 0.1429, step time: 1.0666\n",
      "2/295, train_loss: 0.1004, step time: 1.1184\n",
      "3/295, train_loss: 0.1624, step time: 1.0428\n",
      "4/295, train_loss: 0.0728, step time: 1.0389\n",
      "5/295, train_loss: 0.0811, step time: 1.0509\n",
      "6/295, train_loss: 0.0686, step time: 1.0364\n",
      "7/295, train_loss: 0.0912, step time: 1.1071\n",
      "8/295, train_loss: 0.2158, step time: 1.0466\n",
      "9/295, train_loss: 0.3688, step time: 1.0789\n",
      "10/295, train_loss: 0.0370, step time: 1.0609\n",
      "11/295, train_loss: 0.0790, step time: 1.0418\n",
      "12/295, train_loss: 0.4145, step time: 1.0595\n",
      "13/295, train_loss: 0.0738, step time: 1.1040\n",
      "14/295, train_loss: 0.0743, step time: 1.0554\n",
      "15/295, train_loss: 0.0575, step time: 1.0374\n",
      "16/295, train_loss: 0.0871, step time: 1.0396\n",
      "17/295, train_loss: 0.1959, step time: 1.0430\n",
      "18/295, train_loss: 0.1463, step time: 1.0358\n",
      "19/295, train_loss: 0.0943, step time: 1.0390\n",
      "20/295, train_loss: 0.1299, step time: 1.1050\n",
      "21/295, train_loss: 0.0842, step time: 1.0527\n",
      "22/295, train_loss: 0.0574, step time: 1.0426\n",
      "23/295, train_loss: 0.3935, step time: 1.0397\n",
      "24/295, train_loss: 0.0749, step time: 1.0690\n",
      "25/295, train_loss: 0.1400, step time: 1.1089\n",
      "26/295, train_loss: 0.1191, step time: 1.0380\n",
      "27/295, train_loss: 0.0491, step time: 1.0489\n",
      "28/295, train_loss: 0.0650, step time: 1.0503\n",
      "29/295, train_loss: 0.0579, step time: 1.0539\n",
      "30/295, train_loss: 0.1348, step time: 1.0504\n",
      "31/295, train_loss: 0.4607, step time: 1.0400\n",
      "32/295, train_loss: 0.0447, step time: 1.0535\n",
      "33/295, train_loss: 0.1341, step time: 1.0317\n",
      "34/295, train_loss: 0.0560, step time: 1.0327\n",
      "35/295, train_loss: 0.0606, step time: 1.0356\n",
      "36/295, train_loss: 0.1133, step time: 1.0366\n",
      "37/295, train_loss: 0.4058, step time: 1.0754\n",
      "38/295, train_loss: 0.1171, step time: 1.0403\n",
      "39/295, train_loss: 0.1966, step time: 1.0472\n",
      "40/295, train_loss: 0.1607, step time: 1.0346\n",
      "41/295, train_loss: 0.0648, step time: 1.0391\n",
      "42/295, train_loss: 0.0479, step time: 1.0587\n",
      "43/295, train_loss: 0.0528, step time: 1.0340\n",
      "44/295, train_loss: 0.0389, step time: 1.0329\n",
      "45/295, train_loss: 0.0951, step time: 1.0581\n",
      "46/295, train_loss: 0.0908, step time: 1.0515\n",
      "47/295, train_loss: 0.1111, step time: 1.0619\n",
      "48/295, train_loss: 0.4634, step time: 1.0326\n",
      "49/295, train_loss: 0.4351, step time: 1.0992\n",
      "50/295, train_loss: 0.0601, step time: 1.0426\n",
      "51/295, train_loss: 0.3934, step time: 1.0665\n",
      "52/295, train_loss: 0.1391, step time: 1.0450\n",
      "53/295, train_loss: 0.0466, step time: 1.0322\n",
      "54/295, train_loss: 0.0403, step time: 1.0681\n",
      "55/295, train_loss: 0.2032, step time: 1.0386\n",
      "56/295, train_loss: 0.0841, step time: 1.0423\n",
      "57/295, train_loss: 0.0882, step time: 1.0587\n",
      "58/295, train_loss: 0.2925, step time: 1.0601\n",
      "59/295, train_loss: 0.0837, step time: 1.0398\n",
      "60/295, train_loss: 0.1429, step time: 1.0359\n",
      "61/295, train_loss: 0.1446, step time: 1.0342\n",
      "62/295, train_loss: 0.1016, step time: 1.0373\n",
      "63/295, train_loss: 0.0722, step time: 1.0500\n",
      "64/295, train_loss: 0.3191, step time: 1.0905\n",
      "65/295, train_loss: 0.1784, step time: 1.0346\n",
      "66/295, train_loss: 0.1172, step time: 1.0966\n",
      "67/295, train_loss: 0.0769, step time: 1.0952\n",
      "68/295, train_loss: 0.1514, step time: 1.0333\n",
      "69/295, train_loss: 0.0520, step time: 1.0649\n",
      "70/295, train_loss: 0.1126, step time: 1.0376\n",
      "71/295, train_loss: 0.1221, step time: 1.1116\n",
      "72/295, train_loss: 0.0753, step time: 1.0605\n",
      "73/295, train_loss: 0.0504, step time: 1.0348\n",
      "74/295, train_loss: 0.0781, step time: 1.0443\n",
      "75/295, train_loss: 0.4455, step time: 1.0427\n",
      "76/295, train_loss: 0.0637, step time: 1.0804\n",
      "77/295, train_loss: 0.0795, step time: 1.0308\n",
      "78/295, train_loss: 0.0631, step time: 1.0372\n",
      "79/295, train_loss: 0.3509, step time: 1.0375\n",
      "80/295, train_loss: 0.0622, step time: 1.0613\n",
      "81/295, train_loss: 0.2105, step time: 1.0356\n",
      "82/295, train_loss: 0.1070, step time: 1.0616\n",
      "83/295, train_loss: 0.5182, step time: 1.0405\n",
      "84/295, train_loss: 0.3267, step time: 1.0706\n",
      "85/295, train_loss: 0.1438, step time: 1.0571\n",
      "86/295, train_loss: 0.0745, step time: 1.0390\n",
      "87/295, train_loss: 0.0594, step time: 1.0439\n",
      "88/295, train_loss: 0.0762, step time: 1.0940\n",
      "89/295, train_loss: 0.1456, step time: 1.0428\n",
      "90/295, train_loss: 0.0846, step time: 1.0424\n",
      "91/295, train_loss: 0.0455, step time: 1.0570\n",
      "92/295, train_loss: 0.3763, step time: 1.0362\n",
      "93/295, train_loss: 0.0886, step time: 1.0790\n",
      "94/295, train_loss: 0.0984, step time: 1.0413\n",
      "95/295, train_loss: 0.2217, step time: 1.1149\n",
      "96/295, train_loss: 0.0576, step time: 1.0517\n",
      "97/295, train_loss: 0.0666, step time: 1.0461\n",
      "98/295, train_loss: 0.0672, step time: 1.0341\n",
      "99/295, train_loss: 0.0754, step time: 1.0734\n",
      "100/295, train_loss: 0.0366, step time: 1.0562\n",
      "101/295, train_loss: 0.4001, step time: 1.1103\n",
      "102/295, train_loss: 0.0753, step time: 1.0394\n",
      "103/295, train_loss: 0.0625, step time: 1.0465\n",
      "104/295, train_loss: 0.4128, step time: 1.0466\n",
      "105/295, train_loss: 0.0778, step time: 1.0367\n",
      "106/295, train_loss: 0.1313, step time: 1.0315\n",
      "107/295, train_loss: 0.1500, step time: 1.0490\n",
      "108/295, train_loss: 0.3905, step time: 1.0335\n",
      "109/295, train_loss: 0.0569, step time: 1.0311\n",
      "110/295, train_loss: 0.0606, step time: 1.0457\n",
      "111/295, train_loss: 0.1540, step time: 1.0402\n",
      "112/295, train_loss: 0.1498, step time: 1.0511\n",
      "113/295, train_loss: 0.1049, step time: 1.0475\n",
      "114/295, train_loss: 0.1720, step time: 1.0405\n",
      "115/295, train_loss: 0.0983, step time: 1.0387\n",
      "116/295, train_loss: 0.0789, step time: 1.0391\n",
      "117/295, train_loss: 0.0666, step time: 1.0752\n",
      "118/295, train_loss: 0.0917, step time: 1.1030\n",
      "119/295, train_loss: 0.1506, step time: 1.0508\n",
      "120/295, train_loss: 0.0515, step time: 1.0332\n",
      "121/295, train_loss: 0.1004, step time: 1.0405\n",
      "122/295, train_loss: 0.1523, step time: 1.0479\n",
      "123/295, train_loss: 0.0384, step time: 1.0399\n",
      "124/295, train_loss: 0.1177, step time: 1.0377\n",
      "125/295, train_loss: 0.0487, step time: 1.0551\n",
      "126/295, train_loss: 0.2067, step time: 1.0356\n",
      "127/295, train_loss: 0.0784, step time: 1.0694\n",
      "128/295, train_loss: 0.3078, step time: 1.0630\n",
      "129/295, train_loss: 0.1666, step time: 1.0703\n",
      "130/295, train_loss: 0.1102, step time: 1.0612\n",
      "131/295, train_loss: 0.0944, step time: 1.0300\n",
      "132/295, train_loss: 0.0457, step time: 1.0362\n",
      "133/295, train_loss: 0.1663, step time: 1.0342\n",
      "134/295, train_loss: 0.0974, step time: 1.0581\n",
      "135/295, train_loss: 0.0762, step time: 1.0431\n",
      "136/295, train_loss: 0.0622, step time: 1.0341\n",
      "137/295, train_loss: 0.1206, step time: 1.0859\n",
      "138/295, train_loss: 0.1358, step time: 1.0425\n",
      "139/295, train_loss: 0.0634, step time: 1.0375\n",
      "140/295, train_loss: 0.1716, step time: 1.0649\n",
      "141/295, train_loss: 0.0603, step time: 1.0472\n",
      "142/295, train_loss: 0.0412, step time: 1.0441\n",
      "143/295, train_loss: 0.0460, step time: 1.0427\n",
      "144/295, train_loss: 0.0728, step time: 1.1343\n",
      "145/295, train_loss: 0.1161, step time: 1.0386\n",
      "146/295, train_loss: 0.0607, step time: 1.0348\n",
      "147/295, train_loss: 0.1065, step time: 1.0454\n",
      "148/295, train_loss: 0.0463, step time: 1.0782\n",
      "149/295, train_loss: 0.0833, step time: 1.0367\n",
      "150/295, train_loss: 0.3857, step time: 1.0355\n",
      "151/295, train_loss: 0.0901, step time: 1.0363\n",
      "152/295, train_loss: 0.0490, step time: 1.0751\n",
      "153/295, train_loss: 0.0662, step time: 1.0808\n",
      "154/295, train_loss: 0.2427, step time: 1.0535\n",
      "155/295, train_loss: 0.0878, step time: 1.0321\n",
      "156/295, train_loss: 0.1177, step time: 1.0528\n",
      "157/295, train_loss: 0.0867, step time: 1.0424\n",
      "158/295, train_loss: 0.0470, step time: 1.0331\n",
      "159/295, train_loss: 0.0857, step time: 1.0465\n",
      "160/295, train_loss: 0.0978, step time: 1.0554\n",
      "161/295, train_loss: 0.0590, step time: 1.0707\n",
      "162/295, train_loss: 0.2539, step time: 1.0548\n",
      "163/295, train_loss: 0.0454, step time: 1.0333\n",
      "164/295, train_loss: 0.0789, step time: 1.0488\n",
      "165/295, train_loss: 0.1480, step time: 1.0880\n",
      "166/295, train_loss: 0.0645, step time: 1.0395\n",
      "167/295, train_loss: 0.3867, step time: 1.0387\n",
      "168/295, train_loss: 0.0842, step time: 1.0585\n",
      "169/295, train_loss: 0.1050, step time: 1.0410\n",
      "170/295, train_loss: 0.1624, step time: 1.0393\n",
      "171/295, train_loss: 0.0724, step time: 1.0659\n",
      "172/295, train_loss: 0.1556, step time: 1.0420\n",
      "173/295, train_loss: 0.2554, step time: 1.0348\n",
      "174/295, train_loss: 0.0719, step time: 1.0348\n",
      "175/295, train_loss: 0.3913, step time: 1.0372\n",
      "176/295, train_loss: 0.4167, step time: 1.0387\n",
      "177/295, train_loss: 0.0588, step time: 1.0854\n",
      "178/295, train_loss: 0.1017, step time: 1.0804\n",
      "179/295, train_loss: 0.0881, step time: 1.0359\n",
      "180/295, train_loss: 0.0902, step time: 1.0363\n",
      "181/295, train_loss: 0.1212, step time: 1.0458\n",
      "182/295, train_loss: 0.1445, step time: 1.0312\n",
      "183/295, train_loss: 0.4325, step time: 1.0369\n",
      "184/295, train_loss: 0.3890, step time: 1.0451\n",
      "185/295, train_loss: 0.0826, step time: 1.0410\n",
      "186/295, train_loss: 0.1527, step time: 1.0782\n",
      "187/295, train_loss: 0.0449, step time: 1.0371\n",
      "188/295, train_loss: 0.0532, step time: 1.0438\n",
      "189/295, train_loss: 0.0435, step time: 1.0532\n",
      "190/295, train_loss: 0.1331, step time: 1.1276\n",
      "191/295, train_loss: 0.1218, step time: 1.0353\n",
      "192/295, train_loss: 0.0759, step time: 1.0602\n",
      "193/295, train_loss: 0.0366, step time: 1.0388\n",
      "194/295, train_loss: 0.0963, step time: 1.0456\n",
      "195/295, train_loss: 0.1135, step time: 1.0805\n",
      "196/295, train_loss: 0.0787, step time: 1.0345\n",
      "197/295, train_loss: 0.1435, step time: 1.0356\n",
      "198/295, train_loss: 0.1051, step time: 1.0328\n",
      "199/295, train_loss: 0.3644, step time: 1.0397\n",
      "200/295, train_loss: 0.3745, step time: 1.0679\n",
      "201/295, train_loss: 0.0865, step time: 1.0483\n",
      "202/295, train_loss: 0.1037, step time: 1.1190\n",
      "203/295, train_loss: 0.0556, step time: 1.0350\n",
      "204/295, train_loss: 0.0592, step time: 1.0309\n",
      "205/295, train_loss: 0.1050, step time: 1.0565\n",
      "206/295, train_loss: 0.0544, step time: 1.0821\n",
      "207/295, train_loss: 0.0662, step time: 1.0368\n",
      "208/295, train_loss: 0.1295, step time: 1.0374\n",
      "209/295, train_loss: 0.0920, step time: 1.0320\n",
      "210/295, train_loss: 0.0522, step time: 1.0326\n",
      "211/295, train_loss: 0.2118, step time: 1.0338\n",
      "212/295, train_loss: 0.0490, step time: 1.0341\n",
      "213/295, train_loss: 0.0524, step time: 1.0635\n",
      "214/295, train_loss: 0.1835, step time: 1.0345\n",
      "215/295, train_loss: 0.0545, step time: 1.0606\n",
      "216/295, train_loss: 0.0662, step time: 1.0536\n",
      "217/295, train_loss: 0.0895, step time: 1.0349\n",
      "218/295, train_loss: 0.1105, step time: 1.0595\n",
      "219/295, train_loss: 0.0561, step time: 1.0408\n",
      "220/295, train_loss: 0.0479, step time: 1.0466\n",
      "221/295, train_loss: 0.1174, step time: 1.0453\n",
      "222/295, train_loss: 0.1537, step time: 1.0448\n",
      "223/295, train_loss: 0.1044, step time: 1.0425\n",
      "224/295, train_loss: 0.0617, step time: 1.0314\n",
      "225/295, train_loss: 0.2840, step time: 1.0409\n",
      "226/295, train_loss: 0.1091, step time: 1.0664\n",
      "227/295, train_loss: 0.1376, step time: 1.0731\n",
      "228/295, train_loss: 0.2197, step time: 1.0374\n",
      "229/295, train_loss: 0.1100, step time: 1.0696\n",
      "230/295, train_loss: 0.1111, step time: 1.0385\n",
      "231/295, train_loss: 0.4238, step time: 1.0381\n",
      "232/295, train_loss: 0.0475, step time: 1.0326\n",
      "233/295, train_loss: 0.1332, step time: 1.0401\n",
      "234/295, train_loss: 0.1244, step time: 1.0365\n",
      "235/295, train_loss: 0.1046, step time: 1.0420\n",
      "236/295, train_loss: 0.1505, step time: 1.0732\n",
      "237/295, train_loss: 0.0460, step time: 1.0398\n",
      "238/295, train_loss: 0.0882, step time: 1.0624\n",
      "239/295, train_loss: 0.4094, step time: 1.0955\n",
      "240/295, train_loss: 0.0726, step time: 1.0421\n",
      "241/295, train_loss: 0.4728, step time: 1.0723\n",
      "242/295, train_loss: 0.0904, step time: 1.0449\n",
      "243/295, train_loss: 0.0997, step time: 1.0421\n",
      "244/295, train_loss: 0.1585, step time: 1.0455\n",
      "245/295, train_loss: 0.0512, step time: 1.0345\n",
      "246/295, train_loss: 0.1279, step time: 1.0365\n",
      "247/295, train_loss: 0.0799, step time: 1.0434\n",
      "248/295, train_loss: 0.0499, step time: 1.0360\n",
      "249/295, train_loss: 0.0542, step time: 1.0343\n",
      "250/295, train_loss: 0.1079, step time: 1.0509\n",
      "251/295, train_loss: 0.2914, step time: 1.0513\n",
      "252/295, train_loss: 0.1051, step time: 1.0783\n",
      "253/295, train_loss: 0.0665, step time: 1.0380\n",
      "254/295, train_loss: 0.0703, step time: 1.0372\n",
      "255/295, train_loss: 0.1294, step time: 1.0344\n",
      "256/295, train_loss: 0.0789, step time: 1.0358\n",
      "257/295, train_loss: 0.1169, step time: 1.0345\n",
      "258/295, train_loss: 0.0449, step time: 1.0350\n",
      "259/295, train_loss: 0.1426, step time: 1.0390\n",
      "260/295, train_loss: 0.0628, step time: 1.0353\n",
      "261/295, train_loss: 0.0473, step time: 1.0385\n",
      "262/295, train_loss: 0.0735, step time: 1.0626\n",
      "263/295, train_loss: 0.0406, step time: 1.0521\n",
      "264/295, train_loss: 0.0368, step time: 1.0423\n",
      "265/295, train_loss: 0.0822, step time: 1.0573\n",
      "266/295, train_loss: 0.0464, step time: 1.0312\n",
      "267/295, train_loss: 0.0884, step time: 1.1310\n",
      "268/295, train_loss: 0.0828, step time: 1.0401\n",
      "269/295, train_loss: 0.0812, step time: 1.0706\n",
      "270/295, train_loss: 0.1205, step time: 1.0635\n",
      "271/295, train_loss: 0.0581, step time: 1.0388\n",
      "272/295, train_loss: 0.1052, step time: 1.0402\n",
      "273/295, train_loss: 0.0549, step time: 1.0342\n",
      "274/295, train_loss: 0.1110, step time: 1.0636\n",
      "275/295, train_loss: 0.0392, step time: 1.0406\n",
      "276/295, train_loss: 0.0704, step time: 1.0331\n",
      "277/295, train_loss: 0.0545, step time: 1.0367\n",
      "278/295, train_loss: 0.0451, step time: 1.0359\n",
      "279/295, train_loss: 0.2815, step time: 1.0939\n",
      "280/295, train_loss: 0.0672, step time: 1.1097\n",
      "281/295, train_loss: 0.0941, step time: 1.0408\n",
      "282/295, train_loss: 0.1401, step time: 1.0515\n",
      "283/295, train_loss: 0.0868, step time: 1.0340\n",
      "284/295, train_loss: 0.0620, step time: 1.0532\n",
      "285/295, train_loss: 0.5935, step time: 1.0778\n",
      "286/295, train_loss: 0.1096, step time: 1.0823\n",
      "287/295, train_loss: 0.0616, step time: 1.0340\n",
      "288/295, train_loss: 0.1143, step time: 1.0308\n",
      "289/295, train_loss: 0.1250, step time: 1.0300\n",
      "290/295, train_loss: 0.0635, step time: 1.0304\n",
      "291/295, train_loss: 0.0917, step time: 1.0292\n",
      "292/295, train_loss: 0.1392, step time: 1.0301\n",
      "293/295, train_loss: 0.0441, step time: 1.0290\n",
      "294/295, train_loss: 0.1943, step time: 1.0284\n",
      "295/295, train_loss: 0.2371, step time: 1.0283\n",
      "epoch 25 average loss: 0.1307\n",
      "current epoch: 25 current mean dice: 0.7738 tc: 0.7144 wt: 0.8531 et: 0.7614\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 25 is: 382.4829\n",
      "----------\n",
      "epoch 26/100\n",
      "1/295, train_loss: 0.0803, step time: 1.1036\n",
      "2/295, train_loss: 0.0312, step time: 1.1496\n",
      "3/295, train_loss: 0.0617, step time: 1.0564\n",
      "4/295, train_loss: 0.2481, step time: 1.0554\n",
      "5/295, train_loss: 0.1655, step time: 1.0321\n",
      "6/295, train_loss: 0.0705, step time: 1.0516\n",
      "7/295, train_loss: 0.0737, step time: 1.0374\n",
      "8/295, train_loss: 0.0508, step time: 1.0373\n",
      "9/295, train_loss: 0.0610, step time: 1.0354\n",
      "10/295, train_loss: 0.0731, step time: 1.0861\n",
      "11/295, train_loss: 0.0594, step time: 1.0717\n",
      "12/295, train_loss: 0.0437, step time: 1.0600\n",
      "13/295, train_loss: 0.1268, step time: 1.0573\n",
      "14/295, train_loss: 0.0940, step time: 1.0412\n",
      "15/295, train_loss: 0.3467, step time: 1.0350\n",
      "16/295, train_loss: 0.0895, step time: 1.0822\n",
      "17/295, train_loss: 0.1604, step time: 1.0421\n",
      "18/295, train_loss: 0.2168, step time: 1.0423\n",
      "19/295, train_loss: 0.1566, step time: 1.0683\n",
      "20/295, train_loss: 0.0624, step time: 1.0951\n",
      "21/295, train_loss: 0.0716, step time: 1.0546\n",
      "22/295, train_loss: 0.0858, step time: 1.0424\n",
      "23/295, train_loss: 0.0739, step time: 1.0387\n",
      "24/295, train_loss: 0.0630, step time: 1.0700\n",
      "25/295, train_loss: 0.0493, step time: 1.1468\n",
      "26/295, train_loss: 0.1435, step time: 1.0442\n",
      "27/295, train_loss: 0.0426, step time: 1.0568\n",
      "28/295, train_loss: 0.1678, step time: 1.0608\n",
      "29/295, train_loss: 0.0986, step time: 1.1414\n",
      "30/295, train_loss: 0.0635, step time: 1.0386\n",
      "31/295, train_loss: 0.1356, step time: 1.0315\n",
      "32/295, train_loss: 0.0560, step time: 1.0891\n",
      "33/295, train_loss: 0.0615, step time: 1.0433\n",
      "34/295, train_loss: 0.0726, step time: 1.0420\n",
      "35/295, train_loss: 0.1310, step time: 1.0395\n",
      "36/295, train_loss: 0.0818, step time: 1.0571\n",
      "37/295, train_loss: 0.1398, step time: 1.0327\n",
      "38/295, train_loss: 0.0727, step time: 1.0590\n",
      "39/295, train_loss: 0.0864, step time: 1.0539\n",
      "40/295, train_loss: 0.1225, step time: 1.0640\n",
      "41/295, train_loss: 0.2503, step time: 1.0342\n",
      "42/295, train_loss: 0.4466, step time: 1.0603\n",
      "43/295, train_loss: 0.1180, step time: 1.0854\n",
      "44/295, train_loss: 0.1121, step time: 1.0390\n",
      "45/295, train_loss: 0.0875, step time: 1.0370\n",
      "46/295, train_loss: 0.1144, step time: 1.0368\n",
      "47/295, train_loss: 0.0625, step time: 1.0351\n",
      "48/295, train_loss: 0.0580, step time: 1.0628\n",
      "49/295, train_loss: 0.0462, step time: 1.0397\n",
      "50/295, train_loss: 0.1165, step time: 1.1318\n",
      "51/295, train_loss: 0.1625, step time: 1.0431\n",
      "52/295, train_loss: 0.0773, step time: 1.0425\n",
      "53/295, train_loss: 0.0817, step time: 1.0382\n",
      "54/295, train_loss: 0.0392, step time: 1.0573\n",
      "55/295, train_loss: 0.1060, step time: 1.0464\n",
      "56/295, train_loss: 0.1343, step time: 1.0346\n",
      "57/295, train_loss: 0.0945, step time: 1.0429\n",
      "58/295, train_loss: 0.0779, step time: 1.0824\n",
      "59/295, train_loss: 0.1143, step time: 1.0435\n",
      "60/295, train_loss: 0.0965, step time: 1.0760\n",
      "61/295, train_loss: 0.0637, step time: 1.0345\n",
      "62/295, train_loss: 0.0532, step time: 1.0409\n",
      "63/295, train_loss: 0.1692, step time: 1.0550\n",
      "64/295, train_loss: 0.0904, step time: 1.0392\n",
      "65/295, train_loss: 0.0979, step time: 1.0606\n",
      "66/295, train_loss: 0.4274, step time: 1.0360\n",
      "67/295, train_loss: 0.0535, step time: 1.0345\n",
      "68/295, train_loss: 0.1065, step time: 1.0651\n",
      "69/295, train_loss: 0.1161, step time: 1.0433\n",
      "70/295, train_loss: 0.0867, step time: 1.0547\n",
      "71/295, train_loss: 0.1595, step time: 1.0383\n",
      "72/295, train_loss: 0.1339, step time: 1.0365\n",
      "73/295, train_loss: 0.0460, step time: 1.0518\n",
      "74/295, train_loss: 0.1125, step time: 1.0513\n",
      "75/295, train_loss: 0.0794, step time: 1.0348\n",
      "76/295, train_loss: 0.3936, step time: 1.0588\n",
      "77/295, train_loss: 0.0668, step time: 1.0642\n",
      "78/295, train_loss: 0.1357, step time: 1.0671\n",
      "79/295, train_loss: 0.2127, step time: 1.0328\n",
      "80/295, train_loss: 0.0763, step time: 1.0328\n",
      "81/295, train_loss: 0.4200, step time: 1.0713\n",
      "82/295, train_loss: 0.0461, step time: 1.0372\n",
      "83/295, train_loss: 0.1116, step time: 1.0369\n",
      "84/295, train_loss: 0.1213, step time: 1.0360\n",
      "85/295, train_loss: 0.0695, step time: 1.0564\n",
      "86/295, train_loss: 0.0918, step time: 1.0548\n",
      "87/295, train_loss: 0.0519, step time: 1.0386\n",
      "88/295, train_loss: 0.0966, step time: 1.0589\n",
      "89/295, train_loss: 0.0703, step time: 1.0313\n",
      "90/295, train_loss: 0.0764, step time: 1.0542\n",
      "91/295, train_loss: 0.0524, step time: 1.0732\n",
      "92/295, train_loss: 0.0526, step time: 1.0526\n",
      "93/295, train_loss: 0.0755, step time: 1.0375\n",
      "94/295, train_loss: 0.0864, step time: 1.0449\n",
      "95/295, train_loss: 0.1518, step time: 1.0370\n",
      "96/295, train_loss: 0.2174, step time: 1.0488\n",
      "97/295, train_loss: 0.0716, step time: 1.0452\n",
      "98/295, train_loss: 0.0426, step time: 1.1294\n",
      "99/295, train_loss: 0.0686, step time: 1.0370\n",
      "100/295, train_loss: 0.0949, step time: 1.0319\n",
      "101/295, train_loss: 0.0542, step time: 1.0347\n",
      "102/295, train_loss: 0.1106, step time: 1.0410\n",
      "103/295, train_loss: 0.0469, step time: 1.0409\n",
      "104/295, train_loss: 0.0729, step time: 1.0415\n",
      "105/295, train_loss: 0.0729, step time: 1.0668\n",
      "106/295, train_loss: 0.0705, step time: 1.0363\n",
      "107/295, train_loss: 0.0354, step time: 1.0472\n",
      "108/295, train_loss: 0.0465, step time: 1.0593\n",
      "109/295, train_loss: 0.4872, step time: 1.0407\n",
      "110/295, train_loss: 0.1496, step time: 1.0607\n",
      "111/295, train_loss: 0.0949, step time: 1.0386\n",
      "112/295, train_loss: 0.2188, step time: 1.0399\n",
      "113/295, train_loss: 0.0860, step time: 1.0447\n",
      "114/295, train_loss: 0.1455, step time: 1.0440\n",
      "115/295, train_loss: 0.2320, step time: 1.0306\n",
      "116/295, train_loss: 0.1655, step time: 1.0310\n",
      "117/295, train_loss: 0.0508, step time: 1.0450\n",
      "118/295, train_loss: 0.0652, step time: 1.0444\n",
      "119/295, train_loss: 0.0463, step time: 1.0424\n",
      "120/295, train_loss: 0.1041, step time: 1.0402\n",
      "121/295, train_loss: 0.0443, step time: 1.0411\n",
      "122/295, train_loss: 0.0852, step time: 1.0708\n",
      "123/295, train_loss: 0.0860, step time: 1.0395\n",
      "124/295, train_loss: 0.1323, step time: 1.0520\n",
      "125/295, train_loss: 0.3975, step time: 1.0622\n",
      "126/295, train_loss: 0.3898, step time: 1.0328\n",
      "127/295, train_loss: 0.4434, step time: 1.0445\n",
      "128/295, train_loss: 0.0929, step time: 1.0591\n",
      "129/295, train_loss: 0.2101, step time: 1.0353\n",
      "130/295, train_loss: 0.4271, step time: 1.0722\n",
      "131/295, train_loss: 0.1186, step time: 1.0471\n",
      "132/295, train_loss: 0.2690, step time: 1.0356\n",
      "133/295, train_loss: 0.0786, step time: 1.0437\n",
      "134/295, train_loss: 0.0629, step time: 1.0521\n",
      "135/295, train_loss: 0.0994, step time: 1.0591\n",
      "136/295, train_loss: 0.1060, step time: 1.0325\n",
      "137/295, train_loss: 0.3719, step time: 1.0391\n",
      "138/295, train_loss: 0.1344, step time: 1.0600\n",
      "139/295, train_loss: 0.0358, step time: 1.0335\n",
      "140/295, train_loss: 0.2424, step time: 1.0525\n",
      "141/295, train_loss: 0.3806, step time: 1.0399\n",
      "142/295, train_loss: 0.0380, step time: 1.0386\n",
      "143/295, train_loss: 0.5062, step time: 1.0824\n",
      "144/295, train_loss: 0.0995, step time: 1.0415\n",
      "145/295, train_loss: 0.1649, step time: 1.0451\n",
      "146/295, train_loss: 0.0577, step time: 1.0696\n",
      "147/295, train_loss: 0.1014, step time: 1.0384\n",
      "148/295, train_loss: 0.0560, step time: 1.0533\n",
      "149/295, train_loss: 0.1084, step time: 1.0579\n",
      "150/295, train_loss: 0.4218, step time: 1.0326\n",
      "151/295, train_loss: 0.0503, step time: 1.0384\n",
      "152/295, train_loss: 0.3835, step time: 1.0421\n",
      "153/295, train_loss: 0.0702, step time: 1.0462\n",
      "154/295, train_loss: 0.1320, step time: 1.0583\n",
      "155/295, train_loss: 0.1103, step time: 1.0521\n",
      "156/295, train_loss: 0.0488, step time: 1.0819\n",
      "157/295, train_loss: 0.0452, step time: 1.0325\n",
      "158/295, train_loss: 0.0908, step time: 1.0392\n",
      "159/295, train_loss: 0.0892, step time: 1.0356\n",
      "160/295, train_loss: 0.0366, step time: 1.0364\n",
      "161/295, train_loss: 0.0614, step time: 1.0485\n",
      "162/295, train_loss: 0.0431, step time: 1.0887\n",
      "163/295, train_loss: 0.1452, step time: 1.0411\n",
      "164/295, train_loss: 0.1689, step time: 1.0357\n",
      "165/295, train_loss: 0.2047, step time: 1.0411\n",
      "166/295, train_loss: 0.0765, step time: 1.0887\n",
      "167/295, train_loss: 0.0595, step time: 1.0694\n",
      "168/295, train_loss: 0.0954, step time: 1.0461\n",
      "169/295, train_loss: 0.1076, step time: 1.0415\n",
      "170/295, train_loss: 0.0506, step time: 1.0404\n",
      "171/295, train_loss: 0.0923, step time: 1.0363\n",
      "172/295, train_loss: 0.1069, step time: 1.0372\n",
      "173/295, train_loss: 0.1531, step time: 1.0611\n",
      "174/295, train_loss: 0.0643, step time: 1.0500\n",
      "175/295, train_loss: 0.0987, step time: 1.0365\n",
      "176/295, train_loss: 0.1083, step time: 1.0362\n",
      "177/295, train_loss: 0.0560, step time: 1.0568\n",
      "178/295, train_loss: 0.0674, step time: 1.0386\n",
      "179/295, train_loss: 0.1622, step time: 1.0315\n",
      "180/295, train_loss: 0.0701, step time: 1.0318\n",
      "181/295, train_loss: 0.1264, step time: 1.0394\n",
      "182/295, train_loss: 0.0389, step time: 1.0634\n",
      "183/295, train_loss: 0.1274, step time: 1.0411\n",
      "184/295, train_loss: 0.1586, step time: 1.0321\n",
      "185/295, train_loss: 0.0759, step time: 1.0551\n",
      "186/295, train_loss: 0.0880, step time: 1.1261\n",
      "187/295, train_loss: 0.1251, step time: 1.0648\n",
      "188/295, train_loss: 0.0488, step time: 1.0673\n",
      "189/295, train_loss: 0.4010, step time: 1.0474\n",
      "190/295, train_loss: 0.1333, step time: 1.0371\n",
      "191/295, train_loss: 0.1774, step time: 1.0325\n",
      "192/295, train_loss: 0.4081, step time: 1.0420\n",
      "193/295, train_loss: 0.0565, step time: 1.0753\n",
      "194/295, train_loss: 0.1118, step time: 1.0362\n",
      "195/295, train_loss: 0.0639, step time: 1.0407\n",
      "196/295, train_loss: 0.1532, step time: 1.0395\n",
      "197/295, train_loss: 0.0421, step time: 1.0417\n",
      "198/295, train_loss: 0.1062, step time: 1.0528\n",
      "199/295, train_loss: 0.1138, step time: 1.0320\n",
      "200/295, train_loss: 0.0953, step time: 1.0384\n",
      "201/295, train_loss: 0.0574, step time: 1.0394\n",
      "202/295, train_loss: 0.1225, step time: 1.0349\n",
      "203/295, train_loss: 0.0476, step time: 1.0430\n",
      "204/295, train_loss: 0.1485, step time: 1.0393\n",
      "205/295, train_loss: 0.0441, step time: 1.0500\n",
      "206/295, train_loss: 0.2575, step time: 1.0329\n",
      "207/295, train_loss: 0.0475, step time: 1.0399\n",
      "208/295, train_loss: 0.0740, step time: 1.0308\n",
      "209/295, train_loss: 0.0446, step time: 1.0412\n",
      "210/295, train_loss: 0.1884, step time: 1.0366\n",
      "211/295, train_loss: 0.0603, step time: 1.0329\n",
      "212/295, train_loss: 0.1143, step time: 1.0435\n",
      "213/295, train_loss: 0.0657, step time: 1.0711\n",
      "214/295, train_loss: 0.0559, step time: 1.0621\n",
      "215/295, train_loss: 0.0853, step time: 1.0376\n",
      "216/295, train_loss: 0.4049, step time: 1.0598\n",
      "217/295, train_loss: 0.1002, step time: 1.0550\n",
      "218/295, train_loss: 0.4024, step time: 1.0422\n",
      "219/295, train_loss: 0.1485, step time: 1.0392\n",
      "220/295, train_loss: 0.0542, step time: 1.0456\n",
      "221/295, train_loss: 0.0516, step time: 1.0547\n",
      "222/295, train_loss: 0.0735, step time: 1.0361\n",
      "223/295, train_loss: 0.0921, step time: 1.0356\n",
      "224/295, train_loss: 0.1231, step time: 1.0712\n",
      "225/295, train_loss: 0.0556, step time: 1.0372\n",
      "226/295, train_loss: 0.0860, step time: 1.0623\n",
      "227/295, train_loss: 0.3928, step time: 1.0377\n",
      "228/295, train_loss: 0.0420, step time: 1.0513\n",
      "229/295, train_loss: 0.1985, step time: 1.0436\n",
      "230/295, train_loss: 0.1875, step time: 1.0356\n",
      "231/295, train_loss: 0.1006, step time: 1.0416\n",
      "232/295, train_loss: 0.1451, step time: 1.0586\n",
      "233/295, train_loss: 0.0649, step time: 1.0532\n",
      "234/295, train_loss: 0.0704, step time: 1.0658\n",
      "235/295, train_loss: 0.0369, step time: 1.0621\n",
      "236/295, train_loss: 0.0728, step time: 1.1202\n",
      "237/295, train_loss: 0.1127, step time: 1.0327\n",
      "238/295, train_loss: 0.1791, step time: 1.0480\n",
      "239/295, train_loss: 0.1494, step time: 1.0393\n",
      "240/295, train_loss: 0.1072, step time: 1.0410\n",
      "241/295, train_loss: 0.1114, step time: 1.0341\n",
      "242/295, train_loss: 0.4114, step time: 1.0433\n",
      "243/295, train_loss: 0.0946, step time: 1.0425\n",
      "244/295, train_loss: 0.0984, step time: 1.0845\n",
      "245/295, train_loss: 0.3752, step time: 1.0353\n",
      "246/295, train_loss: 0.0657, step time: 1.0518\n",
      "247/295, train_loss: 0.1104, step time: 1.0461\n",
      "248/295, train_loss: 0.3341, step time: 1.0368\n",
      "249/295, train_loss: 0.0680, step time: 1.0620\n",
      "250/295, train_loss: 0.0989, step time: 1.0817\n",
      "251/295, train_loss: 0.1199, step time: 1.0436\n",
      "252/295, train_loss: 0.2254, step time: 1.0379\n",
      "253/295, train_loss: 0.1541, step time: 1.0361\n",
      "254/295, train_loss: 0.2785, step time: 1.0364\n",
      "255/295, train_loss: 0.0597, step time: 1.0668\n",
      "256/295, train_loss: 0.0510, step time: 1.0479\n",
      "257/295, train_loss: 0.0854, step time: 1.0407\n",
      "258/295, train_loss: 0.0786, step time: 1.0428\n",
      "259/295, train_loss: 0.0384, step time: 1.0356\n",
      "260/295, train_loss: 0.2768, step time: 1.0469\n",
      "261/295, train_loss: 0.0471, step time: 1.0550\n",
      "262/295, train_loss: 0.0706, step time: 1.0342\n",
      "263/295, train_loss: 0.1266, step time: 1.0370\n",
      "264/295, train_loss: 0.4368, step time: 1.0399\n",
      "265/295, train_loss: 0.0767, step time: 1.0375\n",
      "266/295, train_loss: 0.1131, step time: 1.0466\n",
      "267/295, train_loss: 0.0387, step time: 1.0737\n",
      "268/295, train_loss: 0.0649, step time: 1.0342\n",
      "269/295, train_loss: 0.1673, step time: 1.0379\n",
      "270/295, train_loss: 0.1045, step time: 1.0378\n",
      "271/295, train_loss: 0.0410, step time: 1.0445\n",
      "272/295, train_loss: 0.1163, step time: 1.0601\n",
      "273/295, train_loss: 0.1448, step time: 1.0352\n",
      "274/295, train_loss: 0.1116, step time: 1.0419\n",
      "275/295, train_loss: 0.0901, step time: 1.0392\n",
      "276/295, train_loss: 0.0895, step time: 1.0400\n",
      "277/295, train_loss: 0.0788, step time: 1.0450\n",
      "278/295, train_loss: 0.0942, step time: 1.0581\n",
      "279/295, train_loss: 0.0875, step time: 1.0484\n",
      "280/295, train_loss: 0.2983, step time: 1.0678\n",
      "281/295, train_loss: 0.3960, step time: 1.0609\n",
      "282/295, train_loss: 0.4169, step time: 1.0497\n",
      "283/295, train_loss: 0.0824, step time: 1.0553\n",
      "284/295, train_loss: 0.0425, step time: 1.0335\n",
      "285/295, train_loss: 0.0908, step time: 1.0351\n",
      "286/295, train_loss: 0.0717, step time: 1.0532\n",
      "287/295, train_loss: 0.1827, step time: 1.1023\n",
      "288/295, train_loss: 0.0941, step time: 1.0291\n",
      "289/295, train_loss: 0.0716, step time: 1.0285\n",
      "290/295, train_loss: 0.0946, step time: 1.0297\n",
      "291/295, train_loss: 0.0785, step time: 1.0292\n",
      "292/295, train_loss: 0.5728, step time: 1.0294\n",
      "293/295, train_loss: 0.2670, step time: 1.0293\n",
      "294/295, train_loss: 0.4376, step time: 1.0292\n",
      "295/295, train_loss: 0.0812, step time: 1.0294\n",
      "epoch 26 average loss: 0.1303\n",
      "current epoch: 26 current mean dice: 0.7811 tc: 0.7241 wt: 0.8516 et: 0.7679\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 26 is: 384.7299\n",
      "----------\n",
      "epoch 27/100\n",
      "1/295, train_loss: 0.3985, step time: 1.0603\n",
      "2/295, train_loss: 0.0853, step time: 1.0780\n",
      "3/295, train_loss: 0.0543, step time: 1.0659\n",
      "4/295, train_loss: 0.1084, step time: 1.0739\n",
      "5/295, train_loss: 0.0460, step time: 1.0317\n",
      "6/295, train_loss: 0.1145, step time: 1.0311\n",
      "7/295, train_loss: 0.0718, step time: 1.0391\n",
      "8/295, train_loss: 0.5027, step time: 1.0755\n",
      "9/295, train_loss: 0.2086, step time: 1.0311\n",
      "10/295, train_loss: 0.4348, step time: 1.0346\n",
      "11/295, train_loss: 0.0294, step time: 1.0717\n",
      "12/295, train_loss: 0.1421, step time: 1.0724\n",
      "13/295, train_loss: 0.0363, step time: 1.0456\n",
      "14/295, train_loss: 0.0746, step time: 1.0299\n",
      "15/295, train_loss: 0.1472, step time: 1.0335\n",
      "16/295, train_loss: 0.1439, step time: 1.0424\n",
      "17/295, train_loss: 0.2783, step time: 1.0415\n",
      "18/295, train_loss: 0.0765, step time: 1.0577\n",
      "19/295, train_loss: 0.1167, step time: 1.0395\n",
      "20/295, train_loss: 0.0833, step time: 1.0384\n",
      "21/295, train_loss: 0.0371, step time: 1.0320\n",
      "22/295, train_loss: 0.1259, step time: 1.0354\n",
      "23/295, train_loss: 0.0465, step time: 1.0375\n",
      "24/295, train_loss: 0.1481, step time: 1.0647\n",
      "25/295, train_loss: 0.1502, step time: 1.0431\n",
      "26/295, train_loss: 0.3982, step time: 1.0572\n",
      "27/295, train_loss: 0.0721, step time: 1.0736\n",
      "28/295, train_loss: 0.1173, step time: 1.0398\n",
      "29/295, train_loss: 0.0476, step time: 1.0514\n",
      "30/295, train_loss: 0.3109, step time: 1.0420\n",
      "31/295, train_loss: 0.1355, step time: 1.0811\n",
      "32/295, train_loss: 0.1334, step time: 1.0443\n",
      "33/295, train_loss: 0.0657, step time: 1.0342\n",
      "34/295, train_loss: 0.1089, step time: 1.0378\n",
      "35/295, train_loss: 0.1138, step time: 1.0631\n",
      "36/295, train_loss: 0.3804, step time: 1.0464\n",
      "37/295, train_loss: 0.0584, step time: 1.0319\n",
      "38/295, train_loss: 0.0814, step time: 1.0667\n",
      "39/295, train_loss: 0.2021, step time: 1.0326\n",
      "40/295, train_loss: 0.0714, step time: 1.0423\n",
      "41/295, train_loss: 0.0652, step time: 1.0332\n",
      "42/295, train_loss: 0.0594, step time: 1.0476\n",
      "43/295, train_loss: 0.0737, step time: 1.1375\n",
      "44/295, train_loss: 0.0749, step time: 1.0407\n",
      "45/295, train_loss: 0.0594, step time: 1.0993\n",
      "46/295, train_loss: 0.0807, step time: 1.0385\n",
      "47/295, train_loss: 0.0818, step time: 1.0523\n",
      "48/295, train_loss: 0.1495, step time: 1.0314\n",
      "49/295, train_loss: 0.1037, step time: 1.0411\n",
      "50/295, train_loss: 0.0622, step time: 1.0378\n",
      "51/295, train_loss: 0.4346, step time: 1.0721\n",
      "52/295, train_loss: 0.0452, step time: 1.0370\n",
      "53/295, train_loss: 0.0748, step time: 1.0662\n",
      "54/295, train_loss: 0.1031, step time: 1.0398\n",
      "55/295, train_loss: 0.0943, step time: 1.0958\n",
      "56/295, train_loss: 0.1364, step time: 1.0465\n",
      "57/295, train_loss: 0.1498, step time: 1.0366\n",
      "58/295, train_loss: 0.0519, step time: 1.0492\n",
      "59/295, train_loss: 0.0514, step time: 1.0358\n",
      "60/295, train_loss: 0.1435, step time: 1.0549\n",
      "61/295, train_loss: 0.1590, step time: 1.0408\n",
      "62/295, train_loss: 0.0448, step time: 1.0523\n",
      "63/295, train_loss: 0.1083, step time: 1.0579\n",
      "64/295, train_loss: 0.0749, step time: 1.0613\n",
      "65/295, train_loss: 0.0480, step time: 1.0770\n",
      "66/295, train_loss: 0.1673, step time: 1.0517\n",
      "67/295, train_loss: 0.0909, step time: 1.0844\n",
      "68/295, train_loss: 0.1383, step time: 1.0524\n",
      "69/295, train_loss: 0.1002, step time: 1.0534\n",
      "70/295, train_loss: 0.1041, step time: 1.0638\n",
      "71/295, train_loss: 0.1426, step time: 1.0434\n",
      "72/295, train_loss: 0.0810, step time: 1.0595\n",
      "73/295, train_loss: 0.4130, step time: 1.0534\n",
      "74/295, train_loss: 0.0965, step time: 1.0466\n",
      "75/295, train_loss: 0.3775, step time: 1.0538\n",
      "76/295, train_loss: 0.0700, step time: 1.0643\n",
      "77/295, train_loss: 0.0809, step time: 1.0517\n",
      "78/295, train_loss: 0.0406, step time: 1.0364\n",
      "79/295, train_loss: 0.0999, step time: 1.0434\n",
      "80/295, train_loss: 0.0594, step time: 1.0624\n",
      "81/295, train_loss: 0.3654, step time: 1.0581\n",
      "82/295, train_loss: 0.0711, step time: 1.0686\n",
      "83/295, train_loss: 0.0569, step time: 1.1265\n",
      "84/295, train_loss: 0.3012, step time: 1.0433\n",
      "85/295, train_loss: 0.0549, step time: 1.0738\n",
      "86/295, train_loss: 0.0781, step time: 1.0359\n",
      "87/295, train_loss: 0.1178, step time: 1.1187\n",
      "88/295, train_loss: 0.0333, step time: 1.0551\n",
      "89/295, train_loss: 0.1193, step time: 1.0312\n",
      "90/295, train_loss: 0.1099, step time: 1.1129\n",
      "91/295, train_loss: 0.0456, step time: 1.0454\n",
      "92/295, train_loss: 0.1036, step time: 1.0520\n",
      "93/295, train_loss: 0.0925, step time: 1.0331\n",
      "94/295, train_loss: 0.1157, step time: 1.0598\n",
      "95/295, train_loss: 0.0883, step time: 1.0563\n",
      "96/295, train_loss: 0.0697, step time: 1.0394\n",
      "97/295, train_loss: 0.0716, step time: 1.0358\n",
      "98/295, train_loss: 0.1066, step time: 1.0477\n",
      "99/295, train_loss: 0.0522, step time: 1.0383\n",
      "100/295, train_loss: 0.1603, step time: 1.0886\n",
      "101/295, train_loss: 0.0403, step time: 1.0521\n",
      "102/295, train_loss: 0.0969, step time: 1.0355\n",
      "103/295, train_loss: 0.1219, step time: 1.0359\n",
      "104/295, train_loss: 0.0529, step time: 1.0362\n",
      "105/295, train_loss: 0.1060, step time: 1.0331\n",
      "106/295, train_loss: 0.0822, step time: 1.0634\n",
      "107/295, train_loss: 0.2419, step time: 1.0319\n",
      "108/295, train_loss: 0.1586, step time: 1.0352\n",
      "109/295, train_loss: 0.0983, step time: 1.1051\n",
      "110/295, train_loss: 0.0592, step time: 1.0714\n",
      "111/295, train_loss: 0.0678, step time: 1.0474\n",
      "112/295, train_loss: 0.0752, step time: 1.0321\n",
      "113/295, train_loss: 0.0795, step time: 1.1028\n",
      "114/295, train_loss: 0.0630, step time: 1.0330\n",
      "115/295, train_loss: 0.0694, step time: 1.0319\n",
      "116/295, train_loss: 0.1440, step time: 1.0459\n",
      "117/295, train_loss: 0.1004, step time: 1.0556\n",
      "118/295, train_loss: 0.1114, step time: 1.0880\n",
      "119/295, train_loss: 0.0974, step time: 1.0366\n",
      "120/295, train_loss: 0.1677, step time: 1.0327\n",
      "121/295, train_loss: 0.1005, step time: 1.0370\n",
      "122/295, train_loss: 0.0515, step time: 1.0530\n",
      "123/295, train_loss: 0.1012, step time: 1.0356\n",
      "124/295, train_loss: 0.0480, step time: 1.0369\n",
      "125/295, train_loss: 0.2560, step time: 1.0459\n",
      "126/295, train_loss: 0.1741, step time: 1.0513\n",
      "127/295, train_loss: 0.1735, step time: 1.0571\n",
      "128/295, train_loss: 0.1279, step time: 1.1032\n",
      "129/295, train_loss: 0.0854, step time: 1.0353\n",
      "130/295, train_loss: 0.2590, step time: 1.0632\n",
      "131/295, train_loss: 0.1214, step time: 1.0516\n",
      "132/295, train_loss: 0.0558, step time: 1.0671\n",
      "133/295, train_loss: 0.1001, step time: 1.0699\n",
      "134/295, train_loss: 0.1233, step time: 1.0671\n",
      "135/295, train_loss: 0.0923, step time: 1.0747\n",
      "136/295, train_loss: 0.0812, step time: 1.0498\n",
      "137/295, train_loss: 0.0609, step time: 1.0389\n",
      "138/295, train_loss: 0.0565, step time: 1.0429\n",
      "139/295, train_loss: 0.0790, step time: 1.0495\n",
      "140/295, train_loss: 0.0530, step time: 1.0366\n",
      "141/295, train_loss: 0.0788, step time: 1.0380\n",
      "142/295, train_loss: 0.1390, step time: 1.0324\n",
      "143/295, train_loss: 0.0577, step time: 1.0391\n",
      "144/295, train_loss: 0.0431, step time: 1.0359\n",
      "145/295, train_loss: 0.1349, step time: 1.0382\n",
      "146/295, train_loss: 0.0479, step time: 1.0379\n",
      "147/295, train_loss: 0.1129, step time: 1.0391\n",
      "148/295, train_loss: 0.1009, step time: 1.0508\n",
      "149/295, train_loss: 0.1235, step time: 1.0419\n",
      "150/295, train_loss: 0.4081, step time: 1.0520\n",
      "151/295, train_loss: 0.0836, step time: 1.0387\n",
      "152/295, train_loss: 0.2007, step time: 1.0452\n",
      "153/295, train_loss: 0.0541, step time: 1.0361\n",
      "154/295, train_loss: 0.0568, step time: 1.0401\n",
      "155/295, train_loss: 0.0687, step time: 1.0371\n",
      "156/295, train_loss: 0.0462, step time: 1.0371\n",
      "157/295, train_loss: 0.0715, step time: 1.0480\n",
      "158/295, train_loss: 0.0492, step time: 1.0654\n",
      "159/295, train_loss: 0.0688, step time: 1.0426\n",
      "160/295, train_loss: 0.1169, step time: 1.1393\n",
      "161/295, train_loss: 0.0730, step time: 1.1122\n",
      "162/295, train_loss: 0.0450, step time: 1.0757\n",
      "163/295, train_loss: 0.3831, step time: 1.0398\n",
      "164/295, train_loss: 0.0706, step time: 1.0374\n",
      "165/295, train_loss: 0.0386, step time: 1.0449\n",
      "166/295, train_loss: 0.0905, step time: 1.0440\n",
      "167/295, train_loss: 0.0624, step time: 1.0544\n",
      "168/295, train_loss: 0.1062, step time: 1.0333\n",
      "169/295, train_loss: 0.0638, step time: 1.0340\n",
      "170/295, train_loss: 0.1129, step time: 1.0540\n",
      "171/295, train_loss: 0.1613, step time: 1.0381\n",
      "172/295, train_loss: 0.0636, step time: 1.0422\n",
      "173/295, train_loss: 0.0682, step time: 1.0376\n",
      "174/295, train_loss: 0.1446, step time: 1.0326\n",
      "175/295, train_loss: 0.4007, step time: 1.0361\n",
      "176/295, train_loss: 0.0683, step time: 1.0765\n",
      "177/295, train_loss: 0.4020, step time: 1.0396\n",
      "178/295, train_loss: 0.0747, step time: 1.0424\n",
      "179/295, train_loss: 0.1087, step time: 1.0401\n",
      "180/295, train_loss: 0.0417, step time: 1.0363\n",
      "181/295, train_loss: 0.1583, step time: 1.0658\n",
      "182/295, train_loss: 0.0854, step time: 1.0318\n",
      "183/295, train_loss: 0.0984, step time: 1.0939\n",
      "184/295, train_loss: 0.0778, step time: 1.0344\n",
      "185/295, train_loss: 0.2518, step time: 1.0329\n",
      "186/295, train_loss: 0.0371, step time: 1.0310\n",
      "187/295, train_loss: 0.1860, step time: 1.0445\n",
      "188/295, train_loss: 0.1963, step time: 1.0880\n",
      "189/295, train_loss: 0.1823, step time: 1.0481\n",
      "190/295, train_loss: 0.2713, step time: 1.0474\n",
      "191/295, train_loss: 0.0752, step time: 1.0358\n",
      "192/295, train_loss: 0.1137, step time: 1.0418\n",
      "193/295, train_loss: 0.1036, step time: 1.0378\n",
      "194/295, train_loss: 0.0460, step time: 1.0363\n",
      "195/295, train_loss: 0.0545, step time: 1.0334\n",
      "196/295, train_loss: 0.0902, step time: 1.0543\n",
      "197/295, train_loss: 0.1282, step time: 1.1855\n",
      "198/295, train_loss: 0.0485, step time: 1.0415\n",
      "199/295, train_loss: 0.1181, step time: 1.0355\n",
      "200/295, train_loss: 0.0891, step time: 1.0397\n",
      "201/295, train_loss: 0.0415, step time: 1.0388\n",
      "202/295, train_loss: 0.0406, step time: 1.0369\n",
      "203/295, train_loss: 0.0761, step time: 1.0497\n",
      "204/295, train_loss: 0.1609, step time: 1.0319\n",
      "205/295, train_loss: 0.0484, step time: 1.0686\n",
      "206/295, train_loss: 0.0766, step time: 1.0864\n",
      "207/295, train_loss: 0.3838, step time: 1.0879\n",
      "208/295, train_loss: 0.0503, step time: 1.0305\n",
      "209/295, train_loss: 0.0738, step time: 1.0649\n",
      "210/295, train_loss: 0.2114, step time: 1.1195\n",
      "211/295, train_loss: 0.0610, step time: 1.0340\n",
      "212/295, train_loss: 0.0899, step time: 1.0469\n",
      "213/295, train_loss: 0.1237, step time: 1.0418\n",
      "214/295, train_loss: 0.0565, step time: 1.0421\n",
      "215/295, train_loss: 0.0587, step time: 1.0498\n",
      "216/295, train_loss: 0.0617, step time: 1.0615\n",
      "217/295, train_loss: 0.2075, step time: 1.0563\n",
      "218/295, train_loss: 0.0482, step time: 1.0494\n",
      "219/295, train_loss: 0.1380, step time: 1.0341\n",
      "220/295, train_loss: 0.1153, step time: 1.0392\n",
      "221/295, train_loss: 0.0542, step time: 1.0342\n",
      "222/295, train_loss: 0.1170, step time: 1.0372\n",
      "223/295, train_loss: 0.0616, step time: 1.0442\n",
      "224/295, train_loss: 0.0635, step time: 1.0684\n",
      "225/295, train_loss: 0.0386, step time: 1.0707\n",
      "226/295, train_loss: 0.1276, step time: 1.0460\n",
      "227/295, train_loss: 0.1653, step time: 1.0582\n",
      "228/295, train_loss: 0.0765, step time: 1.1247\n",
      "229/295, train_loss: 0.1117, step time: 1.0444\n",
      "230/295, train_loss: 0.0835, step time: 1.0340\n",
      "231/295, train_loss: 0.0808, step time: 1.0519\n",
      "232/295, train_loss: 0.0643, step time: 1.0618\n",
      "233/295, train_loss: 0.0772, step time: 1.0308\n",
      "234/295, train_loss: 0.4080, step time: 1.0326\n",
      "235/295, train_loss: 0.0637, step time: 1.0351\n",
      "236/295, train_loss: 0.0974, step time: 1.0488\n",
      "237/295, train_loss: 0.4185, step time: 1.0730\n",
      "238/295, train_loss: 0.1374, step time: 1.0342\n",
      "239/295, train_loss: 0.2214, step time: 1.0568\n",
      "240/295, train_loss: 0.1496, step time: 1.0430\n",
      "241/295, train_loss: 0.0749, step time: 1.0653\n",
      "242/295, train_loss: 0.0440, step time: 1.0941\n",
      "243/295, train_loss: 0.4238, step time: 1.0584\n",
      "244/295, train_loss: 0.0464, step time: 1.0504\n",
      "245/295, train_loss: 0.0868, step time: 1.0551\n",
      "246/295, train_loss: 0.4129, step time: 1.1509\n",
      "247/295, train_loss: 0.5488, step time: 1.0676\n",
      "248/295, train_loss: 0.0626, step time: 1.0344\n",
      "249/295, train_loss: 0.1001, step time: 1.0755\n",
      "250/295, train_loss: 0.0586, step time: 1.0514\n",
      "251/295, train_loss: 0.0715, step time: 1.0419\n",
      "252/295, train_loss: 0.0545, step time: 1.0389\n",
      "253/295, train_loss: 0.0997, step time: 1.0531\n",
      "254/295, train_loss: 0.1324, step time: 1.0695\n",
      "255/295, train_loss: 0.1681, step time: 1.0351\n",
      "256/295, train_loss: 0.4073, step time: 1.0662\n",
      "257/295, train_loss: 0.1838, step time: 1.0359\n",
      "258/295, train_loss: 0.0498, step time: 1.0411\n",
      "259/295, train_loss: 0.0619, step time: 1.0592\n",
      "260/295, train_loss: 0.2094, step time: 1.0367\n",
      "261/295, train_loss: 0.0649, step time: 1.0421\n",
      "262/295, train_loss: 0.0428, step time: 1.1095\n",
      "263/295, train_loss: 0.1803, step time: 1.0471\n",
      "264/295, train_loss: 0.0632, step time: 1.0472\n",
      "265/295, train_loss: 0.0793, step time: 1.0398\n",
      "266/295, train_loss: 0.1030, step time: 1.0651\n",
      "267/295, train_loss: 0.0482, step time: 1.0547\n",
      "268/295, train_loss: 0.1558, step time: 1.0391\n",
      "269/295, train_loss: 0.0623, step time: 1.0328\n",
      "270/295, train_loss: 0.0366, step time: 1.0387\n",
      "271/295, train_loss: 0.4057, step time: 1.0703\n",
      "272/295, train_loss: 0.0643, step time: 1.0605\n",
      "273/295, train_loss: 0.1516, step time: 1.0503\n",
      "274/295, train_loss: 0.4468, step time: 1.0486\n",
      "275/295, train_loss: 0.0875, step time: 1.0490\n",
      "276/295, train_loss: 0.0537, step time: 1.0501\n",
      "277/295, train_loss: 0.0899, step time: 1.0509\n",
      "278/295, train_loss: 0.2614, step time: 1.0430\n",
      "279/295, train_loss: 0.0663, step time: 1.0502\n",
      "280/295, train_loss: 0.0962, step time: 1.0513\n",
      "281/295, train_loss: 0.1596, step time: 1.0430\n",
      "282/295, train_loss: 0.0878, step time: 1.0698\n",
      "283/295, train_loss: 0.2239, step time: 1.0365\n",
      "284/295, train_loss: 0.2499, step time: 1.0322\n",
      "285/295, train_loss: 0.0923, step time: 1.0464\n",
      "286/295, train_loss: 0.0469, step time: 1.0435\n",
      "287/295, train_loss: 0.0509, step time: 1.0626\n",
      "288/295, train_loss: 0.0915, step time: 1.0303\n",
      "289/295, train_loss: 0.1173, step time: 1.0281\n",
      "290/295, train_loss: 0.3237, step time: 1.0276\n",
      "291/295, train_loss: 0.0483, step time: 1.0291\n",
      "292/295, train_loss: 0.3991, step time: 1.0298\n",
      "293/295, train_loss: 0.1320, step time: 1.0294\n",
      "294/295, train_loss: 0.0538, step time: 1.0292\n",
      "295/295, train_loss: 0.4058, step time: 1.0289\n",
      "epoch 27 average loss: 0.1259\n",
      "current epoch: 27 current mean dice: 0.7680 tc: 0.7106 wt: 0.8587 et: 0.7365\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 27 is: 388.7766\n",
      "----------\n",
      "epoch 28/100\n",
      "1/295, train_loss: 0.0945, step time: 1.0741\n",
      "2/295, train_loss: 0.0448, step time: 1.0533\n",
      "3/295, train_loss: 0.0534, step time: 1.0797\n",
      "4/295, train_loss: 0.0652, step time: 1.0609\n",
      "5/295, train_loss: 0.4077, step time: 1.0362\n",
      "6/295, train_loss: 0.0727, step time: 1.0405\n",
      "7/295, train_loss: 0.0716, step time: 1.0661\n",
      "8/295, train_loss: 0.4456, step time: 1.0322\n",
      "9/295, train_loss: 0.1829, step time: 1.0384\n",
      "10/295, train_loss: 0.1158, step time: 1.0377\n",
      "11/295, train_loss: 0.1088, step time: 1.0288\n",
      "12/295, train_loss: 0.1147, step time: 1.0445\n",
      "13/295, train_loss: 0.4243, step time: 1.0329\n",
      "14/295, train_loss: 0.0528, step time: 1.0459\n",
      "15/295, train_loss: 0.0810, step time: 1.0390\n",
      "16/295, train_loss: 0.1831, step time: 1.0796\n",
      "17/295, train_loss: 0.0399, step time: 1.0323\n",
      "18/295, train_loss: 0.2901, step time: 1.0470\n",
      "19/295, train_loss: 0.0936, step time: 1.0596\n",
      "20/295, train_loss: 0.1371, step time: 1.0369\n",
      "21/295, train_loss: 0.0530, step time: 1.0354\n",
      "22/295, train_loss: 0.1312, step time: 1.1113\n",
      "23/295, train_loss: 0.0430, step time: 1.0534\n",
      "24/295, train_loss: 0.1092, step time: 1.0918\n",
      "25/295, train_loss: 0.1443, step time: 1.0367\n",
      "26/295, train_loss: 0.0825, step time: 1.0469\n",
      "27/295, train_loss: 0.0481, step time: 1.0450\n",
      "28/295, train_loss: 0.1079, step time: 1.0410\n",
      "29/295, train_loss: 0.1315, step time: 1.0378\n",
      "30/295, train_loss: 0.0589, step time: 1.1157\n",
      "31/295, train_loss: 0.1015, step time: 1.0677\n",
      "32/295, train_loss: 0.2812, step time: 1.0762\n",
      "33/295, train_loss: 0.0681, step time: 1.0433\n",
      "34/295, train_loss: 0.5085, step time: 1.0382\n",
      "35/295, train_loss: 0.1189, step time: 1.0401\n",
      "36/295, train_loss: 0.1129, step time: 1.0295\n",
      "37/295, train_loss: 0.1116, step time: 1.0528\n",
      "38/295, train_loss: 0.0888, step time: 1.0417\n",
      "39/295, train_loss: 0.0981, step time: 1.0416\n",
      "40/295, train_loss: 0.0447, step time: 1.0559\n",
      "41/295, train_loss: 0.4470, step time: 1.0768\n",
      "42/295, train_loss: 0.0801, step time: 1.0371\n",
      "43/295, train_loss: 0.0720, step time: 1.0520\n",
      "44/295, train_loss: 0.0639, step time: 1.0620\n",
      "45/295, train_loss: 0.1227, step time: 1.1012\n",
      "46/295, train_loss: 0.0990, step time: 1.0290\n",
      "47/295, train_loss: 0.1880, step time: 1.0314\n",
      "48/295, train_loss: 0.4291, step time: 1.0846\n",
      "49/295, train_loss: 0.1648, step time: 1.0412\n",
      "50/295, train_loss: 0.1075, step time: 1.0508\n",
      "51/295, train_loss: 0.1427, step time: 1.0357\n",
      "52/295, train_loss: 0.0921, step time: 1.0359\n",
      "53/295, train_loss: 0.0980, step time: 1.0516\n",
      "54/295, train_loss: 0.0399, step time: 1.0777\n",
      "55/295, train_loss: 0.3409, step time: 1.0023\n",
      "56/295, train_loss: 0.0687, step time: 1.0380\n",
      "57/295, train_loss: 0.0440, step time: 1.0296\n",
      "58/295, train_loss: 0.0678, step time: 1.0323\n",
      "59/295, train_loss: 0.0943, step time: 1.0516\n",
      "60/295, train_loss: 0.0954, step time: 1.0565\n",
      "61/295, train_loss: 0.1763, step time: 1.0471\n",
      "62/295, train_loss: 0.0731, step time: 1.0343\n",
      "63/295, train_loss: 0.0982, step time: 1.0385\n",
      "64/295, train_loss: 0.0525, step time: 1.0400\n",
      "65/295, train_loss: 0.1052, step time: 1.0424\n",
      "66/295, train_loss: 0.0440, step time: 1.0442\n",
      "67/295, train_loss: 0.1468, step time: 1.0801\n",
      "68/295, train_loss: 0.1172, step time: 1.0386\n",
      "69/295, train_loss: 0.0683, step time: 1.0552\n",
      "70/295, train_loss: 0.0389, step time: 1.0485\n",
      "71/295, train_loss: 0.1035, step time: 1.0489\n",
      "72/295, train_loss: 0.1079, step time: 1.0408\n",
      "73/295, train_loss: 0.1121, step time: 1.0732\n",
      "74/295, train_loss: 0.1214, step time: 1.0412\n",
      "75/295, train_loss: 0.0605, step time: 1.0551\n",
      "76/295, train_loss: 0.0995, step time: 1.0334\n",
      "77/295, train_loss: 0.1620, step time: 1.0333\n",
      "78/295, train_loss: 0.1224, step time: 1.0704\n",
      "79/295, train_loss: 0.0813, step time: 1.0416\n",
      "80/295, train_loss: 0.0782, step time: 1.0556\n",
      "81/295, train_loss: 0.4181, step time: 1.0354\n",
      "82/295, train_loss: 0.1134, step time: 1.0342\n",
      "83/295, train_loss: 0.0772, step time: 1.0532\n",
      "84/295, train_loss: 0.0631, step time: 1.0428\n",
      "85/295, train_loss: 0.2590, step time: 1.0528\n",
      "86/295, train_loss: 0.0976, step time: 1.0723\n",
      "87/295, train_loss: 0.0779, step time: 1.0307\n",
      "88/295, train_loss: 0.1249, step time: 1.0458\n",
      "89/295, train_loss: 0.0680, step time: 1.0353\n",
      "90/295, train_loss: 0.0517, step time: 1.0447\n",
      "91/295, train_loss: 0.0611, step time: 1.0363\n",
      "92/295, train_loss: 0.0254, step time: 1.0371\n",
      "93/295, train_loss: 0.0665, step time: 1.0382\n",
      "94/295, train_loss: 0.1388, step time: 1.0571\n",
      "95/295, train_loss: 0.1177, step time: 1.0311\n",
      "96/295, train_loss: 0.0802, step time: 1.0586\n",
      "97/295, train_loss: 0.1115, step time: 1.0844\n",
      "98/295, train_loss: 0.0556, step time: 1.0369\n",
      "99/295, train_loss: 0.0913, step time: 1.0850\n",
      "100/295, train_loss: 0.1049, step time: 1.0489\n",
      "101/295, train_loss: 0.1946, step time: 1.0493\n",
      "102/295, train_loss: 0.1152, step time: 1.0346\n",
      "103/295, train_loss: 0.0497, step time: 1.0430\n",
      "104/295, train_loss: 0.3916, step time: 1.0346\n",
      "105/295, train_loss: 0.0587, step time: 1.0698\n",
      "106/295, train_loss: 0.0500, step time: 1.0543\n",
      "107/295, train_loss: 0.1074, step time: 1.0356\n",
      "108/295, train_loss: 0.0477, step time: 1.0332\n",
      "109/295, train_loss: 0.0581, step time: 1.0946\n",
      "110/295, train_loss: 0.0735, step time: 1.0396\n",
      "111/295, train_loss: 0.0624, step time: 1.0336\n",
      "112/295, train_loss: 0.1260, step time: 1.0366\n",
      "113/295, train_loss: 0.0549, step time: 1.0371\n",
      "114/295, train_loss: 0.1498, step time: 1.0433\n",
      "115/295, train_loss: 0.1216, step time: 1.0623\n",
      "116/295, train_loss: 0.1746, step time: 1.0428\n",
      "117/295, train_loss: 0.5091, step time: 1.0524\n",
      "118/295, train_loss: 0.0688, step time: 1.0441\n",
      "119/295, train_loss: 0.0633, step time: 1.0395\n",
      "120/295, train_loss: 0.0513, step time: 1.0584\n",
      "121/295, train_loss: 0.1044, step time: 1.0514\n",
      "122/295, train_loss: 0.1351, step time: 1.0628\n",
      "123/295, train_loss: 0.4105, step time: 1.0447\n",
      "124/295, train_loss: 0.0492, step time: 1.0340\n",
      "125/295, train_loss: 0.0608, step time: 1.0355\n",
      "126/295, train_loss: 0.0673, step time: 1.0401\n",
      "127/295, train_loss: 0.0489, step time: 1.0668\n",
      "128/295, train_loss: 0.1724, step time: 1.1176\n",
      "129/295, train_loss: 0.1369, step time: 1.0338\n",
      "130/295, train_loss: 0.0643, step time: 1.0613\n",
      "131/295, train_loss: 0.0725, step time: 1.0540\n",
      "132/295, train_loss: 0.0763, step time: 1.0686\n",
      "133/295, train_loss: 0.1151, step time: 1.0318\n",
      "134/295, train_loss: 0.4262, step time: 1.0539\n",
      "135/295, train_loss: 0.0604, step time: 1.0355\n",
      "136/295, train_loss: 0.0889, step time: 1.0769\n",
      "137/295, train_loss: 0.4215, step time: 1.0455\n",
      "138/295, train_loss: 0.1019, step time: 1.0699\n",
      "139/295, train_loss: 0.0349, step time: 1.0743\n",
      "140/295, train_loss: 0.0680, step time: 1.0853\n",
      "141/295, train_loss: 0.1203, step time: 1.0652\n",
      "142/295, train_loss: 0.0440, step time: 1.0661\n",
      "143/295, train_loss: 0.0477, step time: 1.0607\n",
      "144/295, train_loss: 0.0687, step time: 1.0358\n",
      "145/295, train_loss: 0.1946, step time: 1.0508\n",
      "146/295, train_loss: 0.0450, step time: 1.0499\n",
      "147/295, train_loss: 0.3218, step time: 1.0705\n",
      "148/295, train_loss: 0.3825, step time: 1.0610\n",
      "149/295, train_loss: 0.0569, step time: 1.0440\n",
      "150/295, train_loss: 0.0738, step time: 1.0531\n",
      "151/295, train_loss: 0.3583, step time: 1.0570\n",
      "152/295, train_loss: 0.2270, step time: 1.0344\n",
      "153/295, train_loss: 0.2631, step time: 1.0314\n",
      "154/295, train_loss: 0.4384, step time: 1.0455\n",
      "155/295, train_loss: 0.1304, step time: 1.0577\n",
      "156/295, train_loss: 0.3832, step time: 1.0432\n",
      "157/295, train_loss: 0.0806, step time: 1.1028\n",
      "158/295, train_loss: 0.0780, step time: 1.0314\n",
      "159/295, train_loss: 0.1317, step time: 1.0453\n",
      "160/295, train_loss: 0.0457, step time: 1.0422\n",
      "161/295, train_loss: 0.1062, step time: 1.0743\n",
      "162/295, train_loss: 0.0703, step time: 1.0583\n",
      "163/295, train_loss: 0.1276, step time: 1.0321\n",
      "164/295, train_loss: 0.1797, step time: 1.0450\n",
      "165/295, train_loss: 0.2050, step time: 1.0480\n",
      "166/295, train_loss: 0.2385, step time: 1.0445\n",
      "167/295, train_loss: 0.4353, step time: 1.0556\n",
      "168/295, train_loss: 0.1315, step time: 1.0515\n",
      "169/295, train_loss: 0.1635, step time: 1.0314\n",
      "170/295, train_loss: 0.4043, step time: 1.0309\n",
      "171/295, train_loss: 0.0666, step time: 1.0602\n",
      "172/295, train_loss: 0.0786, step time: 1.0452\n",
      "173/295, train_loss: 0.0856, step time: 1.0453\n",
      "174/295, train_loss: 0.1008, step time: 1.0835\n",
      "175/295, train_loss: 0.0584, step time: 1.0353\n",
      "176/295, train_loss: 0.0908, step time: 1.0339\n",
      "177/295, train_loss: 0.1022, step time: 1.0561\n",
      "178/295, train_loss: 0.1228, step time: 1.0346\n",
      "179/295, train_loss: 0.1030, step time: 1.0325\n",
      "180/295, train_loss: 0.0720, step time: 1.0345\n",
      "181/295, train_loss: 0.0871, step time: 1.0507\n",
      "182/295, train_loss: 0.0605, step time: 1.0370\n",
      "183/295, train_loss: 0.0638, step time: 1.0404\n",
      "184/295, train_loss: 0.3677, step time: 1.0522\n",
      "185/295, train_loss: 0.1002, step time: 1.0310\n",
      "186/295, train_loss: 0.0407, step time: 1.0392\n",
      "187/295, train_loss: 0.0391, step time: 1.0426\n",
      "188/295, train_loss: 0.1059, step time: 1.0395\n",
      "189/295, train_loss: 0.0378, step time: 1.0736\n",
      "190/295, train_loss: 0.0847, step time: 1.0796\n",
      "191/295, train_loss: 0.1423, step time: 1.0350\n",
      "192/295, train_loss: 0.1704, step time: 1.0325\n",
      "193/295, train_loss: 0.0931, step time: 1.0354\n",
      "194/295, train_loss: 0.0496, step time: 1.0492\n",
      "195/295, train_loss: 0.1958, step time: 1.0409\n",
      "196/295, train_loss: 0.1388, step time: 1.0383\n",
      "197/295, train_loss: 0.3749, step time: 1.0465\n",
      "198/295, train_loss: 0.0611, step time: 1.0362\n",
      "199/295, train_loss: 0.0375, step time: 1.0362\n",
      "200/295, train_loss: 0.1549, step time: 1.0427\n",
      "201/295, train_loss: 0.1053, step time: 1.0372\n",
      "202/295, train_loss: 0.0807, step time: 1.0605\n",
      "203/295, train_loss: 0.0595, step time: 1.0355\n",
      "204/295, train_loss: 0.0492, step time: 1.0372\n",
      "205/295, train_loss: 0.1319, step time: 1.0510\n",
      "206/295, train_loss: 0.0473, step time: 1.0319\n",
      "207/295, train_loss: 0.0761, step time: 1.0356\n",
      "208/295, train_loss: 0.0837, step time: 1.0350\n",
      "209/295, train_loss: 0.0945, step time: 1.0363\n",
      "210/295, train_loss: 0.0514, step time: 1.0368\n",
      "211/295, train_loss: 0.1367, step time: 1.0305\n",
      "212/295, train_loss: 0.0879, step time: 1.0987\n",
      "213/295, train_loss: 0.1305, step time: 1.0450\n",
      "214/295, train_loss: 0.0533, step time: 1.0870\n",
      "215/295, train_loss: 0.0697, step time: 1.0328\n",
      "216/295, train_loss: 0.0372, step time: 1.0414\n",
      "217/295, train_loss: 0.0812, step time: 1.0694\n",
      "218/295, train_loss: 0.0588, step time: 1.0322\n",
      "219/295, train_loss: 0.1427, step time: 1.0364\n",
      "220/295, train_loss: 0.3938, step time: 1.0430\n",
      "221/295, train_loss: 0.0611, step time: 1.0695\n",
      "222/295, train_loss: 0.0601, step time: 1.0381\n",
      "223/295, train_loss: 0.1564, step time: 1.0508\n",
      "224/295, train_loss: 0.0533, step time: 1.0394\n",
      "225/295, train_loss: 0.0838, step time: 1.0491\n",
      "226/295, train_loss: 0.0534, step time: 1.0566\n",
      "227/295, train_loss: 0.0691, step time: 1.0878\n",
      "228/295, train_loss: 0.1337, step time: 1.0438\n",
      "229/295, train_loss: 0.1503, step time: 1.0577\n",
      "230/295, train_loss: 0.3969, step time: 1.0390\n",
      "231/295, train_loss: 0.0511, step time: 1.0430\n",
      "232/295, train_loss: 0.1625, step time: 1.0602\n",
      "233/295, train_loss: 0.0552, step time: 1.0455\n",
      "234/295, train_loss: 0.0346, step time: 1.0416\n",
      "235/295, train_loss: 0.1716, step time: 1.0847\n",
      "236/295, train_loss: 0.0855, step time: 1.0326\n",
      "237/295, train_loss: 0.0449, step time: 1.0682\n",
      "238/295, train_loss: 0.0568, step time: 1.0560\n",
      "239/295, train_loss: 0.0817, step time: 1.0525\n",
      "240/295, train_loss: 0.2645, step time: 1.0460\n",
      "241/295, train_loss: 0.0731, step time: 1.0400\n",
      "242/295, train_loss: 0.0874, step time: 1.0411\n",
      "243/295, train_loss: 0.0435, step time: 1.0433\n",
      "244/295, train_loss: 0.1224, step time: 1.0868\n",
      "245/295, train_loss: 0.0364, step time: 1.0346\n",
      "246/295, train_loss: 0.0802, step time: 1.0310\n",
      "247/295, train_loss: 0.1561, step time: 1.0436\n",
      "248/295, train_loss: 0.0470, step time: 1.0403\n",
      "249/295, train_loss: 0.0937, step time: 1.0378\n",
      "250/295, train_loss: 0.0919, step time: 1.0444\n",
      "251/295, train_loss: 0.0871, step time: 1.0661\n",
      "252/295, train_loss: 0.1050, step time: 1.0423\n",
      "253/295, train_loss: 0.1095, step time: 1.0649\n",
      "254/295, train_loss: 0.1742, step time: 1.0599\n",
      "255/295, train_loss: 0.2719, step time: 1.0424\n",
      "256/295, train_loss: 0.0338, step time: 1.0426\n",
      "257/295, train_loss: 0.1011, step time: 1.0434\n",
      "258/295, train_loss: 0.1219, step time: 1.0417\n",
      "259/295, train_loss: 0.1164, step time: 1.0375\n",
      "260/295, train_loss: 0.0740, step time: 1.0352\n",
      "261/295, train_loss: 0.0352, step time: 1.0665\n",
      "262/295, train_loss: 0.0648, step time: 1.0416\n",
      "263/295, train_loss: 0.0417, step time: 1.0651\n",
      "264/295, train_loss: 0.0420, step time: 1.0339\n",
      "265/295, train_loss: 0.1103, step time: 1.0556\n",
      "266/295, train_loss: 0.3748, step time: 1.0538\n",
      "267/295, train_loss: 0.0447, step time: 1.0396\n",
      "268/295, train_loss: 0.0785, step time: 1.0602\n",
      "269/295, train_loss: 0.3756, step time: 1.0486\n",
      "270/295, train_loss: 0.1063, step time: 1.0356\n",
      "271/295, train_loss: 0.2274, step time: 1.1219\n",
      "272/295, train_loss: 0.0648, step time: 1.0425\n",
      "273/295, train_loss: 0.0773, step time: 1.0412\n",
      "274/295, train_loss: 0.0616, step time: 1.0343\n",
      "275/295, train_loss: 0.1371, step time: 1.0544\n",
      "276/295, train_loss: 0.0865, step time: 1.0700\n",
      "277/295, train_loss: 0.0810, step time: 1.0666\n",
      "278/295, train_loss: 0.0796, step time: 1.0601\n",
      "279/295, train_loss: 0.1061, step time: 1.0565\n",
      "280/295, train_loss: 0.0448, step time: 1.0338\n",
      "281/295, train_loss: 0.0519, step time: 1.0580\n",
      "282/295, train_loss: 0.0661, step time: 1.0743\n",
      "283/295, train_loss: 0.0394, step time: 1.0423\n",
      "284/295, train_loss: 0.0519, step time: 1.0350\n",
      "285/295, train_loss: 0.0870, step time: 1.0387\n",
      "286/295, train_loss: 0.0544, step time: 1.0519\n",
      "287/295, train_loss: 0.4325, step time: 1.0794\n",
      "288/295, train_loss: 0.0633, step time: 1.0443\n",
      "289/295, train_loss: 0.4079, step time: 1.0283\n",
      "290/295, train_loss: 0.0680, step time: 1.0293\n",
      "291/295, train_loss: 0.2339, step time: 1.0285\n",
      "292/295, train_loss: 0.0367, step time: 1.0314\n",
      "293/295, train_loss: 0.0532, step time: 1.0300\n",
      "294/295, train_loss: 0.1413, step time: 1.0297\n",
      "295/295, train_loss: 0.0793, step time: 1.0283\n",
      "epoch 28 average loss: 0.1253\n",
      "current epoch: 28 current mean dice: 0.7564 tc: 0.6917 wt: 0.8465 et: 0.7419\n",
      "best mean dice: 0.7821 at epoch: 17\n",
      "time consuming of epoch 28 is: 383.1326\n",
      "----------\n",
      "epoch 29/100\n",
      "1/295, train_loss: 0.0265, step time: 1.1069\n",
      "2/295, train_loss: 0.4029, step time: 1.0354\n",
      "3/295, train_loss: 0.0878, step time: 1.0504\n",
      "4/295, train_loss: 0.0492, step time: 1.0624\n",
      "5/295, train_loss: 0.0496, step time: 1.0590\n",
      "6/295, train_loss: 0.0678, step time: 1.1312\n",
      "7/295, train_loss: 0.4067, step time: 1.0781\n",
      "8/295, train_loss: 0.1480, step time: 1.0599\n",
      "9/295, train_loss: 0.0684, step time: 1.0377\n",
      "10/295, train_loss: 0.1486, step time: 1.0338\n",
      "11/295, train_loss: 0.0375, step time: 1.0355\n",
      "12/295, train_loss: 0.1250, step time: 1.0534\n",
      "13/295, train_loss: 0.0689, step time: 1.1217\n",
      "14/295, train_loss: 0.0341, step time: 1.0310\n",
      "15/295, train_loss: 0.0638, step time: 1.0364\n",
      "16/295, train_loss: 0.0598, step time: 1.0356\n",
      "17/295, train_loss: 0.0672, step time: 1.0442\n",
      "18/295, train_loss: 0.1927, step time: 1.0338\n",
      "19/295, train_loss: 0.0782, step time: 1.1194\n",
      "20/295, train_loss: 0.1489, step time: 1.0437\n",
      "21/295, train_loss: 0.0655, step time: 1.0310\n",
      "22/295, train_loss: 0.0431, step time: 1.0313\n",
      "23/295, train_loss: 0.0445, step time: 1.0835\n",
      "24/295, train_loss: 0.2155, step time: 1.0599\n",
      "25/295, train_loss: 0.2185, step time: 1.0615\n",
      "26/295, train_loss: 0.0594, step time: 1.0539\n",
      "27/295, train_loss: 0.0631, step time: 1.0544\n",
      "28/295, train_loss: 0.1545, step time: 1.0574\n",
      "29/295, train_loss: 0.1058, step time: 1.0459\n",
      "30/295, train_loss: 0.0575, step time: 1.0288\n",
      "31/295, train_loss: 0.2614, step time: 1.0373\n",
      "32/295, train_loss: 0.0560, step time: 1.0380\n",
      "33/295, train_loss: 0.0821, step time: 1.0581\n",
      "34/295, train_loss: 0.1069, step time: 1.0690\n",
      "35/295, train_loss: 0.1421, step time: 1.0546\n",
      "36/295, train_loss: 0.1486, step time: 1.0404\n",
      "37/295, train_loss: 0.1297, step time: 1.0360\n",
      "38/295, train_loss: 0.0407, step time: 1.0346\n",
      "39/295, train_loss: 0.0979, step time: 1.0425\n",
      "40/295, train_loss: 0.1063, step time: 1.0473\n",
      "41/295, train_loss: 0.3823, step time: 1.1224\n",
      "42/295, train_loss: 0.0944, step time: 1.0366\n",
      "43/295, train_loss: 0.0690, step time: 1.0370\n",
      "44/295, train_loss: 0.0501, step time: 1.0387\n",
      "45/295, train_loss: 0.1846, step time: 1.0312\n",
      "46/295, train_loss: 0.0527, step time: 1.0549\n",
      "47/295, train_loss: 0.0783, step time: 1.0330\n",
      "48/295, train_loss: 0.0830, step time: 1.0536\n",
      "49/295, train_loss: 0.4146, step time: 1.0333\n",
      "50/295, train_loss: 0.1512, step time: 1.0477\n",
      "51/295, train_loss: 0.0788, step time: 1.0537\n",
      "52/295, train_loss: 0.0354, step time: 1.1481\n",
      "53/295, train_loss: 0.0488, step time: 1.0640\n",
      "54/295, train_loss: 0.1792, step time: 1.0461\n",
      "55/295, train_loss: 0.0724, step time: 1.0592\n",
      "56/295, train_loss: 0.0524, step time: 1.0632\n",
      "57/295, train_loss: 0.0763, step time: 1.0331\n",
      "58/295, train_loss: 0.0698, step time: 1.0610\n",
      "59/295, train_loss: 0.0618, step time: 1.0380\n",
      "60/295, train_loss: 0.1537, step time: 1.0400\n",
      "61/295, train_loss: 0.2656, step time: 1.0558\n",
      "62/295, train_loss: 0.1593, step time: 1.0759\n",
      "63/295, train_loss: 0.1904, step time: 1.0342\n",
      "64/295, train_loss: 0.0923, step time: 1.0321\n",
      "65/295, train_loss: 0.0779, step time: 1.0689\n",
      "66/295, train_loss: 0.0510, step time: 1.0737\n",
      "67/295, train_loss: 0.1987, step time: 1.0481\n",
      "68/295, train_loss: 0.0823, step time: 1.0312\n",
      "69/295, train_loss: 0.1483, step time: 1.0329\n",
      "70/295, train_loss: 0.1132, step time: 1.0395\n",
      "71/295, train_loss: 0.0996, step time: 1.0525\n",
      "72/295, train_loss: 0.0603, step time: 1.0497\n",
      "73/295, train_loss: 0.0450, step time: 1.0410\n",
      "74/295, train_loss: 0.1477, step time: 1.0587\n",
      "75/295, train_loss: 0.0914, step time: 1.0550\n",
      "76/295, train_loss: 0.0641, step time: 1.0487\n",
      "77/295, train_loss: 0.2944, step time: 1.0384\n",
      "78/295, train_loss: 0.4372, step time: 1.0753\n",
      "79/295, train_loss: 0.0847, step time: 1.0623\n",
      "80/295, train_loss: 0.1525, step time: 1.0385\n",
      "81/295, train_loss: 0.1470, step time: 1.0854\n",
      "82/295, train_loss: 0.0441, step time: 1.0379\n",
      "83/295, train_loss: 0.0451, step time: 1.0367\n",
      "84/295, train_loss: 0.0661, step time: 1.1444\n",
      "85/295, train_loss: 0.1009, step time: 1.0381\n",
      "86/295, train_loss: 0.0613, step time: 1.0378\n",
      "87/295, train_loss: 0.0614, step time: 1.0618\n",
      "88/295, train_loss: 0.1240, step time: 1.0378\n",
      "89/295, train_loss: 0.0484, step time: 1.0400\n",
      "90/295, train_loss: 0.1067, step time: 1.1037\n",
      "91/295, train_loss: 0.1039, step time: 1.0364\n",
      "92/295, train_loss: 0.0693, step time: 1.0332\n",
      "93/295, train_loss: 0.4985, step time: 1.0619\n",
      "94/295, train_loss: 0.0720, step time: 1.0542\n",
      "95/295, train_loss: 0.0820, step time: 1.0632\n",
      "96/295, train_loss: 0.1064, step time: 1.0312\n",
      "97/295, train_loss: 0.0769, step time: 1.0363\n",
      "98/295, train_loss: 0.0822, step time: 1.0709\n",
      "99/295, train_loss: 0.0687, step time: 1.0961\n",
      "100/295, train_loss: 0.0339, step time: 1.0877\n",
      "101/295, train_loss: 0.0455, step time: 1.0424\n",
      "102/295, train_loss: 0.1080, step time: 1.0644\n",
      "103/295, train_loss: 0.1083, step time: 1.0336\n",
      "104/295, train_loss: 0.3503, step time: 1.0951\n",
      "105/295, train_loss: 0.1169, step time: 1.1638\n",
      "106/295, train_loss: 0.0782, step time: 1.0382\n",
      "107/295, train_loss: 0.3161, step time: 1.0378\n",
      "108/295, train_loss: 0.0586, step time: 1.0857\n",
      "109/295, train_loss: 0.1126, step time: 1.0418\n",
      "110/295, train_loss: 0.0927, step time: 1.0581\n",
      "111/295, train_loss: 0.0404, step time: 1.0623\n",
      "112/295, train_loss: 0.1284, step time: 1.0366\n",
      "113/295, train_loss: 0.3924, step time: 1.0343\n",
      "114/295, train_loss: 0.0410, step time: 1.0974\n",
      "115/295, train_loss: 0.1136, step time: 1.0574\n",
      "116/295, train_loss: 0.0978, step time: 1.0343\n",
      "117/295, train_loss: 0.1079, step time: 1.0351\n",
      "118/295, train_loss: 0.1416, step time: 1.0433\n",
      "119/295, train_loss: 0.1244, step time: 1.0549\n",
      "120/295, train_loss: 0.0702, step time: 1.0389\n",
      "121/295, train_loss: 0.1275, step time: 1.0403\n",
      "122/295, train_loss: 0.0479, step time: 1.0591\n",
      "123/295, train_loss: 0.0617, step time: 1.0641\n",
      "124/295, train_loss: 0.1208, step time: 1.0555\n",
      "125/295, train_loss: 0.1796, step time: 1.0603\n",
      "126/295, train_loss: 0.1644, step time: 1.0694\n",
      "127/295, train_loss: 0.1200, step time: 1.0340\n",
      "128/295, train_loss: 0.1447, step time: 1.0399\n",
      "129/295, train_loss: 0.2228, step time: 1.0420\n",
      "130/295, train_loss: 0.0654, step time: 1.0474\n",
      "131/295, train_loss: 0.0640, step time: 1.0549\n",
      "132/295, train_loss: 0.1096, step time: 1.0359\n",
      "133/295, train_loss: 0.1092, step time: 1.0576\n",
      "134/295, train_loss: 0.0595, step time: 1.0634\n",
      "135/295, train_loss: 0.0715, step time: 1.0590\n",
      "136/295, train_loss: 0.4341, step time: 1.0392\n",
      "137/295, train_loss: 0.0599, step time: 1.0377\n",
      "138/295, train_loss: 0.0609, step time: 1.1104\n",
      "139/295, train_loss: 0.0497, step time: 1.0518\n",
      "140/295, train_loss: 0.0681, step time: 1.0541\n",
      "141/295, train_loss: 0.3884, step time: 1.0436\n",
      "142/295, train_loss: 0.1051, step time: 1.0322\n",
      "143/295, train_loss: 0.0388, step time: 1.0804\n",
      "144/295, train_loss: 0.1455, step time: 1.0381\n",
      "145/295, train_loss: 0.0824, step time: 1.0543\n",
      "146/295, train_loss: 0.4371, step time: 1.0856\n",
      "147/295, train_loss: 0.0512, step time: 1.0636\n",
      "148/295, train_loss: 0.0817, step time: 1.0511\n",
      "149/295, train_loss: 0.2448, step time: 1.0306\n",
      "150/295, train_loss: 0.0585, step time: 1.0331\n",
      "151/295, train_loss: 0.0701, step time: 1.0379\n",
      "152/295, train_loss: 0.1222, step time: 1.0483\n",
      "153/295, train_loss: 0.1328, step time: 1.0405\n",
      "154/295, train_loss: 0.3949, step time: 1.0634\n",
      "155/295, train_loss: 0.0483, step time: 1.0368\n",
      "156/295, train_loss: 0.0686, step time: 1.0345\n",
      "157/295, train_loss: 0.4265, step time: 1.0475\n",
      "158/295, train_loss: 0.1480, step time: 1.0409\n",
      "159/295, train_loss: 0.0603, step time: 1.0411\n",
      "160/295, train_loss: 0.0724, step time: 1.0801\n",
      "161/295, train_loss: 0.0482, step time: 1.0439\n",
      "162/295, train_loss: 0.3851, step time: 1.0408\n",
      "163/295, train_loss: 0.3645, step time: 1.0452\n",
      "164/295, train_loss: 0.1211, step time: 1.0736\n",
      "165/295, train_loss: 0.0642, step time: 1.0335\n",
      "166/295, train_loss: 0.0672, step time: 1.0359\n",
      "167/295, train_loss: 0.1092, step time: 1.0380\n",
      "168/295, train_loss: 0.3190, step time: 1.0439\n",
      "169/295, train_loss: 0.0585, step time: 1.0491\n",
      "170/295, train_loss: 0.0395, step time: 1.0361\n",
      "171/295, train_loss: 0.1413, step time: 1.0595\n",
      "172/295, train_loss: 0.4120, step time: 1.0376\n",
      "173/295, train_loss: 0.1470, step time: 1.0374\n",
      "174/295, train_loss: 0.0942, step time: 1.0326\n",
      "175/295, train_loss: 0.1324, step time: 1.0309\n",
      "176/295, train_loss: 0.4438, step time: 1.0324\n",
      "177/295, train_loss: 0.0409, step time: 1.0337\n",
      "178/295, train_loss: 0.1111, step time: 1.0412\n",
      "179/295, train_loss: 0.0681, step time: 1.0513\n",
      "180/295, train_loss: 0.0810, step time: 1.1120\n",
      "181/295, train_loss: 0.1309, step time: 1.0317\n",
      "182/295, train_loss: 0.1085, step time: 1.0385\n",
      "183/295, train_loss: 0.4303, step time: 1.0401\n",
      "184/295, train_loss: 0.3928, step time: 1.0547\n",
      "185/295, train_loss: 0.2195, step time: 1.0445\n",
      "186/295, train_loss: 0.1523, step time: 1.0481\n",
      "187/295, train_loss: 0.0678, step time: 1.0587\n",
      "188/295, train_loss: 0.0527, step time: 1.0425\n",
      "189/295, train_loss: 0.2357, step time: 1.0325\n",
      "190/295, train_loss: 0.0485, step time: 1.0449\n",
      "191/295, train_loss: 0.0940, step time: 1.0516\n",
      "192/295, train_loss: 0.0669, step time: 1.0442\n",
      "193/295, train_loss: 0.0469, step time: 1.0424\n",
      "194/295, train_loss: 0.1590, step time: 1.0506\n",
      "195/295, train_loss: 0.1649, step time: 1.0457\n",
      "196/295, train_loss: 0.3857, step time: 1.0405\n",
      "197/295, train_loss: 0.1125, step time: 1.0869\n",
      "198/295, train_loss: 0.0619, step time: 1.0435\n",
      "199/295, train_loss: 0.1345, step time: 1.0766\n",
      "200/295, train_loss: 0.2375, step time: 1.0795\n",
      "201/295, train_loss: 0.1202, step time: 1.0342\n",
      "202/295, train_loss: 0.0710, step time: 1.0341\n",
      "203/295, train_loss: 0.1971, step time: 1.0341\n",
      "204/295, train_loss: 0.3524, step time: 1.0370\n",
      "205/295, train_loss: 0.0507, step time: 1.0338\n",
      "206/295, train_loss: 0.0815, step time: 1.0367\n",
      "207/295, train_loss: 0.0575, step time: 1.0345\n",
      "208/295, train_loss: 0.1087, step time: 1.0427\n",
      "209/295, train_loss: 0.1035, step time: 1.0394\n",
      "210/295, train_loss: 0.0637, step time: 1.0341\n",
      "211/295, train_loss: 0.0641, step time: 1.0374\n",
      "212/295, train_loss: 0.0684, step time: 1.0420\n",
      "213/295, train_loss: 0.0678, step time: 1.0557\n",
      "214/295, train_loss: 0.0585, step time: 1.0536\n",
      "215/295, train_loss: 0.0727, step time: 1.0335\n",
      "216/295, train_loss: 0.1450, step time: 1.0387\n",
      "217/295, train_loss: 0.0807, step time: 1.0358\n",
      "218/295, train_loss: 0.0397, step time: 1.0662\n",
      "219/295, train_loss: 0.0483, step time: 1.0361\n",
      "220/295, train_loss: 0.1557, step time: 1.0384\n",
      "221/295, train_loss: 0.1282, step time: 1.0480\n",
      "222/295, train_loss: 0.0984, step time: 1.0337\n",
      "223/295, train_loss: 0.0862, step time: 1.0417\n",
      "224/295, train_loss: 0.0840, step time: 1.0434\n",
      "225/295, train_loss: 0.0789, step time: 1.0584\n",
      "226/295, train_loss: 0.0826, step time: 1.0349\n",
      "227/295, train_loss: 0.0432, step time: 1.1090\n",
      "228/295, train_loss: 0.2092, step time: 1.0359\n",
      "229/295, train_loss: 0.1167, step time: 1.0311\n",
      "230/295, train_loss: 0.1809, step time: 1.0429\n",
      "231/295, train_loss: 0.0939, step time: 1.0448\n",
      "232/295, train_loss: 0.2242, step time: 1.0648\n",
      "233/295, train_loss: 0.0704, step time: 1.1054\n",
      "234/295, train_loss: 0.0730, step time: 1.0440\n",
      "235/295, train_loss: 0.1067, step time: 1.0362\n",
      "236/295, train_loss: 0.2374, step time: 1.0353\n",
      "237/295, train_loss: 0.0843, step time: 1.0518\n",
      "238/295, train_loss: 0.1091, step time: 1.0638\n",
      "239/295, train_loss: 0.0719, step time: 1.0362\n",
      "240/295, train_loss: 0.2974, step time: 1.0798\n",
      "241/295, train_loss: 0.0628, step time: 1.0363\n",
      "242/295, train_loss: 0.1032, step time: 1.0333\n",
      "243/295, train_loss: 0.0684, step time: 1.0670\n",
      "244/295, train_loss: 0.1186, step time: 1.1392\n",
      "245/295, train_loss: 0.0548, step time: 1.0443\n",
      "246/295, train_loss: 0.0444, step time: 1.0440\n",
      "247/295, train_loss: 0.1222, step time: 1.0396\n",
      "248/295, train_loss: 0.1049, step time: 1.0618\n",
      "249/295, train_loss: 0.1017, step time: 1.0432\n",
      "250/295, train_loss: 0.0408, step time: 1.0515\n",
      "251/295, train_loss: 0.0571, step time: 1.0678\n",
      "252/295, train_loss: 0.5006, step time: 1.0332\n",
      "253/295, train_loss: 0.1450, step time: 1.0759\n",
      "254/295, train_loss: 0.0660, step time: 1.0360\n",
      "255/295, train_loss: 0.0393, step time: 1.0347\n",
      "256/295, train_loss: 0.4463, step time: 1.0559\n",
      "257/295, train_loss: 0.4224, step time: 1.0892\n",
      "258/295, train_loss: 0.0600, step time: 1.0354\n",
      "259/295, train_loss: 0.1123, step time: 1.0659\n",
      "260/295, train_loss: 0.1066, step time: 1.0650\n",
      "261/295, train_loss: 0.0454, step time: 1.0425\n",
      "262/295, train_loss: 0.1506, step time: 1.0441\n",
      "263/295, train_loss: 0.0633, step time: 1.0585\n",
      "264/295, train_loss: 0.1172, step time: 1.0621\n",
      "265/295, train_loss: 0.0672, step time: 1.0335\n",
      "266/295, train_loss: 0.0661, step time: 1.0428\n",
      "267/295, train_loss: 0.0847, step time: 1.1062\n",
      "268/295, train_loss: 0.0772, step time: 1.0699\n",
      "269/295, train_loss: 0.0605, step time: 1.0639\n",
      "270/295, train_loss: 0.0333, step time: 1.0657\n",
      "271/295, train_loss: 0.1177, step time: 1.0381\n",
      "272/295, train_loss: 0.0594, step time: 1.0701\n",
      "273/295, train_loss: 0.1075, step time: 1.0483\n",
      "274/295, train_loss: 0.0721, step time: 1.0478\n",
      "275/295, train_loss: 0.1348, step time: 1.0577\n",
      "276/295, train_loss: 0.0688, step time: 1.0371\n",
      "277/295, train_loss: 0.2335, step time: 1.0707\n",
      "278/295, train_loss: 0.0967, step time: 1.0889\n",
      "279/295, train_loss: 0.0523, step time: 1.0460\n",
      "280/295, train_loss: 0.0723, step time: 1.0338\n",
      "281/295, train_loss: 0.2249, step time: 1.0564\n",
      "282/295, train_loss: 0.0780, step time: 1.0415\n",
      "283/295, train_loss: 0.1386, step time: 1.0404\n",
      "284/295, train_loss: 0.0912, step time: 1.0387\n",
      "285/295, train_loss: 0.1591, step time: 1.0628\n",
      "286/295, train_loss: 0.0794, step time: 1.0607\n",
      "287/295, train_loss: 0.0722, step time: 1.0366\n",
      "288/295, train_loss: 0.0488, step time: 1.0287\n",
      "289/295, train_loss: 0.0465, step time: 1.0286\n",
      "290/295, train_loss: 0.1584, step time: 1.0298\n",
      "291/295, train_loss: 0.0885, step time: 1.0292\n",
      "292/295, train_loss: 0.3806, step time: 1.0285\n",
      "293/295, train_loss: 0.0972, step time: 1.0288\n",
      "294/295, train_loss: 0.1896, step time: 1.0286\n",
      "295/295, train_loss: 0.3762, step time: 1.0282\n",
      "epoch 29 average loss: 0.1280\n",
      "saved new best metric model\n",
      "current epoch: 29 current mean dice: 0.8045 tc: 0.7618 wt: 0.8650 et: 0.7936\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 29 is: 387.4119\n",
      "----------\n",
      "epoch 30/100\n",
      "1/295, train_loss: 0.1401, step time: 1.0652\n",
      "2/295, train_loss: 0.0681, step time: 1.1243\n",
      "3/295, train_loss: 0.0613, step time: 1.0575\n",
      "4/295, train_loss: 0.0874, step time: 1.0531\n",
      "5/295, train_loss: 0.0547, step time: 1.0381\n",
      "6/295, train_loss: 0.0517, step time: 1.0404\n",
      "7/295, train_loss: 0.2018, step time: 1.0636\n",
      "8/295, train_loss: 0.1536, step time: 1.0722\n",
      "9/295, train_loss: 0.0872, step time: 1.0351\n",
      "10/295, train_loss: 0.1546, step time: 1.0356\n",
      "11/295, train_loss: 0.0427, step time: 1.0560\n",
      "12/295, train_loss: 0.1325, step time: 1.0345\n",
      "13/295, train_loss: 0.0606, step time: 1.0444\n",
      "14/295, train_loss: 0.0930, step time: 1.0333\n",
      "15/295, train_loss: 0.0899, step time: 1.0573\n",
      "16/295, train_loss: 0.1012, step time: 1.0563\n",
      "17/295, train_loss: 0.0872, step time: 1.0351\n",
      "18/295, train_loss: 0.0435, step time: 1.0331\n",
      "19/295, train_loss: 0.3629, step time: 1.0341\n",
      "20/295, train_loss: 0.4386, step time: 1.0518\n",
      "21/295, train_loss: 0.0660, step time: 1.0314\n",
      "22/295, train_loss: 0.0742, step time: 1.0365\n",
      "23/295, train_loss: 0.1202, step time: 1.0665\n",
      "24/295, train_loss: 0.0445, step time: 1.0454\n",
      "25/295, train_loss: 0.1242, step time: 1.0652\n",
      "26/295, train_loss: 0.0539, step time: 1.0555\n",
      "27/295, train_loss: 0.1025, step time: 1.0358\n",
      "28/295, train_loss: 0.0667, step time: 1.0358\n",
      "29/295, train_loss: 0.0580, step time: 1.0343\n",
      "30/295, train_loss: 0.0665, step time: 1.0673\n",
      "31/295, train_loss: 0.4102, step time: 1.0516\n",
      "32/295, train_loss: 0.1946, step time: 1.0553\n",
      "33/295, train_loss: 0.2094, step time: 1.0351\n",
      "34/295, train_loss: 0.1479, step time: 1.0369\n",
      "35/295, train_loss: 0.1339, step time: 1.0411\n",
      "36/295, train_loss: 0.0475, step time: 1.0423\n",
      "37/295, train_loss: 0.1064, step time: 1.0333\n",
      "38/295, train_loss: 0.1431, step time: 1.0643\n",
      "39/295, train_loss: 0.0600, step time: 1.0586\n",
      "40/295, train_loss: 0.0951, step time: 1.0456\n",
      "41/295, train_loss: 0.0701, step time: 1.0357\n",
      "42/295, train_loss: 0.0998, step time: 1.0853\n",
      "43/295, train_loss: 0.1046, step time: 1.1063\n",
      "44/295, train_loss: 0.1090, step time: 1.0529\n",
      "45/295, train_loss: 0.0667, step time: 1.0379\n",
      "46/295, train_loss: 0.4392, step time: 1.0382\n",
      "47/295, train_loss: 0.0329, step time: 1.0578\n",
      "48/295, train_loss: 0.1095, step time: 1.0422\n",
      "49/295, train_loss: 0.0579, step time: 1.0583\n",
      "50/295, train_loss: 0.0602, step time: 1.0352\n",
      "51/295, train_loss: 0.3611, step time: 1.0327\n",
      "52/295, train_loss: 0.0432, step time: 1.0435\n",
      "53/295, train_loss: 0.0970, step time: 1.1006\n",
      "54/295, train_loss: 0.4088, step time: 1.0405\n",
      "55/295, train_loss: 0.0988, step time: 1.1103\n",
      "56/295, train_loss: 0.1285, step time: 1.0592\n",
      "57/295, train_loss: 0.3414, step time: 1.0431\n",
      "58/295, train_loss: 0.0879, step time: 1.0784\n",
      "59/295, train_loss: 0.1303, step time: 1.0399\n",
      "60/295, train_loss: 0.3454, step time: 1.0381\n",
      "61/295, train_loss: 0.1353, step time: 1.0932\n",
      "62/295, train_loss: 0.2659, step time: 1.0345\n",
      "63/295, train_loss: 0.0435, step time: 1.0583\n",
      "64/295, train_loss: 0.0972, step time: 1.0450\n",
      "65/295, train_loss: 0.0615, step time: 1.0427\n",
      "66/295, train_loss: 0.1736, step time: 1.0406\n",
      "67/295, train_loss: 0.4192, step time: 1.0924\n",
      "68/295, train_loss: 0.1269, step time: 1.1036\n",
      "69/295, train_loss: 0.3851, step time: 1.0323\n",
      "70/295, train_loss: 0.0820, step time: 1.0405\n",
      "71/295, train_loss: 0.0672, step time: 1.0637\n",
      "72/295, train_loss: 0.1548, step time: 1.0593\n",
      "73/295, train_loss: 0.1575, step time: 1.0429\n",
      "74/295, train_loss: 0.3766, step time: 1.0569\n",
      "75/295, train_loss: 0.0927, step time: 1.0898\n",
      "76/295, train_loss: 0.1558, step time: 1.0331\n",
      "77/295, train_loss: 0.0790, step time: 1.0752\n",
      "78/295, train_loss: 0.1315, step time: 1.0330\n",
      "79/295, train_loss: 0.0483, step time: 1.0574\n",
      "80/295, train_loss: 0.0554, step time: 1.0404\n",
      "81/295, train_loss: 0.0283, step time: 1.0523\n",
      "82/295, train_loss: 0.1078, step time: 1.0500\n",
      "83/295, train_loss: 0.0697, step time: 1.0329\n",
      "84/295, train_loss: 0.0844, step time: 1.0377\n",
      "85/295, train_loss: 0.0779, step time: 1.0333\n",
      "86/295, train_loss: 0.4324, step time: 1.0696\n",
      "87/295, train_loss: 0.0423, step time: 1.0441\n",
      "88/295, train_loss: 0.0415, step time: 1.0389\n",
      "89/295, train_loss: 0.0380, step time: 1.0356\n",
      "90/295, train_loss: 0.4477, step time: 1.0739\n",
      "91/295, train_loss: 0.1583, step time: 1.0323\n",
      "92/295, train_loss: 0.1446, step time: 1.0322\n",
      "93/295, train_loss: 0.0562, step time: 1.0388\n",
      "94/295, train_loss: 0.0846, step time: 1.0373\n",
      "95/295, train_loss: 0.1114, step time: 1.0373\n",
      "96/295, train_loss: 0.1384, step time: 1.0655\n",
      "97/295, train_loss: 0.4021, step time: 1.0311\n",
      "98/295, train_loss: 0.0852, step time: 1.0343\n",
      "99/295, train_loss: 0.1239, step time: 1.0365\n",
      "100/295, train_loss: 0.0691, step time: 1.0361\n",
      "101/295, train_loss: 0.1068, step time: 1.0485\n",
      "102/295, train_loss: 0.0329, step time: 1.0637\n",
      "103/295, train_loss: 0.0997, step time: 1.0454\n",
      "104/295, train_loss: 0.0838, step time: 1.0401\n",
      "105/295, train_loss: 0.0474, step time: 1.0356\n",
      "106/295, train_loss: 0.0691, step time: 1.0348\n",
      "107/295, train_loss: 0.0549, step time: 1.0379\n",
      "108/295, train_loss: 0.0715, step time: 1.0518\n",
      "109/295, train_loss: 0.4981, step time: 1.0662\n",
      "110/295, train_loss: 0.0344, step time: 1.0443\n",
      "111/295, train_loss: 0.1114, step time: 1.0480\n",
      "112/295, train_loss: 0.0580, step time: 1.0381\n",
      "113/295, train_loss: 0.0924, step time: 1.0476\n",
      "114/295, train_loss: 0.1313, step time: 1.0442\n",
      "115/295, train_loss: 0.0719, step time: 1.0737\n",
      "116/295, train_loss: 0.1312, step time: 1.0439\n",
      "117/295, train_loss: 0.0885, step time: 1.0402\n",
      "118/295, train_loss: 0.2381, step time: 1.0478\n",
      "119/295, train_loss: 0.1421, step time: 1.0787\n",
      "120/295, train_loss: 0.0921, step time: 1.0933\n",
      "121/295, train_loss: 0.1392, step time: 1.0749\n",
      "122/295, train_loss: 0.1150, step time: 1.0352\n",
      "123/295, train_loss: 0.1223, step time: 1.0359\n",
      "124/295, train_loss: 0.0369, step time: 1.0438\n",
      "125/295, train_loss: 0.0660, step time: 1.0352\n",
      "126/295, train_loss: 0.1797, step time: 1.0430\n",
      "127/295, train_loss: 0.3740, step time: 1.0527\n",
      "128/295, train_loss: 0.1231, step time: 1.0301\n",
      "129/295, train_loss: 0.0574, step time: 1.0455\n",
      "130/295, train_loss: 0.0357, step time: 1.0888\n",
      "131/295, train_loss: 0.0492, step time: 1.0578\n",
      "132/295, train_loss: 0.0697, step time: 1.0526\n",
      "133/295, train_loss: 0.2118, step time: 1.0377\n",
      "134/295, train_loss: 0.0753, step time: 1.0330\n",
      "135/295, train_loss: 0.0769, step time: 1.0600\n",
      "136/295, train_loss: 0.0457, step time: 1.0689\n",
      "137/295, train_loss: 0.0862, step time: 1.0352\n",
      "138/295, train_loss: 0.1063, step time: 1.0496\n",
      "139/295, train_loss: 0.1140, step time: 1.0416\n",
      "140/295, train_loss: 0.0704, step time: 1.0442\n",
      "141/295, train_loss: 0.2046, step time: 1.0392\n",
      "142/295, train_loss: 0.1417, step time: 1.0504\n",
      "143/295, train_loss: 0.0465, step time: 1.0360\n",
      "144/295, train_loss: 0.3778, step time: 1.0413\n",
      "145/295, train_loss: 0.0773, step time: 1.0299\n",
      "146/295, train_loss: 0.0554, step time: 1.0384\n",
      "147/295, train_loss: 0.0476, step time: 1.0344\n",
      "148/295, train_loss: 0.0672, step time: 1.0357\n",
      "149/295, train_loss: 0.2590, step time: 1.0501\n",
      "150/295, train_loss: 0.4318, step time: 1.0610\n",
      "151/295, train_loss: 0.1031, step time: 1.0823\n",
      "152/295, train_loss: 0.0533, step time: 1.0323\n",
      "153/295, train_loss: 0.4400, step time: 1.0335\n",
      "154/295, train_loss: 0.0614, step time: 1.0331\n",
      "155/295, train_loss: 0.0623, step time: 1.0387\n",
      "156/295, train_loss: 0.0904, step time: 1.1029\n",
      "157/295, train_loss: 0.0908, step time: 1.0587\n",
      "158/295, train_loss: 0.1458, step time: 1.0442\n",
      "159/295, train_loss: 0.0736, step time: 1.0527\n",
      "160/295, train_loss: 0.0956, step time: 1.0351\n",
      "161/295, train_loss: 0.0699, step time: 1.0417\n",
      "162/295, train_loss: 0.0516, step time: 1.0686\n",
      "163/295, train_loss: 0.1016, step time: 1.0325\n",
      "164/295, train_loss: 0.1713, step time: 1.0353\n",
      "165/295, train_loss: 0.0549, step time: 1.1073\n",
      "166/295, train_loss: 0.0687, step time: 1.0312\n",
      "167/295, train_loss: 0.0353, step time: 1.0480\n",
      "168/295, train_loss: 0.1629, step time: 1.0372\n",
      "169/295, train_loss: 0.0631, step time: 1.0384\n",
      "170/295, train_loss: 0.1191, step time: 1.0672\n",
      "171/295, train_loss: 0.0826, step time: 1.0462\n",
      "172/295, train_loss: 0.0792, step time: 1.0505\n",
      "173/295, train_loss: 0.4313, step time: 1.0497\n",
      "174/295, train_loss: 0.0992, step time: 1.0575\n",
      "175/295, train_loss: 0.0692, step time: 1.0380\n",
      "176/295, train_loss: 0.1100, step time: 1.0515\n",
      "177/295, train_loss: 0.0566, step time: 1.0404\n",
      "178/295, train_loss: 0.0733, step time: 1.0592\n",
      "179/295, train_loss: 0.1244, step time: 1.0371\n",
      "180/295, train_loss: 0.0680, step time: 1.0362\n",
      "181/295, train_loss: 0.5682, step time: 1.0393\n",
      "182/295, train_loss: 0.0872, step time: 1.0376\n",
      "183/295, train_loss: 0.3752, step time: 1.0375\n",
      "184/295, train_loss: 0.0422, step time: 1.0357\n",
      "185/295, train_loss: 0.1153, step time: 1.0592\n",
      "186/295, train_loss: 0.1211, step time: 1.0361\n",
      "187/295, train_loss: 0.0717, step time: 1.0347\n",
      "188/295, train_loss: 0.1064, step time: 1.0540\n",
      "189/295, train_loss: 0.1398, step time: 1.0693\n",
      "190/295, train_loss: 0.4801, step time: 1.0516\n",
      "191/295, train_loss: 0.1012, step time: 1.0387\n",
      "192/295, train_loss: 0.1540, step time: 1.0427\n",
      "193/295, train_loss: 0.0979, step time: 1.0321\n",
      "194/295, train_loss: 0.0949, step time: 1.0325\n",
      "195/295, train_loss: 0.0559, step time: 1.0495\n",
      "196/295, train_loss: 0.1121, step time: 1.0476\n",
      "197/295, train_loss: 0.1176, step time: 1.0328\n",
      "198/295, train_loss: 0.1391, step time: 1.0333\n",
      "199/295, train_loss: 0.0900, step time: 1.0411\n",
      "200/295, train_loss: 0.0884, step time: 1.0484\n",
      "201/295, train_loss: 0.2385, step time: 1.0395\n",
      "202/295, train_loss: 0.3931, step time: 1.0348\n",
      "203/295, train_loss: 0.1615, step time: 1.0379\n",
      "204/295, train_loss: 0.0727, step time: 1.0416\n",
      "205/295, train_loss: 0.1175, step time: 1.0376\n",
      "206/295, train_loss: 0.0677, step time: 1.0334\n",
      "207/295, train_loss: 0.0378, step time: 1.0571\n",
      "208/295, train_loss: 0.1440, step time: 1.0335\n",
      "209/295, train_loss: 0.1053, step time: 1.0428\n",
      "210/295, train_loss: 0.1012, step time: 1.0380\n",
      "211/295, train_loss: 0.0867, step time: 1.0397\n",
      "212/295, train_loss: 0.1219, step time: 1.0857\n",
      "213/295, train_loss: 0.1283, step time: 1.0776\n",
      "214/295, train_loss: 0.1164, step time: 1.0490\n",
      "215/295, train_loss: 0.2492, step time: 1.0777\n",
      "216/295, train_loss: 0.0909, step time: 1.0415\n",
      "217/295, train_loss: 0.0424, step time: 1.0600\n",
      "218/295, train_loss: 0.0555, step time: 1.0476\n",
      "219/295, train_loss: 0.0809, step time: 1.0384\n",
      "220/295, train_loss: 0.0415, step time: 1.0643\n",
      "221/295, train_loss: 0.1517, step time: 1.0518\n",
      "222/295, train_loss: 0.0650, step time: 1.0399\n",
      "223/295, train_loss: 0.0484, step time: 1.0480\n",
      "224/295, train_loss: 0.1300, step time: 1.0502\n",
      "225/295, train_loss: 0.2038, step time: 1.0470\n",
      "226/295, train_loss: 0.1280, step time: 1.0333\n",
      "227/295, train_loss: 0.0914, step time: 1.0346\n",
      "228/295, train_loss: 0.0381, step time: 1.0340\n",
      "229/295, train_loss: 0.0768, step time: 1.0490\n",
      "230/295, train_loss: 0.1006, step time: 1.0372\n",
      "231/295, train_loss: 0.0689, step time: 1.0363\n",
      "232/295, train_loss: 0.0405, step time: 1.0365\n",
      "233/295, train_loss: 0.1914, step time: 1.0379\n",
      "234/295, train_loss: 0.0730, step time: 1.0852\n",
      "235/295, train_loss: 0.0828, step time: 1.0729\n",
      "236/295, train_loss: 0.0407, step time: 1.0384\n",
      "237/295, train_loss: 0.1086, step time: 1.0559\n",
      "238/295, train_loss: 0.2659, step time: 1.0662\n",
      "239/295, train_loss: 0.0588, step time: 1.0358\n",
      "240/295, train_loss: 0.0792, step time: 1.0372\n",
      "241/295, train_loss: 0.1387, step time: 1.1336\n",
      "242/295, train_loss: 0.0698, step time: 1.0678\n",
      "243/295, train_loss: 0.1082, step time: 1.0387\n",
      "244/295, train_loss: 0.1161, step time: 1.0589\n",
      "245/295, train_loss: 0.1007, step time: 1.0365\n",
      "246/295, train_loss: 0.0673, step time: 1.0440\n",
      "247/295, train_loss: 0.0465, step time: 1.0395\n",
      "248/295, train_loss: 0.0593, step time: 1.0431\n",
      "249/295, train_loss: 0.0881, step time: 1.1378\n",
      "250/295, train_loss: 0.0782, step time: 1.1020\n",
      "251/295, train_loss: 0.0499, step time: 1.0345\n",
      "252/295, train_loss: 0.0720, step time: 1.0656\n",
      "253/295, train_loss: 0.0591, step time: 1.0517\n",
      "254/295, train_loss: 0.0581, step time: 1.0374\n",
      "255/295, train_loss: 0.1465, step time: 1.0343\n",
      "256/295, train_loss: 0.1462, step time: 1.0606\n",
      "257/295, train_loss: 0.2041, step time: 1.0554\n",
      "258/295, train_loss: 0.0764, step time: 1.0512\n",
      "259/295, train_loss: 0.1252, step time: 1.1153\n",
      "260/295, train_loss: 0.4065, step time: 1.0370\n",
      "261/295, train_loss: 0.0423, step time: 1.0374\n",
      "262/295, train_loss: 0.0743, step time: 1.0383\n",
      "263/295, train_loss: 0.1181, step time: 1.0320\n",
      "264/295, train_loss: 0.0402, step time: 1.0362\n",
      "265/295, train_loss: 0.4641, step time: 1.0532\n",
      "266/295, train_loss: 0.0774, step time: 1.1099\n",
      "267/295, train_loss: 0.1910, step time: 1.0349\n",
      "268/295, train_loss: 0.0747, step time: 1.0400\n",
      "269/295, train_loss: 0.1707, step time: 1.0484\n",
      "270/295, train_loss: 0.0883, step time: 1.0506\n",
      "271/295, train_loss: 0.0643, step time: 1.0718\n",
      "272/295, train_loss: 0.0492, step time: 1.0428\n",
      "273/295, train_loss: 0.3763, step time: 1.0361\n",
      "274/295, train_loss: 0.0555, step time: 1.0362\n",
      "275/295, train_loss: 0.1874, step time: 1.0963\n",
      "276/295, train_loss: 0.0438, step time: 1.0568\n",
      "277/295, train_loss: 0.1949, step time: 1.0587\n",
      "278/295, train_loss: 0.0485, step time: 1.0816\n",
      "279/295, train_loss: 0.4328, step time: 1.0413\n",
      "280/295, train_loss: 0.1558, step time: 1.0386\n",
      "281/295, train_loss: 0.0417, step time: 1.0370\n",
      "282/295, train_loss: 0.0648, step time: 1.0428\n",
      "283/295, train_loss: 0.1402, step time: 1.0705\n",
      "284/295, train_loss: 0.0537, step time: 1.0336\n",
      "285/295, train_loss: 0.0808, step time: 1.0347\n",
      "286/295, train_loss: 0.0850, step time: 1.0353\n",
      "287/295, train_loss: 0.0429, step time: 1.0383\n",
      "288/295, train_loss: 0.0644, step time: 1.0365\n",
      "289/295, train_loss: 0.1043, step time: 1.0290\n",
      "290/295, train_loss: 0.0549, step time: 1.0288\n",
      "291/295, train_loss: 0.0527, step time: 1.0292\n",
      "292/295, train_loss: 0.0423, step time: 1.0292\n",
      "293/295, train_loss: 0.0506, step time: 1.0293\n",
      "294/295, train_loss: 0.1184, step time: 1.0286\n",
      "295/295, train_loss: 0.3949, step time: 1.0295\n",
      "epoch 30 average loss: 0.1272\n",
      "current epoch: 30 current mean dice: 0.8040 tc: 0.7601 wt: 0.8687 et: 0.7932\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 30 is: 384.2864\n",
      "----------\n",
      "epoch 31/100\n",
      "1/295, train_loss: 0.0582, step time: 1.0418\n",
      "2/295, train_loss: 0.0816, step time: 1.0413\n",
      "3/295, train_loss: 0.4052, step time: 1.0864\n",
      "4/295, train_loss: 0.1957, step time: 1.0409\n",
      "5/295, train_loss: 0.1610, step time: 1.0481\n",
      "6/295, train_loss: 0.2325, step time: 1.0364\n",
      "7/295, train_loss: 0.3853, step time: 1.0321\n",
      "8/295, train_loss: 0.4217, step time: 1.0531\n",
      "9/295, train_loss: 0.0546, step time: 1.0326\n",
      "10/295, train_loss: 0.0439, step time: 1.0349\n",
      "11/295, train_loss: 0.1089, step time: 1.0388\n",
      "12/295, train_loss: 0.0675, step time: 1.0643\n",
      "13/295, train_loss: 0.4010, step time: 1.0382\n",
      "14/295, train_loss: 0.0897, step time: 1.0588\n",
      "15/295, train_loss: 0.0511, step time: 1.0377\n",
      "16/295, train_loss: 0.1945, step time: 1.0519\n",
      "17/295, train_loss: 0.0488, step time: 1.0670\n",
      "18/295, train_loss: 0.0676, step time: 1.0383\n",
      "19/295, train_loss: 0.1137, step time: 1.0357\n",
      "20/295, train_loss: 0.1706, step time: 1.0488\n",
      "21/295, train_loss: 0.1167, step time: 1.0525\n",
      "22/295, train_loss: 0.0592, step time: 1.0339\n",
      "23/295, train_loss: 0.1022, step time: 1.0343\n",
      "24/295, train_loss: 0.1386, step time: 1.0322\n",
      "25/295, train_loss: 0.0948, step time: 1.0563\n",
      "26/295, train_loss: 0.2573, step time: 1.1316\n",
      "27/295, train_loss: 0.0592, step time: 1.0337\n",
      "28/295, train_loss: 0.0550, step time: 1.0559\n",
      "29/295, train_loss: 0.0577, step time: 1.0653\n",
      "30/295, train_loss: 0.0622, step time: 1.0452\n",
      "31/295, train_loss: 0.0515, step time: 1.0327\n",
      "32/295, train_loss: 0.0363, step time: 1.0343\n",
      "33/295, train_loss: 0.0441, step time: 1.0336\n",
      "34/295, train_loss: 0.1529, step time: 1.0616\n",
      "35/295, train_loss: 0.1487, step time: 1.0445\n",
      "36/295, train_loss: 0.1150, step time: 1.0601\n",
      "37/295, train_loss: 0.1202, step time: 1.0358\n",
      "38/295, train_loss: 0.0622, step time: 1.0400\n",
      "39/295, train_loss: 0.3890, step time: 1.0821\n",
      "40/295, train_loss: 0.4822, step time: 1.0553\n",
      "41/295, train_loss: 0.1753, step time: 1.0494\n",
      "42/295, train_loss: 0.0951, step time: 1.0435\n",
      "43/295, train_loss: 0.1307, step time: 1.0901\n",
      "44/295, train_loss: 0.0759, step time: 1.0509\n",
      "45/295, train_loss: 0.0668, step time: 1.0370\n",
      "46/295, train_loss: 0.3756, step time: 1.0751\n",
      "47/295, train_loss: 0.1260, step time: 1.0401\n",
      "48/295, train_loss: 0.1804, step time: 1.0320\n",
      "49/295, train_loss: 0.1000, step time: 1.0425\n",
      "50/295, train_loss: 0.0550, step time: 1.0320\n",
      "51/295, train_loss: 0.0659, step time: 1.0481\n",
      "52/295, train_loss: 0.3798, step time: 1.0346\n",
      "53/295, train_loss: 0.0827, step time: 1.0769\n",
      "54/295, train_loss: 0.0863, step time: 1.0402\n",
      "55/295, train_loss: 0.0580, step time: 1.0318\n",
      "56/295, train_loss: 0.3409, step time: 1.0379\n",
      "57/295, train_loss: 0.0389, step time: 1.0375\n",
      "58/295, train_loss: 0.0884, step time: 1.1064\n",
      "59/295, train_loss: 0.2504, step time: 1.0387\n",
      "60/295, train_loss: 0.0421, step time: 1.0395\n",
      "61/295, train_loss: 0.0410, step time: 1.0600\n",
      "62/295, train_loss: 0.0727, step time: 1.0444\n",
      "63/295, train_loss: 0.1355, step time: 1.0556\n",
      "64/295, train_loss: 0.3875, step time: 1.0864\n",
      "65/295, train_loss: 0.0535, step time: 1.0464\n",
      "66/295, train_loss: 0.1068, step time: 1.0323\n",
      "67/295, train_loss: 0.1474, step time: 1.0379\n",
      "68/295, train_loss: 0.2041, step time: 1.0887\n",
      "69/295, train_loss: 0.1920, step time: 1.1900\n",
      "70/295, train_loss: 0.1211, step time: 1.0317\n",
      "71/295, train_loss: 0.0746, step time: 1.0463\n",
      "72/295, train_loss: 0.0890, step time: 1.0410\n",
      "73/295, train_loss: 0.1175, step time: 1.0391\n",
      "74/295, train_loss: 0.0539, step time: 1.0335\n",
      "75/295, train_loss: 0.1512, step time: 1.0383\n",
      "76/295, train_loss: 0.0625, step time: 1.0930\n",
      "77/295, train_loss: 0.3955, step time: 1.0313\n",
      "78/295, train_loss: 0.0948, step time: 1.0587\n",
      "79/295, train_loss: 0.0732, step time: 1.0835\n",
      "80/295, train_loss: 0.0715, step time: 1.1435\n",
      "81/295, train_loss: 0.0621, step time: 1.0447\n",
      "82/295, train_loss: 0.0456, step time: 1.0417\n",
      "83/295, train_loss: 0.0514, step time: 1.0390\n",
      "84/295, train_loss: 0.4236, step time: 1.0580\n",
      "85/295, train_loss: 0.1319, step time: 1.0884\n",
      "86/295, train_loss: 0.0703, step time: 1.0563\n",
      "87/295, train_loss: 0.1856, step time: 1.0470\n",
      "88/295, train_loss: 0.0834, step time: 1.0353\n",
      "89/295, train_loss: 0.4156, step time: 1.0419\n",
      "90/295, train_loss: 0.0949, step time: 1.0571\n",
      "91/295, train_loss: 0.0548, step time: 1.0634\n",
      "92/295, train_loss: 0.0833, step time: 1.0429\n",
      "93/295, train_loss: 0.3216, step time: 1.0314\n",
      "94/295, train_loss: 0.0885, step time: 1.0391\n",
      "95/295, train_loss: 0.0718, step time: 1.0522\n",
      "96/295, train_loss: 0.0845, step time: 1.0469\n",
      "97/295, train_loss: 0.1252, step time: 1.0495\n",
      "98/295, train_loss: 0.0659, step time: 1.0400\n",
      "99/295, train_loss: 0.0957, step time: 1.0974\n",
      "100/295, train_loss: 0.3903, step time: 1.0479\n",
      "101/295, train_loss: 0.1309, step time: 1.0520\n",
      "102/295, train_loss: 0.4290, step time: 1.0519\n",
      "103/295, train_loss: 0.1171, step time: 1.0826\n",
      "104/295, train_loss: 0.0419, step time: 1.0367\n",
      "105/295, train_loss: 0.1440, step time: 1.0365\n",
      "106/295, train_loss: 0.1212, step time: 1.0351\n",
      "107/295, train_loss: 0.0576, step time: 1.0730\n",
      "108/295, train_loss: 0.0762, step time: 1.0426\n",
      "109/295, train_loss: 0.1772, step time: 1.0374\n",
      "110/295, train_loss: 0.1165, step time: 1.0632\n",
      "111/295, train_loss: 0.0850, step time: 1.1159\n",
      "112/295, train_loss: 0.1073, step time: 1.0336\n",
      "113/295, train_loss: 0.0395, step time: 1.0350\n",
      "114/295, train_loss: 0.0695, step time: 1.0352\n",
      "115/295, train_loss: 0.2639, step time: 1.0367\n",
      "116/295, train_loss: 0.0414, step time: 1.0355\n",
      "117/295, train_loss: 0.2092, step time: 1.0390\n",
      "118/295, train_loss: 0.1382, step time: 1.0409\n",
      "119/295, train_loss: 0.1091, step time: 1.0621\n",
      "120/295, train_loss: 0.1073, step time: 1.0595\n",
      "121/295, train_loss: 0.4754, step time: 1.0762\n",
      "122/295, train_loss: 0.0567, step time: 1.0486\n",
      "123/295, train_loss: 0.0712, step time: 1.0553\n",
      "124/295, train_loss: 0.0565, step time: 1.0391\n",
      "125/295, train_loss: 0.1203, step time: 1.0336\n",
      "126/295, train_loss: 0.0638, step time: 1.0499\n",
      "127/295, train_loss: 0.4050, step time: 1.0853\n",
      "128/295, train_loss: 0.2266, step time: 1.0861\n",
      "129/295, train_loss: 0.1043, step time: 1.0730\n",
      "130/295, train_loss: 0.0327, step time: 1.0351\n",
      "131/295, train_loss: 0.1512, step time: 1.0387\n",
      "132/295, train_loss: 0.0741, step time: 1.0390\n",
      "133/295, train_loss: 0.0737, step time: 1.0492\n",
      "134/295, train_loss: 0.0856, step time: 1.0397\n",
      "135/295, train_loss: 0.0373, step time: 1.0477\n",
      "136/295, train_loss: 0.1425, step time: 1.0514\n",
      "137/295, train_loss: 0.1181, step time: 1.0351\n",
      "138/295, train_loss: 0.0736, step time: 1.0416\n",
      "139/295, train_loss: 0.0452, step time: 1.1331\n",
      "140/295, train_loss: 0.0540, step time: 1.0370\n",
      "141/295, train_loss: 0.0929, step time: 1.0528\n",
      "142/295, train_loss: 0.1331, step time: 1.0421\n",
      "143/295, train_loss: 0.0779, step time: 1.0648\n",
      "144/295, train_loss: 0.1040, step time: 1.0862\n",
      "145/295, train_loss: 0.1309, step time: 1.0430\n",
      "146/295, train_loss: 0.1301, step time: 1.0396\n",
      "147/295, train_loss: 0.1178, step time: 1.0419\n",
      "148/295, train_loss: 0.0757, step time: 1.0456\n",
      "149/295, train_loss: 0.0834, step time: 1.0411\n",
      "150/295, train_loss: 0.0348, step time: 1.0621\n",
      "151/295, train_loss: 0.0889, step time: 1.0423\n",
      "152/295, train_loss: 0.0609, step time: 1.0504\n",
      "153/295, train_loss: 0.2031, step time: 1.0540\n",
      "154/295, train_loss: 0.0569, step time: 1.1024\n",
      "155/295, train_loss: 0.1383, step time: 1.0478\n",
      "156/295, train_loss: 0.0782, step time: 1.0590\n",
      "157/295, train_loss: 0.1005, step time: 1.0502\n",
      "158/295, train_loss: 0.1032, step time: 1.0396\n",
      "159/295, train_loss: 0.1039, step time: 1.0340\n",
      "160/295, train_loss: 0.1035, step time: 1.0327\n",
      "161/295, train_loss: 0.4029, step time: 1.0389\n",
      "162/295, train_loss: 0.0559, step time: 1.1413\n",
      "163/295, train_loss: 0.0858, step time: 1.0479\n",
      "164/295, train_loss: 0.0506, step time: 1.0504\n",
      "165/295, train_loss: 0.0844, step time: 1.0338\n",
      "166/295, train_loss: 0.3942, step time: 1.0313\n",
      "167/295, train_loss: 0.0698, step time: 1.0318\n",
      "168/295, train_loss: 0.0918, step time: 1.0377\n",
      "169/295, train_loss: 0.0406, step time: 1.0376\n",
      "170/295, train_loss: 0.1315, step time: 1.0334\n",
      "171/295, train_loss: 0.0559, step time: 1.0639\n",
      "172/295, train_loss: 0.0484, step time: 1.0322\n",
      "173/295, train_loss: 0.1311, step time: 1.0332\n",
      "174/295, train_loss: 0.0467, step time: 1.0461\n",
      "175/295, train_loss: 0.0889, step time: 1.0364\n",
      "176/295, train_loss: 0.0435, step time: 1.0336\n",
      "177/295, train_loss: 0.1105, step time: 1.0319\n",
      "178/295, train_loss: 0.1626, step time: 1.0632\n",
      "179/295, train_loss: 0.5173, step time: 1.0782\n",
      "180/295, train_loss: 0.0412, step time: 1.0365\n",
      "181/295, train_loss: 0.0990, step time: 1.0795\n",
      "182/295, train_loss: 0.0616, step time: 1.0670\n",
      "183/295, train_loss: 0.1328, step time: 1.0365\n",
      "184/295, train_loss: 0.1342, step time: 1.0437\n",
      "185/295, train_loss: 0.0891, step time: 1.0992\n",
      "186/295, train_loss: 0.0653, step time: 1.0632\n",
      "187/295, train_loss: 0.0877, step time: 1.0999\n",
      "188/295, train_loss: 0.0681, step time: 1.1126\n",
      "189/295, train_loss: 0.1263, step time: 1.0467\n",
      "190/295, train_loss: 0.1131, step time: 1.0397\n",
      "191/295, train_loss: 0.1203, step time: 1.0809\n",
      "192/295, train_loss: 0.0628, step time: 1.0397\n",
      "193/295, train_loss: 0.1269, step time: 1.0380\n",
      "194/295, train_loss: 0.0628, step time: 1.0387\n",
      "195/295, train_loss: 0.0595, step time: 1.0426\n",
      "196/295, train_loss: 0.2149, step time: 1.0464\n",
      "197/295, train_loss: 0.1313, step time: 1.0459\n",
      "198/295, train_loss: 0.2191, step time: 1.0346\n",
      "199/295, train_loss: 0.0751, step time: 1.0350\n",
      "200/295, train_loss: 0.0806, step time: 1.0437\n",
      "201/295, train_loss: 0.0920, step time: 1.0800\n",
      "202/295, train_loss: 0.0690, step time: 1.0337\n",
      "203/295, train_loss: 0.0400, step time: 1.0468\n",
      "204/295, train_loss: 0.0479, step time: 1.0612\n",
      "205/295, train_loss: 0.0462, step time: 1.0863\n",
      "206/295, train_loss: 0.4643, step time: 1.0323\n",
      "207/295, train_loss: 0.1709, step time: 1.0314\n",
      "208/295, train_loss: 0.0791, step time: 1.0374\n",
      "209/295, train_loss: 0.0708, step time: 1.0629\n",
      "210/295, train_loss: 0.0716, step time: 1.0985\n",
      "211/295, train_loss: 0.0667, step time: 1.0631\n",
      "212/295, train_loss: 0.0976, step time: 1.0607\n",
      "213/295, train_loss: 0.1378, step time: 1.0628\n",
      "214/295, train_loss: 0.0995, step time: 1.0466\n",
      "215/295, train_loss: 0.0527, step time: 1.0822\n",
      "216/295, train_loss: 0.1051, step time: 1.0588\n",
      "217/295, train_loss: 0.4159, step time: 1.0480\n",
      "218/295, train_loss: 0.1258, step time: 1.0483\n",
      "219/295, train_loss: 0.1245, step time: 1.0739\n",
      "220/295, train_loss: 0.3749, step time: 1.0383\n",
      "221/295, train_loss: 0.0607, step time: 1.0335\n",
      "222/295, train_loss: 0.0895, step time: 1.0474\n",
      "223/295, train_loss: 0.1173, step time: 1.0415\n",
      "224/295, train_loss: 0.0344, step time: 1.0559\n",
      "225/295, train_loss: 0.0654, step time: 1.0807\n",
      "226/295, train_loss: 0.0467, step time: 1.1052\n",
      "227/295, train_loss: 0.0773, step time: 1.0795\n",
      "228/295, train_loss: 0.0480, step time: 1.0554\n",
      "229/295, train_loss: 0.0776, step time: 1.0349\n",
      "230/295, train_loss: 0.0397, step time: 1.0563\n",
      "231/295, train_loss: 0.4512, step time: 1.0425\n",
      "232/295, train_loss: 0.0855, step time: 1.0771\n",
      "233/295, train_loss: 0.0353, step time: 1.0574\n",
      "234/295, train_loss: 0.1549, step time: 1.0367\n",
      "235/295, train_loss: 0.1151, step time: 1.0619\n",
      "236/295, train_loss: 0.0688, step time: 1.0575\n",
      "237/295, train_loss: 0.2322, step time: 1.0379\n",
      "238/295, train_loss: 0.0549, step time: 1.0839\n",
      "239/295, train_loss: 0.0441, step time: 1.0353\n",
      "240/295, train_loss: 0.0405, step time: 1.0426\n",
      "241/295, train_loss: 0.0473, step time: 1.1092\n",
      "242/295, train_loss: 0.1015, step time: 1.0490\n",
      "243/295, train_loss: 0.0584, step time: 1.0571\n",
      "244/295, train_loss: 0.0664, step time: 1.0419\n",
      "245/295, train_loss: 0.1379, step time: 1.0386\n",
      "246/295, train_loss: 0.1095, step time: 1.0462\n",
      "247/295, train_loss: 0.0347, step time: 1.0318\n",
      "248/295, train_loss: 0.0943, step time: 1.0333\n",
      "249/295, train_loss: 0.0825, step time: 1.0373\n",
      "250/295, train_loss: 0.0950, step time: 1.0483\n",
      "251/295, train_loss: 0.1511, step time: 1.0372\n",
      "252/295, train_loss: 0.0948, step time: 1.0445\n",
      "253/295, train_loss: 0.0799, step time: 1.0550\n",
      "254/295, train_loss: 0.0362, step time: 1.0415\n",
      "255/295, train_loss: 0.1300, step time: 1.0967\n",
      "256/295, train_loss: 0.0561, step time: 1.0736\n",
      "257/295, train_loss: 0.0827, step time: 1.0405\n",
      "258/295, train_loss: 0.1606, step time: 1.0340\n",
      "259/295, train_loss: 0.0673, step time: 1.0443\n",
      "260/295, train_loss: 0.0383, step time: 1.0384\n",
      "261/295, train_loss: 0.0622, step time: 1.0530\n",
      "262/295, train_loss: 0.2599, step time: 1.0444\n",
      "263/295, train_loss: 0.1030, step time: 1.0549\n",
      "264/295, train_loss: 0.1511, step time: 1.0797\n",
      "265/295, train_loss: 0.0713, step time: 1.0373\n",
      "266/295, train_loss: 0.0717, step time: 1.0545\n",
      "267/295, train_loss: 0.0683, step time: 1.0447\n",
      "268/295, train_loss: 0.0526, step time: 1.0426\n",
      "269/295, train_loss: 0.0493, step time: 1.0667\n",
      "270/295, train_loss: 0.1475, step time: 1.0726\n",
      "271/295, train_loss: 0.0720, step time: 1.0454\n",
      "272/295, train_loss: 0.0983, step time: 1.0620\n",
      "273/295, train_loss: 0.0468, step time: 1.0508\n",
      "274/295, train_loss: 0.1374, step time: 1.0391\n",
      "275/295, train_loss: 0.0513, step time: 1.0419\n",
      "276/295, train_loss: 0.2363, step time: 1.0802\n",
      "277/295, train_loss: 0.1178, step time: 1.0344\n",
      "278/295, train_loss: 0.0471, step time: 1.0477\n",
      "279/295, train_loss: 0.1018, step time: 1.0680\n",
      "280/295, train_loss: 0.4153, step time: 1.0417\n",
      "281/295, train_loss: 0.5168, step time: 1.0354\n",
      "282/295, train_loss: 0.0432, step time: 1.0384\n",
      "283/295, train_loss: 0.1688, step time: 1.0415\n",
      "284/295, train_loss: 0.1018, step time: 1.0396\n",
      "285/295, train_loss: 0.0498, step time: 1.0847\n",
      "286/295, train_loss: 0.1285, step time: 1.0722\n",
      "287/295, train_loss: 0.1036, step time: 1.0451\n",
      "288/295, train_loss: 0.0531, step time: 1.0805\n",
      "289/295, train_loss: 0.0835, step time: 1.0291\n",
      "290/295, train_loss: 0.0512, step time: 1.0313\n",
      "291/295, train_loss: 0.0456, step time: 1.0353\n",
      "292/295, train_loss: 0.0361, step time: 1.0303\n",
      "293/295, train_loss: 0.0649, step time: 1.0290\n",
      "294/295, train_loss: 0.0686, step time: 1.0291\n",
      "295/295, train_loss: 0.0841, step time: 1.0292\n",
      "epoch 31 average loss: 0.1240\n",
      "current epoch: 31 current mean dice: 0.7509 tc: 0.6894 wt: 0.8368 et: 0.7310\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 31 is: 386.5934\n",
      "----------\n",
      "epoch 32/100\n",
      "1/295, train_loss: 0.1099, step time: 1.0788\n",
      "2/295, train_loss: 0.0936, step time: 1.0556\n",
      "3/295, train_loss: 0.5054, step time: 1.0883\n",
      "4/295, train_loss: 0.1091, step time: 1.0795\n",
      "5/295, train_loss: 0.0785, step time: 1.0417\n",
      "6/295, train_loss: 0.1280, step time: 1.0687\n",
      "7/295, train_loss: 0.1047, step time: 1.0490\n",
      "8/295, train_loss: 0.0347, step time: 1.0690\n",
      "9/295, train_loss: 0.0610, step time: 1.0333\n",
      "10/295, train_loss: 0.0581, step time: 1.0319\n",
      "11/295, train_loss: 0.0379, step time: 1.0409\n",
      "12/295, train_loss: 0.0734, step time: 1.0399\n",
      "13/295, train_loss: 0.1186, step time: 1.0762\n",
      "14/295, train_loss: 0.1667, step time: 1.0363\n",
      "15/295, train_loss: 0.0926, step time: 1.0615\n",
      "16/295, train_loss: 0.0676, step time: 1.0401\n",
      "17/295, train_loss: 0.1869, step time: 1.0341\n",
      "18/295, train_loss: 0.1283, step time: 1.0385\n",
      "19/295, train_loss: 0.0925, step time: 1.0409\n",
      "20/295, train_loss: 0.1393, step time: 1.0536\n",
      "21/295, train_loss: 0.1794, step time: 1.0459\n",
      "22/295, train_loss: 0.0587, step time: 1.0299\n",
      "23/295, train_loss: 0.0585, step time: 1.0333\n",
      "24/295, train_loss: 0.1159, step time: 1.0341\n",
      "25/295, train_loss: 0.1395, step time: 1.0439\n",
      "26/295, train_loss: 0.1113, step time: 1.0352\n",
      "27/295, train_loss: 0.0455, step time: 1.0416\n",
      "28/295, train_loss: 0.0672, step time: 1.0323\n",
      "29/295, train_loss: 0.0418, step time: 1.0568\n",
      "30/295, train_loss: 0.1073, step time: 1.0300\n",
      "31/295, train_loss: 0.4072, step time: 1.0649\n",
      "32/295, train_loss: 0.0727, step time: 1.0365\n",
      "33/295, train_loss: 0.1409, step time: 1.0646\n",
      "34/295, train_loss: 0.3932, step time: 1.0430\n",
      "35/295, train_loss: 0.2606, step time: 1.0656\n",
      "36/295, train_loss: 0.0574, step time: 1.0733\n",
      "37/295, train_loss: 0.5607, step time: 1.0358\n",
      "38/295, train_loss: 0.0977, step time: 1.0377\n",
      "39/295, train_loss: 0.0926, step time: 1.0657\n",
      "40/295, train_loss: 0.1173, step time: 1.0434\n",
      "41/295, train_loss: 0.1286, step time: 1.0522\n",
      "42/295, train_loss: 0.0702, step time: 1.0704\n",
      "43/295, train_loss: 0.1215, step time: 1.0587\n",
      "44/295, train_loss: 0.1593, step time: 1.0380\n",
      "45/295, train_loss: 0.0705, step time: 1.0476\n",
      "46/295, train_loss: 0.4052, step time: 1.0866\n",
      "47/295, train_loss: 0.1347, step time: 1.0380\n",
      "48/295, train_loss: 0.0360, step time: 1.0379\n",
      "49/295, train_loss: 0.0396, step time: 1.0302\n",
      "50/295, train_loss: 0.1243, step time: 1.0332\n",
      "51/295, train_loss: 0.0548, step time: 1.0610\n",
      "52/295, train_loss: 0.4008, step time: 1.0384\n",
      "53/295, train_loss: 0.0394, step time: 1.0435\n",
      "54/295, train_loss: 0.0649, step time: 1.0318\n",
      "55/295, train_loss: 0.0361, step time: 1.0705\n",
      "56/295, train_loss: 0.0786, step time: 1.0494\n",
      "57/295, train_loss: 0.4155, step time: 1.0348\n",
      "58/295, train_loss: 0.0661, step time: 1.0320\n",
      "59/295, train_loss: 0.0427, step time: 1.0334\n",
      "60/295, train_loss: 0.1311, step time: 1.0378\n",
      "61/295, train_loss: 0.1202, step time: 1.0529\n",
      "62/295, train_loss: 0.0567, step time: 1.0411\n",
      "63/295, train_loss: 0.0491, step time: 1.0370\n",
      "64/295, train_loss: 0.0733, step time: 1.0314\n",
      "65/295, train_loss: 0.0884, step time: 1.0390\n",
      "66/295, train_loss: 0.0942, step time: 1.0441\n",
      "67/295, train_loss: 0.0966, step time: 1.0401\n",
      "68/295, train_loss: 0.1202, step time: 1.0339\n",
      "69/295, train_loss: 0.0363, step time: 1.0403\n",
      "70/295, train_loss: 0.2003, step time: 1.0587\n",
      "71/295, train_loss: 0.0565, step time: 1.0819\n",
      "72/295, train_loss: 0.0462, step time: 1.0683\n",
      "73/295, train_loss: 0.0930, step time: 1.0375\n",
      "74/295, train_loss: 0.1652, step time: 1.0310\n",
      "75/295, train_loss: 0.0481, step time: 1.0357\n",
      "76/295, train_loss: 0.0454, step time: 1.0505\n",
      "77/295, train_loss: 0.0770, step time: 1.1400\n",
      "78/295, train_loss: 0.0585, step time: 1.0351\n",
      "79/295, train_loss: 0.0581, step time: 1.0398\n",
      "80/295, train_loss: 0.0843, step time: 1.0423\n",
      "81/295, train_loss: 0.0420, step time: 1.0761\n",
      "82/295, train_loss: 0.0636, step time: 1.0354\n",
      "83/295, train_loss: 0.1237, step time: 1.0333\n",
      "84/295, train_loss: 0.0764, step time: 1.0616\n",
      "85/295, train_loss: 0.0498, step time: 1.0660\n",
      "86/295, train_loss: 0.0732, step time: 1.0849\n",
      "87/295, train_loss: 0.1165, step time: 1.0461\n",
      "88/295, train_loss: 0.0845, step time: 1.0370\n",
      "89/295, train_loss: 0.4044, step time: 1.0435\n",
      "90/295, train_loss: 0.0547, step time: 1.0541\n",
      "91/295, train_loss: 0.0833, step time: 1.0386\n",
      "92/295, train_loss: 0.0453, step time: 1.0516\n",
      "93/295, train_loss: 0.0975, step time: 1.0404\n",
      "94/295, train_loss: 0.0954, step time: 1.0366\n",
      "95/295, train_loss: 0.0716, step time: 1.0502\n",
      "96/295, train_loss: 0.0923, step time: 1.0421\n",
      "97/295, train_loss: 0.0758, step time: 1.0684\n",
      "98/295, train_loss: 0.3246, step time: 1.0627\n",
      "99/295, train_loss: 0.1040, step time: 1.0699\n",
      "100/295, train_loss: 0.0626, step time: 1.0718\n",
      "101/295, train_loss: 0.0717, step time: 1.0335\n",
      "102/295, train_loss: 0.0862, step time: 1.1067\n",
      "103/295, train_loss: 0.0424, step time: 1.0679\n",
      "104/295, train_loss: 0.1806, step time: 1.0483\n",
      "105/295, train_loss: 0.0372, step time: 1.0386\n",
      "106/295, train_loss: 0.1584, step time: 1.0426\n",
      "107/295, train_loss: 0.3744, step time: 1.0624\n",
      "108/295, train_loss: 0.0458, step time: 1.0380\n",
      "109/295, train_loss: 0.0424, step time: 1.0561\n",
      "110/295, train_loss: 0.1430, step time: 1.0347\n",
      "111/295, train_loss: 0.2060, step time: 1.0984\n",
      "112/295, train_loss: 0.0721, step time: 1.0343\n",
      "113/295, train_loss: 0.4044, step time: 1.0589\n",
      "114/295, train_loss: 0.1060, step time: 1.0430\n",
      "115/295, train_loss: 0.4171, step time: 1.0544\n",
      "116/295, train_loss: 0.0553, step time: 1.0437\n",
      "117/295, train_loss: 0.0770, step time: 1.0352\n",
      "118/295, train_loss: 0.0458, step time: 1.0356\n",
      "119/295, train_loss: 0.0443, step time: 1.0423\n",
      "120/295, train_loss: 0.2639, step time: 1.0439\n",
      "121/295, train_loss: 0.0722, step time: 1.0352\n",
      "122/295, train_loss: 0.0997, step time: 1.0375\n",
      "123/295, train_loss: 0.2827, step time: 1.0392\n",
      "124/295, train_loss: 0.0878, step time: 1.0733\n",
      "125/295, train_loss: 0.0709, step time: 1.0897\n",
      "126/295, train_loss: 0.0548, step time: 1.0454\n",
      "127/295, train_loss: 0.1310, step time: 1.0324\n",
      "128/295, train_loss: 0.1448, step time: 1.1128\n",
      "129/295, train_loss: 0.0735, step time: 1.0343\n",
      "130/295, train_loss: 0.0712, step time: 1.0322\n",
      "131/295, train_loss: 0.3813, step time: 1.0412\n",
      "132/295, train_loss: 0.2101, step time: 1.0402\n",
      "133/295, train_loss: 0.0937, step time: 1.0642\n",
      "134/295, train_loss: 0.0547, step time: 1.0846\n",
      "135/295, train_loss: 0.1131, step time: 1.0370\n",
      "136/295, train_loss: 0.0808, step time: 1.0386\n",
      "137/295, train_loss: 0.0776, step time: 1.0400\n",
      "138/295, train_loss: 0.1101, step time: 1.0699\n",
      "139/295, train_loss: 0.1209, step time: 1.0551\n",
      "140/295, train_loss: 0.4317, step time: 1.1069\n",
      "141/295, train_loss: 0.3781, step time: 1.0381\n",
      "142/295, train_loss: 0.0784, step time: 1.0559\n",
      "143/295, train_loss: 0.0925, step time: 1.0626\n",
      "144/295, train_loss: 0.1300, step time: 1.0422\n",
      "145/295, train_loss: 0.0826, step time: 1.0408\n",
      "146/295, train_loss: 0.4783, step time: 1.0724\n",
      "147/295, train_loss: 0.2270, step time: 1.0468\n",
      "148/295, train_loss: 0.4632, step time: 1.0552\n",
      "149/295, train_loss: 0.0571, step time: 1.0598\n",
      "150/295, train_loss: 0.1973, step time: 1.0504\n",
      "151/295, train_loss: 0.0586, step time: 1.0516\n",
      "152/295, train_loss: 0.0416, step time: 1.0397\n",
      "153/295, train_loss: 0.0721, step time: 1.0560\n",
      "154/295, train_loss: 0.3649, step time: 1.1267\n",
      "155/295, train_loss: 0.1226, step time: 1.0383\n",
      "156/295, train_loss: 0.1214, step time: 1.0368\n",
      "157/295, train_loss: 0.1821, step time: 1.0367\n",
      "158/295, train_loss: 0.1029, step time: 1.0416\n",
      "159/295, train_loss: 0.0610, step time: 1.0393\n",
      "160/295, train_loss: 0.0959, step time: 1.0369\n",
      "161/295, train_loss: 0.0620, step time: 1.0360\n",
      "162/295, train_loss: 0.0654, step time: 1.0615\n",
      "163/295, train_loss: 0.4555, step time: 1.0529\n",
      "164/295, train_loss: 0.1683, step time: 1.0324\n",
      "165/295, train_loss: 0.3854, step time: 1.0396\n",
      "166/295, train_loss: 0.2251, step time: 1.0497\n",
      "167/295, train_loss: 0.0673, step time: 1.0459\n",
      "168/295, train_loss: 0.2869, step time: 1.0345\n",
      "169/295, train_loss: 0.1024, step time: 1.0401\n",
      "170/295, train_loss: 0.1427, step time: 1.0443\n",
      "171/295, train_loss: 0.0710, step time: 1.0419\n",
      "172/295, train_loss: 0.0658, step time: 1.0945\n",
      "173/295, train_loss: 0.1873, step time: 1.0590\n",
      "174/295, train_loss: 0.2155, step time: 1.0411\n",
      "175/295, train_loss: 0.0713, step time: 1.1170\n",
      "176/295, train_loss: 0.0434, step time: 1.0425\n",
      "177/295, train_loss: 0.0345, step time: 1.0432\n",
      "178/295, train_loss: 0.0399, step time: 1.0378\n",
      "179/295, train_loss: 0.0461, step time: 1.0442\n",
      "180/295, train_loss: 0.1398, step time: 1.1043\n",
      "181/295, train_loss: 0.1707, step time: 1.0444\n",
      "182/295, train_loss: 0.1443, step time: 1.0432\n",
      "183/295, train_loss: 0.0444, step time: 1.0382\n",
      "184/295, train_loss: 0.2809, step time: 1.0448\n",
      "185/295, train_loss: 0.0587, step time: 1.0687\n",
      "186/295, train_loss: 0.0731, step time: 1.0636\n",
      "187/295, train_loss: 0.4033, step time: 1.0539\n",
      "188/295, train_loss: 0.1141, step time: 1.0589\n",
      "189/295, train_loss: 0.3406, step time: 1.0346\n",
      "190/295, train_loss: 0.0571, step time: 1.0373\n",
      "191/295, train_loss: 0.0572, step time: 1.0497\n",
      "192/295, train_loss: 0.0771, step time: 1.1013\n",
      "193/295, train_loss: 0.5122, step time: 1.0398\n",
      "194/295, train_loss: 0.0724, step time: 1.0826\n",
      "195/295, train_loss: 0.0615, step time: 1.0429\n",
      "196/295, train_loss: 0.2211, step time: 1.0459\n",
      "197/295, train_loss: 0.0728, step time: 1.0574\n",
      "198/295, train_loss: 0.1565, step time: 1.0391\n",
      "199/295, train_loss: 0.0815, step time: 1.0478\n",
      "200/295, train_loss: 0.1655, step time: 1.0385\n",
      "201/295, train_loss: 0.0454, step time: 1.0399\n",
      "202/295, train_loss: 0.0526, step time: 1.0408\n",
      "203/295, train_loss: 0.2108, step time: 1.0368\n",
      "204/295, train_loss: 0.1340, step time: 1.0474\n",
      "205/295, train_loss: 0.0576, step time: 1.0694\n",
      "206/295, train_loss: 0.0705, step time: 1.0326\n",
      "207/295, train_loss: 0.1401, step time: 1.0451\n",
      "208/295, train_loss: 0.0935, step time: 1.0529\n",
      "209/295, train_loss: 0.1392, step time: 1.0417\n",
      "210/295, train_loss: 0.2010, step time: 1.0390\n",
      "211/295, train_loss: 0.1348, step time: 1.0387\n",
      "212/295, train_loss: 0.0973, step time: 1.0376\n",
      "213/295, train_loss: 0.1337, step time: 1.0732\n",
      "214/295, train_loss: 0.1165, step time: 1.0554\n",
      "215/295, train_loss: 0.0631, step time: 1.0456\n",
      "216/295, train_loss: 0.0400, step time: 1.0626\n",
      "217/295, train_loss: 0.0676, step time: 1.0326\n",
      "218/295, train_loss: 0.6211, step time: 1.0609\n",
      "219/295, train_loss: 0.0584, step time: 1.0440\n",
      "220/295, train_loss: 0.1137, step time: 1.0350\n",
      "221/295, train_loss: 0.1206, step time: 1.0414\n",
      "222/295, train_loss: 0.0532, step time: 1.0717\n",
      "223/295, train_loss: 0.0635, step time: 1.0435\n",
      "224/295, train_loss: 0.0358, step time: 1.0589\n",
      "225/295, train_loss: 0.0499, step time: 1.0440\n",
      "226/295, train_loss: 0.1110, step time: 1.0347\n",
      "227/295, train_loss: 0.1928, step time: 1.0453\n",
      "228/295, train_loss: 0.3738, step time: 1.0635\n",
      "229/295, train_loss: 0.3583, step time: 1.0317\n",
      "230/295, train_loss: 0.0624, step time: 1.0371\n",
      "231/295, train_loss: 0.1077, step time: 1.0446\n",
      "232/295, train_loss: 0.0755, step time: 1.0424\n",
      "233/295, train_loss: 0.0404, step time: 1.0444\n",
      "234/295, train_loss: 0.1133, step time: 1.0450\n",
      "235/295, train_loss: 0.1638, step time: 1.0633\n",
      "236/295, train_loss: 0.0699, step time: 1.0763\n",
      "237/295, train_loss: 0.0879, step time: 1.0674\n",
      "238/295, train_loss: 0.1159, step time: 1.0400\n",
      "239/295, train_loss: 0.0439, step time: 1.0461\n",
      "240/295, train_loss: 0.1380, step time: 1.0452\n",
      "241/295, train_loss: 0.0517, step time: 1.0672\n",
      "242/295, train_loss: 0.0778, step time: 1.0402\n",
      "243/295, train_loss: 0.0532, step time: 1.0464\n",
      "244/295, train_loss: 0.0582, step time: 1.0372\n",
      "245/295, train_loss: 0.1122, step time: 1.0324\n",
      "246/295, train_loss: 0.0931, step time: 1.0348\n",
      "247/295, train_loss: 0.2025, step time: 1.0433\n",
      "248/295, train_loss: 0.2218, step time: 1.0476\n",
      "249/295, train_loss: 0.0577, step time: 1.0369\n",
      "250/295, train_loss: 0.0643, step time: 1.1098\n",
      "251/295, train_loss: 0.0876, step time: 1.0841\n",
      "252/295, train_loss: 0.1395, step time: 1.0658\n",
      "253/295, train_loss: 0.0350, step time: 1.1175\n",
      "254/295, train_loss: 0.4301, step time: 1.0422\n",
      "255/295, train_loss: 0.0738, step time: 1.0641\n",
      "256/295, train_loss: 0.1276, step time: 1.0320\n",
      "257/295, train_loss: 0.0946, step time: 1.0373\n",
      "258/295, train_loss: 0.0599, step time: 1.0531\n",
      "259/295, train_loss: 0.0565, step time: 1.0359\n",
      "260/295, train_loss: 0.1666, step time: 1.0385\n",
      "261/295, train_loss: 0.3996, step time: 1.0441\n",
      "262/295, train_loss: 0.0935, step time: 1.0589\n",
      "263/295, train_loss: 0.0844, step time: 1.0433\n",
      "264/295, train_loss: 0.0775, step time: 1.0486\n",
      "265/295, train_loss: 0.1101, step time: 1.0401\n",
      "266/295, train_loss: 0.1561, step time: 1.0470\n",
      "267/295, train_loss: 0.4172, step time: 1.0367\n",
      "268/295, train_loss: 0.1556, step time: 1.0657\n",
      "269/295, train_loss: 0.0451, step time: 1.0398\n",
      "270/295, train_loss: 0.0853, step time: 1.0602\n",
      "271/295, train_loss: 0.0484, step time: 1.0770\n",
      "272/295, train_loss: 0.0559, step time: 1.0437\n",
      "273/295, train_loss: 0.0468, step time: 1.0660\n",
      "274/295, train_loss: 0.0932, step time: 1.0542\n",
      "275/295, train_loss: 0.1384, step time: 1.0489\n",
      "276/295, train_loss: 0.1657, step time: 1.0555\n",
      "277/295, train_loss: 0.0756, step time: 1.0335\n",
      "278/295, train_loss: 0.1001, step time: 1.0409\n",
      "279/295, train_loss: 0.1708, step time: 1.0557\n",
      "280/295, train_loss: 0.0613, step time: 1.0576\n",
      "281/295, train_loss: 0.1045, step time: 1.0434\n",
      "282/295, train_loss: 0.1452, step time: 1.0385\n",
      "283/295, train_loss: 0.0832, step time: 1.0410\n",
      "284/295, train_loss: 0.0902, step time: 1.0790\n",
      "285/295, train_loss: 0.0601, step time: 1.0462\n",
      "286/295, train_loss: 0.0814, step time: 1.0383\n",
      "287/295, train_loss: 0.0683, step time: 1.0342\n",
      "288/295, train_loss: 0.0584, step time: 1.0318\n",
      "289/295, train_loss: 0.0703, step time: 1.0290\n",
      "290/295, train_loss: 0.1015, step time: 1.0298\n",
      "291/295, train_loss: 0.0903, step time: 1.0303\n",
      "292/295, train_loss: 0.0711, step time: 1.0287\n",
      "293/295, train_loss: 0.1533, step time: 1.0302\n",
      "294/295, train_loss: 0.0948, step time: 1.0288\n",
      "295/295, train_loss: 0.2120, step time: 1.0314\n",
      "epoch 32 average loss: 0.1299\n",
      "current epoch: 32 current mean dice: 0.7798 tc: 0.7261 wt: 0.8548 et: 0.7633\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 32 is: 385.7937\n",
      "----------\n",
      "epoch 33/100\n",
      "1/295, train_loss: 0.2455, step time: 1.0579\n",
      "2/295, train_loss: 0.0446, step time: 1.0617\n",
      "3/295, train_loss: 0.4415, step time: 1.0895\n",
      "4/295, train_loss: 0.0915, step time: 1.0724\n",
      "5/295, train_loss: 0.1419, step time: 1.0501\n",
      "6/295, train_loss: 0.2529, step time: 1.0494\n",
      "7/295, train_loss: 0.0489, step time: 1.0558\n",
      "8/295, train_loss: 0.1186, step time: 1.0428\n",
      "9/295, train_loss: 0.0590, step time: 1.0399\n",
      "10/295, train_loss: 0.0963, step time: 1.0605\n",
      "11/295, train_loss: 0.1557, step time: 1.0800\n",
      "12/295, train_loss: 0.0633, step time: 1.0605\n",
      "13/295, train_loss: 0.4019, step time: 1.0324\n",
      "14/295, train_loss: 0.1844, step time: 1.0389\n",
      "15/295, train_loss: 0.1013, step time: 1.0394\n",
      "16/295, train_loss: 0.0805, step time: 1.0702\n",
      "17/295, train_loss: 0.1514, step time: 1.0344\n",
      "18/295, train_loss: 0.0411, step time: 1.0344\n",
      "19/295, train_loss: 0.4352, step time: 1.0412\n",
      "20/295, train_loss: 0.4090, step time: 1.1060\n",
      "21/295, train_loss: 0.0600, step time: 1.0324\n",
      "22/295, train_loss: 0.0417, step time: 1.0340\n",
      "23/295, train_loss: 0.0461, step time: 1.0739\n",
      "24/295, train_loss: 0.0828, step time: 1.0335\n",
      "25/295, train_loss: 0.0535, step time: 1.0470\n",
      "26/295, train_loss: 0.2845, step time: 1.0392\n",
      "27/295, train_loss: 0.1930, step time: 1.0578\n",
      "28/295, train_loss: 0.1027, step time: 1.0477\n",
      "29/295, train_loss: 0.0798, step time: 1.0341\n",
      "30/295, train_loss: 0.1254, step time: 1.0592\n",
      "31/295, train_loss: 0.0790, step time: 1.0364\n",
      "32/295, train_loss: 0.4204, step time: 1.0347\n",
      "33/295, train_loss: 0.0985, step time: 1.0498\n",
      "34/295, train_loss: 0.3745, step time: 1.0375\n",
      "35/295, train_loss: 0.0635, step time: 1.0399\n",
      "36/295, train_loss: 0.0561, step time: 1.0866\n",
      "37/295, train_loss: 0.0712, step time: 1.0458\n",
      "38/295, train_loss: 0.0680, step time: 1.0392\n",
      "39/295, train_loss: 0.0719, step time: 1.0361\n",
      "40/295, train_loss: 0.1002, step time: 1.1013\n",
      "41/295, train_loss: 0.0898, step time: 1.0727\n",
      "42/295, train_loss: 0.1490, step time: 1.0668\n",
      "43/295, train_loss: 0.0592, step time: 1.0375\n",
      "44/295, train_loss: 0.1066, step time: 1.0487\n",
      "45/295, train_loss: 0.1481, step time: 1.0570\n",
      "46/295, train_loss: 0.0845, step time: 1.0445\n",
      "47/295, train_loss: 0.0993, step time: 1.0331\n",
      "48/295, train_loss: 0.0669, step time: 1.0316\n",
      "49/295, train_loss: 0.0601, step time: 1.0409\n",
      "50/295, train_loss: 0.0387, step time: 1.0446\n",
      "51/295, train_loss: 0.0586, step time: 1.0928\n",
      "52/295, train_loss: 0.0741, step time: 1.0345\n",
      "53/295, train_loss: 0.0705, step time: 1.0675\n",
      "54/295, train_loss: 0.0618, step time: 1.0342\n",
      "55/295, train_loss: 0.0363, step time: 1.0397\n",
      "56/295, train_loss: 0.0600, step time: 1.0534\n",
      "57/295, train_loss: 0.2303, step time: 1.0476\n",
      "58/295, train_loss: 0.0640, step time: 1.0422\n",
      "59/295, train_loss: 0.1454, step time: 1.0356\n",
      "60/295, train_loss: 0.1387, step time: 1.0316\n",
      "61/295, train_loss: 0.0465, step time: 1.0362\n",
      "62/295, train_loss: 0.0653, step time: 1.0373\n",
      "63/295, train_loss: 0.1208, step time: 1.0467\n",
      "64/295, train_loss: 0.0562, step time: 1.0749\n",
      "65/295, train_loss: 0.0744, step time: 1.0350\n",
      "66/295, train_loss: 0.1686, step time: 1.0603\n",
      "67/295, train_loss: 0.1129, step time: 1.0640\n",
      "68/295, train_loss: 0.0800, step time: 1.0846\n",
      "69/295, train_loss: 0.0631, step time: 1.0421\n",
      "70/295, train_loss: 0.2249, step time: 1.0763\n",
      "71/295, train_loss: 0.2311, step time: 1.0346\n",
      "72/295, train_loss: 0.0914, step time: 1.0311\n",
      "73/295, train_loss: 0.3919, step time: 1.0327\n",
      "74/295, train_loss: 0.0904, step time: 1.0417\n",
      "75/295, train_loss: 0.1064, step time: 1.0613\n",
      "76/295, train_loss: 0.1342, step time: 1.0369\n",
      "77/295, train_loss: 0.0539, step time: 1.0404\n",
      "78/295, train_loss: 0.0896, step time: 1.0366\n",
      "79/295, train_loss: 0.1525, step time: 1.0607\n",
      "80/295, train_loss: 0.0375, step time: 1.0413\n",
      "81/295, train_loss: 0.0987, step time: 1.0426\n",
      "82/295, train_loss: 0.0968, step time: 1.0604\n",
      "83/295, train_loss: 0.0711, step time: 1.0482\n",
      "84/295, train_loss: 0.0722, step time: 1.0360\n",
      "85/295, train_loss: 0.0854, step time: 1.0337\n",
      "86/295, train_loss: 0.1443, step time: 1.0342\n",
      "87/295, train_loss: 0.0543, step time: 1.0443\n",
      "88/295, train_loss: 0.0605, step time: 1.0313\n",
      "89/295, train_loss: 0.0566, step time: 1.0727\n",
      "90/295, train_loss: 0.0522, step time: 1.0624\n",
      "91/295, train_loss: 0.0966, step time: 1.0840\n",
      "92/295, train_loss: 0.0662, step time: 1.0373\n",
      "93/295, train_loss: 0.1977, step time: 1.0381\n",
      "94/295, train_loss: 0.1714, step time: 1.0434\n",
      "95/295, train_loss: 0.4128, step time: 1.0675\n",
      "96/295, train_loss: 0.0669, step time: 1.0450\n",
      "97/295, train_loss: 0.4229, step time: 1.0335\n",
      "98/295, train_loss: 0.0838, step time: 1.0575\n",
      "99/295, train_loss: 0.1030, step time: 1.0406\n",
      "100/295, train_loss: 0.1072, step time: 1.0489\n",
      "101/295, train_loss: 0.0500, step time: 1.0416\n",
      "102/295, train_loss: 0.0844, step time: 1.0345\n",
      "103/295, train_loss: 0.0971, step time: 1.0402\n",
      "104/295, train_loss: 0.3454, step time: 1.0494\n",
      "105/295, train_loss: 0.0622, step time: 1.0756\n",
      "106/295, train_loss: 0.0523, step time: 1.0762\n",
      "107/295, train_loss: 0.0941, step time: 1.0565\n",
      "108/295, train_loss: 0.0629, step time: 1.0350\n",
      "109/295, train_loss: 0.0460, step time: 1.0403\n",
      "110/295, train_loss: 0.1978, step time: 1.0372\n",
      "111/295, train_loss: 0.1038, step time: 1.0742\n",
      "112/295, train_loss: 0.0926, step time: 1.0342\n",
      "113/295, train_loss: 0.1167, step time: 1.0476\n",
      "114/295, train_loss: 0.1484, step time: 1.0352\n",
      "115/295, train_loss: 0.1144, step time: 1.0338\n",
      "116/295, train_loss: 0.0988, step time: 1.0552\n",
      "117/295, train_loss: 0.0559, step time: 1.0426\n",
      "118/295, train_loss: 0.3993, step time: 1.0698\n",
      "119/295, train_loss: 0.0988, step time: 1.0372\n",
      "120/295, train_loss: 0.1135, step time: 1.0373\n",
      "121/295, train_loss: 0.0655, step time: 1.0405\n",
      "122/295, train_loss: 0.0668, step time: 1.0742\n",
      "123/295, train_loss: 0.0665, step time: 1.0472\n",
      "124/295, train_loss: 0.0902, step time: 1.0635\n",
      "125/295, train_loss: 0.1890, step time: 1.0479\n",
      "126/295, train_loss: 0.0686, step time: 1.0470\n",
      "127/295, train_loss: 0.4493, step time: 1.0435\n",
      "128/295, train_loss: 0.0540, step time: 1.0712\n",
      "129/295, train_loss: 0.4166, step time: 1.0368\n",
      "130/295, train_loss: 0.0600, step time: 1.0487\n",
      "131/295, train_loss: 0.1012, step time: 1.0388\n",
      "132/295, train_loss: 0.0448, step time: 1.0354\n",
      "133/295, train_loss: 0.4064, step time: 1.0381\n",
      "134/295, train_loss: 0.0570, step time: 1.0381\n",
      "135/295, train_loss: 0.0452, step time: 1.0710\n",
      "136/295, train_loss: 0.1505, step time: 1.0408\n",
      "137/295, train_loss: 0.0479, step time: 1.0554\n",
      "138/295, train_loss: 0.1587, step time: 1.0635\n",
      "139/295, train_loss: 0.0386, step time: 1.0372\n",
      "140/295, train_loss: 0.0730, step time: 1.0388\n",
      "141/295, train_loss: 0.0658, step time: 1.0325\n",
      "142/295, train_loss: 0.2327, step time: 1.0341\n",
      "143/295, train_loss: 0.1129, step time: 1.0645\n",
      "144/295, train_loss: 0.0650, step time: 1.0398\n",
      "145/295, train_loss: 0.0396, step time: 1.0645\n",
      "146/295, train_loss: 0.0835, step time: 1.0327\n",
      "147/295, train_loss: 0.0392, step time: 1.0356\n",
      "148/295, train_loss: 0.0812, step time: 1.0312\n",
      "149/295, train_loss: 0.0940, step time: 1.0491\n",
      "150/295, train_loss: 0.0420, step time: 1.0468\n",
      "151/295, train_loss: 0.0958, step time: 1.0456\n",
      "152/295, train_loss: 0.0929, step time: 1.0497\n",
      "153/295, train_loss: 0.3538, step time: 1.0463\n",
      "154/295, train_loss: 0.0658, step time: 1.0368\n",
      "155/295, train_loss: 0.0484, step time: 1.0369\n",
      "156/295, train_loss: 0.0582, step time: 1.0460\n",
      "157/295, train_loss: 0.0385, step time: 1.1029\n",
      "158/295, train_loss: 0.0549, step time: 1.0676\n",
      "159/295, train_loss: 0.0963, step time: 1.0386\n",
      "160/295, train_loss: 0.0689, step time: 1.0344\n",
      "161/295, train_loss: 0.0577, step time: 1.0435\n",
      "162/295, train_loss: 0.0523, step time: 1.0438\n",
      "163/295, train_loss: 0.0508, step time: 1.1131\n",
      "164/295, train_loss: 0.0443, step time: 1.0366\n",
      "165/295, train_loss: 0.1040, step time: 1.0350\n",
      "166/295, train_loss: 0.0793, step time: 1.0719\n",
      "167/295, train_loss: 0.0547, step time: 1.0361\n",
      "168/295, train_loss: 0.0627, step time: 1.0447\n",
      "169/295, train_loss: 0.0670, step time: 1.0400\n",
      "170/295, train_loss: 0.1487, step time: 1.0449\n",
      "171/295, train_loss: 0.5050, step time: 1.0312\n",
      "172/295, train_loss: 0.0997, step time: 1.0418\n",
      "173/295, train_loss: 0.0346, step time: 1.1538\n",
      "174/295, train_loss: 0.1242, step time: 1.0390\n",
      "175/295, train_loss: 0.0735, step time: 1.0436\n",
      "176/295, train_loss: 0.0710, step time: 1.0406\n",
      "177/295, train_loss: 0.0585, step time: 1.0499\n",
      "178/295, train_loss: 0.0700, step time: 1.0384\n",
      "179/295, train_loss: 0.0612, step time: 1.0407\n",
      "180/295, train_loss: 0.1729, step time: 1.0362\n",
      "181/295, train_loss: 0.0684, step time: 1.0621\n",
      "182/295, train_loss: 0.1291, step time: 1.0740\n",
      "183/295, train_loss: 0.1104, step time: 1.0609\n",
      "184/295, train_loss: 0.0648, step time: 1.0371\n",
      "185/295, train_loss: 0.1207, step time: 1.0432\n",
      "186/295, train_loss: 0.0780, step time: 1.0439\n",
      "187/295, train_loss: 0.1529, step time: 1.0579\n",
      "188/295, train_loss: 0.0652, step time: 1.0392\n",
      "189/295, train_loss: 0.2557, step time: 1.0450\n",
      "190/295, train_loss: 0.1029, step time: 1.0384\n",
      "191/295, train_loss: 0.0884, step time: 1.0359\n",
      "192/295, train_loss: 0.1949, step time: 1.0369\n",
      "193/295, train_loss: 0.0896, step time: 1.0454\n",
      "194/295, train_loss: 0.1002, step time: 1.0420\n",
      "195/295, train_loss: 0.4184, step time: 1.0899\n",
      "196/295, train_loss: 0.1219, step time: 1.0324\n",
      "197/295, train_loss: 0.0992, step time: 1.0368\n",
      "198/295, train_loss: 0.1391, step time: 1.0396\n",
      "199/295, train_loss: 0.0600, step time: 1.0340\n",
      "200/295, train_loss: 0.1066, step time: 1.0509\n",
      "201/295, train_loss: 0.0680, step time: 1.0396\n",
      "202/295, train_loss: 0.3956, step time: 1.0380\n",
      "203/295, train_loss: 0.0362, step time: 1.0344\n",
      "204/295, train_loss: 0.0830, step time: 1.0799\n",
      "205/295, train_loss: 0.0365, step time: 1.0816\n",
      "206/295, train_loss: 0.0554, step time: 1.0393\n",
      "207/295, train_loss: 0.1271, step time: 1.0308\n",
      "208/295, train_loss: 0.0879, step time: 1.0412\n",
      "209/295, train_loss: 0.1202, step time: 1.0660\n",
      "210/295, train_loss: 0.0778, step time: 1.0522\n",
      "211/295, train_loss: 0.0498, step time: 1.0473\n",
      "212/295, train_loss: 0.0859, step time: 1.1180\n",
      "213/295, train_loss: 0.1356, step time: 1.0509\n",
      "214/295, train_loss: 0.0563, step time: 1.0337\n",
      "215/295, train_loss: 0.1346, step time: 1.0363\n",
      "216/295, train_loss: 0.0437, step time: 1.0585\n",
      "217/295, train_loss: 0.0913, step time: 1.0473\n",
      "218/295, train_loss: 0.0944, step time: 1.0337\n",
      "219/295, train_loss: 0.4187, step time: 1.0492\n",
      "220/295, train_loss: 0.1017, step time: 1.0936\n",
      "221/295, train_loss: 0.0377, step time: 1.0427\n",
      "222/295, train_loss: 0.1418, step time: 1.1115\n",
      "223/295, train_loss: 0.0448, step time: 1.0466\n",
      "224/295, train_loss: 0.0251, step time: 1.0376\n",
      "225/295, train_loss: 0.3854, step time: 1.0385\n",
      "226/295, train_loss: 0.1033, step time: 1.0325\n",
      "227/295, train_loss: 0.1255, step time: 1.0987\n",
      "228/295, train_loss: 0.0525, step time: 1.0871\n",
      "229/295, train_loss: 0.0360, step time: 1.0523\n",
      "230/295, train_loss: 0.1022, step time: 1.0404\n",
      "231/295, train_loss: 0.0890, step time: 1.0393\n",
      "232/295, train_loss: 0.0499, step time: 1.0356\n",
      "233/295, train_loss: 0.0750, step time: 1.0451\n",
      "234/295, train_loss: 0.0910, step time: 1.0471\n",
      "235/295, train_loss: 0.1089, step time: 1.0375\n",
      "236/295, train_loss: 0.0978, step time: 1.0602\n",
      "237/295, train_loss: 0.0885, step time: 1.1195\n",
      "238/295, train_loss: 0.2349, step time: 1.0537\n",
      "239/295, train_loss: 0.0629, step time: 1.0372\n",
      "240/295, train_loss: 0.0569, step time: 1.0576\n",
      "241/295, train_loss: 0.0964, step time: 1.0307\n",
      "242/295, train_loss: 0.0410, step time: 1.1040\n",
      "243/295, train_loss: 0.1536, step time: 1.0380\n",
      "244/295, train_loss: 0.0574, step time: 1.0368\n",
      "245/295, train_loss: 0.1506, step time: 1.0443\n",
      "246/295, train_loss: 0.0408, step time: 1.0402\n",
      "247/295, train_loss: 0.5182, step time: 1.0376\n",
      "248/295, train_loss: 0.0464, step time: 1.0489\n",
      "249/295, train_loss: 0.0698, step time: 1.0660\n",
      "250/295, train_loss: 0.1357, step time: 1.0577\n",
      "251/295, train_loss: 0.0742, step time: 1.0362\n",
      "252/295, train_loss: 0.1041, step time: 1.0783\n",
      "253/295, train_loss: 0.1199, step time: 1.1133\n",
      "254/295, train_loss: 0.0674, step time: 1.0302\n",
      "255/295, train_loss: 0.0867, step time: 1.0359\n",
      "256/295, train_loss: 0.0382, step time: 1.0615\n",
      "257/295, train_loss: 0.0365, step time: 1.0442\n",
      "258/295, train_loss: 0.1125, step time: 1.0361\n",
      "259/295, train_loss: 0.0466, step time: 1.0436\n",
      "260/295, train_loss: 0.0677, step time: 1.0659\n",
      "261/295, train_loss: 0.2235, step time: 1.0346\n",
      "262/295, train_loss: 0.0577, step time: 1.0401\n",
      "263/295, train_loss: 0.0699, step time: 1.0375\n",
      "264/295, train_loss: 0.0483, step time: 1.0413\n",
      "265/295, train_loss: 0.1004, step time: 1.0696\n",
      "266/295, train_loss: 0.0462, step time: 1.0654\n",
      "267/295, train_loss: 0.1366, step time: 1.0428\n",
      "268/295, train_loss: 0.0718, step time: 1.0345\n",
      "269/295, train_loss: 0.1372, step time: 1.0368\n",
      "270/295, train_loss: 0.0730, step time: 1.0630\n",
      "271/295, train_loss: 0.0964, step time: 1.0709\n",
      "272/295, train_loss: 0.3950, step time: 1.0499\n",
      "273/295, train_loss: 0.3693, step time: 1.0552\n",
      "274/295, train_loss: 0.0510, step time: 1.0340\n",
      "275/295, train_loss: 0.1062, step time: 1.0343\n",
      "276/295, train_loss: 0.0370, step time: 1.0420\n",
      "277/295, train_loss: 0.2113, step time: 1.0534\n",
      "278/295, train_loss: 0.0873, step time: 1.0552\n",
      "279/295, train_loss: 0.0532, step time: 1.0380\n",
      "280/295, train_loss: 0.1823, step time: 1.0324\n",
      "281/295, train_loss: 0.3738, step time: 1.0724\n",
      "282/295, train_loss: 0.3865, step time: 1.0394\n",
      "283/295, train_loss: 0.0562, step time: 1.0372\n",
      "284/295, train_loss: 0.1009, step time: 1.0420\n",
      "285/295, train_loss: 0.1358, step time: 1.0491\n",
      "286/295, train_loss: 0.0911, step time: 1.0411\n",
      "287/295, train_loss: 0.1390, step time: 1.1037\n",
      "288/295, train_loss: 0.0409, step time: 1.0377\n",
      "289/295, train_loss: 0.0417, step time: 1.0288\n",
      "290/295, train_loss: 0.0737, step time: 1.0294\n",
      "291/295, train_loss: 0.1163, step time: 1.0291\n",
      "292/295, train_loss: 0.2429, step time: 1.0284\n",
      "293/295, train_loss: 0.0340, step time: 1.0301\n",
      "294/295, train_loss: 0.0687, step time: 1.0296\n",
      "295/295, train_loss: 0.4158, step time: 1.0286\n",
      "epoch 33 average loss: 0.1204\n",
      "current epoch: 33 current mean dice: 0.8019 tc: 0.7501 wt: 0.8700 et: 0.7987\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 33 is: 386.3581\n",
      "----------\n",
      "epoch 34/100\n",
      "1/295, train_loss: 0.0454, step time: 1.0910\n",
      "2/295, train_loss: 0.1451, step time: 1.1717\n",
      "3/295, train_loss: 0.0380, step time: 1.0975\n",
      "4/295, train_loss: 0.4171, step time: 1.0819\n",
      "5/295, train_loss: 0.0483, step time: 1.0716\n",
      "6/295, train_loss: 0.0660, step time: 1.1130\n",
      "7/295, train_loss: 0.0851, step time: 1.0390\n",
      "8/295, train_loss: 0.0608, step time: 1.0724\n",
      "9/295, train_loss: 0.2812, step time: 1.0378\n",
      "10/295, train_loss: 0.1022, step time: 1.0456\n",
      "11/295, train_loss: 0.1543, step time: 1.0565\n",
      "12/295, train_loss: 0.0692, step time: 1.0379\n",
      "13/295, train_loss: 0.1327, step time: 1.0601\n",
      "14/295, train_loss: 0.0698, step time: 1.0461\n",
      "15/295, train_loss: 0.1576, step time: 1.0376\n",
      "16/295, train_loss: 0.0659, step time: 1.0411\n",
      "17/295, train_loss: 0.0693, step time: 1.1111\n",
      "18/295, train_loss: 0.3987, step time: 1.0347\n",
      "19/295, train_loss: 0.1154, step time: 1.0431\n",
      "20/295, train_loss: 0.0752, step time: 1.0422\n",
      "21/295, train_loss: 0.1364, step time: 1.0412\n",
      "22/295, train_loss: 0.0484, step time: 1.0544\n",
      "23/295, train_loss: 0.0872, step time: 1.0403\n",
      "24/295, train_loss: 0.1322, step time: 1.0545\n",
      "25/295, train_loss: 0.0333, step time: 1.0545\n",
      "26/295, train_loss: 0.4248, step time: 1.0388\n",
      "27/295, train_loss: 0.1207, step time: 1.0389\n",
      "28/295, train_loss: 0.1190, step time: 1.0457\n",
      "29/295, train_loss: 0.1067, step time: 1.0434\n",
      "30/295, train_loss: 0.1306, step time: 1.0425\n",
      "31/295, train_loss: 0.1129, step time: 1.0771\n",
      "32/295, train_loss: 0.1215, step time: 1.0411\n",
      "33/295, train_loss: 0.0484, step time: 1.0763\n",
      "34/295, train_loss: 0.0422, step time: 1.0352\n",
      "35/295, train_loss: 0.0976, step time: 1.0530\n",
      "36/295, train_loss: 0.0685, step time: 1.0376\n",
      "37/295, train_loss: 0.0593, step time: 1.0541\n",
      "38/295, train_loss: 0.0883, step time: 1.0393\n",
      "39/295, train_loss: 0.3974, step time: 1.0430\n",
      "40/295, train_loss: 0.0501, step time: 1.0372\n",
      "41/295, train_loss: 0.0393, step time: 1.0382\n",
      "42/295, train_loss: 0.0373, step time: 1.0438\n",
      "43/295, train_loss: 0.0968, step time: 1.0574\n",
      "44/295, train_loss: 0.0587, step time: 1.0518\n",
      "45/295, train_loss: 0.0700, step time: 1.0394\n",
      "46/295, train_loss: 0.0908, step time: 1.0543\n",
      "47/295, train_loss: 0.0978, step time: 1.0425\n",
      "48/295, train_loss: 0.0618, step time: 1.0409\n",
      "49/295, train_loss: 0.1102, step time: 1.0731\n",
      "50/295, train_loss: 0.0552, step time: 1.0548\n",
      "51/295, train_loss: 0.1194, step time: 1.0636\n",
      "52/295, train_loss: 0.1862, step time: 1.0785\n",
      "53/295, train_loss: 0.0713, step time: 1.0416\n",
      "54/295, train_loss: 0.4102, step time: 1.0399\n",
      "55/295, train_loss: 0.0449, step time: 1.0329\n",
      "56/295, train_loss: 0.0475, step time: 1.1287\n",
      "57/295, train_loss: 0.0392, step time: 1.0319\n",
      "58/295, train_loss: 0.1207, step time: 1.0341\n",
      "59/295, train_loss: 0.1220, step time: 1.0430\n",
      "60/295, train_loss: 0.0751, step time: 1.0641\n",
      "61/295, train_loss: 0.1130, step time: 1.0457\n",
      "62/295, train_loss: 0.0359, step time: 1.0445\n",
      "63/295, train_loss: 0.0645, step time: 1.0495\n",
      "64/295, train_loss: 0.0684, step time: 1.0845\n",
      "65/295, train_loss: 0.0923, step time: 1.0527\n",
      "66/295, train_loss: 0.0386, step time: 1.0326\n",
      "67/295, train_loss: 0.1641, step time: 1.0520\n",
      "68/295, train_loss: 0.0519, step time: 1.0462\n",
      "69/295, train_loss: 0.1860, step time: 1.0326\n",
      "70/295, train_loss: 0.0866, step time: 1.0747\n",
      "71/295, train_loss: 0.0406, step time: 1.0408\n",
      "72/295, train_loss: 0.0875, step time: 1.0576\n",
      "73/295, train_loss: 0.2781, step time: 1.0602\n",
      "74/295, train_loss: 0.0697, step time: 1.0628\n",
      "75/295, train_loss: 0.0973, step time: 1.0526\n",
      "76/295, train_loss: 0.0859, step time: 1.0400\n",
      "77/295, train_loss: 0.1255, step time: 1.0378\n",
      "78/295, train_loss: 0.0476, step time: 1.1252\n",
      "79/295, train_loss: 0.0738, step time: 1.0335\n",
      "80/295, train_loss: 0.0677, step time: 1.0568\n",
      "81/295, train_loss: 0.0689, step time: 1.0775\n",
      "82/295, train_loss: 0.4503, step time: 1.1241\n",
      "83/295, train_loss: 0.0836, step time: 1.0642\n",
      "84/295, train_loss: 0.0474, step time: 1.0858\n",
      "85/295, train_loss: 0.0591, step time: 1.0384\n",
      "86/295, train_loss: 0.1066, step time: 1.0403\n",
      "87/295, train_loss: 0.0536, step time: 1.0415\n",
      "88/295, train_loss: 0.0686, step time: 1.0556\n",
      "89/295, train_loss: 0.0634, step time: 1.0953\n",
      "90/295, train_loss: 0.1650, step time: 1.1161\n",
      "91/295, train_loss: 0.0660, step time: 1.0403\n",
      "92/295, train_loss: 0.0839, step time: 1.0625\n",
      "93/295, train_loss: 0.1091, step time: 1.0353\n",
      "94/295, train_loss: 0.0602, step time: 1.0516\n",
      "95/295, train_loss: 0.1391, step time: 1.0840\n",
      "96/295, train_loss: 0.1638, step time: 1.0341\n",
      "97/295, train_loss: 0.0810, step time: 1.0533\n",
      "98/295, train_loss: 0.1167, step time: 1.0362\n",
      "99/295, train_loss: 0.1143, step time: 1.0323\n",
      "100/295, train_loss: 0.0932, step time: 1.0570\n",
      "101/295, train_loss: 0.0847, step time: 1.0404\n",
      "102/295, train_loss: 0.0560, step time: 1.0392\n",
      "103/295, train_loss: 0.1145, step time: 1.0446\n",
      "104/295, train_loss: 0.0395, step time: 1.0597\n",
      "105/295, train_loss: 0.0601, step time: 1.0525\n",
      "106/295, train_loss: 0.0469, step time: 1.0449\n",
      "107/295, train_loss: 0.0655, step time: 1.0413\n",
      "108/295, train_loss: 0.1056, step time: 1.1179\n",
      "109/295, train_loss: 0.4003, step time: 1.0713\n",
      "110/295, train_loss: 0.1059, step time: 1.0480\n",
      "111/295, train_loss: 0.0962, step time: 1.0356\n",
      "112/295, train_loss: 0.0696, step time: 1.0452\n",
      "113/295, train_loss: 0.0889, step time: 1.0634\n",
      "114/295, train_loss: 0.1276, step time: 1.0417\n",
      "115/295, train_loss: 0.1754, step time: 1.0355\n",
      "116/295, train_loss: 0.0943, step time: 1.0423\n",
      "117/295, train_loss: 0.0930, step time: 1.0387\n",
      "118/295, train_loss: 0.0670, step time: 1.0597\n",
      "119/295, train_loss: 0.0665, step time: 1.0399\n",
      "120/295, train_loss: 0.0470, step time: 1.0579\n",
      "121/295, train_loss: 0.3797, step time: 1.0391\n",
      "122/295, train_loss: 0.0529, step time: 1.0345\n",
      "123/295, train_loss: 0.0241, step time: 1.0514\n",
      "124/295, train_loss: 0.0615, step time: 1.0381\n",
      "125/295, train_loss: 0.0580, step time: 1.0340\n",
      "126/295, train_loss: 0.1675, step time: 1.0490\n",
      "127/295, train_loss: 0.1365, step time: 1.0474\n",
      "128/295, train_loss: 0.1030, step time: 1.0458\n",
      "129/295, train_loss: 0.1164, step time: 1.0546\n",
      "130/295, train_loss: 0.0397, step time: 1.0634\n",
      "131/295, train_loss: 0.0426, step time: 1.0351\n",
      "132/295, train_loss: 0.1088, step time: 1.0789\n",
      "133/295, train_loss: 0.1024, step time: 1.0773\n",
      "134/295, train_loss: 0.0911, step time: 1.0387\n",
      "135/295, train_loss: 0.0465, step time: 1.0531\n",
      "136/295, train_loss: 0.0449, step time: 1.0527\n",
      "137/295, train_loss: 0.1114, step time: 1.0485\n",
      "138/295, train_loss: 0.1070, step time: 1.0320\n",
      "139/295, train_loss: 0.1351, step time: 1.0466\n",
      "140/295, train_loss: 0.1031, step time: 1.0413\n",
      "141/295, train_loss: 0.1453, step time: 1.0487\n",
      "142/295, train_loss: 0.0518, step time: 1.0659\n",
      "143/295, train_loss: 0.3937, step time: 1.0656\n",
      "144/295, train_loss: 0.0654, step time: 1.0397\n",
      "145/295, train_loss: 0.1115, step time: 1.0610\n",
      "146/295, train_loss: 0.1203, step time: 1.0442\n",
      "147/295, train_loss: 0.0936, step time: 1.0575\n",
      "148/295, train_loss: 0.0419, step time: 1.0449\n",
      "149/295, train_loss: 0.0707, step time: 1.0372\n",
      "150/295, train_loss: 0.0775, step time: 1.0627\n",
      "151/295, train_loss: 0.0340, step time: 1.0582\n",
      "152/295, train_loss: 0.1962, step time: 1.0848\n",
      "153/295, train_loss: 0.1138, step time: 1.0543\n",
      "154/295, train_loss: 0.0508, step time: 1.0414\n",
      "155/295, train_loss: 0.3658, step time: 1.0442\n",
      "156/295, train_loss: 0.0867, step time: 1.0426\n",
      "157/295, train_loss: 0.0760, step time: 1.0600\n",
      "158/295, train_loss: 0.0550, step time: 1.0334\n",
      "159/295, train_loss: 0.0763, step time: 1.1045\n",
      "160/295, train_loss: 0.1278, step time: 1.0423\n",
      "161/295, train_loss: 0.4048, step time: 1.0389\n",
      "162/295, train_loss: 0.0523, step time: 1.0544\n",
      "163/295, train_loss: 0.0811, step time: 1.0382\n",
      "164/295, train_loss: 0.1317, step time: 1.0608\n",
      "165/295, train_loss: 0.1097, step time: 1.0583\n",
      "166/295, train_loss: 0.0608, step time: 1.0466\n",
      "167/295, train_loss: 0.0938, step time: 1.0423\n",
      "168/295, train_loss: 0.0356, step time: 1.0574\n",
      "169/295, train_loss: 0.1444, step time: 1.0664\n",
      "170/295, train_loss: 0.3904, step time: 1.0396\n",
      "171/295, train_loss: 0.2934, step time: 1.0523\n",
      "172/295, train_loss: 0.0944, step time: 1.0490\n",
      "173/295, train_loss: 0.3183, step time: 1.0665\n",
      "174/295, train_loss: 0.0658, step time: 1.0819\n",
      "175/295, train_loss: 0.0704, step time: 1.0332\n",
      "176/295, train_loss: 0.0429, step time: 1.0491\n",
      "177/295, train_loss: 0.0580, step time: 1.0557\n",
      "178/295, train_loss: 0.1714, step time: 1.0331\n",
      "179/295, train_loss: 0.0889, step time: 1.0393\n",
      "180/295, train_loss: 0.1497, step time: 1.0332\n",
      "181/295, train_loss: 0.0328, step time: 1.0432\n",
      "182/295, train_loss: 0.0779, step time: 1.0320\n",
      "183/295, train_loss: 0.1200, step time: 1.0340\n",
      "184/295, train_loss: 0.4944, step time: 1.0336\n",
      "185/295, train_loss: 0.0991, step time: 1.0638\n",
      "186/295, train_loss: 0.0798, step time: 1.0340\n",
      "187/295, train_loss: 0.0371, step time: 1.0473\n",
      "188/295, train_loss: 0.0537, step time: 1.1028\n",
      "189/295, train_loss: 0.1392, step time: 1.0467\n",
      "190/295, train_loss: 0.0680, step time: 1.0375\n",
      "191/295, train_loss: 0.0560, step time: 1.0559\n",
      "192/295, train_loss: 0.4166, step time: 1.0872\n",
      "193/295, train_loss: 0.3655, step time: 1.0483\n",
      "194/295, train_loss: 0.2419, step time: 1.0489\n",
      "195/295, train_loss: 0.1489, step time: 1.0400\n",
      "196/295, train_loss: 0.0854, step time: 1.0394\n",
      "197/295, train_loss: 0.1018, step time: 1.0371\n",
      "198/295, train_loss: 0.1084, step time: 1.0463\n",
      "199/295, train_loss: 0.1111, step time: 1.0497\n",
      "200/295, train_loss: 0.0661, step time: 1.0447\n",
      "201/295, train_loss: 0.1885, step time: 1.0454\n",
      "202/295, train_loss: 0.2122, step time: 1.0402\n",
      "203/295, train_loss: 0.0375, step time: 1.0786\n",
      "204/295, train_loss: 0.0887, step time: 1.0401\n",
      "205/295, train_loss: 0.3848, step time: 1.0438\n",
      "206/295, train_loss: 0.0348, step time: 1.0614\n",
      "207/295, train_loss: 0.1042, step time: 1.0338\n",
      "208/295, train_loss: 0.0432, step time: 1.0607\n",
      "209/295, train_loss: 0.0632, step time: 1.0371\n",
      "210/295, train_loss: 0.0544, step time: 1.0735\n",
      "211/295, train_loss: 0.0907, step time: 1.0618\n",
      "212/295, train_loss: 0.1381, step time: 1.0603\n",
      "213/295, train_loss: 0.1550, step time: 1.0695\n",
      "214/295, train_loss: 0.1172, step time: 1.0425\n",
      "215/295, train_loss: 0.3356, step time: 1.0372\n",
      "216/295, train_loss: 0.1153, step time: 1.0387\n",
      "217/295, train_loss: 0.0606, step time: 1.0876\n",
      "218/295, train_loss: 0.0771, step time: 1.1234\n",
      "219/295, train_loss: 0.0623, step time: 1.0380\n",
      "220/295, train_loss: 0.4155, step time: 1.0449\n",
      "221/295, train_loss: 0.0527, step time: 1.0488\n",
      "222/295, train_loss: 0.0695, step time: 1.0470\n",
      "223/295, train_loss: 0.0583, step time: 1.0391\n",
      "224/295, train_loss: 0.0827, step time: 1.0703\n",
      "225/295, train_loss: 0.2542, step time: 1.0379\n",
      "226/295, train_loss: 0.1391, step time: 1.0430\n",
      "227/295, train_loss: 0.1011, step time: 1.0438\n",
      "228/295, train_loss: 0.0667, step time: 1.0662\n",
      "229/295, train_loss: 0.4103, step time: 1.0418\n",
      "230/295, train_loss: 0.1925, step time: 1.0423\n",
      "231/295, train_loss: 0.0936, step time: 1.0685\n",
      "232/295, train_loss: 0.0615, step time: 1.0673\n",
      "233/295, train_loss: 0.1252, step time: 1.0432\n",
      "234/295, train_loss: 0.0730, step time: 1.0570\n",
      "235/295, train_loss: 0.0973, step time: 1.0417\n",
      "236/295, train_loss: 0.0828, step time: 1.0440\n",
      "237/295, train_loss: 0.1432, step time: 1.0375\n",
      "238/295, train_loss: 0.0890, step time: 1.0480\n",
      "239/295, train_loss: 0.2383, step time: 1.0661\n",
      "240/295, train_loss: 0.0777, step time: 1.0598\n",
      "241/295, train_loss: 0.0394, step time: 1.0711\n",
      "242/295, train_loss: 0.1351, step time: 1.0450\n",
      "243/295, train_loss: 0.2410, step time: 1.0947\n",
      "244/295, train_loss: 0.3808, step time: 1.0489\n",
      "245/295, train_loss: 0.0999, step time: 1.0721\n",
      "246/295, train_loss: 0.0515, step time: 1.0458\n",
      "247/295, train_loss: 0.0606, step time: 1.0595\n",
      "248/295, train_loss: 0.0902, step time: 1.0800\n",
      "249/295, train_loss: 0.0878, step time: 1.0448\n",
      "250/295, train_loss: 0.0759, step time: 1.0373\n",
      "251/295, train_loss: 0.0520, step time: 1.0334\n",
      "252/295, train_loss: 0.0844, step time: 1.0370\n",
      "253/295, train_loss: 0.1058, step time: 1.1204\n",
      "254/295, train_loss: 0.0636, step time: 1.0374\n",
      "255/295, train_loss: 0.0549, step time: 1.0596\n",
      "256/295, train_loss: 0.0395, step time: 1.0483\n",
      "257/295, train_loss: 0.0617, step time: 1.0339\n",
      "258/295, train_loss: 0.1094, step time: 1.0372\n",
      "259/295, train_loss: 0.1951, step time: 1.0403\n",
      "260/295, train_loss: 0.0632, step time: 1.0589\n",
      "261/295, train_loss: 0.3835, step time: 1.0438\n",
      "262/295, train_loss: 0.0837, step time: 1.0996\n",
      "263/295, train_loss: 0.0358, step time: 1.0419\n",
      "264/295, train_loss: 0.0446, step time: 1.0688\n",
      "265/295, train_loss: 0.0568, step time: 1.0449\n",
      "266/295, train_loss: 0.4019, step time: 1.0393\n",
      "267/295, train_loss: 0.0838, step time: 1.0401\n",
      "268/295, train_loss: 0.0703, step time: 1.0346\n",
      "269/295, train_loss: 0.0620, step time: 1.0382\n",
      "270/295, train_loss: 0.0349, step time: 1.0376\n",
      "271/295, train_loss: 0.0797, step time: 1.0569\n",
      "272/295, train_loss: 0.1059, step time: 1.0427\n",
      "273/295, train_loss: 0.1480, step time: 1.0725\n",
      "274/295, train_loss: 0.4128, step time: 1.0440\n",
      "275/295, train_loss: 0.1344, step time: 1.0600\n",
      "276/295, train_loss: 0.2359, step time: 1.1051\n",
      "277/295, train_loss: 0.0603, step time: 1.0337\n",
      "278/295, train_loss: 0.1989, step time: 1.0448\n",
      "279/295, train_loss: 0.1552, step time: 1.0381\n",
      "280/295, train_loss: 0.0497, step time: 1.0435\n",
      "281/295, train_loss: 0.0703, step time: 1.0624\n",
      "282/295, train_loss: 0.3731, step time: 1.1081\n",
      "283/295, train_loss: 0.0344, step time: 1.0440\n",
      "284/295, train_loss: 0.0669, step time: 1.0502\n",
      "285/295, train_loss: 0.1112, step time: 1.0586\n",
      "286/295, train_loss: 0.1584, step time: 1.0581\n",
      "287/295, train_loss: 0.0916, step time: 1.0398\n",
      "288/295, train_loss: 0.0954, step time: 1.0420\n",
      "289/295, train_loss: 0.0521, step time: 1.0319\n",
      "290/295, train_loss: 0.1342, step time: 1.0298\n",
      "291/295, train_loss: 0.1699, step time: 1.0287\n",
      "292/295, train_loss: 0.3483, step time: 1.0292\n",
      "293/295, train_loss: 0.4676, step time: 1.0308\n",
      "294/295, train_loss: 0.4493, step time: 1.0301\n",
      "295/295, train_loss: 0.0766, step time: 1.0293\n",
      "epoch 34 average loss: 0.1224\n",
      "current epoch: 34 current mean dice: 0.7744 tc: 0.7256 wt: 0.8391 et: 0.7614\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 34 is: 386.3015\n",
      "----------\n",
      "epoch 35/100\n",
      "1/295, train_loss: 0.0283, step time: 1.0921\n",
      "2/295, train_loss: 0.0740, step time: 1.1038\n",
      "3/295, train_loss: 0.0653, step time: 1.0666\n",
      "4/295, train_loss: 0.1037, step time: 1.0981\n",
      "5/295, train_loss: 0.0988, step time: 1.0685\n",
      "6/295, train_loss: 0.0618, step time: 1.0411\n",
      "7/295, train_loss: 0.0825, step time: 1.0978\n",
      "8/295, train_loss: 0.1304, step time: 1.0801\n",
      "9/295, train_loss: 0.1125, step time: 1.0641\n",
      "10/295, train_loss: 0.0785, step time: 1.0580\n",
      "11/295, train_loss: 0.4630, step time: 1.0389\n",
      "12/295, train_loss: 0.1387, step time: 1.0416\n",
      "13/295, train_loss: 0.4073, step time: 1.0368\n",
      "14/295, train_loss: 0.0686, step time: 1.0651\n",
      "15/295, train_loss: 0.0546, step time: 1.0409\n",
      "16/295, train_loss: 0.0357, step time: 1.0504\n",
      "17/295, train_loss: 0.4265, step time: 1.0371\n",
      "18/295, train_loss: 0.3569, step time: 1.0752\n",
      "19/295, train_loss: 0.1095, step time: 1.0392\n",
      "20/295, train_loss: 0.0986, step time: 1.0949\n",
      "21/295, train_loss: 0.0656, step time: 1.0356\n",
      "22/295, train_loss: 0.0959, step time: 1.0395\n",
      "23/295, train_loss: 0.0415, step time: 1.0493\n",
      "24/295, train_loss: 0.4329, step time: 1.0672\n",
      "25/295, train_loss: 0.0644, step time: 1.1091\n",
      "26/295, train_loss: 0.0831, step time: 1.0671\n",
      "27/295, train_loss: 0.0514, step time: 1.0612\n",
      "28/295, train_loss: 0.1037, step time: 1.0452\n",
      "29/295, train_loss: 0.1030, step time: 1.0360\n",
      "30/295, train_loss: 0.1921, step time: 1.0363\n",
      "31/295, train_loss: 0.3849, step time: 1.0320\n",
      "32/295, train_loss: 0.0467, step time: 1.0411\n",
      "33/295, train_loss: 0.0636, step time: 1.0962\n",
      "34/295, train_loss: 0.1497, step time: 1.0731\n",
      "35/295, train_loss: 0.0562, step time: 1.0326\n",
      "36/295, train_loss: 0.0612, step time: 1.0546\n",
      "37/295, train_loss: 0.0581, step time: 1.0368\n",
      "38/295, train_loss: 0.0851, step time: 1.0388\n",
      "39/295, train_loss: 0.0674, step time: 1.0717\n",
      "40/295, train_loss: 0.0779, step time: 1.0331\n",
      "41/295, train_loss: 0.0784, step time: 1.0370\n",
      "42/295, train_loss: 0.1019, step time: 1.0650\n",
      "43/295, train_loss: 0.0575, step time: 1.0633\n",
      "44/295, train_loss: 0.0459, step time: 1.0548\n",
      "45/295, train_loss: 0.1077, step time: 1.0408\n",
      "46/295, train_loss: 0.0528, step time: 1.0639\n",
      "47/295, train_loss: 0.0716, step time: 1.0373\n",
      "48/295, train_loss: 0.0824, step time: 1.0560\n",
      "49/295, train_loss: 0.0975, step time: 1.0386\n",
      "50/295, train_loss: 0.0527, step time: 1.0542\n",
      "51/295, train_loss: 0.0815, step time: 1.0420\n",
      "52/295, train_loss: 0.0625, step time: 1.0552\n",
      "53/295, train_loss: 0.0505, step time: 1.0605\n",
      "54/295, train_loss: 0.1082, step time: 1.0572\n",
      "55/295, train_loss: 0.0455, step time: 1.0449\n",
      "56/295, train_loss: 0.1705, step time: 1.0464\n",
      "57/295, train_loss: 0.0601, step time: 1.0961\n",
      "58/295, train_loss: 0.1922, step time: 1.0753\n",
      "59/295, train_loss: 0.1104, step time: 1.0458\n",
      "60/295, train_loss: 0.1574, step time: 1.0339\n",
      "61/295, train_loss: 0.5252, step time: 1.0332\n",
      "62/295, train_loss: 0.4216, step time: 1.0355\n",
      "63/295, train_loss: 0.0504, step time: 1.0354\n",
      "64/295, train_loss: 0.0738, step time: 1.0431\n",
      "65/295, train_loss: 0.3856, step time: 1.0582\n",
      "66/295, train_loss: 0.0540, step time: 1.0647\n",
      "67/295, train_loss: 0.0801, step time: 1.0729\n",
      "68/295, train_loss: 0.0465, step time: 1.0431\n",
      "69/295, train_loss: 0.0894, step time: 1.0470\n",
      "70/295, train_loss: 0.0427, step time: 1.0437\n",
      "71/295, train_loss: 0.0378, step time: 1.0344\n",
      "72/295, train_loss: 0.0783, step time: 1.0568\n",
      "73/295, train_loss: 0.0615, step time: 1.0534\n",
      "74/295, train_loss: 0.1342, step time: 1.0695\n",
      "75/295, train_loss: 0.0439, step time: 1.0375\n",
      "76/295, train_loss: 0.1271, step time: 1.0671\n",
      "77/295, train_loss: 0.2667, step time: 1.0421\n",
      "78/295, train_loss: 0.0556, step time: 1.0508\n",
      "79/295, train_loss: 0.0521, step time: 1.0361\n",
      "80/295, train_loss: 0.1921, step time: 1.0388\n",
      "81/295, train_loss: 0.1386, step time: 1.0798\n",
      "82/295, train_loss: 0.0596, step time: 1.0481\n",
      "83/295, train_loss: 0.1025, step time: 1.0364\n",
      "84/295, train_loss: 0.3875, step time: 1.0384\n",
      "85/295, train_loss: 0.0652, step time: 1.0503\n",
      "86/295, train_loss: 0.3976, step time: 1.0408\n",
      "87/295, train_loss: 0.1086, step time: 1.0558\n",
      "88/295, train_loss: 0.1619, step time: 1.0625\n",
      "89/295, train_loss: 0.1592, step time: 1.0479\n",
      "90/295, train_loss: 0.2500, step time: 1.0378\n",
      "91/295, train_loss: 0.1489, step time: 1.0433\n",
      "92/295, train_loss: 0.1041, step time: 1.0479\n",
      "93/295, train_loss: 0.0641, step time: 1.0451\n",
      "94/295, train_loss: 0.0739, step time: 1.1039\n",
      "95/295, train_loss: 0.1209, step time: 1.0376\n",
      "96/295, train_loss: 0.0993, step time: 1.0461\n",
      "97/295, train_loss: 0.0473, step time: 1.0341\n",
      "98/295, train_loss: 0.4130, step time: 1.0012\n",
      "99/295, train_loss: 0.0710, step time: 1.0819\n",
      "100/295, train_loss: 0.1001, step time: 1.0498\n",
      "101/295, train_loss: 0.3922, step time: 1.0459\n",
      "102/295, train_loss: 0.1353, step time: 1.0405\n",
      "103/295, train_loss: 0.1413, step time: 1.0452\n",
      "104/295, train_loss: 0.1131, step time: 1.1509\n",
      "105/295, train_loss: 0.1364, step time: 1.0622\n",
      "106/295, train_loss: 0.0654, step time: 1.0389\n",
      "107/295, train_loss: 0.1092, step time: 1.0411\n",
      "108/295, train_loss: 0.0402, step time: 1.0456\n",
      "109/295, train_loss: 0.0571, step time: 1.0595\n",
      "110/295, train_loss: 0.0673, step time: 1.0570\n",
      "111/295, train_loss: 0.0798, step time: 1.0346\n",
      "112/295, train_loss: 0.0715, step time: 1.0528\n",
      "113/295, train_loss: 0.2043, step time: 1.0654\n",
      "114/295, train_loss: 0.0828, step time: 1.0585\n",
      "115/295, train_loss: 0.0559, step time: 1.0538\n",
      "116/295, train_loss: 0.0911, step time: 1.0508\n",
      "117/295, train_loss: 0.0471, step time: 1.0635\n",
      "118/295, train_loss: 0.2895, step time: 1.0776\n",
      "119/295, train_loss: 0.0973, step time: 1.0343\n",
      "120/295, train_loss: 0.0681, step time: 1.0499\n",
      "121/295, train_loss: 0.0625, step time: 1.0710\n",
      "122/295, train_loss: 0.1526, step time: 1.0339\n",
      "123/295, train_loss: 0.1006, step time: 1.0364\n",
      "124/295, train_loss: 0.1016, step time: 1.0390\n",
      "125/295, train_loss: 0.1896, step time: 1.0366\n",
      "126/295, train_loss: 0.1065, step time: 1.0401\n",
      "127/295, train_loss: 0.1759, step time: 1.0397\n",
      "128/295, train_loss: 0.0504, step time: 1.0623\n",
      "129/295, train_loss: 0.1097, step time: 1.1167\n",
      "130/295, train_loss: 0.0649, step time: 1.0734\n",
      "131/295, train_loss: 0.0861, step time: 1.0403\n",
      "132/295, train_loss: 0.0513, step time: 1.0359\n",
      "133/295, train_loss: 0.3670, step time: 1.0467\n",
      "134/295, train_loss: 0.0561, step time: 1.0452\n",
      "135/295, train_loss: 0.4373, step time: 1.0630\n",
      "136/295, train_loss: 0.4864, step time: 1.0579\n",
      "137/295, train_loss: 0.0608, step time: 1.0414\n",
      "138/295, train_loss: 0.4136, step time: 1.0366\n",
      "139/295, train_loss: 0.1241, step time: 1.0638\n",
      "140/295, train_loss: 0.1332, step time: 1.0476\n",
      "141/295, train_loss: 0.0388, step time: 1.0514\n",
      "142/295, train_loss: 0.0716, step time: 1.0608\n",
      "143/295, train_loss: 0.0556, step time: 1.0521\n",
      "144/295, train_loss: 0.1097, step time: 1.0324\n",
      "145/295, train_loss: 0.0500, step time: 1.0603\n",
      "146/295, train_loss: 0.0753, step time: 1.0583\n",
      "147/295, train_loss: 0.0942, step time: 1.1044\n",
      "148/295, train_loss: 0.1298, step time: 1.0633\n",
      "149/295, train_loss: 0.0398, step time: 1.0333\n",
      "150/295, train_loss: 0.1269, step time: 1.0384\n",
      "151/295, train_loss: 0.0569, step time: 1.0675\n",
      "152/295, train_loss: 0.0983, step time: 1.0348\n",
      "153/295, train_loss: 0.1509, step time: 1.0461\n",
      "154/295, train_loss: 0.2110, step time: 1.0558\n",
      "155/295, train_loss: 0.0609, step time: 1.0662\n",
      "156/295, train_loss: 0.0812, step time: 1.0424\n",
      "157/295, train_loss: 0.1164, step time: 1.0534\n",
      "158/295, train_loss: 0.0652, step time: 1.0539\n",
      "159/295, train_loss: 0.1958, step time: 1.0547\n",
      "160/295, train_loss: 0.0882, step time: 1.0787\n",
      "161/295, train_loss: 0.1476, step time: 1.0320\n",
      "162/295, train_loss: 0.2794, step time: 1.0392\n",
      "163/295, train_loss: 0.0417, step time: 1.0944\n",
      "164/295, train_loss: 0.1045, step time: 1.0556\n",
      "165/295, train_loss: 0.0340, step time: 1.1155\n",
      "166/295, train_loss: 0.1059, step time: 1.0626\n",
      "167/295, train_loss: 0.0508, step time: 1.0333\n",
      "168/295, train_loss: 0.1141, step time: 1.0317\n",
      "169/295, train_loss: 0.0936, step time: 1.0373\n",
      "170/295, train_loss: 0.0801, step time: 1.0429\n",
      "171/295, train_loss: 0.0793, step time: 1.0603\n",
      "172/295, train_loss: 0.1068, step time: 1.0608\n",
      "173/295, train_loss: 0.0412, step time: 1.0465\n",
      "174/295, train_loss: 0.0925, step time: 1.0850\n",
      "175/295, train_loss: 0.0744, step time: 1.0452\n",
      "176/295, train_loss: 0.3368, step time: 1.0408\n",
      "177/295, train_loss: 0.1159, step time: 1.0441\n",
      "178/295, train_loss: 0.1259, step time: 1.0455\n",
      "179/295, train_loss: 0.0404, step time: 1.0478\n",
      "180/295, train_loss: 0.0333, step time: 1.0610\n",
      "181/295, train_loss: 0.4139, step time: 1.0381\n",
      "182/295, train_loss: 0.0688, step time: 1.0373\n",
      "183/295, train_loss: 0.0622, step time: 1.0362\n",
      "184/295, train_loss: 0.0889, step time: 1.0323\n",
      "185/295, train_loss: 0.0479, step time: 1.0446\n",
      "186/295, train_loss: 0.1186, step time: 1.0363\n",
      "187/295, train_loss: 0.0832, step time: 1.0378\n",
      "188/295, train_loss: 0.0474, step time: 1.0600\n",
      "189/295, train_loss: 0.1022, step time: 1.0397\n",
      "190/295, train_loss: 0.1412, step time: 1.0345\n",
      "191/295, train_loss: 0.0731, step time: 1.0384\n",
      "192/295, train_loss: 0.0436, step time: 1.0428\n",
      "193/295, train_loss: 0.1171, step time: 1.0431\n",
      "194/295, train_loss: 0.4017, step time: 1.0497\n",
      "195/295, train_loss: 0.0651, step time: 1.0462\n",
      "196/295, train_loss: 0.1184, step time: 1.0375\n",
      "197/295, train_loss: 0.1607, step time: 1.0416\n",
      "198/295, train_loss: 0.1152, step time: 1.0412\n",
      "199/295, train_loss: 0.0636, step time: 1.0391\n",
      "200/295, train_loss: 0.0524, step time: 1.0402\n",
      "201/295, train_loss: 0.1717, step time: 1.0319\n",
      "202/295, train_loss: 0.0288, step time: 1.0373\n",
      "203/295, train_loss: 0.0540, step time: 1.0572\n",
      "204/295, train_loss: 0.0655, step time: 1.0376\n",
      "205/295, train_loss: 0.1327, step time: 1.0401\n",
      "206/295, train_loss: 0.0918, step time: 1.0340\n",
      "207/295, train_loss: 0.1536, step time: 1.0404\n",
      "208/295, train_loss: 0.4044, step time: 1.0319\n",
      "209/295, train_loss: 0.1137, step time: 1.0330\n",
      "210/295, train_loss: 0.0938, step time: 1.0338\n",
      "211/295, train_loss: 0.0638, step time: 1.0318\n",
      "212/295, train_loss: 0.4135, step time: 1.0374\n",
      "213/295, train_loss: 0.0424, step time: 1.0349\n",
      "214/295, train_loss: 0.0468, step time: 1.0614\n",
      "215/295, train_loss: 0.1037, step time: 1.0551\n",
      "216/295, train_loss: 0.2071, step time: 1.1092\n",
      "217/295, train_loss: 0.1024, step time: 1.0666\n",
      "218/295, train_loss: 0.0937, step time: 1.0408\n",
      "219/295, train_loss: 0.1237, step time: 1.0745\n",
      "220/295, train_loss: 0.0991, step time: 1.0382\n",
      "221/295, train_loss: 0.0392, step time: 1.0786\n",
      "222/295, train_loss: 0.0930, step time: 1.0879\n",
      "223/295, train_loss: 0.1059, step time: 1.0806\n",
      "224/295, train_loss: 0.1329, step time: 1.0990\n",
      "225/295, train_loss: 0.0461, step time: 1.0435\n",
      "226/295, train_loss: 0.1092, step time: 1.0473\n",
      "227/295, train_loss: 0.0610, step time: 1.1251\n",
      "228/295, train_loss: 0.0688, step time: 1.0357\n",
      "229/295, train_loss: 0.0763, step time: 1.0357\n",
      "230/295, train_loss: 0.0740, step time: 1.0701\n",
      "231/295, train_loss: 0.1178, step time: 1.0460\n",
      "232/295, train_loss: 0.0388, step time: 1.1251\n",
      "233/295, train_loss: 0.0597, step time: 1.0393\n",
      "234/295, train_loss: 0.0522, step time: 1.0375\n",
      "235/295, train_loss: 0.0481, step time: 1.0331\n",
      "236/295, train_loss: 0.0419, step time: 1.0433\n",
      "237/295, train_loss: 0.0397, step time: 1.0405\n",
      "238/295, train_loss: 0.1049, step time: 1.0388\n",
      "239/295, train_loss: 0.0363, step time: 1.0364\n",
      "240/295, train_loss: 0.0741, step time: 1.0491\n",
      "241/295, train_loss: 0.1144, step time: 1.0349\n",
      "242/295, train_loss: 0.3675, step time: 1.0351\n",
      "243/295, train_loss: 0.0495, step time: 1.0919\n",
      "244/295, train_loss: 0.0441, step time: 1.0312\n",
      "245/295, train_loss: 0.4231, step time: 1.0410\n",
      "246/295, train_loss: 0.1360, step time: 1.0554\n",
      "247/295, train_loss: 0.4311, step time: 1.0424\n",
      "248/295, train_loss: 0.0842, step time: 1.0381\n",
      "249/295, train_loss: 0.0608, step time: 1.0575\n",
      "250/295, train_loss: 0.1903, step time: 1.0458\n",
      "251/295, train_loss: 0.1059, step time: 1.0387\n",
      "252/295, train_loss: 0.1466, step time: 1.0444\n",
      "253/295, train_loss: 0.0794, step time: 1.0644\n",
      "254/295, train_loss: 0.0840, step time: 1.0567\n",
      "255/295, train_loss: 0.0377, step time: 1.0389\n",
      "256/295, train_loss: 0.0683, step time: 1.0366\n",
      "257/295, train_loss: 0.0632, step time: 1.0471\n",
      "258/295, train_loss: 0.0508, step time: 1.0365\n",
      "259/295, train_loss: 0.0840, step time: 1.0615\n",
      "260/295, train_loss: 0.1407, step time: 1.0966\n",
      "261/295, train_loss: 0.1415, step time: 1.0439\n",
      "262/295, train_loss: 0.0935, step time: 1.0401\n",
      "263/295, train_loss: 0.4061, step time: 1.0470\n",
      "264/295, train_loss: 0.0441, step time: 1.0718\n",
      "265/295, train_loss: 0.3962, step time: 1.0743\n",
      "266/295, train_loss: 0.1686, step time: 1.0371\n",
      "267/295, train_loss: 0.0861, step time: 1.0449\n",
      "268/295, train_loss: 0.1332, step time: 1.0839\n",
      "269/295, train_loss: 0.0618, step time: 1.0392\n",
      "270/295, train_loss: 0.0556, step time: 1.0437\n",
      "271/295, train_loss: 0.0400, step time: 1.0456\n",
      "272/295, train_loss: 0.0649, step time: 1.1232\n",
      "273/295, train_loss: 0.1590, step time: 1.0564\n",
      "274/295, train_loss: 0.0950, step time: 1.0592\n",
      "275/295, train_loss: 0.0370, step time: 1.0377\n",
      "276/295, train_loss: 0.2179, step time: 1.0343\n",
      "277/295, train_loss: 0.0536, step time: 1.0362\n",
      "278/295, train_loss: 0.0746, step time: 1.0483\n",
      "279/295, train_loss: 0.1678, step time: 1.0797\n",
      "280/295, train_loss: 0.0704, step time: 1.0515\n",
      "281/295, train_loss: 0.1851, step time: 1.0649\n",
      "282/295, train_loss: 0.0652, step time: 1.0443\n",
      "283/295, train_loss: 0.0681, step time: 1.0401\n",
      "284/295, train_loss: 0.0956, step time: 1.0396\n",
      "285/295, train_loss: 0.0934, step time: 1.0448\n",
      "286/295, train_loss: 0.0863, step time: 1.0776\n",
      "287/295, train_loss: 0.1249, step time: 1.0335\n",
      "288/295, train_loss: 0.0834, step time: 1.0425\n",
      "289/295, train_loss: 0.1416, step time: 1.0303\n",
      "290/295, train_loss: 0.0674, step time: 1.0303\n",
      "291/295, train_loss: 0.1210, step time: 1.0312\n",
      "292/295, train_loss: 0.0394, step time: 1.0288\n",
      "293/295, train_loss: 0.0393, step time: 1.0296\n",
      "294/295, train_loss: 0.0427, step time: 1.0303\n",
      "295/295, train_loss: 0.0982, step time: 1.0307\n",
      "epoch 35 average loss: 0.1212\n",
      "current epoch: 35 current mean dice: 0.7875 tc: 0.7482 wt: 0.8472 et: 0.7737\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 35 is: 382.2806\n",
      "----------\n",
      "epoch 36/100\n",
      "1/295, train_loss: 0.1059, step time: 1.1616\n",
      "2/295, train_loss: 0.0532, step time: 1.0952\n",
      "3/295, train_loss: 0.0856, step time: 1.1342\n",
      "4/295, train_loss: 0.0888, step time: 1.0672\n",
      "5/295, train_loss: 0.0985, step time: 1.0363\n",
      "6/295, train_loss: 0.0602, step time: 1.0336\n",
      "7/295, train_loss: 0.4021, step time: 1.0400\n",
      "8/295, train_loss: 0.0463, step time: 1.1571\n",
      "9/295, train_loss: 0.3446, step time: 1.0395\n",
      "10/295, train_loss: 0.0767, step time: 1.0634\n",
      "11/295, train_loss: 0.0391, step time: 1.0324\n",
      "12/295, train_loss: 0.0449, step time: 1.0322\n",
      "13/295, train_loss: 0.0641, step time: 1.0599\n",
      "14/295, train_loss: 0.1000, step time: 1.0483\n",
      "15/295, train_loss: 0.0404, step time: 1.0700\n",
      "16/295, train_loss: 0.0447, step time: 1.0831\n",
      "17/295, train_loss: 0.1283, step time: 1.0377\n",
      "18/295, train_loss: 0.3758, step time: 1.0503\n",
      "19/295, train_loss: 0.0608, step time: 1.0583\n",
      "20/295, train_loss: 0.1197, step time: 1.1161\n",
      "21/295, train_loss: 0.1257, step time: 1.0390\n",
      "22/295, train_loss: 0.1260, step time: 1.0393\n",
      "23/295, train_loss: 0.0521, step time: 1.0558\n",
      "24/295, train_loss: 0.0631, step time: 1.0300\n",
      "25/295, train_loss: 0.0631, step time: 1.0909\n",
      "26/295, train_loss: 0.3538, step time: 1.0395\n",
      "27/295, train_loss: 0.0381, step time: 1.0899\n",
      "28/295, train_loss: 0.1264, step time: 1.0359\n",
      "29/295, train_loss: 0.3831, step time: 1.0539\n",
      "30/295, train_loss: 0.4251, step time: 1.0419\n",
      "31/295, train_loss: 0.0982, step time: 1.0769\n",
      "32/295, train_loss: 0.0405, step time: 1.0387\n",
      "33/295, train_loss: 0.1374, step time: 1.0396\n",
      "34/295, train_loss: 0.0614, step time: 1.0383\n",
      "35/295, train_loss: 0.4177, step time: 1.0476\n",
      "36/295, train_loss: 0.0725, step time: 1.0566\n",
      "37/295, train_loss: 0.0860, step time: 1.0793\n",
      "38/295, train_loss: 0.1860, step time: 1.0545\n",
      "39/295, train_loss: 0.0681, step time: 1.0684\n",
      "40/295, train_loss: 0.1905, step time: 1.0471\n",
      "41/295, train_loss: 0.0423, step time: 1.0373\n",
      "42/295, train_loss: 0.0479, step time: 1.0610\n",
      "43/295, train_loss: 0.1039, step time: 1.0463\n",
      "44/295, train_loss: 0.0970, step time: 1.0530\n",
      "45/295, train_loss: 0.1585, step time: 1.0871\n",
      "46/295, train_loss: 0.0356, step time: 1.0385\n",
      "47/295, train_loss: 0.0467, step time: 1.1107\n",
      "48/295, train_loss: 0.1321, step time: 1.0540\n",
      "49/295, train_loss: 0.0793, step time: 1.0510\n",
      "50/295, train_loss: 0.1455, step time: 1.0345\n",
      "51/295, train_loss: 0.0333, step time: 1.0597\n",
      "52/295, train_loss: 0.0768, step time: 1.0351\n",
      "53/295, train_loss: 0.1267, step time: 1.0448\n",
      "54/295, train_loss: 0.0577, step time: 1.0365\n",
      "55/295, train_loss: 0.1244, step time: 1.0400\n",
      "56/295, train_loss: 0.1849, step time: 1.0579\n",
      "57/295, train_loss: 0.0548, step time: 1.0375\n",
      "58/295, train_loss: 0.0235, step time: 1.0341\n",
      "59/295, train_loss: 0.1722, step time: 1.0751\n",
      "60/295, train_loss: 0.0987, step time: 1.0570\n",
      "61/295, train_loss: 0.1450, step time: 1.0463\n",
      "62/295, train_loss: 0.0362, step time: 1.0584\n",
      "63/295, train_loss: 0.0647, step time: 1.0510\n",
      "64/295, train_loss: 0.0808, step time: 1.0639\n",
      "65/295, train_loss: 0.1034, step time: 1.0346\n",
      "66/295, train_loss: 0.1061, step time: 1.0328\n",
      "67/295, train_loss: 0.1009, step time: 1.0360\n",
      "68/295, train_loss: 0.0693, step time: 1.0348\n",
      "69/295, train_loss: 0.0971, step time: 1.0569\n",
      "70/295, train_loss: 0.0824, step time: 1.0445\n",
      "71/295, train_loss: 0.3669, step time: 1.0386\n",
      "72/295, train_loss: 0.0968, step time: 1.0578\n",
      "73/295, train_loss: 0.4189, step time: 1.0589\n",
      "74/295, train_loss: 0.0332, step time: 1.0413\n",
      "75/295, train_loss: 0.0825, step time: 1.0378\n",
      "76/295, train_loss: 0.0498, step time: 1.0401\n",
      "77/295, train_loss: 0.3961, step time: 1.0526\n",
      "78/295, train_loss: 0.0316, step time: 1.0337\n",
      "79/295, train_loss: 0.2353, step time: 1.0436\n",
      "80/295, train_loss: 0.0668, step time: 1.0530\n",
      "81/295, train_loss: 0.0520, step time: 1.1235\n",
      "82/295, train_loss: 0.3815, step time: 1.0428\n",
      "83/295, train_loss: 0.0782, step time: 1.0467\n",
      "84/295, train_loss: 0.1810, step time: 1.0527\n",
      "85/295, train_loss: 0.0690, step time: 1.0475\n",
      "86/295, train_loss: 0.0692, step time: 1.0538\n",
      "87/295, train_loss: 0.0773, step time: 1.0589\n",
      "88/295, train_loss: 0.0864, step time: 1.0709\n",
      "89/295, train_loss: 0.1367, step time: 1.0696\n",
      "90/295, train_loss: 0.5113, step time: 1.0521\n",
      "91/295, train_loss: 0.1563, step time: 1.0440\n",
      "92/295, train_loss: 0.0497, step time: 1.0418\n",
      "93/295, train_loss: 0.0672, step time: 1.0370\n",
      "94/295, train_loss: 0.0748, step time: 1.0338\n",
      "95/295, train_loss: 0.0959, step time: 1.0556\n",
      "96/295, train_loss: 0.0422, step time: 1.0331\n",
      "97/295, train_loss: 0.0946, step time: 1.0679\n",
      "98/295, train_loss: 0.2707, step time: 1.1162\n",
      "99/295, train_loss: 0.0411, step time: 1.0477\n",
      "100/295, train_loss: 0.1111, step time: 1.0371\n",
      "101/295, train_loss: 0.0828, step time: 1.0582\n",
      "102/295, train_loss: 0.0367, step time: 1.0385\n",
      "103/295, train_loss: 0.0731, step time: 1.0390\n",
      "104/295, train_loss: 0.4532, step time: 1.0536\n",
      "105/295, train_loss: 0.0997, step time: 1.0675\n",
      "106/295, train_loss: 0.0880, step time: 1.0461\n",
      "107/295, train_loss: 0.3895, step time: 1.0643\n",
      "108/295, train_loss: 0.0959, step time: 1.0750\n",
      "109/295, train_loss: 0.1008, step time: 1.0922\n",
      "110/295, train_loss: 0.0933, step time: 1.0412\n",
      "111/295, train_loss: 0.1376, step time: 1.0595\n",
      "112/295, train_loss: 0.1067, step time: 1.0481\n",
      "113/295, train_loss: 0.0694, step time: 1.0436\n",
      "114/295, train_loss: 0.0547, step time: 1.0359\n",
      "115/295, train_loss: 0.1381, step time: 1.1091\n",
      "116/295, train_loss: 0.0653, step time: 1.0453\n",
      "117/295, train_loss: 0.0460, step time: 1.0638\n",
      "118/295, train_loss: 0.0537, step time: 1.0397\n",
      "119/295, train_loss: 0.0728, step time: 1.0464\n",
      "120/295, train_loss: 0.1024, step time: 1.0617\n",
      "121/295, train_loss: 0.3840, step time: 1.0323\n",
      "122/295, train_loss: 0.0956, step time: 1.0461\n",
      "123/295, train_loss: 0.0979, step time: 1.0633\n",
      "124/295, train_loss: 0.1387, step time: 1.0625\n",
      "125/295, train_loss: 0.0659, step time: 1.0764\n",
      "126/295, train_loss: 0.0502, step time: 1.0427\n",
      "127/295, train_loss: 0.0376, step time: 1.1136\n",
      "128/295, train_loss: 0.0691, step time: 1.0447\n",
      "129/295, train_loss: 0.0865, step time: 1.0483\n",
      "130/295, train_loss: 0.0515, step time: 1.0350\n",
      "131/295, train_loss: 0.0399, step time: 1.0448\n",
      "132/295, train_loss: 0.0966, step time: 1.0350\n",
      "133/295, train_loss: 0.0665, step time: 1.0336\n",
      "134/295, train_loss: 0.0343, step time: 1.0379\n",
      "135/295, train_loss: 0.0501, step time: 1.0736\n",
      "136/295, train_loss: 0.1153, step time: 1.0459\n",
      "137/295, train_loss: 0.1558, step time: 1.0442\n",
      "138/295, train_loss: 0.1020, step time: 1.0482\n",
      "139/295, train_loss: 0.2234, step time: 1.0638\n",
      "140/295, train_loss: 0.0670, step time: 1.0349\n",
      "141/295, train_loss: 0.0539, step time: 1.0419\n",
      "142/295, train_loss: 0.0713, step time: 1.0535\n",
      "143/295, train_loss: 0.0959, step time: 1.0388\n",
      "144/295, train_loss: 0.0954, step time: 1.0740\n",
      "145/295, train_loss: 0.1304, step time: 1.0405\n",
      "146/295, train_loss: 0.1216, step time: 1.0307\n",
      "147/295, train_loss: 0.1075, step time: 1.0348\n",
      "148/295, train_loss: 0.1757, step time: 1.0436\n",
      "149/295, train_loss: 0.0664, step time: 1.0371\n",
      "150/295, train_loss: 0.1259, step time: 1.0393\n",
      "151/295, train_loss: 0.0754, step time: 1.0456\n",
      "152/295, train_loss: 0.1814, step time: 1.0459\n",
      "153/295, train_loss: 0.0437, step time: 1.0394\n",
      "154/295, train_loss: 0.0404, step time: 1.0460\n",
      "155/295, train_loss: 0.0661, step time: 1.0888\n",
      "156/295, train_loss: 0.0561, step time: 1.0564\n",
      "157/295, train_loss: 0.0492, step time: 1.0464\n",
      "158/295, train_loss: 0.1331, step time: 1.1391\n",
      "159/295, train_loss: 0.0600, step time: 1.0515\n",
      "160/295, train_loss: 0.0516, step time: 1.1186\n",
      "161/295, train_loss: 0.0529, step time: 1.0610\n",
      "162/295, train_loss: 0.0754, step time: 1.0495\n",
      "163/295, train_loss: 0.0990, step time: 1.0480\n",
      "164/295, train_loss: 0.0769, step time: 1.0787\n",
      "165/295, train_loss: 0.0762, step time: 1.0367\n",
      "166/295, train_loss: 0.0588, step time: 1.0507\n",
      "167/295, train_loss: 0.1128, step time: 1.0407\n",
      "168/295, train_loss: 0.1189, step time: 1.0564\n",
      "169/295, train_loss: 0.0503, step time: 1.0744\n",
      "170/295, train_loss: 0.4068, step time: 1.0749\n",
      "171/295, train_loss: 0.1954, step time: 1.0364\n",
      "172/295, train_loss: 0.0698, step time: 1.0533\n",
      "173/295, train_loss: 0.0777, step time: 1.0546\n",
      "174/295, train_loss: 0.1906, step time: 1.0355\n",
      "175/295, train_loss: 0.0644, step time: 1.0912\n",
      "176/295, train_loss: 0.1079, step time: 1.0315\n",
      "177/295, train_loss: 0.0582, step time: 1.0390\n",
      "178/295, train_loss: 0.1139, step time: 1.0709\n",
      "179/295, train_loss: 0.1986, step time: 1.0673\n",
      "180/295, train_loss: 0.4505, step time: 1.0527\n",
      "181/295, train_loss: 0.0836, step time: 1.0410\n",
      "182/295, train_loss: 0.1088, step time: 1.0635\n",
      "183/295, train_loss: 0.1155, step time: 1.0559\n",
      "184/295, train_loss: 0.1363, step time: 1.0460\n",
      "185/295, train_loss: 0.0945, step time: 1.0561\n",
      "186/295, train_loss: 0.0870, step time: 1.0326\n",
      "187/295, train_loss: 0.1043, step time: 1.0365\n",
      "188/295, train_loss: 0.0432, step time: 1.0337\n",
      "189/295, train_loss: 0.0386, step time: 1.0371\n",
      "190/295, train_loss: 0.0796, step time: 1.0469\n",
      "191/295, train_loss: 0.0836, step time: 1.0959\n",
      "192/295, train_loss: 0.0661, step time: 1.0758\n",
      "193/295, train_loss: 0.1191, step time: 1.0636\n",
      "194/295, train_loss: 0.0615, step time: 1.0417\n",
      "195/295, train_loss: 0.0825, step time: 1.0695\n",
      "196/295, train_loss: 0.1243, step time: 1.0417\n",
      "197/295, train_loss: 0.1236, step time: 1.0396\n",
      "198/295, train_loss: 0.0513, step time: 1.1048\n",
      "199/295, train_loss: 0.1160, step time: 1.0711\n",
      "200/295, train_loss: 0.4194, step time: 1.0397\n",
      "201/295, train_loss: 0.0566, step time: 1.0978\n",
      "202/295, train_loss: 0.2066, step time: 1.0404\n",
      "203/295, train_loss: 0.0489, step time: 1.0992\n",
      "204/295, train_loss: 0.0368, step time: 1.0398\n",
      "205/295, train_loss: 0.0903, step time: 1.0776\n",
      "206/295, train_loss: 0.1454, step time: 1.0571\n",
      "207/295, train_loss: 0.1255, step time: 1.0375\n",
      "208/295, train_loss: 0.0628, step time: 1.0859\n",
      "209/295, train_loss: 0.0825, step time: 1.0486\n",
      "210/295, train_loss: 0.0572, step time: 1.0819\n",
      "211/295, train_loss: 0.0651, step time: 1.0992\n",
      "212/295, train_loss: 0.0538, step time: 1.0429\n",
      "213/295, train_loss: 0.0636, step time: 1.0368\n",
      "214/295, train_loss: 0.0730, step time: 1.0423\n",
      "215/295, train_loss: 0.2687, step time: 1.0333\n",
      "216/295, train_loss: 0.0457, step time: 1.0842\n",
      "217/295, train_loss: 0.1495, step time: 1.0529\n",
      "218/295, train_loss: 0.0505, step time: 1.0644\n",
      "219/295, train_loss: 0.0336, step time: 1.0659\n",
      "220/295, train_loss: 0.0786, step time: 1.0374\n",
      "221/295, train_loss: 0.0838, step time: 1.0653\n",
      "222/295, train_loss: 0.1068, step time: 1.1100\n",
      "223/295, train_loss: 0.0811, step time: 1.0557\n",
      "224/295, train_loss: 0.0516, step time: 1.0387\n",
      "225/295, train_loss: 0.3840, step time: 1.0978\n",
      "226/295, train_loss: 0.1591, step time: 1.0570\n",
      "227/295, train_loss: 0.0416, step time: 1.0389\n",
      "228/295, train_loss: 0.0798, step time: 1.0383\n",
      "229/295, train_loss: 0.0854, step time: 1.0716\n",
      "230/295, train_loss: 0.0978, step time: 1.0389\n",
      "231/295, train_loss: 0.0802, step time: 1.0343\n",
      "232/295, train_loss: 0.0408, step time: 1.0417\n",
      "233/295, train_loss: 0.1470, step time: 1.0372\n",
      "234/295, train_loss: 0.0568, step time: 1.0340\n",
      "235/295, train_loss: 0.4078, step time: 1.0393\n",
      "236/295, train_loss: 0.1016, step time: 1.0431\n",
      "237/295, train_loss: 0.0703, step time: 1.0479\n",
      "238/295, train_loss: 0.0645, step time: 1.0487\n",
      "239/295, train_loss: 0.2165, step time: 1.0360\n",
      "240/295, train_loss: 0.0902, step time: 1.0586\n",
      "241/295, train_loss: 0.3445, step time: 1.0610\n",
      "242/295, train_loss: 0.0954, step time: 1.0338\n",
      "243/295, train_loss: 0.1691, step time: 1.0393\n",
      "244/295, train_loss: 0.0606, step time: 1.0392\n",
      "245/295, train_loss: 0.0489, step time: 1.0427\n",
      "246/295, train_loss: 0.1290, step time: 1.0337\n",
      "247/295, train_loss: 0.4248, step time: 1.0349\n",
      "248/295, train_loss: 0.0661, step time: 1.0707\n",
      "249/295, train_loss: 0.3939, step time: 1.0644\n",
      "250/295, train_loss: 0.0580, step time: 1.0588\n",
      "251/295, train_loss: 0.0708, step time: 1.0749\n",
      "252/295, train_loss: 0.0614, step time: 1.0334\n",
      "253/295, train_loss: 0.4219, step time: 1.0524\n",
      "254/295, train_loss: 0.0923, step time: 1.0421\n",
      "255/295, train_loss: 0.0369, step time: 1.0436\n",
      "256/295, train_loss: 0.1383, step time: 1.0774\n",
      "257/295, train_loss: 0.0979, step time: 1.0690\n",
      "258/295, train_loss: 0.1346, step time: 1.0539\n",
      "259/295, train_loss: 0.0541, step time: 1.0593\n",
      "260/295, train_loss: 0.0652, step time: 1.0364\n",
      "261/295, train_loss: 0.2066, step time: 1.0432\n",
      "262/295, train_loss: 0.0372, step time: 1.0476\n",
      "263/295, train_loss: 0.0444, step time: 1.0418\n",
      "264/295, train_loss: 0.0574, step time: 1.0709\n",
      "265/295, train_loss: 0.0447, step time: 1.0423\n",
      "266/295, train_loss: 0.0551, step time: 1.0611\n",
      "267/295, train_loss: 0.1411, step time: 1.0444\n",
      "268/295, train_loss: 0.0367, step time: 1.0507\n",
      "269/295, train_loss: 0.0839, step time: 1.0872\n",
      "270/295, train_loss: 0.0732, step time: 1.0351\n",
      "271/295, train_loss: 0.0617, step time: 1.0405\n",
      "272/295, train_loss: 0.0605, step time: 1.1278\n",
      "273/295, train_loss: 0.0456, step time: 1.0418\n",
      "274/295, train_loss: 0.0626, step time: 1.0368\n",
      "275/295, train_loss: 0.0557, step time: 1.0373\n",
      "276/295, train_loss: 0.0817, step time: 1.0335\n",
      "277/295, train_loss: 0.1318, step time: 1.0360\n",
      "278/295, train_loss: 0.1133, step time: 1.0442\n",
      "279/295, train_loss: 0.0448, step time: 1.0708\n",
      "280/295, train_loss: 0.0494, step time: 1.0438\n",
      "281/295, train_loss: 0.0481, step time: 1.0469\n",
      "282/295, train_loss: 0.0538, step time: 1.0665\n",
      "283/295, train_loss: 0.2433, step time: 1.0681\n",
      "284/295, train_loss: 0.0941, step time: 1.1201\n",
      "285/295, train_loss: 0.0464, step time: 1.0360\n",
      "286/295, train_loss: 0.0844, step time: 1.0378\n",
      "287/295, train_loss: 0.0810, step time: 1.0521\n",
      "288/295, train_loss: 0.4194, step time: 1.0618\n",
      "289/295, train_loss: 0.1144, step time: 1.0321\n",
      "290/295, train_loss: 0.0604, step time: 1.0299\n",
      "291/295, train_loss: 0.1691, step time: 1.0298\n",
      "292/295, train_loss: 0.0703, step time: 1.0297\n",
      "293/295, train_loss: 0.0884, step time: 1.0296\n",
      "294/295, train_loss: 0.1272, step time: 1.0286\n",
      "295/295, train_loss: 0.0570, step time: 1.0299\n",
      "epoch 36 average loss: 0.1155\n",
      "current epoch: 36 current mean dice: 0.7715 tc: 0.7211 wt: 0.8486 et: 0.7488\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 36 is: 390.8955\n",
      "----------\n",
      "epoch 37/100\n",
      "1/295, train_loss: 0.0639, step time: 1.0607\n",
      "2/295, train_loss: 0.0526, step time: 1.0979\n",
      "3/295, train_loss: 0.1424, step time: 1.0885\n",
      "4/295, train_loss: 0.1403, step time: 1.0424\n",
      "5/295, train_loss: 0.1203, step time: 1.0725\n",
      "6/295, train_loss: 0.0601, step time: 1.1178\n",
      "7/295, train_loss: 0.0592, step time: 1.0456\n",
      "8/295, train_loss: 0.0456, step time: 1.0380\n",
      "9/295, train_loss: 0.0676, step time: 1.0378\n",
      "10/295, train_loss: 0.0609, step time: 1.0332\n",
      "11/295, train_loss: 0.0523, step time: 1.0393\n",
      "12/295, train_loss: 0.1508, step time: 1.0362\n",
      "13/295, train_loss: 0.0643, step time: 1.0542\n",
      "14/295, train_loss: 0.0319, step time: 1.0599\n",
      "15/295, train_loss: 0.1105, step time: 1.0457\n",
      "16/295, train_loss: 0.0965, step time: 1.0320\n",
      "17/295, train_loss: 0.0444, step time: 1.0619\n",
      "18/295, train_loss: 0.1342, step time: 1.0339\n",
      "19/295, train_loss: 0.0503, step time: 1.0423\n",
      "20/295, train_loss: 0.0958, step time: 1.1005\n",
      "21/295, train_loss: 0.3800, step time: 1.0397\n",
      "22/295, train_loss: 0.0714, step time: 1.0371\n",
      "23/295, train_loss: 0.0358, step time: 1.0569\n",
      "24/295, train_loss: 0.4911, step time: 1.0358\n",
      "25/295, train_loss: 0.0523, step time: 1.0392\n",
      "26/295, train_loss: 0.4001, step time: 1.0380\n",
      "27/295, train_loss: 0.0904, step time: 1.0837\n",
      "28/295, train_loss: 0.0921, step time: 1.0378\n",
      "29/295, train_loss: 0.0353, step time: 1.0394\n",
      "30/295, train_loss: 0.0611, step time: 1.0426\n",
      "31/295, train_loss: 0.0860, step time: 1.0360\n",
      "32/295, train_loss: 0.0234, step time: 1.0705\n",
      "33/295, train_loss: 0.0514, step time: 1.0417\n",
      "34/295, train_loss: 0.0748, step time: 1.0618\n",
      "35/295, train_loss: 0.0559, step time: 1.0310\n",
      "36/295, train_loss: 0.1028, step time: 1.0302\n",
      "37/295, train_loss: 0.1057, step time: 1.0323\n",
      "38/295, train_loss: 0.0752, step time: 1.0329\n",
      "39/295, train_loss: 0.0673, step time: 1.0483\n",
      "40/295, train_loss: 0.0791, step time: 1.0542\n",
      "41/295, train_loss: 0.0425, step time: 1.0361\n",
      "42/295, train_loss: 0.3834, step time: 1.0516\n",
      "43/295, train_loss: 0.1474, step time: 1.0668\n",
      "44/295, train_loss: 0.1006, step time: 1.0383\n",
      "45/295, train_loss: 0.1004, step time: 1.0366\n",
      "46/295, train_loss: 0.4017, step time: 1.0386\n",
      "47/295, train_loss: 0.1080, step time: 1.0744\n",
      "48/295, train_loss: 0.3984, step time: 1.1348\n",
      "49/295, train_loss: 0.0745, step time: 1.0363\n",
      "50/295, train_loss: 0.0733, step time: 1.0382\n",
      "51/295, train_loss: 0.0381, step time: 1.0436\n",
      "52/295, train_loss: 0.3971, step time: 1.0486\n",
      "53/295, train_loss: 0.1259, step time: 1.0530\n",
      "54/295, train_loss: 0.0811, step time: 1.0376\n",
      "55/295, train_loss: 0.1210, step time: 1.0578\n",
      "56/295, train_loss: 0.1017, step time: 1.0355\n",
      "57/295, train_loss: 0.0933, step time: 1.0578\n",
      "58/295, train_loss: 0.4211, step time: 1.0505\n",
      "59/295, train_loss: 0.0331, step time: 1.0327\n",
      "60/295, train_loss: 0.0983, step time: 1.0346\n",
      "61/295, train_loss: 0.0923, step time: 1.0431\n",
      "62/295, train_loss: 0.0549, step time: 1.1121\n",
      "63/295, train_loss: 0.3767, step time: 1.0347\n",
      "64/295, train_loss: 0.0657, step time: 1.0322\n",
      "65/295, train_loss: 0.1121, step time: 1.0704\n",
      "66/295, train_loss: 0.0989, step time: 1.0623\n",
      "67/295, train_loss: 0.0607, step time: 1.0372\n",
      "68/295, train_loss: 0.0478, step time: 1.0313\n",
      "69/295, train_loss: 0.0434, step time: 1.0392\n",
      "70/295, train_loss: 0.1030, step time: 1.0369\n",
      "71/295, train_loss: 0.1487, step time: 1.0832\n",
      "72/295, train_loss: 0.0709, step time: 1.0777\n",
      "73/295, train_loss: 0.0657, step time: 1.0368\n",
      "74/295, train_loss: 0.0506, step time: 1.0403\n",
      "75/295, train_loss: 0.0852, step time: 1.0920\n",
      "76/295, train_loss: 0.0485, step time: 1.0620\n",
      "77/295, train_loss: 0.0497, step time: 1.0395\n",
      "78/295, train_loss: 0.1881, step time: 1.1005\n",
      "79/295, train_loss: 0.0393, step time: 1.0953\n",
      "80/295, train_loss: 0.0932, step time: 1.0420\n",
      "81/295, train_loss: 0.0588, step time: 1.0471\n",
      "82/295, train_loss: 0.1561, step time: 1.0435\n",
      "83/295, train_loss: 0.0893, step time: 1.0533\n",
      "84/295, train_loss: 0.1060, step time: 1.0412\n",
      "85/295, train_loss: 0.0516, step time: 1.0636\n",
      "86/295, train_loss: 0.0328, step time: 1.0580\n",
      "87/295, train_loss: 0.0964, step time: 1.0717\n",
      "88/295, train_loss: 0.0558, step time: 1.0570\n",
      "89/295, train_loss: 0.1090, step time: 1.0546\n",
      "90/295, train_loss: 0.0492, step time: 1.0335\n",
      "91/295, train_loss: 0.4007, step time: 1.0500\n",
      "92/295, train_loss: 0.1645, step time: 1.0558\n",
      "93/295, train_loss: 0.0894, step time: 1.0398\n",
      "94/295, train_loss: 0.0443, step time: 1.0420\n",
      "95/295, train_loss: 0.0656, step time: 1.0599\n",
      "96/295, train_loss: 0.2094, step time: 1.0401\n",
      "97/295, train_loss: 0.1270, step time: 1.0392\n",
      "98/295, train_loss: 0.0367, step time: 1.0859\n",
      "99/295, train_loss: 0.0856, step time: 1.0438\n",
      "100/295, train_loss: 0.0836, step time: 1.0442\n",
      "101/295, train_loss: 0.1189, step time: 1.0546\n",
      "102/295, train_loss: 0.1577, step time: 1.0346\n",
      "103/295, train_loss: 0.1023, step time: 1.0403\n",
      "104/295, train_loss: 0.0679, step time: 1.0451\n",
      "105/295, train_loss: 0.0501, step time: 1.0448\n",
      "106/295, train_loss: 0.0587, step time: 1.0635\n",
      "107/295, train_loss: 0.0506, step time: 1.0558\n",
      "108/295, train_loss: 0.0890, step time: 1.0386\n",
      "109/295, train_loss: 0.0616, step time: 1.0445\n",
      "110/295, train_loss: 0.2003, step time: 1.0390\n",
      "111/295, train_loss: 0.0989, step time: 1.0472\n",
      "112/295, train_loss: 0.0496, step time: 1.0342\n",
      "113/295, train_loss: 0.1532, step time: 1.0498\n",
      "114/295, train_loss: 0.0567, step time: 1.0667\n",
      "115/295, train_loss: 0.0768, step time: 1.0378\n",
      "116/295, train_loss: 0.0368, step time: 1.0448\n",
      "117/295, train_loss: 0.4271, step time: 1.0439\n",
      "118/295, train_loss: 0.0996, step time: 1.0648\n",
      "119/295, train_loss: 0.0356, step time: 1.0988\n",
      "120/295, train_loss: 0.0917, step time: 1.0422\n",
      "121/295, train_loss: 0.0342, step time: 1.0936\n",
      "122/295, train_loss: 0.0490, step time: 1.1111\n",
      "123/295, train_loss: 0.3997, step time: 1.0334\n",
      "124/295, train_loss: 0.0304, step time: 1.0482\n",
      "125/295, train_loss: 0.0992, step time: 1.0382\n",
      "126/295, train_loss: 0.0722, step time: 1.0372\n",
      "127/295, train_loss: 0.0992, step time: 1.0418\n",
      "128/295, train_loss: 0.0598, step time: 1.0638\n",
      "129/295, train_loss: 0.3011, step time: 1.0849\n",
      "130/295, train_loss: 0.1822, step time: 1.0542\n",
      "131/295, train_loss: 0.0746, step time: 1.0455\n",
      "132/295, train_loss: 0.0663, step time: 1.0406\n",
      "133/295, train_loss: 0.1363, step time: 1.0329\n",
      "134/295, train_loss: 0.1025, step time: 1.0397\n",
      "135/295, train_loss: 0.1191, step time: 1.0556\n",
      "136/295, train_loss: 0.3768, step time: 1.0803\n",
      "137/295, train_loss: 0.0896, step time: 1.0656\n",
      "138/295, train_loss: 0.1596, step time: 1.1035\n",
      "139/295, train_loss: 0.0519, step time: 1.0392\n",
      "140/295, train_loss: 0.0729, step time: 1.0429\n",
      "141/295, train_loss: 0.0823, step time: 1.0844\n",
      "142/295, train_loss: 0.0466, step time: 1.0354\n",
      "143/295, train_loss: 0.3929, step time: 1.1839\n",
      "144/295, train_loss: 0.0847, step time: 1.0361\n",
      "145/295, train_loss: 0.1886, step time: 1.0329\n",
      "146/295, train_loss: 0.4228, step time: 1.0626\n",
      "147/295, train_loss: 0.0912, step time: 1.0489\n",
      "148/295, train_loss: 0.1559, step time: 1.0405\n",
      "149/295, train_loss: 0.0850, step time: 1.0426\n",
      "150/295, train_loss: 0.1009, step time: 1.0459\n",
      "151/295, train_loss: 0.0468, step time: 1.0486\n",
      "152/295, train_loss: 0.0697, step time: 1.0650\n",
      "153/295, train_loss: 0.0608, step time: 1.0731\n",
      "154/295, train_loss: 0.3583, step time: 1.0326\n",
      "155/295, train_loss: 0.0433, step time: 1.0427\n",
      "156/295, train_loss: 0.0483, step time: 1.0756\n",
      "157/295, train_loss: 0.0575, step time: 1.0544\n",
      "158/295, train_loss: 0.1153, step time: 1.0343\n",
      "159/295, train_loss: 0.1199, step time: 1.0485\n",
      "160/295, train_loss: 0.4191, step time: 1.0390\n",
      "161/295, train_loss: 0.0542, step time: 1.0356\n",
      "162/295, train_loss: 0.0450, step time: 1.0603\n",
      "163/295, train_loss: 0.0427, step time: 1.0471\n",
      "164/295, train_loss: 0.1302, step time: 1.1311\n",
      "165/295, train_loss: 0.1261, step time: 1.0523\n",
      "166/295, train_loss: 0.1184, step time: 1.0866\n",
      "167/295, train_loss: 0.0549, step time: 1.0459\n",
      "168/295, train_loss: 0.1967, step time: 1.0694\n",
      "169/295, train_loss: 0.0880, step time: 1.0563\n",
      "170/295, train_loss: 0.0744, step time: 1.0665\n",
      "171/295, train_loss: 0.1231, step time: 1.0826\n",
      "172/295, train_loss: 0.0709, step time: 1.0569\n",
      "173/295, train_loss: 0.1170, step time: 1.0346\n",
      "174/295, train_loss: 0.0655, step time: 1.0379\n",
      "175/295, train_loss: 0.0845, step time: 1.0607\n",
      "176/295, train_loss: 0.4240, step time: 1.0689\n",
      "177/295, train_loss: 0.0616, step time: 1.0504\n",
      "178/295, train_loss: 0.0782, step time: 1.0332\n",
      "179/295, train_loss: 0.1221, step time: 1.0417\n",
      "180/295, train_loss: 0.0938, step time: 1.0602\n",
      "181/295, train_loss: 0.0580, step time: 1.0353\n",
      "182/295, train_loss: 0.0837, step time: 1.0341\n",
      "183/295, train_loss: 0.4237, step time: 1.0440\n",
      "184/295, train_loss: 0.0514, step time: 1.0516\n",
      "185/295, train_loss: 0.0678, step time: 1.0329\n",
      "186/295, train_loss: 0.0523, step time: 1.0317\n",
      "187/295, train_loss: 0.1165, step time: 1.0378\n",
      "188/295, train_loss: 0.0820, step time: 1.0328\n",
      "189/295, train_loss: 0.0940, step time: 1.0528\n",
      "190/295, train_loss: 0.0588, step time: 1.0361\n",
      "191/295, train_loss: 0.0360, step time: 1.0790\n",
      "192/295, train_loss: 0.0849, step time: 1.0558\n",
      "193/295, train_loss: 0.0941, step time: 1.0518\n",
      "194/295, train_loss: 0.0786, step time: 1.0409\n",
      "195/295, train_loss: 0.0436, step time: 1.0838\n",
      "196/295, train_loss: 0.1471, step time: 1.0412\n",
      "197/295, train_loss: 0.0691, step time: 1.0629\n",
      "198/295, train_loss: 0.0601, step time: 1.0656\n",
      "199/295, train_loss: 0.0969, step time: 1.0654\n",
      "200/295, train_loss: 0.3880, step time: 1.0375\n",
      "201/295, train_loss: 0.0492, step time: 1.0550\n",
      "202/295, train_loss: 0.2026, step time: 1.0431\n",
      "203/295, train_loss: 0.1378, step time: 1.0371\n",
      "204/295, train_loss: 0.3532, step time: 1.0659\n",
      "205/295, train_loss: 0.0630, step time: 1.0344\n",
      "206/295, train_loss: 0.0581, step time: 1.0627\n",
      "207/295, train_loss: 0.1338, step time: 1.0394\n",
      "208/295, train_loss: 0.1755, step time: 1.0365\n",
      "209/295, train_loss: 0.4115, step time: 1.0522\n",
      "210/295, train_loss: 0.1020, step time: 1.0801\n",
      "211/295, train_loss: 0.0938, step time: 1.0429\n",
      "212/295, train_loss: 0.1316, step time: 1.1558\n",
      "213/295, train_loss: 0.4160, step time: 1.0394\n",
      "214/295, train_loss: 0.0926, step time: 1.0417\n",
      "215/295, train_loss: 0.0340, step time: 1.0882\n",
      "216/295, train_loss: 0.1669, step time: 1.1056\n",
      "217/295, train_loss: 0.0963, step time: 1.0339\n",
      "218/295, train_loss: 0.0959, step time: 1.0382\n",
      "219/295, train_loss: 0.1113, step time: 1.0587\n",
      "220/295, train_loss: 0.0592, step time: 1.1212\n",
      "221/295, train_loss: 0.0589, step time: 1.0362\n",
      "222/295, train_loss: 0.0472, step time: 1.0336\n",
      "223/295, train_loss: 0.0763, step time: 1.0448\n",
      "224/295, train_loss: 0.2219, step time: 1.0704\n",
      "225/295, train_loss: 0.1795, step time: 1.0366\n",
      "226/295, train_loss: 0.0489, step time: 1.1352\n",
      "227/295, train_loss: 0.2016, step time: 1.1379\n",
      "228/295, train_loss: 0.0899, step time: 1.0395\n",
      "229/295, train_loss: 0.0513, step time: 1.0872\n",
      "230/295, train_loss: 0.0643, step time: 1.0891\n",
      "231/295, train_loss: 0.0455, step time: 1.0321\n",
      "232/295, train_loss: 0.0641, step time: 1.0474\n",
      "233/295, train_loss: 0.2480, step time: 1.0958\n",
      "234/295, train_loss: 0.1230, step time: 1.0438\n",
      "235/295, train_loss: 0.0400, step time: 1.0432\n",
      "236/295, train_loss: 0.1559, step time: 1.0324\n",
      "237/295, train_loss: 0.0715, step time: 1.0459\n",
      "238/295, train_loss: 0.1114, step time: 1.0431\n",
      "239/295, train_loss: 0.0717, step time: 1.0451\n",
      "240/295, train_loss: 0.0750, step time: 1.0606\n",
      "241/295, train_loss: 0.1049, step time: 1.0528\n",
      "242/295, train_loss: 0.0830, step time: 1.0370\n",
      "243/295, train_loss: 0.4084, step time: 1.0622\n",
      "244/295, train_loss: 0.0390, step time: 1.0420\n",
      "245/295, train_loss: 0.0617, step time: 1.0435\n",
      "246/295, train_loss: 0.0413, step time: 1.0522\n",
      "247/295, train_loss: 0.1327, step time: 1.0588\n",
      "248/295, train_loss: 0.0356, step time: 1.0666\n",
      "249/295, train_loss: 0.0631, step time: 1.0540\n",
      "250/295, train_loss: 0.1900, step time: 1.0453\n",
      "251/295, train_loss: 0.1089, step time: 1.0563\n",
      "252/295, train_loss: 0.0678, step time: 1.0487\n",
      "253/295, train_loss: 0.0976, step time: 1.0433\n",
      "254/295, train_loss: 0.0875, step time: 1.0493\n",
      "255/295, train_loss: 0.0429, step time: 1.0995\n",
      "256/295, train_loss: 0.1334, step time: 1.0367\n",
      "257/295, train_loss: 0.1474, step time: 1.0442\n",
      "258/295, train_loss: 0.1347, step time: 1.0339\n",
      "259/295, train_loss: 0.0719, step time: 1.0661\n",
      "260/295, train_loss: 0.0415, step time: 1.0358\n",
      "261/295, train_loss: 0.0864, step time: 1.0429\n",
      "262/295, train_loss: 0.0369, step time: 1.0353\n",
      "263/295, train_loss: 0.0533, step time: 1.0409\n",
      "264/295, train_loss: 0.0724, step time: 1.0941\n",
      "265/295, train_loss: 0.0476, step time: 1.0407\n",
      "266/295, train_loss: 0.0975, step time: 1.0312\n",
      "267/295, train_loss: 0.0744, step time: 1.0374\n",
      "268/295, train_loss: 0.1321, step time: 1.0429\n",
      "269/295, train_loss: 0.3983, step time: 1.0905\n",
      "270/295, train_loss: 0.2578, step time: 1.0668\n",
      "271/295, train_loss: 0.0597, step time: 1.0378\n",
      "272/295, train_loss: 0.1055, step time: 1.0424\n",
      "273/295, train_loss: 0.0575, step time: 1.1333\n",
      "274/295, train_loss: 0.0778, step time: 1.0419\n",
      "275/295, train_loss: 0.0493, step time: 1.0330\n",
      "276/295, train_loss: 0.0908, step time: 1.0353\n",
      "277/295, train_loss: 0.2315, step time: 1.0440\n",
      "278/295, train_loss: 0.1444, step time: 1.0657\n",
      "279/295, train_loss: 0.1383, step time: 1.0971\n",
      "280/295, train_loss: 0.0357, step time: 1.0525\n",
      "281/295, train_loss: 0.0463, step time: 1.0400\n",
      "282/295, train_loss: 0.1113, step time: 1.0361\n",
      "283/295, train_loss: 0.3063, step time: 1.0351\n",
      "284/295, train_loss: 0.0960, step time: 1.0398\n",
      "285/295, train_loss: 0.0558, step time: 1.0637\n",
      "286/295, train_loss: 0.0847, step time: 1.0356\n",
      "287/295, train_loss: 0.1386, step time: 1.0473\n",
      "288/295, train_loss: 0.1163, step time: 1.0318\n",
      "289/295, train_loss: 0.1424, step time: 1.0301\n",
      "290/295, train_loss: 0.0862, step time: 1.0293\n",
      "291/295, train_loss: 0.1689, step time: 1.0300\n",
      "292/295, train_loss: 0.0465, step time: 1.0294\n",
      "293/295, train_loss: 0.1182, step time: 1.0287\n",
      "294/295, train_loss: 0.0640, step time: 1.0298\n",
      "295/295, train_loss: 0.0676, step time: 1.0299\n",
      "epoch 37 average loss: 0.1168\n",
      "current epoch: 37 current mean dice: 0.7702 tc: 0.7167 wt: 0.8502 et: 0.7484\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 37 is: 389.9568\n",
      "----------\n",
      "epoch 38/100\n",
      "1/295, train_loss: 0.0595, step time: 1.1030\n",
      "2/295, train_loss: 0.1086, step time: 1.0669\n",
      "3/295, train_loss: 0.4233, step time: 1.1226\n",
      "4/295, train_loss: 0.0655, step time: 1.1676\n",
      "5/295, train_loss: 0.1627, step time: 1.0716\n",
      "6/295, train_loss: 0.1750, step time: 1.0562\n",
      "7/295, train_loss: 0.1171, step time: 1.0319\n",
      "8/295, train_loss: 0.0884, step time: 1.0394\n",
      "9/295, train_loss: 0.0956, step time: 1.1101\n",
      "10/295, train_loss: 0.1186, step time: 1.0369\n",
      "11/295, train_loss: 0.0828, step time: 1.0535\n",
      "12/295, train_loss: 0.0244, step time: 1.0300\n",
      "13/295, train_loss: 0.3972, step time: 1.0344\n",
      "14/295, train_loss: 0.1840, step time: 1.0840\n",
      "15/295, train_loss: 0.0680, step time: 1.1151\n",
      "16/295, train_loss: 0.0498, step time: 1.0341\n",
      "17/295, train_loss: 0.1545, step time: 1.1036\n",
      "18/295, train_loss: 0.4353, step time: 1.0555\n",
      "19/295, train_loss: 0.1847, step time: 1.0333\n",
      "20/295, train_loss: 0.0369, step time: 1.0469\n",
      "21/295, train_loss: 0.0825, step time: 1.0596\n",
      "22/295, train_loss: 0.0706, step time: 1.0616\n",
      "23/295, train_loss: 0.1167, step time: 1.0413\n",
      "24/295, train_loss: 0.5166, step time: 1.0358\n",
      "25/295, train_loss: 0.0510, step time: 1.0325\n",
      "26/295, train_loss: 0.1012, step time: 1.0741\n",
      "27/295, train_loss: 0.1001, step time: 1.0328\n",
      "28/295, train_loss: 0.0601, step time: 1.0386\n",
      "29/295, train_loss: 0.0739, step time: 1.0408\n",
      "30/295, train_loss: 0.0351, step time: 1.1073\n",
      "31/295, train_loss: 0.0634, step time: 1.0360\n",
      "32/295, train_loss: 0.0573, step time: 1.0356\n",
      "33/295, train_loss: 0.1589, step time: 1.0410\n",
      "34/295, train_loss: 0.0500, step time: 1.0336\n",
      "35/295, train_loss: 0.1262, step time: 1.0604\n",
      "36/295, train_loss: 0.0516, step time: 1.0650\n",
      "37/295, train_loss: 0.4158, step time: 1.0555\n",
      "38/295, train_loss: 0.0519, step time: 1.0466\n",
      "39/295, train_loss: 0.1238, step time: 1.0478\n",
      "40/295, train_loss: 0.0813, step time: 1.0326\n",
      "41/295, train_loss: 0.0805, step time: 1.0694\n",
      "42/295, train_loss: 0.0995, step time: 1.0552\n",
      "43/295, train_loss: 0.1763, step time: 1.0635\n",
      "44/295, train_loss: 0.0377, step time: 1.0512\n",
      "45/295, train_loss: 0.0563, step time: 1.0335\n",
      "46/295, train_loss: 0.0616, step time: 1.0745\n",
      "47/295, train_loss: 0.0668, step time: 1.0354\n",
      "48/295, train_loss: 0.0730, step time: 1.0386\n",
      "49/295, train_loss: 0.1061, step time: 1.0466\n",
      "50/295, train_loss: 0.0348, step time: 1.0614\n",
      "51/295, train_loss: 0.1375, step time: 1.0614\n",
      "52/295, train_loss: 0.4179, step time: 1.0537\n",
      "53/295, train_loss: 0.0566, step time: 1.0393\n",
      "54/295, train_loss: 0.0582, step time: 1.0442\n",
      "55/295, train_loss: 0.3842, step time: 1.0596\n",
      "56/295, train_loss: 0.3854, step time: 1.0365\n",
      "57/295, train_loss: 0.0978, step time: 1.0457\n",
      "58/295, train_loss: 0.1541, step time: 1.0430\n",
      "59/295, train_loss: 0.0338, step time: 1.0595\n",
      "60/295, train_loss: 0.0351, step time: 1.0443\n",
      "61/295, train_loss: 0.1831, step time: 1.0462\n",
      "62/295, train_loss: 0.0870, step time: 1.0395\n",
      "63/295, train_loss: 0.0413, step time: 1.0737\n",
      "64/295, train_loss: 0.0648, step time: 1.0703\n",
      "65/295, train_loss: 0.1464, step time: 1.0417\n",
      "66/295, train_loss: 0.0582, step time: 1.0422\n",
      "67/295, train_loss: 0.0448, step time: 1.0629\n",
      "68/295, train_loss: 0.0411, step time: 1.0409\n",
      "69/295, train_loss: 0.0327, step time: 1.0821\n",
      "70/295, train_loss: 0.0378, step time: 1.0494\n",
      "71/295, train_loss: 0.0620, step time: 1.0427\n",
      "72/295, train_loss: 0.0522, step time: 1.0367\n",
      "73/295, train_loss: 0.1033, step time: 1.1341\n",
      "74/295, train_loss: 0.0465, step time: 1.0415\n",
      "75/295, train_loss: 0.0343, step time: 1.0410\n",
      "76/295, train_loss: 0.0984, step time: 1.0352\n",
      "77/295, train_loss: 0.0508, step time: 1.0682\n",
      "78/295, train_loss: 0.1527, step time: 1.0421\n",
      "79/295, train_loss: 0.0828, step time: 1.0633\n",
      "80/295, train_loss: 0.0322, step time: 1.0437\n",
      "81/295, train_loss: 0.0938, step time: 1.0419\n",
      "82/295, train_loss: 0.1114, step time: 1.1154\n",
      "83/295, train_loss: 0.0459, step time: 1.0338\n",
      "84/295, train_loss: 0.0965, step time: 1.0430\n",
      "85/295, train_loss: 0.0527, step time: 1.0646\n",
      "86/295, train_loss: 0.0674, step time: 1.0506\n",
      "87/295, train_loss: 0.0396, step time: 1.0428\n",
      "88/295, train_loss: 0.0419, step time: 1.0604\n",
      "89/295, train_loss: 0.0587, step time: 1.0336\n",
      "90/295, train_loss: 0.0742, step time: 1.0533\n",
      "91/295, train_loss: 0.0623, step time: 1.0558\n",
      "92/295, train_loss: 0.0655, step time: 1.0341\n",
      "93/295, train_loss: 0.1689, step time: 1.0374\n",
      "94/295, train_loss: 0.1454, step time: 1.0506\n",
      "95/295, train_loss: 0.0683, step time: 1.0374\n",
      "96/295, train_loss: 0.0457, step time: 1.0686\n",
      "97/295, train_loss: 0.1763, step time: 1.1846\n",
      "98/295, train_loss: 0.3848, step time: 1.0356\n",
      "99/295, train_loss: 0.0426, step time: 1.0589\n",
      "100/295, train_loss: 0.1315, step time: 1.0570\n",
      "101/295, train_loss: 0.1056, step time: 1.0379\n",
      "102/295, train_loss: 0.0568, step time: 1.0355\n",
      "103/295, train_loss: 0.0504, step time: 1.0374\n",
      "104/295, train_loss: 0.0908, step time: 1.0403\n",
      "105/295, train_loss: 0.0563, step time: 1.0663\n",
      "106/295, train_loss: 0.0390, step time: 1.0360\n",
      "107/295, train_loss: 0.0717, step time: 1.0476\n",
      "108/295, train_loss: 0.0833, step time: 1.0521\n",
      "109/295, train_loss: 0.0364, step time: 1.0466\n",
      "110/295, train_loss: 0.3294, step time: 1.0398\n",
      "111/295, train_loss: 0.0568, step time: 1.0737\n",
      "112/295, train_loss: 0.0913, step time: 1.0407\n",
      "113/295, train_loss: 0.1001, step time: 1.0579\n",
      "114/295, train_loss: 0.1026, step time: 1.0332\n",
      "115/295, train_loss: 0.1197, step time: 1.0355\n",
      "116/295, train_loss: 0.0928, step time: 1.0426\n",
      "117/295, train_loss: 0.3778, step time: 1.0571\n",
      "118/295, train_loss: 0.0537, step time: 1.0661\n",
      "119/295, train_loss: 0.1503, step time: 1.0311\n",
      "120/295, train_loss: 0.0849, step time: 1.0359\n",
      "121/295, train_loss: 0.1256, step time: 1.0392\n",
      "122/295, train_loss: 0.0575, step time: 1.0380\n",
      "123/295, train_loss: 0.0675, step time: 1.0371\n",
      "124/295, train_loss: 0.0785, step time: 1.0445\n",
      "125/295, train_loss: 0.0737, step time: 1.0944\n",
      "126/295, train_loss: 0.1014, step time: 1.0455\n",
      "127/295, train_loss: 0.0891, step time: 1.0540\n",
      "128/295, train_loss: 0.0943, step time: 1.0869\n",
      "129/295, train_loss: 0.4081, step time: 1.0480\n",
      "130/295, train_loss: 0.0994, step time: 1.0377\n",
      "131/295, train_loss: 0.1253, step time: 1.0332\n",
      "132/295, train_loss: 0.0317, step time: 1.0418\n",
      "133/295, train_loss: 0.0465, step time: 1.0519\n",
      "134/295, train_loss: 0.3786, step time: 1.0449\n",
      "135/295, train_loss: 0.0711, step time: 1.0458\n",
      "136/295, train_loss: 0.1146, step time: 1.0499\n",
      "137/295, train_loss: 0.1233, step time: 1.0651\n",
      "138/295, train_loss: 0.2175, step time: 1.0375\n",
      "139/295, train_loss: 0.1346, step time: 1.0415\n",
      "140/295, train_loss: 0.0873, step time: 1.0859\n",
      "141/295, train_loss: 0.3644, step time: 1.0384\n",
      "142/295, train_loss: 0.0536, step time: 1.0822\n",
      "143/295, train_loss: 0.0688, step time: 1.0652\n",
      "144/295, train_loss: 0.1694, step time: 1.0460\n",
      "145/295, train_loss: 0.0721, step time: 1.0578\n",
      "146/295, train_loss: 0.0633, step time: 1.0401\n",
      "147/295, train_loss: 0.0657, step time: 1.0962\n",
      "148/295, train_loss: 0.0457, step time: 1.0398\n",
      "149/295, train_loss: 0.0920, step time: 1.0557\n",
      "150/295, train_loss: 0.0925, step time: 1.0484\n",
      "151/295, train_loss: 0.0533, step time: 1.0319\n",
      "152/295, train_loss: 0.3968, step time: 1.0499\n",
      "153/295, train_loss: 0.0550, step time: 1.0400\n",
      "154/295, train_loss: 0.1056, step time: 1.0345\n",
      "155/295, train_loss: 0.0965, step time: 1.0709\n",
      "156/295, train_loss: 0.0405, step time: 1.0372\n",
      "157/295, train_loss: 0.1377, step time: 1.0389\n",
      "158/295, train_loss: 0.1300, step time: 1.0462\n",
      "159/295, train_loss: 0.0390, step time: 1.0395\n",
      "160/295, train_loss: 0.0394, step time: 1.0366\n",
      "161/295, train_loss: 0.0717, step time: 1.0648\n",
      "162/295, train_loss: 0.1424, step time: 1.0456\n",
      "163/295, train_loss: 0.0575, step time: 1.0504\n",
      "164/295, train_loss: 0.0375, step time: 1.0390\n",
      "165/295, train_loss: 0.0331, step time: 1.0414\n",
      "166/295, train_loss: 0.0649, step time: 1.0486\n",
      "167/295, train_loss: 0.2007, step time: 1.0739\n",
      "168/295, train_loss: 0.0440, step time: 1.0411\n",
      "169/295, train_loss: 0.0486, step time: 1.0407\n",
      "170/295, train_loss: 0.0641, step time: 1.0380\n",
      "171/295, train_loss: 0.0526, step time: 1.0359\n",
      "172/295, train_loss: 0.3624, step time: 1.0599\n",
      "173/295, train_loss: 0.0733, step time: 1.0523\n",
      "174/295, train_loss: 0.0647, step time: 1.0423\n",
      "175/295, train_loss: 0.1408, step time: 1.0382\n",
      "176/295, train_loss: 0.1272, step time: 1.0389\n",
      "177/295, train_loss: 0.0845, step time: 1.0451\n",
      "178/295, train_loss: 0.0699, step time: 1.0408\n",
      "179/295, train_loss: 0.0960, step time: 1.0453\n",
      "180/295, train_loss: 0.0901, step time: 1.0448\n",
      "181/295, train_loss: 0.4234, step time: 1.0662\n",
      "182/295, train_loss: 0.0652, step time: 1.0381\n",
      "183/295, train_loss: 0.0435, step time: 1.0346\n",
      "184/295, train_loss: 0.0349, step time: 1.0528\n",
      "185/295, train_loss: 0.0643, step time: 1.0543\n",
      "186/295, train_loss: 0.1015, step time: 1.0387\n",
      "187/295, train_loss: 0.1238, step time: 1.0403\n",
      "188/295, train_loss: 0.1121, step time: 1.0409\n",
      "189/295, train_loss: 0.0667, step time: 1.0445\n",
      "190/295, train_loss: 0.0916, step time: 1.0412\n",
      "191/295, train_loss: 0.3972, step time: 1.0515\n",
      "192/295, train_loss: 0.0444, step time: 1.0823\n",
      "193/295, train_loss: 0.0917, step time: 1.0633\n",
      "194/295, train_loss: 0.0885, step time: 1.0465\n",
      "195/295, train_loss: 0.0679, step time: 1.0396\n",
      "196/295, train_loss: 0.0825, step time: 1.0335\n",
      "197/295, train_loss: 0.0711, step time: 1.0844\n",
      "198/295, train_loss: 0.3102, step time: 1.0668\n",
      "199/295, train_loss: 0.0915, step time: 1.0618\n",
      "200/295, train_loss: 0.4191, step time: 1.0686\n",
      "201/295, train_loss: 0.4045, step time: 1.0453\n",
      "202/295, train_loss: 0.4018, step time: 1.0378\n",
      "203/295, train_loss: 0.1142, step time: 1.0393\n",
      "204/295, train_loss: 0.3389, step time: 1.0593\n",
      "205/295, train_loss: 0.0543, step time: 1.0475\n",
      "206/295, train_loss: 0.1618, step time: 1.0374\n",
      "207/295, train_loss: 0.0870, step time: 1.0387\n",
      "208/295, train_loss: 0.0484, step time: 1.0712\n",
      "209/295, train_loss: 0.0585, step time: 1.0537\n",
      "210/295, train_loss: 0.0734, step time: 1.0482\n",
      "211/295, train_loss: 0.1300, step time: 1.0495\n",
      "212/295, train_loss: 0.0441, step time: 1.0380\n",
      "213/295, train_loss: 0.1422, step time: 1.0523\n",
      "214/295, train_loss: 0.1428, step time: 1.0468\n",
      "215/295, train_loss: 0.2389, step time: 1.0953\n",
      "216/295, train_loss: 0.0328, step time: 1.0363\n",
      "217/295, train_loss: 0.1529, step time: 1.0380\n",
      "218/295, train_loss: 0.0723, step time: 1.0597\n",
      "219/295, train_loss: 0.0588, step time: 1.0459\n",
      "220/295, train_loss: 0.0358, step time: 1.0362\n",
      "221/295, train_loss: 0.0864, step time: 1.0523\n",
      "222/295, train_loss: 0.0926, step time: 1.1006\n",
      "223/295, train_loss: 0.0647, step time: 1.0500\n",
      "224/295, train_loss: 0.0535, step time: 1.0464\n",
      "225/295, train_loss: 0.1459, step time: 1.0629\n",
      "226/295, train_loss: 0.0571, step time: 1.0419\n",
      "227/295, train_loss: 0.0387, step time: 1.0588\n",
      "228/295, train_loss: 0.1241, step time: 1.1133\n",
      "229/295, train_loss: 0.0607, step time: 1.0360\n",
      "230/295, train_loss: 0.0411, step time: 1.0651\n",
      "231/295, train_loss: 0.1280, step time: 1.0423\n",
      "232/295, train_loss: 0.0800, step time: 1.0471\n",
      "233/295, train_loss: 0.0486, step time: 1.0429\n",
      "234/295, train_loss: 0.0802, step time: 1.0500\n",
      "235/295, train_loss: 0.0884, step time: 1.0368\n",
      "236/295, train_loss: 0.0806, step time: 1.0389\n",
      "237/295, train_loss: 0.1256, step time: 1.0345\n",
      "238/295, train_loss: 0.0652, step time: 1.0341\n",
      "239/295, train_loss: 0.0734, step time: 1.0615\n",
      "240/295, train_loss: 0.0860, step time: 1.0447\n",
      "241/295, train_loss: 0.0911, step time: 1.0392\n",
      "242/295, train_loss: 0.0588, step time: 1.0877\n",
      "243/295, train_loss: 0.0982, step time: 1.1293\n",
      "244/295, train_loss: 0.1501, step time: 1.0855\n",
      "245/295, train_loss: 0.2522, step time: 1.0674\n",
      "246/295, train_loss: 0.0528, step time: 1.0372\n",
      "247/295, train_loss: 0.0679, step time: 1.0800\n",
      "248/295, train_loss: 0.0710, step time: 1.0942\n",
      "249/295, train_loss: 0.0899, step time: 1.1373\n",
      "250/295, train_loss: 0.0952, step time: 1.0882\n",
      "251/295, train_loss: 0.1148, step time: 1.0387\n",
      "252/295, train_loss: 0.1036, step time: 1.0516\n",
      "253/295, train_loss: 0.1244, step time: 1.1073\n",
      "254/295, train_loss: 0.0574, step time: 1.0417\n",
      "255/295, train_loss: 0.1102, step time: 1.0679\n",
      "256/295, train_loss: 0.1306, step time: 1.0366\n",
      "257/295, train_loss: 0.0483, step time: 1.0384\n",
      "258/295, train_loss: 0.0749, step time: 1.0461\n",
      "259/295, train_loss: 0.0427, step time: 1.0385\n",
      "260/295, train_loss: 0.0435, step time: 1.0333\n",
      "261/295, train_loss: 0.0741, step time: 1.0428\n",
      "262/295, train_loss: 0.1239, step time: 1.0376\n",
      "263/295, train_loss: 0.0508, step time: 1.0375\n",
      "264/295, train_loss: 0.1383, step time: 1.0612\n",
      "265/295, train_loss: 0.1276, step time: 1.0637\n",
      "266/295, train_loss: 0.3831, step time: 1.0456\n",
      "267/295, train_loss: 0.0495, step time: 1.0641\n",
      "268/295, train_loss: 0.3793, step time: 1.0425\n",
      "269/295, train_loss: 0.0813, step time: 1.0594\n",
      "270/295, train_loss: 0.0401, step time: 1.0750\n",
      "271/295, train_loss: 0.0542, step time: 1.0684\n",
      "272/295, train_loss: 0.0405, step time: 1.0489\n",
      "273/295, train_loss: 0.0953, step time: 1.0384\n",
      "274/295, train_loss: 0.1493, step time: 1.0442\n",
      "275/295, train_loss: 0.3985, step time: 1.0452\n",
      "276/295, train_loss: 0.0415, step time: 1.0378\n",
      "277/295, train_loss: 0.1273, step time: 1.0422\n",
      "278/295, train_loss: 0.0557, step time: 1.0929\n",
      "279/295, train_loss: 0.0883, step time: 1.0457\n",
      "280/295, train_loss: 0.1020, step time: 1.0451\n",
      "281/295, train_loss: 0.0895, step time: 1.0361\n",
      "282/295, train_loss: 0.0552, step time: 1.0334\n",
      "283/295, train_loss: 0.0635, step time: 1.0358\n",
      "284/295, train_loss: 0.0989, step time: 1.0327\n",
      "285/295, train_loss: 0.0630, step time: 1.0458\n",
      "286/295, train_loss: 0.0725, step time: 1.0437\n",
      "287/295, train_loss: 0.0346, step time: 1.0347\n",
      "288/295, train_loss: 0.0614, step time: 1.0426\n",
      "289/295, train_loss: 0.0582, step time: 1.0360\n",
      "290/295, train_loss: 0.1250, step time: 1.0301\n",
      "291/295, train_loss: 0.2281, step time: 1.0296\n",
      "292/295, train_loss: 0.0665, step time: 1.0300\n",
      "293/295, train_loss: 0.1085, step time: 1.0302\n",
      "294/295, train_loss: 0.1683, step time: 1.0304\n",
      "295/295, train_loss: 0.0835, step time: 1.0296\n",
      "epoch 38 average loss: 0.1127\n",
      "current epoch: 38 current mean dice: 0.8037 tc: 0.7566 wt: 0.8688 et: 0.7918\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 38 is: 382.6881\n",
      "----------\n",
      "epoch 39/100\n",
      "1/295, train_loss: 0.1023, step time: 1.0733\n",
      "2/295, train_loss: 0.0461, step time: 1.1242\n",
      "3/295, train_loss: 0.0389, step time: 1.0708\n",
      "4/295, train_loss: 0.0657, step time: 1.0664\n",
      "5/295, train_loss: 0.1023, step time: 1.0657\n",
      "6/295, train_loss: 0.1035, step time: 1.0491\n",
      "7/295, train_loss: 0.0378, step time: 1.0378\n",
      "8/295, train_loss: 0.3870, step time: 1.0495\n",
      "9/295, train_loss: 0.0772, step time: 1.0513\n",
      "10/295, train_loss: 0.0591, step time: 1.0496\n",
      "11/295, train_loss: 0.0891, step time: 1.0350\n",
      "12/295, train_loss: 0.0592, step time: 1.0374\n",
      "13/295, train_loss: 0.0715, step time: 1.0493\n",
      "14/295, train_loss: 0.0403, step time: 1.0472\n",
      "15/295, train_loss: 0.0509, step time: 1.0377\n",
      "16/295, train_loss: 0.1044, step time: 1.0536\n",
      "17/295, train_loss: 0.0662, step time: 1.0399\n",
      "18/295, train_loss: 0.0668, step time: 1.0388\n",
      "19/295, train_loss: 0.4291, step time: 1.0334\n",
      "20/295, train_loss: 0.4090, step time: 1.0394\n",
      "21/295, train_loss: 0.1239, step time: 1.1114\n",
      "22/295, train_loss: 0.1542, step time: 1.0365\n",
      "23/295, train_loss: 0.0938, step time: 1.1255\n",
      "24/295, train_loss: 0.0255, step time: 1.0756\n",
      "25/295, train_loss: 0.3909, step time: 1.0366\n",
      "26/295, train_loss: 0.1094, step time: 1.0376\n",
      "27/295, train_loss: 0.4129, step time: 1.0511\n",
      "28/295, train_loss: 0.1119, step time: 1.1103\n",
      "29/295, train_loss: 0.0998, step time: 1.0731\n",
      "30/295, train_loss: 0.4061, step time: 1.0438\n",
      "31/295, train_loss: 0.0783, step time: 1.0609\n",
      "32/295, train_loss: 0.0673, step time: 1.1012\n",
      "33/295, train_loss: 0.0386, step time: 1.1034\n",
      "34/295, train_loss: 0.3963, step time: 1.0430\n",
      "35/295, train_loss: 0.0657, step time: 1.0634\n",
      "36/295, train_loss: 0.0832, step time: 1.0471\n",
      "37/295, train_loss: 0.0950, step time: 1.0494\n",
      "38/295, train_loss: 0.2221, step time: 1.0509\n",
      "39/295, train_loss: 0.0934, step time: 1.0422\n",
      "40/295, train_loss: 0.0760, step time: 1.0332\n",
      "41/295, train_loss: 0.0746, step time: 1.0325\n",
      "42/295, train_loss: 0.0577, step time: 1.0397\n",
      "43/295, train_loss: 0.0691, step time: 1.0380\n",
      "44/295, train_loss: 0.0665, step time: 1.0332\n",
      "45/295, train_loss: 0.3747, step time: 1.0324\n",
      "46/295, train_loss: 0.1622, step time: 1.0669\n",
      "47/295, train_loss: 0.1056, step time: 1.0443\n",
      "48/295, train_loss: 0.0816, step time: 1.0368\n",
      "49/295, train_loss: 0.0624, step time: 1.0510\n",
      "50/295, train_loss: 0.0384, step time: 1.0833\n",
      "51/295, train_loss: 0.0745, step time: 1.0412\n",
      "52/295, train_loss: 0.0448, step time: 1.0587\n",
      "53/295, train_loss: 0.1075, step time: 1.0487\n",
      "54/295, train_loss: 0.0334, step time: 1.0531\n",
      "55/295, train_loss: 0.4173, step time: 1.0357\n",
      "56/295, train_loss: 0.0559, step time: 1.0983\n",
      "57/295, train_loss: 0.0322, step time: 1.0310\n",
      "58/295, train_loss: 0.0791, step time: 1.0321\n",
      "59/295, train_loss: 0.0896, step time: 1.0462\n",
      "60/295, train_loss: 0.0921, step time: 1.0390\n",
      "61/295, train_loss: 0.0600, step time: 1.0572\n",
      "62/295, train_loss: 0.0848, step time: 1.0747\n",
      "63/295, train_loss: 0.1819, step time: 1.0565\n",
      "64/295, train_loss: 0.0909, step time: 1.0397\n",
      "65/295, train_loss: 0.0523, step time: 1.0798\n",
      "66/295, train_loss: 0.0569, step time: 1.0452\n",
      "67/295, train_loss: 0.0479, step time: 1.0467\n",
      "68/295, train_loss: 0.0636, step time: 1.0895\n",
      "69/295, train_loss: 0.3996, step time: 1.0472\n",
      "70/295, train_loss: 0.0885, step time: 1.0293\n",
      "71/295, train_loss: 0.0634, step time: 1.0838\n",
      "72/295, train_loss: 0.0529, step time: 1.0466\n",
      "73/295, train_loss: 0.0711, step time: 1.0601\n",
      "74/295, train_loss: 0.0949, step time: 1.0319\n",
      "75/295, train_loss: 0.0333, step time: 1.0385\n",
      "76/295, train_loss: 0.0379, step time: 1.0332\n",
      "77/295, train_loss: 0.1565, step time: 1.1175\n",
      "78/295, train_loss: 0.1171, step time: 1.1102\n",
      "79/295, train_loss: 0.0630, step time: 1.0425\n",
      "80/295, train_loss: 0.3986, step time: 1.0763\n",
      "81/295, train_loss: 0.2010, step time: 1.0394\n",
      "82/295, train_loss: 0.0869, step time: 1.1011\n",
      "83/295, train_loss: 0.0954, step time: 1.0491\n",
      "84/295, train_loss: 0.1219, step time: 1.1165\n",
      "85/295, train_loss: 0.0728, step time: 1.0610\n",
      "86/295, train_loss: 0.0622, step time: 1.0485\n",
      "87/295, train_loss: 0.3957, step time: 1.0715\n",
      "88/295, train_loss: 0.0540, step time: 1.0556\n",
      "89/295, train_loss: 0.1218, step time: 1.0449\n",
      "90/295, train_loss: 0.0809, step time: 1.0627\n",
      "91/295, train_loss: 0.0404, step time: 1.0593\n",
      "92/295, train_loss: 0.0630, step time: 1.0817\n",
      "93/295, train_loss: 0.0544, step time: 1.0571\n",
      "94/295, train_loss: 0.1106, step time: 1.0398\n",
      "95/295, train_loss: 0.0476, step time: 1.0356\n",
      "96/295, train_loss: 0.1383, step time: 1.0372\n",
      "97/295, train_loss: 0.0398, step time: 1.0607\n",
      "98/295, train_loss: 0.0947, step time: 1.0617\n",
      "99/295, train_loss: 0.0816, step time: 1.0344\n",
      "100/295, train_loss: 0.0440, step time: 1.0809\n",
      "101/295, train_loss: 0.1401, step time: 1.0423\n",
      "102/295, train_loss: 0.0627, step time: 1.0384\n",
      "103/295, train_loss: 0.0619, step time: 1.0724\n",
      "104/295, train_loss: 0.0823, step time: 1.0385\n",
      "105/295, train_loss: 0.1310, step time: 1.0354\n",
      "106/295, train_loss: 0.0963, step time: 1.0399\n",
      "107/295, train_loss: 0.0617, step time: 1.0419\n",
      "108/295, train_loss: 0.1530, step time: 1.0592\n",
      "109/295, train_loss: 0.0907, step time: 1.0420\n",
      "110/295, train_loss: 0.1154, step time: 1.1094\n",
      "111/295, train_loss: 0.0602, step time: 1.0387\n",
      "112/295, train_loss: 0.0600, step time: 1.0476\n",
      "113/295, train_loss: 0.0386, step time: 1.0400\n",
      "114/295, train_loss: 0.0493, step time: 1.0404\n",
      "115/295, train_loss: 0.0372, step time: 1.0447\n",
      "116/295, train_loss: 0.0553, step time: 1.0332\n",
      "117/295, train_loss: 0.1263, step time: 1.0455\n",
      "118/295, train_loss: 0.1238, step time: 1.0859\n",
      "119/295, train_loss: 0.1043, step time: 1.0328\n",
      "120/295, train_loss: 0.3780, step time: 1.0421\n",
      "121/295, train_loss: 0.1248, step time: 1.0419\n",
      "122/295, train_loss: 0.0792, step time: 1.0382\n",
      "123/295, train_loss: 0.0437, step time: 1.0380\n",
      "124/295, train_loss: 0.4977, step time: 1.0348\n",
      "125/295, train_loss: 0.4235, step time: 1.1433\n",
      "126/295, train_loss: 0.0489, step time: 1.0459\n",
      "127/295, train_loss: 0.0665, step time: 1.0324\n",
      "128/295, train_loss: 0.0674, step time: 1.0395\n",
      "129/295, train_loss: 0.0935, step time: 1.0333\n",
      "130/295, train_loss: 0.0382, step time: 1.0398\n",
      "131/295, train_loss: 0.0639, step time: 1.0509\n",
      "132/295, train_loss: 0.1011, step time: 1.0715\n",
      "133/295, train_loss: 0.0910, step time: 1.0701\n",
      "134/295, train_loss: 0.0585, step time: 1.0937\n",
      "135/295, train_loss: 0.1580, step time: 1.0596\n",
      "136/295, train_loss: 0.0419, step time: 1.0519\n",
      "137/295, train_loss: 0.0441, step time: 1.0449\n",
      "138/295, train_loss: 0.0672, step time: 1.0357\n",
      "139/295, train_loss: 0.0669, step time: 1.0338\n",
      "140/295, train_loss: 0.1212, step time: 1.0418\n",
      "141/295, train_loss: 0.1262, step time: 1.0334\n",
      "142/295, train_loss: 0.0347, step time: 1.0380\n",
      "143/295, train_loss: 0.0542, step time: 1.1232\n",
      "144/295, train_loss: 0.1729, step time: 1.0337\n",
      "145/295, train_loss: 0.1029, step time: 1.0359\n",
      "146/295, train_loss: 0.4098, step time: 1.0373\n",
      "147/295, train_loss: 0.0329, step time: 1.0451\n",
      "148/295, train_loss: 0.0825, step time: 1.0648\n",
      "149/295, train_loss: 0.0541, step time: 1.0624\n",
      "150/295, train_loss: 0.0817, step time: 1.0638\n",
      "151/295, train_loss: 0.0934, step time: 1.0559\n",
      "152/295, train_loss: 0.1101, step time: 1.0411\n",
      "153/295, train_loss: 0.1377, step time: 1.0399\n",
      "154/295, train_loss: 0.0656, step time: 1.0699\n",
      "155/295, train_loss: 0.0535, step time: 1.0828\n",
      "156/295, train_loss: 0.1201, step time: 1.0429\n",
      "157/295, train_loss: 0.1400, step time: 1.0314\n",
      "158/295, train_loss: 0.0639, step time: 1.0353\n",
      "159/295, train_loss: 0.2066, step time: 1.0400\n",
      "160/295, train_loss: 0.1537, step time: 1.0732\n",
      "161/295, train_loss: 0.1033, step time: 1.0367\n",
      "162/295, train_loss: 0.4167, step time: 1.0468\n",
      "163/295, train_loss: 0.0941, step time: 1.0639\n",
      "164/295, train_loss: 0.0798, step time: 1.0503\n",
      "165/295, train_loss: 0.0537, step time: 1.0990\n",
      "166/295, train_loss: 0.0451, step time: 1.0383\n",
      "167/295, train_loss: 0.0493, step time: 1.0558\n",
      "168/295, train_loss: 0.0594, step time: 1.0563\n",
      "169/295, train_loss: 0.1054, step time: 1.0446\n",
      "170/295, train_loss: 0.1036, step time: 1.1367\n",
      "171/295, train_loss: 0.0524, step time: 1.0593\n",
      "172/295, train_loss: 0.0956, step time: 1.0430\n",
      "173/295, train_loss: 0.0810, step time: 1.1016\n",
      "174/295, train_loss: 0.0356, step time: 1.0797\n",
      "175/295, train_loss: 0.0843, step time: 1.0390\n",
      "176/295, train_loss: 0.1291, step time: 1.0674\n",
      "177/295, train_loss: 0.1992, step time: 1.1001\n",
      "178/295, train_loss: 0.0349, step time: 1.0344\n",
      "179/295, train_loss: 0.4018, step time: 1.0393\n",
      "180/295, train_loss: 0.0771, step time: 1.0324\n",
      "181/295, train_loss: 0.0871, step time: 1.0759\n",
      "182/295, train_loss: 0.0504, step time: 1.0356\n",
      "183/295, train_loss: 0.1292, step time: 1.0544\n",
      "184/295, train_loss: 0.0805, step time: 1.0403\n",
      "185/295, train_loss: 0.0308, step time: 1.0816\n",
      "186/295, train_loss: 0.0486, step time: 1.0329\n",
      "187/295, train_loss: 0.0560, step time: 1.0624\n",
      "188/295, train_loss: 0.0557, step time: 1.0432\n",
      "189/295, train_loss: 0.0677, step time: 1.0665\n",
      "190/295, train_loss: 0.0957, step time: 1.0815\n",
      "191/295, train_loss: 0.0823, step time: 1.0327\n",
      "192/295, train_loss: 0.1426, step time: 1.0403\n",
      "193/295, train_loss: 0.0392, step time: 1.0366\n",
      "194/295, train_loss: 0.0556, step time: 1.0329\n",
      "195/295, train_loss: 0.1027, step time: 1.0330\n",
      "196/295, train_loss: 0.0298, step time: 1.0347\n",
      "197/295, train_loss: 0.1114, step time: 1.0591\n",
      "198/295, train_loss: 0.0725, step time: 1.0383\n",
      "199/295, train_loss: 0.3646, step time: 1.0400\n",
      "200/295, train_loss: 0.0915, step time: 1.0640\n",
      "201/295, train_loss: 0.0379, step time: 1.0368\n",
      "202/295, train_loss: 0.0537, step time: 1.0405\n",
      "203/295, train_loss: 0.0663, step time: 1.0350\n",
      "204/295, train_loss: 0.3969, step time: 1.0478\n",
      "205/295, train_loss: 0.0490, step time: 1.0552\n",
      "206/295, train_loss: 0.1637, step time: 1.0704\n",
      "207/295, train_loss: 0.0555, step time: 1.0358\n",
      "208/295, train_loss: 0.0398, step time: 1.0387\n",
      "209/295, train_loss: 0.0463, step time: 1.0564\n",
      "210/295, train_loss: 0.1253, step time: 1.1036\n",
      "211/295, train_loss: 0.0837, step time: 1.0402\n",
      "212/295, train_loss: 0.3811, step time: 1.0644\n",
      "213/295, train_loss: 0.1081, step time: 1.0576\n",
      "214/295, train_loss: 0.0809, step time: 1.0422\n",
      "215/295, train_loss: 0.0459, step time: 1.0428\n",
      "216/295, train_loss: 0.0369, step time: 1.0544\n",
      "217/295, train_loss: 0.3395, step time: 1.0682\n",
      "218/295, train_loss: 0.0643, step time: 1.0337\n",
      "219/295, train_loss: 0.0921, step time: 1.0340\n",
      "220/295, train_loss: 0.0550, step time: 1.0423\n",
      "221/295, train_loss: 0.1788, step time: 1.0608\n",
      "222/295, train_loss: 0.0787, step time: 1.0365\n",
      "223/295, train_loss: 0.0798, step time: 1.0627\n",
      "224/295, train_loss: 0.0657, step time: 1.0526\n",
      "225/295, train_loss: 0.0575, step time: 1.0318\n",
      "226/295, train_loss: 0.0424, step time: 1.0734\n",
      "227/295, train_loss: 0.1907, step time: 1.0578\n",
      "228/295, train_loss: 0.3858, step time: 1.0690\n",
      "229/295, train_loss: 0.1963, step time: 1.0368\n",
      "230/295, train_loss: 0.1009, step time: 1.0336\n",
      "231/295, train_loss: 0.0953, step time: 1.0648\n",
      "232/295, train_loss: 0.1742, step time: 1.0546\n",
      "233/295, train_loss: 0.0420, step time: 1.0652\n",
      "234/295, train_loss: 0.0507, step time: 1.0374\n",
      "235/295, train_loss: 0.1132, step time: 1.0319\n",
      "236/295, train_loss: 0.0502, step time: 1.0362\n",
      "237/295, train_loss: 0.0464, step time: 1.0359\n",
      "238/295, train_loss: 0.0685, step time: 1.0881\n",
      "239/295, train_loss: 0.0818, step time: 1.0360\n",
      "240/295, train_loss: 0.0358, step time: 1.0517\n",
      "241/295, train_loss: 0.0525, step time: 1.0310\n",
      "242/295, train_loss: 0.0985, step time: 1.0400\n",
      "243/295, train_loss: 0.0577, step time: 1.0394\n",
      "244/295, train_loss: 0.0678, step time: 1.0431\n",
      "245/295, train_loss: 0.1775, step time: 1.0435\n",
      "246/295, train_loss: 0.0571, step time: 1.0595\n",
      "247/295, train_loss: 0.0832, step time: 1.0623\n",
      "248/295, train_loss: 0.0707, step time: 1.0527\n",
      "249/295, train_loss: 0.0951, step time: 1.0743\n",
      "250/295, train_loss: 0.3163, step time: 1.0445\n",
      "251/295, train_loss: 0.0500, step time: 1.0444\n",
      "252/295, train_loss: 0.0633, step time: 1.0556\n",
      "253/295, train_loss: 0.0391, step time: 1.0477\n",
      "254/295, train_loss: 0.0608, step time: 1.0487\n",
      "255/295, train_loss: 0.1342, step time: 1.0473\n",
      "256/295, train_loss: 0.1110, step time: 1.0365\n",
      "257/295, train_loss: 0.1418, step time: 1.0357\n",
      "258/295, train_loss: 0.0826, step time: 1.0423\n",
      "259/295, train_loss: 0.2286, step time: 1.0323\n",
      "260/295, train_loss: 0.0831, step time: 1.0550\n",
      "261/295, train_loss: 0.1059, step time: 1.0636\n",
      "262/295, train_loss: 0.0461, step time: 1.0391\n",
      "263/295, train_loss: 0.0420, step time: 1.0383\n",
      "264/295, train_loss: 0.1186, step time: 1.0451\n",
      "265/295, train_loss: 0.1072, step time: 1.0456\n",
      "266/295, train_loss: 0.0611, step time: 1.0578\n",
      "267/295, train_loss: 0.2080, step time: 1.0369\n",
      "268/295, train_loss: 0.0867, step time: 1.0463\n",
      "269/295, train_loss: 0.0530, step time: 1.0581\n",
      "270/295, train_loss: 0.1100, step time: 1.0650\n",
      "271/295, train_loss: 0.0370, step time: 1.0606\n",
      "272/295, train_loss: 0.0553, step time: 1.0478\n",
      "273/295, train_loss: 0.0618, step time: 1.1190\n",
      "274/295, train_loss: 0.0580, step time: 1.0369\n",
      "275/295, train_loss: 0.0686, step time: 1.0598\n",
      "276/295, train_loss: 0.1167, step time: 1.0467\n",
      "277/295, train_loss: 0.1372, step time: 1.0610\n",
      "278/295, train_loss: 0.0722, step time: 1.0567\n",
      "279/295, train_loss: 0.0433, step time: 1.0354\n",
      "280/295, train_loss: 0.0390, step time: 1.0457\n",
      "281/295, train_loss: 0.1058, step time: 1.0399\n",
      "282/295, train_loss: 0.0522, step time: 1.0368\n",
      "283/295, train_loss: 0.3203, step time: 1.0359\n",
      "284/295, train_loss: 0.0408, step time: 1.0982\n",
      "285/295, train_loss: 0.0824, step time: 1.0475\n",
      "286/295, train_loss: 0.1288, step time: 1.0448\n",
      "287/295, train_loss: 0.1301, step time: 1.0344\n",
      "288/295, train_loss: 0.0829, step time: 1.0380\n",
      "289/295, train_loss: 0.0935, step time: 1.0295\n",
      "290/295, train_loss: 0.0369, step time: 1.0300\n",
      "291/295, train_loss: 0.1245, step time: 1.0296\n",
      "292/295, train_loss: 0.3726, step time: 1.0300\n",
      "293/295, train_loss: 0.0831, step time: 1.0287\n",
      "294/295, train_loss: 0.1641, step time: 1.0299\n",
      "295/295, train_loss: 0.0489, step time: 1.0277\n",
      "epoch 39 average loss: 0.1106\n",
      "current epoch: 39 current mean dice: 0.7756 tc: 0.7315 wt: 0.8463 et: 0.7583\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 39 is: 380.0209\n",
      "----------\n",
      "epoch 40/100\n",
      "1/295, train_loss: 0.0754, step time: 1.1164\n",
      "2/295, train_loss: 0.0595, step time: 1.0418\n",
      "3/295, train_loss: 0.0681, step time: 1.0580\n",
      "4/295, train_loss: 0.0564, step time: 1.0799\n",
      "5/295, train_loss: 0.0504, step time: 1.0392\n",
      "6/295, train_loss: 0.1552, step time: 1.0559\n",
      "7/295, train_loss: 0.0654, step time: 1.0459\n",
      "8/295, train_loss: 0.0522, step time: 1.1071\n",
      "9/295, train_loss: 0.0476, step time: 1.0680\n",
      "10/295, train_loss: 0.0666, step time: 1.0515\n",
      "11/295, train_loss: 0.0481, step time: 1.0664\n",
      "12/295, train_loss: 0.0459, step time: 1.0773\n",
      "13/295, train_loss: 0.1235, step time: 1.0440\n",
      "14/295, train_loss: 0.0562, step time: 1.1171\n",
      "15/295, train_loss: 0.1077, step time: 1.0560\n",
      "16/295, train_loss: 0.0371, step time: 1.0530\n",
      "17/295, train_loss: 0.0939, step time: 1.0420\n",
      "18/295, train_loss: 0.1472, step time: 1.0305\n",
      "19/295, train_loss: 0.1319, step time: 1.0390\n",
      "20/295, train_loss: 0.1008, step time: 1.0553\n",
      "21/295, train_loss: 0.0310, step time: 1.0714\n",
      "22/295, train_loss: 0.0718, step time: 1.0559\n",
      "23/295, train_loss: 0.0346, step time: 1.0435\n",
      "24/295, train_loss: 0.1565, step time: 1.0674\n",
      "25/295, train_loss: 0.4345, step time: 1.0444\n",
      "26/295, train_loss: 0.0652, step time: 1.0397\n",
      "27/295, train_loss: 0.4231, step time: 1.0455\n",
      "28/295, train_loss: 0.1250, step time: 1.0401\n",
      "29/295, train_loss: 0.3747, step time: 1.0979\n",
      "30/295, train_loss: 0.0719, step time: 1.0615\n",
      "31/295, train_loss: 0.0449, step time: 1.0327\n",
      "32/295, train_loss: 0.0420, step time: 1.1067\n",
      "33/295, train_loss: 0.0579, step time: 1.0410\n",
      "34/295, train_loss: 0.1059, step time: 1.0365\n",
      "35/295, train_loss: 0.0739, step time: 1.0462\n",
      "36/295, train_loss: 0.0840, step time: 1.0491\n",
      "37/295, train_loss: 0.0742, step time: 1.0350\n",
      "38/295, train_loss: 0.3940, step time: 1.0482\n",
      "39/295, train_loss: 0.1333, step time: 1.0647\n",
      "40/295, train_loss: 0.1188, step time: 1.0506\n",
      "41/295, train_loss: 0.0947, step time: 1.0589\n",
      "42/295, train_loss: 0.0637, step time: 1.0390\n",
      "43/295, train_loss: 0.0567, step time: 1.0353\n",
      "44/295, train_loss: 0.0887, step time: 1.0390\n",
      "45/295, train_loss: 0.0716, step time: 1.0366\n",
      "46/295, train_loss: 0.1279, step time: 1.0606\n",
      "47/295, train_loss: 0.1153, step time: 1.0858\n",
      "48/295, train_loss: 0.0492, step time: 1.1002\n",
      "49/295, train_loss: 0.1008, step time: 1.0343\n",
      "50/295, train_loss: 0.0792, step time: 1.0389\n",
      "51/295, train_loss: 0.4406, step time: 1.0439\n",
      "52/295, train_loss: 0.0338, step time: 1.0620\n",
      "53/295, train_loss: 0.0790, step time: 1.0694\n",
      "54/295, train_loss: 0.1048, step time: 1.0427\n",
      "55/295, train_loss: 0.0872, step time: 1.0629\n",
      "56/295, train_loss: 0.0834, step time: 1.0516\n",
      "57/295, train_loss: 0.0614, step time: 1.0325\n",
      "58/295, train_loss: 0.0622, step time: 1.0358\n",
      "59/295, train_loss: 0.0776, step time: 1.0855\n",
      "60/295, train_loss: 0.0535, step time: 1.0530\n",
      "61/295, train_loss: 0.0921, step time: 1.0676\n",
      "62/295, train_loss: 0.1009, step time: 1.0450\n",
      "63/295, train_loss: 0.1090, step time: 1.0463\n",
      "64/295, train_loss: 0.0804, step time: 1.1269\n",
      "65/295, train_loss: 0.0607, step time: 1.0389\n",
      "66/295, train_loss: 0.1488, step time: 1.0476\n",
      "67/295, train_loss: 0.0504, step time: 1.0578\n",
      "68/295, train_loss: 0.0332, step time: 1.0357\n",
      "69/295, train_loss: 0.3634, step time: 1.0415\n",
      "70/295, train_loss: 0.0680, step time: 1.0426\n",
      "71/295, train_loss: 0.3579, step time: 1.0545\n",
      "72/295, train_loss: 0.1097, step time: 1.0386\n",
      "73/295, train_loss: 0.0444, step time: 1.0368\n",
      "74/295, train_loss: 0.0334, step time: 1.0662\n",
      "75/295, train_loss: 0.1199, step time: 1.0608\n",
      "76/295, train_loss: 0.1654, step time: 1.0424\n",
      "77/295, train_loss: 0.0760, step time: 1.0439\n",
      "78/295, train_loss: 0.0441, step time: 1.0594\n",
      "79/295, train_loss: 0.2230, step time: 1.0629\n",
      "80/295, train_loss: 0.1655, step time: 1.1346\n",
      "81/295, train_loss: 0.1083, step time: 1.0843\n",
      "82/295, train_loss: 0.1282, step time: 1.0489\n",
      "83/295, train_loss: 0.0898, step time: 1.0550\n",
      "84/295, train_loss: 0.3937, step time: 1.0453\n",
      "85/295, train_loss: 0.1205, step time: 1.0472\n",
      "86/295, train_loss: 0.1020, step time: 1.0603\n",
      "87/295, train_loss: 0.1151, step time: 1.0368\n",
      "88/295, train_loss: 0.2096, step time: 1.0346\n",
      "89/295, train_loss: 0.0346, step time: 1.0476\n",
      "90/295, train_loss: 0.0701, step time: 1.0930\n",
      "91/295, train_loss: 0.0536, step time: 1.0833\n",
      "92/295, train_loss: 0.0413, step time: 1.0552\n",
      "93/295, train_loss: 0.0722, step time: 1.0381\n",
      "94/295, train_loss: 0.0959, step time: 1.0639\n",
      "95/295, train_loss: 0.4095, step time: 1.0359\n",
      "96/295, train_loss: 0.0763, step time: 1.0647\n",
      "97/295, train_loss: 0.1125, step time: 1.0571\n",
      "98/295, train_loss: 0.0689, step time: 1.0953\n",
      "99/295, train_loss: 0.0392, step time: 1.0352\n",
      "100/295, train_loss: 0.0434, step time: 1.0374\n",
      "101/295, train_loss: 0.1025, step time: 1.0395\n",
      "102/295, train_loss: 0.0439, step time: 1.0533\n",
      "103/295, train_loss: 0.3680, step time: 1.0856\n",
      "104/295, train_loss: 0.0549, step time: 1.0492\n",
      "105/295, train_loss: 0.0486, step time: 1.0669\n",
      "106/295, train_loss: 0.0751, step time: 1.1454\n",
      "107/295, train_loss: 0.0466, step time: 1.0498\n",
      "108/295, train_loss: 0.0403, step time: 1.0391\n",
      "109/295, train_loss: 0.1261, step time: 1.0368\n",
      "110/295, train_loss: 0.0957, step time: 1.0338\n",
      "111/295, train_loss: 0.0660, step time: 1.0379\n",
      "112/295, train_loss: 0.1433, step time: 1.0430\n",
      "113/295, train_loss: 0.0626, step time: 1.0408\n",
      "114/295, train_loss: 0.0988, step time: 1.0725\n",
      "115/295, train_loss: 0.0462, step time: 1.0401\n",
      "116/295, train_loss: 0.0636, step time: 1.0399\n",
      "117/295, train_loss: 0.3970, step time: 1.0391\n",
      "118/295, train_loss: 0.0347, step time: 1.0497\n",
      "119/295, train_loss: 0.0605, step time: 1.0375\n",
      "120/295, train_loss: 0.0763, step time: 1.0345\n",
      "121/295, train_loss: 0.0323, step time: 1.0628\n",
      "122/295, train_loss: 0.0646, step time: 1.0334\n",
      "123/295, train_loss: 0.0920, step time: 1.0729\n",
      "124/295, train_loss: 0.1906, step time: 1.0313\n",
      "125/295, train_loss: 0.1144, step time: 1.0991\n",
      "126/295, train_loss: 0.0559, step time: 1.0316\n",
      "127/295, train_loss: 0.0568, step time: 1.0444\n",
      "128/295, train_loss: 0.0785, step time: 1.0805\n",
      "129/295, train_loss: 0.0360, step time: 1.0503\n",
      "130/295, train_loss: 0.0390, step time: 1.0579\n",
      "131/295, train_loss: 0.1066, step time: 1.0344\n",
      "132/295, train_loss: 0.0789, step time: 1.0675\n",
      "133/295, train_loss: 0.0592, step time: 1.0423\n",
      "134/295, train_loss: 0.0804, step time: 1.0653\n",
      "135/295, train_loss: 0.0665, step time: 1.0361\n",
      "136/295, train_loss: 0.0497, step time: 1.0462\n",
      "137/295, train_loss: 0.0671, step time: 1.0582\n",
      "138/295, train_loss: 0.0373, step time: 1.0517\n",
      "139/295, train_loss: 0.1274, step time: 1.0401\n",
      "140/295, train_loss: 0.1149, step time: 1.0567\n",
      "141/295, train_loss: 0.0596, step time: 1.0346\n",
      "142/295, train_loss: 0.0767, step time: 1.0395\n",
      "143/295, train_loss: 0.0336, step time: 1.0363\n",
      "144/295, train_loss: 0.3778, step time: 1.0339\n",
      "145/295, train_loss: 0.0849, step time: 1.0360\n",
      "146/295, train_loss: 0.0444, step time: 1.0350\n",
      "147/295, train_loss: 0.1073, step time: 1.0692\n",
      "148/295, train_loss: 0.3831, step time: 1.0428\n",
      "149/295, train_loss: 0.1169, step time: 1.0359\n",
      "150/295, train_loss: 0.0686, step time: 1.0396\n",
      "151/295, train_loss: 0.0378, step time: 1.0403\n",
      "152/295, train_loss: 0.4047, step time: 1.0333\n",
      "153/295, train_loss: 0.1070, step time: 1.0363\n",
      "154/295, train_loss: 0.0704, step time: 1.0615\n",
      "155/295, train_loss: 0.0657, step time: 1.0394\n",
      "156/295, train_loss: 0.0615, step time: 1.0453\n",
      "157/295, train_loss: 0.1195, step time: 1.0525\n",
      "158/295, train_loss: 0.0553, step time: 1.0483\n",
      "159/295, train_loss: 0.0521, step time: 1.0336\n",
      "160/295, train_loss: 0.1264, step time: 1.0416\n",
      "161/295, train_loss: 0.1002, step time: 1.0351\n",
      "162/295, train_loss: 0.0483, step time: 1.0436\n",
      "163/295, train_loss: 0.0975, step time: 1.0597\n",
      "164/295, train_loss: 0.1924, step time: 1.0379\n",
      "165/295, train_loss: 0.0571, step time: 1.0387\n",
      "166/295, train_loss: 0.0641, step time: 1.0780\n",
      "167/295, train_loss: 0.0945, step time: 1.0351\n",
      "168/295, train_loss: 0.1318, step time: 1.0812\n",
      "169/295, train_loss: 0.1354, step time: 1.0525\n",
      "170/295, train_loss: 0.0583, step time: 1.0450\n",
      "171/295, train_loss: 0.0741, step time: 1.0583\n",
      "172/295, train_loss: 0.1117, step time: 1.0825\n",
      "173/295, train_loss: 0.2713, step time: 1.0476\n",
      "174/295, train_loss: 0.4166, step time: 1.0551\n",
      "175/295, train_loss: 0.0573, step time: 1.0336\n",
      "176/295, train_loss: 0.0327, step time: 1.0362\n",
      "177/295, train_loss: 0.0568, step time: 1.0369\n",
      "178/295, train_loss: 0.0445, step time: 1.0392\n",
      "179/295, train_loss: 0.0983, step time: 1.0360\n",
      "180/295, train_loss: 0.3029, step time: 1.0489\n",
      "181/295, train_loss: 0.2115, step time: 1.0531\n",
      "182/295, train_loss: 0.0997, step time: 1.0389\n",
      "183/295, train_loss: 0.1157, step time: 1.0440\n",
      "184/295, train_loss: 0.1235, step time: 1.0670\n",
      "185/295, train_loss: 0.1041, step time: 1.0946\n",
      "186/295, train_loss: 0.0453, step time: 1.0778\n",
      "187/295, train_loss: 0.1275, step time: 1.0480\n",
      "188/295, train_loss: 0.0509, step time: 1.0380\n",
      "189/295, train_loss: 0.1849, step time: 1.0583\n",
      "190/295, train_loss: 0.0410, step time: 1.0448\n",
      "191/295, train_loss: 0.0529, step time: 1.0459\n",
      "192/295, train_loss: 0.0665, step time: 1.0707\n",
      "193/295, train_loss: 0.1188, step time: 1.0939\n",
      "194/295, train_loss: 0.3349, step time: 1.0606\n",
      "195/295, train_loss: 0.0904, step time: 1.0437\n",
      "196/295, train_loss: 0.1189, step time: 1.0637\n",
      "197/295, train_loss: 0.0968, step time: 1.0455\n",
      "198/295, train_loss: 0.0436, step time: 1.0421\n",
      "199/295, train_loss: 0.0650, step time: 1.0508\n",
      "200/295, train_loss: 0.0504, step time: 1.0573\n",
      "201/295, train_loss: 0.3938, step time: 1.0666\n",
      "202/295, train_loss: 0.1524, step time: 1.0373\n",
      "203/295, train_loss: 0.0388, step time: 1.0940\n",
      "204/295, train_loss: 0.2813, step time: 1.0508\n",
      "205/295, train_loss: 0.1400, step time: 1.0311\n",
      "206/295, train_loss: 0.0832, step time: 1.0524\n",
      "207/295, train_loss: 0.0714, step time: 1.0377\n",
      "208/295, train_loss: 0.0936, step time: 1.0436\n",
      "209/295, train_loss: 0.0368, step time: 1.0411\n",
      "210/295, train_loss: 0.0420, step time: 1.0371\n",
      "211/295, train_loss: 0.0759, step time: 1.0682\n",
      "212/295, train_loss: 0.0416, step time: 1.0330\n",
      "213/295, train_loss: 0.1114, step time: 1.0328\n",
      "214/295, train_loss: 0.0463, step time: 1.0409\n",
      "215/295, train_loss: 0.1174, step time: 1.0782\n",
      "216/295, train_loss: 0.0848, step time: 1.0581\n",
      "217/295, train_loss: 0.2008, step time: 1.0363\n",
      "218/295, train_loss: 0.0572, step time: 1.0371\n",
      "219/295, train_loss: 0.0417, step time: 1.0413\n",
      "220/295, train_loss: 0.0232, step time: 1.0493\n",
      "221/295, train_loss: 0.0941, step time: 1.0668\n",
      "222/295, train_loss: 0.0343, step time: 1.0353\n",
      "223/295, train_loss: 0.0788, step time: 1.0433\n",
      "224/295, train_loss: 0.0522, step time: 1.0774\n",
      "225/295, train_loss: 0.1389, step time: 1.0630\n",
      "226/295, train_loss: 0.0496, step time: 1.0692\n",
      "227/295, train_loss: 0.0601, step time: 1.0358\n",
      "228/295, train_loss: 0.0789, step time: 1.0372\n",
      "229/295, train_loss: 0.1587, step time: 1.0580\n",
      "230/295, train_loss: 0.0686, step time: 1.0436\n",
      "231/295, train_loss: 0.2082, step time: 1.0435\n",
      "232/295, train_loss: 0.0945, step time: 1.0465\n",
      "233/295, train_loss: 0.1031, step time: 1.0450\n",
      "234/295, train_loss: 0.1873, step time: 1.0404\n",
      "235/295, train_loss: 0.0569, step time: 1.0872\n",
      "236/295, train_loss: 0.0931, step time: 1.0494\n",
      "237/295, train_loss: 0.1163, step time: 1.0466\n",
      "238/295, train_loss: 0.1100, step time: 1.0345\n",
      "239/295, train_loss: 0.1198, step time: 1.0433\n",
      "240/295, train_loss: 0.1806, step time: 1.0458\n",
      "241/295, train_loss: 0.0631, step time: 1.0427\n",
      "242/295, train_loss: 0.1107, step time: 1.0644\n",
      "243/295, train_loss: 0.0908, step time: 1.0575\n",
      "244/295, train_loss: 0.0536, step time: 1.0680\n",
      "245/295, train_loss: 0.0869, step time: 1.0398\n",
      "246/295, train_loss: 0.0823, step time: 1.0792\n",
      "247/295, train_loss: 0.0549, step time: 1.0526\n",
      "248/295, train_loss: 0.0525, step time: 1.0592\n",
      "249/295, train_loss: 0.0557, step time: 1.0585\n",
      "250/295, train_loss: 0.0569, step time: 1.0358\n",
      "251/295, train_loss: 0.0499, step time: 1.0379\n",
      "252/295, train_loss: 0.1350, step time: 1.0328\n",
      "253/295, train_loss: 0.4055, step time: 1.0469\n",
      "254/295, train_loss: 0.2363, step time: 1.0873\n",
      "255/295, train_loss: 0.1312, step time: 1.0623\n",
      "256/295, train_loss: 0.1090, step time: 1.0929\n",
      "257/295, train_loss: 0.0559, step time: 1.0496\n",
      "258/295, train_loss: 0.4802, step time: 1.0674\n",
      "259/295, train_loss: 0.1019, step time: 1.0444\n",
      "260/295, train_loss: 0.4028, step time: 1.0378\n",
      "261/295, train_loss: 0.1540, step time: 1.0385\n",
      "262/295, train_loss: 0.0712, step time: 1.0464\n",
      "263/295, train_loss: 0.0976, step time: 1.0414\n",
      "264/295, train_loss: 0.1081, step time: 1.0405\n",
      "265/295, train_loss: 0.0733, step time: 1.1830\n",
      "266/295, train_loss: 0.3998, step time: 1.0364\n",
      "267/295, train_loss: 0.1341, step time: 1.0591\n",
      "268/295, train_loss: 0.0931, step time: 1.0798\n",
      "269/295, train_loss: 0.1323, step time: 1.0362\n",
      "270/295, train_loss: 0.0419, step time: 1.0405\n",
      "271/295, train_loss: 0.0656, step time: 1.0422\n",
      "272/295, train_loss: 0.0792, step time: 1.0421\n",
      "273/295, train_loss: 0.0993, step time: 1.0575\n",
      "274/295, train_loss: 0.0332, step time: 1.1002\n",
      "275/295, train_loss: 0.0524, step time: 1.0666\n",
      "276/295, train_loss: 0.3968, step time: 1.0401\n",
      "277/295, train_loss: 0.0440, step time: 1.1064\n",
      "278/295, train_loss: 0.0924, step time: 1.0961\n",
      "279/295, train_loss: 0.0533, step time: 1.0527\n",
      "280/295, train_loss: 0.4181, step time: 1.0386\n",
      "281/295, train_loss: 0.0343, step time: 1.0443\n",
      "282/295, train_loss: 0.1248, step time: 1.0338\n",
      "283/295, train_loss: 0.0732, step time: 1.0311\n",
      "284/295, train_loss: 0.1106, step time: 1.0367\n",
      "285/295, train_loss: 0.0374, step time: 1.0565\n",
      "286/295, train_loss: 0.0627, step time: 1.0528\n",
      "287/295, train_loss: 0.0440, step time: 1.0341\n",
      "288/295, train_loss: 0.3846, step time: 1.0412\n",
      "289/295, train_loss: 0.0786, step time: 1.0297\n",
      "290/295, train_loss: 0.0556, step time: 1.0286\n",
      "291/295, train_loss: 0.1036, step time: 1.0282\n",
      "292/295, train_loss: 0.0908, step time: 1.0288\n",
      "293/295, train_loss: 0.0422, step time: 1.0298\n",
      "294/295, train_loss: 0.0375, step time: 1.0294\n",
      "295/295, train_loss: 0.1322, step time: 1.0299\n",
      "epoch 40 average loss: 0.1118\n",
      "current epoch: 40 current mean dice: 0.7574 tc: 0.7099 wt: 0.8346 et: 0.7328\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 40 is: 383.5407\n",
      "----------\n",
      "epoch 41/100\n",
      "1/295, train_loss: 0.0507, step time: 1.1098\n",
      "2/295, train_loss: 0.0731, step time: 1.0934\n",
      "3/295, train_loss: 0.0673, step time: 1.0677\n",
      "4/295, train_loss: 0.0848, step time: 1.0721\n",
      "5/295, train_loss: 0.0342, step time: 1.0941\n",
      "6/295, train_loss: 0.0711, step time: 1.0519\n",
      "7/295, train_loss: 0.0517, step time: 1.0675\n",
      "8/295, train_loss: 0.4016, step time: 1.0416\n",
      "9/295, train_loss: 0.1244, step time: 1.0394\n",
      "10/295, train_loss: 0.0359, step time: 1.0420\n",
      "11/295, train_loss: 0.0989, step time: 1.0373\n",
      "12/295, train_loss: 0.0589, step time: 1.0336\n",
      "13/295, train_loss: 0.0646, step time: 1.0428\n",
      "14/295, train_loss: 0.0707, step time: 1.0553\n",
      "15/295, train_loss: 0.0375, step time: 1.0293\n",
      "16/295, train_loss: 0.0644, step time: 1.0469\n",
      "17/295, train_loss: 0.0875, step time: 1.0317\n",
      "18/295, train_loss: 0.2796, step time: 1.0564\n",
      "19/295, train_loss: 0.1527, step time: 1.0843\n",
      "20/295, train_loss: 0.0625, step time: 1.0413\n",
      "21/295, train_loss: 0.1143, step time: 1.1126\n",
      "22/295, train_loss: 0.0547, step time: 1.0610\n",
      "23/295, train_loss: 0.1017, step time: 1.0352\n",
      "24/295, train_loss: 0.0385, step time: 1.0359\n",
      "25/295, train_loss: 0.1355, step time: 1.0351\n",
      "26/295, train_loss: 0.1137, step time: 1.0526\n",
      "27/295, train_loss: 0.0759, step time: 1.0948\n",
      "28/295, train_loss: 0.1680, step time: 1.0401\n",
      "29/295, train_loss: 0.0634, step time: 1.0389\n",
      "30/295, train_loss: 0.1886, step time: 1.0585\n",
      "31/295, train_loss: 0.0630, step time: 1.0336\n",
      "32/295, train_loss: 0.0680, step time: 1.0649\n",
      "33/295, train_loss: 0.0624, step time: 1.0389\n",
      "34/295, train_loss: 0.1112, step time: 1.0360\n",
      "35/295, train_loss: 0.1276, step time: 1.0442\n",
      "36/295, train_loss: 0.1560, step time: 1.0899\n",
      "37/295, train_loss: 0.1712, step time: 1.0768\n",
      "38/295, train_loss: 0.1052, step time: 1.0418\n",
      "39/295, train_loss: 0.0903, step time: 1.0441\n",
      "40/295, train_loss: 0.3978, step time: 1.0421\n",
      "41/295, train_loss: 0.1231, step time: 1.0370\n",
      "42/295, train_loss: 0.0720, step time: 1.0398\n",
      "43/295, train_loss: 0.0521, step time: 1.0440\n",
      "44/295, train_loss: 0.0579, step time: 1.0412\n",
      "45/295, train_loss: 0.1390, step time: 1.0325\n",
      "46/295, train_loss: 0.0997, step time: 1.0587\n",
      "47/295, train_loss: 0.0923, step time: 1.0532\n",
      "48/295, train_loss: 0.0329, step time: 1.0425\n",
      "49/295, train_loss: 0.1142, step time: 1.0346\n",
      "50/295, train_loss: 0.1784, step time: 1.0589\n",
      "51/295, train_loss: 0.1180, step time: 1.0387\n",
      "52/295, train_loss: 0.1570, step time: 1.0627\n",
      "53/295, train_loss: 0.0875, step time: 1.0535\n",
      "54/295, train_loss: 0.0664, step time: 1.0594\n",
      "55/295, train_loss: 0.3338, step time: 1.0405\n",
      "56/295, train_loss: 0.0788, step time: 1.0539\n",
      "57/295, train_loss: 0.0556, step time: 1.0358\n",
      "58/295, train_loss: 0.0881, step time: 1.0534\n",
      "59/295, train_loss: 0.1224, step time: 1.0481\n",
      "60/295, train_loss: 0.0377, step time: 1.0399\n",
      "61/295, train_loss: 0.0232, step time: 1.0402\n",
      "62/295, train_loss: 0.0380, step time: 1.0385\n",
      "63/295, train_loss: 0.0900, step time: 1.0317\n",
      "64/295, train_loss: 0.0912, step time: 1.0361\n",
      "65/295, train_loss: 0.0711, step time: 1.0366\n",
      "66/295, train_loss: 0.0643, step time: 1.0299\n",
      "67/295, train_loss: 0.4144, step time: 1.0323\n",
      "68/295, train_loss: 0.4125, step time: 1.0458\n",
      "69/295, train_loss: 0.1133, step time: 1.0554\n",
      "70/295, train_loss: 0.0888, step time: 1.0355\n",
      "71/295, train_loss: 0.0505, step time: 1.0472\n",
      "72/295, train_loss: 0.0517, step time: 1.0308\n",
      "73/295, train_loss: 0.3801, step time: 1.0527\n",
      "74/295, train_loss: 0.0611, step time: 1.0426\n",
      "75/295, train_loss: 0.3631, step time: 1.0779\n",
      "76/295, train_loss: 0.0395, step time: 1.0402\n",
      "77/295, train_loss: 0.0935, step time: 1.0348\n",
      "78/295, train_loss: 0.0432, step time: 1.0422\n",
      "79/295, train_loss: 0.0495, step time: 1.0336\n",
      "80/295, train_loss: 0.1002, step time: 1.0354\n",
      "81/295, train_loss: 0.0340, step time: 1.0576\n",
      "82/295, train_loss: 0.0449, step time: 1.0778\n",
      "83/295, train_loss: 0.0681, step time: 1.0709\n",
      "84/295, train_loss: 0.0599, step time: 1.0575\n",
      "85/295, train_loss: 0.0916, step time: 1.0366\n",
      "86/295, train_loss: 0.0364, step time: 1.0357\n",
      "87/295, train_loss: 0.0380, step time: 1.0497\n",
      "88/295, train_loss: 0.0836, step time: 1.0400\n",
      "89/295, train_loss: 0.0463, step time: 1.0312\n",
      "90/295, train_loss: 0.1226, step time: 1.0365\n",
      "91/295, train_loss: 0.0387, step time: 1.0480\n",
      "92/295, train_loss: 0.0550, step time: 1.0553\n",
      "93/295, train_loss: 0.0545, step time: 1.0351\n",
      "94/295, train_loss: 0.0821, step time: 1.0390\n",
      "95/295, train_loss: 0.0558, step time: 1.0462\n",
      "96/295, train_loss: 0.0738, step time: 1.1261\n",
      "97/295, train_loss: 0.0481, step time: 1.0475\n",
      "98/295, train_loss: 0.0524, step time: 1.0401\n",
      "99/295, train_loss: 0.0516, step time: 1.0809\n",
      "100/295, train_loss: 0.1248, step time: 1.0478\n",
      "101/295, train_loss: 0.0364, step time: 1.0415\n",
      "102/295, train_loss: 0.0960, step time: 1.0823\n",
      "103/295, train_loss: 0.1916, step time: 1.0375\n",
      "104/295, train_loss: 0.1820, step time: 1.0995\n",
      "105/295, train_loss: 0.0443, step time: 1.0356\n",
      "106/295, train_loss: 0.0778, step time: 1.0342\n",
      "107/295, train_loss: 0.0846, step time: 1.0411\n",
      "108/295, train_loss: 0.0553, step time: 1.0700\n",
      "109/295, train_loss: 0.1406, step time: 1.0336\n",
      "110/295, train_loss: 0.1288, step time: 1.0431\n",
      "111/295, train_loss: 0.1569, step time: 1.0751\n",
      "112/295, train_loss: 0.0764, step time: 1.0489\n",
      "113/295, train_loss: 0.0745, step time: 1.0359\n",
      "114/295, train_loss: 0.0568, step time: 1.0465\n",
      "115/295, train_loss: 0.0495, step time: 1.0404\n",
      "116/295, train_loss: 0.0451, step time: 1.0701\n",
      "117/295, train_loss: 0.0694, step time: 1.0335\n",
      "118/295, train_loss: 0.0727, step time: 1.0357\n",
      "119/295, train_loss: 0.4064, step time: 1.0341\n",
      "120/295, train_loss: 0.0350, step time: 1.0368\n",
      "121/295, train_loss: 0.0667, step time: 1.0428\n",
      "122/295, train_loss: 0.0524, step time: 1.0326\n",
      "123/295, train_loss: 0.1307, step time: 1.0391\n",
      "124/295, train_loss: 0.4185, step time: 1.0359\n",
      "125/295, train_loss: 0.1364, step time: 1.0717\n",
      "126/295, train_loss: 0.0955, step time: 1.0424\n",
      "127/295, train_loss: 0.1513, step time: 1.0468\n",
      "128/295, train_loss: 0.0629, step time: 1.0404\n",
      "129/295, train_loss: 0.0308, step time: 1.0378\n",
      "130/295, train_loss: 0.1740, step time: 1.0332\n",
      "131/295, train_loss: 0.0608, step time: 1.0348\n",
      "132/295, train_loss: 0.0432, step time: 1.0321\n",
      "133/295, train_loss: 0.0720, step time: 1.0377\n",
      "134/295, train_loss: 0.3289, step time: 1.0487\n",
      "135/295, train_loss: 0.0803, step time: 1.0538\n",
      "136/295, train_loss: 0.1628, step time: 1.0392\n",
      "137/295, train_loss: 0.0911, step time: 1.0576\n",
      "138/295, train_loss: 0.0927, step time: 1.0389\n",
      "139/295, train_loss: 0.2326, step time: 1.0553\n",
      "140/295, train_loss: 0.0654, step time: 1.0671\n",
      "141/295, train_loss: 0.0974, step time: 1.0695\n",
      "142/295, train_loss: 0.0908, step time: 1.0391\n",
      "143/295, train_loss: 0.1269, step time: 1.0319\n",
      "144/295, train_loss: 0.0940, step time: 1.0674\n",
      "145/295, train_loss: 0.0589, step time: 1.0665\n",
      "146/295, train_loss: 0.0480, step time: 1.0461\n",
      "147/295, train_loss: 0.0831, step time: 1.0319\n",
      "148/295, train_loss: 0.0587, step time: 1.0386\n",
      "149/295, train_loss: 0.0729, step time: 1.0416\n",
      "150/295, train_loss: 0.1125, step time: 1.0405\n",
      "151/295, train_loss: 0.0611, step time: 1.1167\n",
      "152/295, train_loss: 0.0983, step time: 1.0389\n",
      "153/295, train_loss: 0.1218, step time: 1.0455\n",
      "154/295, train_loss: 0.1013, step time: 1.0954\n",
      "155/295, train_loss: 0.0786, step time: 1.0329\n",
      "156/295, train_loss: 0.0523, step time: 1.0490\n",
      "157/295, train_loss: 0.4978, step time: 1.0532\n",
      "158/295, train_loss: 0.0903, step time: 1.1373\n",
      "159/295, train_loss: 0.0493, step time: 1.0463\n",
      "160/295, train_loss: 0.1980, step time: 1.0359\n",
      "161/295, train_loss: 0.1014, step time: 1.0820\n",
      "162/295, train_loss: 0.3782, step time: 1.0402\n",
      "163/295, train_loss: 0.2151, step time: 1.0409\n",
      "164/295, train_loss: 0.0798, step time: 1.0468\n",
      "165/295, train_loss: 0.0354, step time: 1.0450\n",
      "166/295, train_loss: 0.1169, step time: 1.0684\n",
      "167/295, train_loss: 0.0798, step time: 1.0691\n",
      "168/295, train_loss: 0.3668, step time: 1.0431\n",
      "169/295, train_loss: 0.0887, step time: 1.0387\n",
      "170/295, train_loss: 0.3743, step time: 1.0908\n",
      "171/295, train_loss: 0.0480, step time: 1.0446\n",
      "172/295, train_loss: 0.0678, step time: 1.0442\n",
      "173/295, train_loss: 0.1118, step time: 1.0567\n",
      "174/295, train_loss: 0.0405, step time: 1.0795\n",
      "175/295, train_loss: 0.0710, step time: 1.0473\n",
      "176/295, train_loss: 0.0335, step time: 1.0580\n",
      "177/295, train_loss: 0.0927, step time: 1.0427\n",
      "178/295, train_loss: 0.0626, step time: 1.0463\n",
      "179/295, train_loss: 0.3968, step time: 1.0970\n",
      "180/295, train_loss: 0.1214, step time: 1.0649\n",
      "181/295, train_loss: 0.1043, step time: 1.0357\n",
      "182/295, train_loss: 0.0913, step time: 1.0455\n",
      "183/295, train_loss: 0.0656, step time: 1.0565\n",
      "184/295, train_loss: 0.1895, step time: 1.0527\n",
      "185/295, train_loss: 0.0622, step time: 1.0378\n",
      "186/295, train_loss: 0.0539, step time: 1.0432\n",
      "187/295, train_loss: 0.0631, step time: 1.0772\n",
      "188/295, train_loss: 0.0664, step time: 1.0729\n",
      "189/295, train_loss: 0.0348, step time: 1.0410\n",
      "190/295, train_loss: 0.0828, step time: 1.0796\n",
      "191/295, train_loss: 0.0488, step time: 1.0944\n",
      "192/295, train_loss: 0.0316, step time: 1.0637\n",
      "193/295, train_loss: 0.3859, step time: 1.1099\n",
      "194/295, train_loss: 0.0503, step time: 1.0358\n",
      "195/295, train_loss: 0.1514, step time: 1.0396\n",
      "196/295, train_loss: 0.0391, step time: 1.0416\n",
      "197/295, train_loss: 0.0411, step time: 1.0440\n",
      "198/295, train_loss: 0.4446, step time: 1.0565\n",
      "199/295, train_loss: 0.1387, step time: 1.0687\n",
      "200/295, train_loss: 0.0917, step time: 1.0480\n",
      "201/295, train_loss: 0.0756, step time: 1.0350\n",
      "202/295, train_loss: 0.0461, step time: 1.0516\n",
      "203/295, train_loss: 0.0766, step time: 1.0543\n",
      "204/295, train_loss: 0.0559, step time: 1.0827\n",
      "205/295, train_loss: 0.0861, step time: 1.0734\n",
      "206/295, train_loss: 0.0384, step time: 1.0438\n",
      "207/295, train_loss: 0.1314, step time: 1.0654\n",
      "208/295, train_loss: 0.0748, step time: 1.0393\n",
      "209/295, train_loss: 0.0772, step time: 1.0637\n",
      "210/295, train_loss: 0.1813, step time: 1.0338\n",
      "211/295, train_loss: 0.1191, step time: 1.0328\n",
      "212/295, train_loss: 0.1059, step time: 1.0906\n",
      "213/295, train_loss: 0.1475, step time: 1.0424\n",
      "214/295, train_loss: 0.0338, step time: 1.0688\n",
      "215/295, train_loss: 0.0400, step time: 1.0419\n",
      "216/295, train_loss: 0.4020, step time: 1.0415\n",
      "217/295, train_loss: 0.1557, step time: 1.0411\n",
      "218/295, train_loss: 0.1242, step time: 1.0425\n",
      "219/295, train_loss: 0.1451, step time: 1.0437\n",
      "220/295, train_loss: 0.0400, step time: 1.0686\n",
      "221/295, train_loss: 0.0674, step time: 1.0450\n",
      "222/295, train_loss: 0.0491, step time: 1.0477\n",
      "223/295, train_loss: 0.0663, step time: 1.0846\n",
      "224/295, train_loss: 0.1234, step time: 1.0631\n",
      "225/295, train_loss: 0.1526, step time: 1.0589\n",
      "226/295, train_loss: 0.1063, step time: 1.0317\n",
      "227/295, train_loss: 0.0789, step time: 1.0557\n",
      "228/295, train_loss: 0.0602, step time: 1.0611\n",
      "229/295, train_loss: 0.1141, step time: 1.0329\n",
      "230/295, train_loss: 0.0928, step time: 1.0338\n",
      "231/295, train_loss: 0.1065, step time: 1.0471\n",
      "232/295, train_loss: 0.0842, step time: 1.0414\n",
      "233/295, train_loss: 0.0692, step time: 1.0338\n",
      "234/295, train_loss: 0.0647, step time: 1.0520\n",
      "235/295, train_loss: 0.0518, step time: 1.0385\n",
      "236/295, train_loss: 0.4220, step time: 1.0598\n",
      "237/295, train_loss: 0.0927, step time: 1.0566\n",
      "238/295, train_loss: 0.3823, step time: 1.0577\n",
      "239/295, train_loss: 0.1080, step time: 1.0323\n",
      "240/295, train_loss: 0.0915, step time: 1.0382\n",
      "241/295, train_loss: 0.4209, step time: 1.0428\n",
      "242/295, train_loss: 0.0365, step time: 1.0519\n",
      "243/295, train_loss: 0.1097, step time: 1.0742\n",
      "244/295, train_loss: 0.4117, step time: 1.0406\n",
      "245/295, train_loss: 0.1021, step time: 1.1176\n",
      "246/295, train_loss: 0.0498, step time: 1.0497\n",
      "247/295, train_loss: 0.0694, step time: 1.0411\n",
      "248/295, train_loss: 0.0522, step time: 1.0339\n",
      "249/295, train_loss: 0.0379, step time: 1.0441\n",
      "250/295, train_loss: 0.0444, step time: 1.0462\n",
      "251/295, train_loss: 0.0652, step time: 1.0604\n",
      "252/295, train_loss: 0.0807, step time: 1.0316\n",
      "253/295, train_loss: 0.0713, step time: 1.0386\n",
      "254/295, train_loss: 0.3831, step time: 1.0500\n",
      "255/295, train_loss: 0.0795, step time: 1.0412\n",
      "256/295, train_loss: 0.2003, step time: 1.0391\n",
      "257/295, train_loss: 0.0972, step time: 1.0331\n",
      "258/295, train_loss: 0.0499, step time: 1.1185\n",
      "259/295, train_loss: 0.0462, step time: 1.0348\n",
      "260/295, train_loss: 0.0968, step time: 1.0383\n",
      "261/295, train_loss: 0.0449, step time: 1.0696\n",
      "262/295, train_loss: 0.0655, step time: 1.0386\n",
      "263/295, train_loss: 0.1310, step time: 1.0855\n",
      "264/295, train_loss: 0.0664, step time: 1.0359\n",
      "265/295, train_loss: 0.1204, step time: 1.0389\n",
      "266/295, train_loss: 0.0824, step time: 1.0319\n",
      "267/295, train_loss: 0.1107, step time: 1.0348\n",
      "268/295, train_loss: 0.3775, step time: 1.0334\n",
      "269/295, train_loss: 0.1069, step time: 1.0633\n",
      "270/295, train_loss: 0.1282, step time: 1.0378\n",
      "271/295, train_loss: 0.0638, step time: 1.0379\n",
      "272/295, train_loss: 0.1143, step time: 1.0375\n",
      "273/295, train_loss: 0.0515, step time: 1.0677\n",
      "274/295, train_loss: 0.0363, step time: 1.0400\n",
      "275/295, train_loss: 0.0355, step time: 1.0369\n",
      "276/295, train_loss: 0.1073, step time: 1.0559\n",
      "277/295, train_loss: 0.1214, step time: 1.0404\n",
      "278/295, train_loss: 0.3874, step time: 1.0467\n",
      "279/295, train_loss: 0.0721, step time: 1.0636\n",
      "280/295, train_loss: 0.0799, step time: 1.0382\n",
      "281/295, train_loss: 0.1060, step time: 1.0473\n",
      "282/295, train_loss: 0.0754, step time: 1.0422\n",
      "283/295, train_loss: 0.0430, step time: 1.0439\n",
      "284/295, train_loss: 0.0400, step time: 1.0878\n",
      "285/295, train_loss: 0.0789, step time: 1.0451\n",
      "286/295, train_loss: 0.1093, step time: 1.0646\n",
      "287/295, train_loss: 0.0501, step time: 1.0493\n",
      "288/295, train_loss: 0.0472, step time: 1.0326\n",
      "289/295, train_loss: 0.1144, step time: 1.1037\n",
      "290/295, train_loss: 0.1658, step time: 1.0293\n",
      "291/295, train_loss: 0.0641, step time: 1.0285\n",
      "292/295, train_loss: 0.0799, step time: 1.0294\n",
      "293/295, train_loss: 0.2185, step time: 1.0301\n",
      "294/295, train_loss: 0.0485, step time: 1.0292\n",
      "295/295, train_loss: 0.0525, step time: 1.0312\n",
      "epoch 41 average loss: 0.1118\n",
      "current epoch: 41 current mean dice: 0.7623 tc: 0.7083 wt: 0.8401 et: 0.7463\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 41 is: 389.2434\n",
      "----------\n",
      "epoch 42/100\n",
      "1/295, train_loss: 0.2061, step time: 1.1659\n",
      "2/295, train_loss: 0.0890, step time: 1.0822\n",
      "3/295, train_loss: 0.0571, step time: 1.0357\n",
      "4/295, train_loss: 0.0786, step time: 1.0401\n",
      "5/295, train_loss: 0.0493, step time: 1.0529\n",
      "6/295, train_loss: 0.0713, step time: 1.1146\n",
      "7/295, train_loss: 0.0917, step time: 1.0488\n",
      "8/295, train_loss: 0.0292, step time: 1.0334\n",
      "9/295, train_loss: 0.3629, step time: 1.0594\n",
      "10/295, train_loss: 0.0476, step time: 1.0492\n",
      "11/295, train_loss: 0.0542, step time: 1.0634\n",
      "12/295, train_loss: 0.3809, step time: 1.0860\n",
      "13/295, train_loss: 0.0395, step time: 1.0413\n",
      "14/295, train_loss: 0.0780, step time: 1.0765\n",
      "15/295, train_loss: 0.0483, step time: 1.0595\n",
      "16/295, train_loss: 0.1400, step time: 1.0345\n",
      "17/295, train_loss: 0.0978, step time: 1.0699\n",
      "18/295, train_loss: 0.0451, step time: 1.1459\n",
      "19/295, train_loss: 0.0661, step time: 1.0325\n",
      "20/295, train_loss: 0.0394, step time: 1.0408\n",
      "21/295, train_loss: 0.0958, step time: 1.0518\n",
      "22/295, train_loss: 0.1008, step time: 1.0443\n",
      "23/295, train_loss: 0.0573, step time: 1.0728\n",
      "24/295, train_loss: 0.0681, step time: 1.0403\n",
      "25/295, train_loss: 0.3855, step time: 1.0673\n",
      "26/295, train_loss: 0.1004, step time: 1.0449\n",
      "27/295, train_loss: 0.0395, step time: 1.0377\n",
      "28/295, train_loss: 0.1629, step time: 1.0434\n",
      "29/295, train_loss: 0.0480, step time: 1.0411\n",
      "30/295, train_loss: 0.1350, step time: 1.0593\n",
      "31/295, train_loss: 0.0496, step time: 1.0778\n",
      "32/295, train_loss: 0.0355, step time: 1.0359\n",
      "33/295, train_loss: 0.3858, step time: 1.0466\n",
      "34/295, train_loss: 0.0747, step time: 1.0382\n",
      "35/295, train_loss: 0.0333, step time: 1.0552\n",
      "36/295, train_loss: 0.0930, step time: 1.0626\n",
      "37/295, train_loss: 0.0879, step time: 1.0493\n",
      "38/295, train_loss: 0.0375, step time: 1.0371\n",
      "39/295, train_loss: 0.0559, step time: 1.0340\n",
      "40/295, train_loss: 0.3800, step time: 1.0468\n",
      "41/295, train_loss: 0.0869, step time: 1.0597\n",
      "42/295, train_loss: 0.0599, step time: 1.0319\n",
      "43/295, train_loss: 0.0464, step time: 1.0607\n",
      "44/295, train_loss: 0.0960, step time: 1.0810\n",
      "45/295, train_loss: 0.0555, step time: 1.0523\n",
      "46/295, train_loss: 0.3746, step time: 1.0342\n",
      "47/295, train_loss: 0.0551, step time: 1.0395\n",
      "48/295, train_loss: 0.0473, step time: 1.0313\n",
      "49/295, train_loss: 0.0703, step time: 1.0392\n",
      "50/295, train_loss: 0.1559, step time: 1.0423\n",
      "51/295, train_loss: 0.0332, step time: 1.0564\n",
      "52/295, train_loss: 0.4157, step time: 1.0492\n",
      "53/295, train_loss: 0.4035, step time: 1.0507\n",
      "54/295, train_loss: 0.0508, step time: 1.0536\n",
      "55/295, train_loss: 0.0360, step time: 1.0976\n",
      "56/295, train_loss: 0.0475, step time: 1.1115\n",
      "57/295, train_loss: 0.0616, step time: 1.1152\n",
      "58/295, train_loss: 0.0360, step time: 1.0591\n",
      "59/295, train_loss: 0.0615, step time: 1.0438\n",
      "60/295, train_loss: 0.1168, step time: 1.0566\n",
      "61/295, train_loss: 0.0655, step time: 1.0311\n",
      "62/295, train_loss: 0.0641, step time: 1.0338\n",
      "63/295, train_loss: 0.0887, step time: 1.0436\n",
      "64/295, train_loss: 0.0811, step time: 1.0539\n",
      "65/295, train_loss: 0.1444, step time: 1.0627\n",
      "66/295, train_loss: 0.1902, step time: 1.0634\n",
      "67/295, train_loss: 0.3628, step time: 1.0586\n",
      "68/295, train_loss: 0.0488, step time: 1.0909\n",
      "69/295, train_loss: 0.4220, step time: 1.0338\n",
      "70/295, train_loss: 0.0726, step time: 1.0468\n",
      "71/295, train_loss: 0.0329, step time: 1.0476\n",
      "72/295, train_loss: 0.0442, step time: 1.0335\n",
      "73/295, train_loss: 0.0760, step time: 1.0395\n",
      "74/295, train_loss: 0.0733, step time: 1.0425\n",
      "75/295, train_loss: 0.1127, step time: 1.0872\n",
      "76/295, train_loss: 0.1860, step time: 1.0357\n",
      "77/295, train_loss: 0.0445, step time: 1.0553\n",
      "78/295, train_loss: 0.1116, step time: 1.0374\n",
      "79/295, train_loss: 0.0473, step time: 1.0550\n",
      "80/295, train_loss: 0.0357, step time: 1.0538\n",
      "81/295, train_loss: 0.0352, step time: 1.0385\n",
      "82/295, train_loss: 0.1285, step time: 1.1298\n",
      "83/295, train_loss: 0.1239, step time: 1.0715\n",
      "84/295, train_loss: 0.1206, step time: 1.0375\n",
      "85/295, train_loss: 0.0881, step time: 1.0303\n",
      "86/295, train_loss: 0.0808, step time: 1.0347\n",
      "87/295, train_loss: 0.0978, step time: 1.0768\n",
      "88/295, train_loss: 0.0614, step time: 1.0785\n",
      "89/295, train_loss: 0.0881, step time: 1.0378\n",
      "90/295, train_loss: 0.0752, step time: 1.0369\n",
      "91/295, train_loss: 0.0625, step time: 1.0430\n",
      "92/295, train_loss: 0.0921, step time: 1.0377\n",
      "93/295, train_loss: 0.0441, step time: 1.0540\n",
      "94/295, train_loss: 0.1245, step time: 1.0500\n",
      "95/295, train_loss: 0.0674, step time: 1.0832\n",
      "96/295, train_loss: 0.0356, step time: 1.0662\n",
      "97/295, train_loss: 0.0380, step time: 1.0609\n",
      "98/295, train_loss: 0.1103, step time: 1.0628\n",
      "99/295, train_loss: 0.0735, step time: 1.1004\n",
      "100/295, train_loss: 0.0861, step time: 1.0363\n",
      "101/295, train_loss: 0.0976, step time: 1.0373\n",
      "102/295, train_loss: 0.0356, step time: 1.0321\n",
      "103/295, train_loss: 0.0342, step time: 1.0958\n",
      "104/295, train_loss: 0.0430, step time: 1.0775\n",
      "105/295, train_loss: 0.0321, step time: 1.0333\n",
      "106/295, train_loss: 0.0698, step time: 1.0625\n",
      "107/295, train_loss: 0.1481, step time: 1.0824\n",
      "108/295, train_loss: 0.0683, step time: 1.0664\n",
      "109/295, train_loss: 0.4112, step time: 1.0376\n",
      "110/295, train_loss: 0.3869, step time: 1.0391\n",
      "111/295, train_loss: 0.0762, step time: 1.0496\n",
      "112/295, train_loss: 0.0717, step time: 1.0526\n",
      "113/295, train_loss: 0.0810, step time: 1.0393\n",
      "114/295, train_loss: 0.0584, step time: 1.0636\n",
      "115/295, train_loss: 0.0496, step time: 1.0744\n",
      "116/295, train_loss: 0.0919, step time: 1.0445\n",
      "117/295, train_loss: 0.1076, step time: 1.0696\n",
      "118/295, train_loss: 0.0588, step time: 1.0368\n",
      "119/295, train_loss: 0.0423, step time: 1.0391\n",
      "120/295, train_loss: 0.0553, step time: 1.0432\n",
      "121/295, train_loss: 0.1716, step time: 1.0649\n",
      "122/295, train_loss: 0.0928, step time: 1.1036\n",
      "123/295, train_loss: 0.0603, step time: 1.0684\n",
      "124/295, train_loss: 0.0876, step time: 1.0424\n",
      "125/295, train_loss: 0.1031, step time: 1.0414\n",
      "126/295, train_loss: 0.0819, step time: 1.0369\n",
      "127/295, train_loss: 0.1226, step time: 1.0414\n",
      "128/295, train_loss: 0.0339, step time: 1.0949\n",
      "129/295, train_loss: 0.0353, step time: 1.0418\n",
      "130/295, train_loss: 0.0420, step time: 1.0450\n",
      "131/295, train_loss: 0.0334, step time: 1.0935\n",
      "132/295, train_loss: 0.1209, step time: 1.0509\n",
      "133/295, train_loss: 0.1015, step time: 1.0450\n",
      "134/295, train_loss: 0.1060, step time: 1.0458\n",
      "135/295, train_loss: 0.3961, step time: 1.0345\n",
      "136/295, train_loss: 0.1026, step time: 1.0372\n",
      "137/295, train_loss: 0.0763, step time: 1.0496\n",
      "138/295, train_loss: 0.1437, step time: 1.0542\n",
      "139/295, train_loss: 0.0572, step time: 1.0446\n",
      "140/295, train_loss: 0.1000, step time: 1.0544\n",
      "141/295, train_loss: 0.0933, step time: 1.0582\n",
      "142/295, train_loss: 0.0387, step time: 1.0390\n",
      "143/295, train_loss: 0.1249, step time: 1.0989\n",
      "144/295, train_loss: 0.0542, step time: 1.0325\n",
      "145/295, train_loss: 0.0229, step time: 1.0460\n",
      "146/295, train_loss: 0.1215, step time: 1.0365\n",
      "147/295, train_loss: 0.0572, step time: 1.0394\n",
      "148/295, train_loss: 0.1371, step time: 1.0411\n",
      "149/295, train_loss: 0.1202, step time: 1.0458\n",
      "150/295, train_loss: 0.0799, step time: 1.0463\n",
      "151/295, train_loss: 0.3925, step time: 1.0655\n",
      "152/295, train_loss: 0.0380, step time: 1.0332\n",
      "153/295, train_loss: 0.1397, step time: 1.0517\n",
      "154/295, train_loss: 0.0560, step time: 1.0377\n",
      "155/295, train_loss: 0.0921, step time: 1.0371\n",
      "156/295, train_loss: 0.3772, step time: 1.0366\n",
      "157/295, train_loss: 0.3984, step time: 1.0418\n",
      "158/295, train_loss: 0.0644, step time: 1.0405\n",
      "159/295, train_loss: 0.0629, step time: 1.0390\n",
      "160/295, train_loss: 0.0763, step time: 1.0550\n",
      "161/295, train_loss: 0.1253, step time: 1.0373\n",
      "162/295, train_loss: 0.1043, step time: 1.0592\n",
      "163/295, train_loss: 0.1177, step time: 1.0526\n",
      "164/295, train_loss: 0.0753, step time: 1.0551\n",
      "165/295, train_loss: 0.0959, step time: 1.0447\n",
      "166/295, train_loss: 0.0324, step time: 1.0416\n",
      "167/295, train_loss: 0.0561, step time: 1.0483\n",
      "168/295, train_loss: 0.0660, step time: 1.0381\n",
      "169/295, train_loss: 0.0480, step time: 1.0340\n",
      "170/295, train_loss: 0.0546, step time: 1.0331\n",
      "171/295, train_loss: 0.0601, step time: 1.0384\n",
      "172/295, train_loss: 0.0895, step time: 1.0360\n",
      "173/295, train_loss: 0.3992, step time: 1.0381\n",
      "174/295, train_loss: 0.0406, step time: 1.0662\n",
      "175/295, train_loss: 0.0764, step time: 1.0565\n",
      "176/295, train_loss: 0.0770, step time: 1.0553\n",
      "177/295, train_loss: 0.1371, step time: 1.0452\n",
      "178/295, train_loss: 0.4167, step time: 1.0400\n",
      "179/295, train_loss: 0.0509, step time: 1.0423\n",
      "180/295, train_loss: 0.0466, step time: 1.0581\n",
      "181/295, train_loss: 0.0575, step time: 1.0452\n",
      "182/295, train_loss: 0.0737, step time: 1.1395\n",
      "183/295, train_loss: 0.0396, step time: 1.0830\n",
      "184/295, train_loss: 0.1082, step time: 1.0390\n",
      "185/295, train_loss: 0.1078, step time: 1.0523\n",
      "186/295, train_loss: 0.0891, step time: 1.0404\n",
      "187/295, train_loss: 0.1409, step time: 1.0407\n",
      "188/295, train_loss: 0.0467, step time: 1.0409\n",
      "189/295, train_loss: 0.0375, step time: 1.0603\n",
      "190/295, train_loss: 0.0652, step time: 1.0383\n",
      "191/295, train_loss: 0.1570, step time: 1.0394\n",
      "192/295, train_loss: 0.0636, step time: 1.0332\n",
      "193/295, train_loss: 0.0463, step time: 1.0473\n",
      "194/295, train_loss: 0.0438, step time: 1.0381\n",
      "195/295, train_loss: 0.0458, step time: 1.0470\n",
      "196/295, train_loss: 0.0931, step time: 1.0418\n",
      "197/295, train_loss: 0.0904, step time: 1.1197\n",
      "198/295, train_loss: 0.0471, step time: 1.0412\n",
      "199/295, train_loss: 0.1663, step time: 1.0745\n",
      "200/295, train_loss: 0.0528, step time: 1.0548\n",
      "201/295, train_loss: 0.0904, step time: 1.0526\n",
      "202/295, train_loss: 0.0698, step time: 1.0615\n",
      "203/295, train_loss: 0.0935, step time: 1.0353\n",
      "204/295, train_loss: 0.0592, step time: 1.0406\n",
      "205/295, train_loss: 0.0725, step time: 1.0410\n",
      "206/295, train_loss: 0.0777, step time: 1.0368\n",
      "207/295, train_loss: 0.1306, step time: 1.0430\n",
      "208/295, train_loss: 0.1769, step time: 1.0492\n",
      "209/295, train_loss: 0.0415, step time: 1.0374\n",
      "210/295, train_loss: 0.0661, step time: 1.0401\n",
      "211/295, train_loss: 0.0572, step time: 1.0524\n",
      "212/295, train_loss: 0.0422, step time: 1.0611\n",
      "213/295, train_loss: 0.0942, step time: 1.0488\n",
      "214/295, train_loss: 0.0425, step time: 1.0396\n",
      "215/295, train_loss: 0.0578, step time: 1.1098\n",
      "216/295, train_loss: 0.1471, step time: 1.0493\n",
      "217/295, train_loss: 0.1190, step time: 1.0446\n",
      "218/295, train_loss: 0.0545, step time: 1.0852\n",
      "219/295, train_loss: 0.0562, step time: 1.0422\n",
      "220/295, train_loss: 0.0866, step time: 1.0403\n",
      "221/295, train_loss: 0.0640, step time: 1.0371\n",
      "222/295, train_loss: 0.4006, step time: 1.0436\n",
      "223/295, train_loss: 0.1236, step time: 1.1408\n",
      "224/295, train_loss: 0.0928, step time: 1.0376\n",
      "225/295, train_loss: 0.0955, step time: 1.0471\n",
      "226/295, train_loss: 0.4917, step time: 1.0406\n",
      "227/295, train_loss: 0.0633, step time: 1.0333\n",
      "228/295, train_loss: 0.0453, step time: 1.0347\n",
      "229/295, train_loss: 0.0520, step time: 1.0467\n",
      "230/295, train_loss: 0.2311, step time: 1.0519\n",
      "231/295, train_loss: 0.1065, step time: 1.0955\n",
      "232/295, train_loss: 0.0462, step time: 1.0336\n",
      "233/295, train_loss: 0.0589, step time: 1.0396\n",
      "234/295, train_loss: 0.0345, step time: 1.0347\n",
      "235/295, train_loss: 0.1221, step time: 1.0456\n",
      "236/295, train_loss: 0.1763, step time: 1.1043\n",
      "237/295, train_loss: 0.0530, step time: 1.0538\n",
      "238/295, train_loss: 0.0792, step time: 1.0339\n",
      "239/295, train_loss: 0.0820, step time: 1.0513\n",
      "240/295, train_loss: 0.0380, step time: 1.0490\n",
      "241/295, train_loss: 0.0583, step time: 1.0581\n",
      "242/295, train_loss: 0.1328, step time: 1.0746\n",
      "243/295, train_loss: 0.0501, step time: 1.0467\n",
      "244/295, train_loss: 0.0902, step time: 1.0361\n",
      "245/295, train_loss: 0.1085, step time: 1.0647\n",
      "246/295, train_loss: 0.0982, step time: 1.0359\n",
      "247/295, train_loss: 0.0657, step time: 1.0351\n",
      "248/295, train_loss: 0.0439, step time: 1.0395\n",
      "249/295, train_loss: 0.1893, step time: 1.0399\n",
      "250/295, train_loss: 0.0968, step time: 1.0360\n",
      "251/295, train_loss: 0.0903, step time: 1.0706\n",
      "252/295, train_loss: 0.0586, step time: 1.0559\n",
      "253/295, train_loss: 0.1181, step time: 1.0401\n",
      "254/295, train_loss: 0.0831, step time: 1.0345\n",
      "255/295, train_loss: 0.3332, step time: 1.0639\n",
      "256/295, train_loss: 0.4089, step time: 1.0381\n",
      "257/295, train_loss: 0.1148, step time: 1.0451\n",
      "258/295, train_loss: 0.1029, step time: 1.0557\n",
      "259/295, train_loss: 0.0686, step time: 1.0390\n",
      "260/295, train_loss: 0.1158, step time: 1.0509\n",
      "261/295, train_loss: 0.0335, step time: 1.0561\n",
      "262/295, train_loss: 0.0808, step time: 1.0360\n",
      "263/295, train_loss: 0.0434, step time: 1.0416\n",
      "264/295, train_loss: 0.3548, step time: 1.0637\n",
      "265/295, train_loss: 0.0505, step time: 1.0820\n",
      "266/295, train_loss: 0.1543, step time: 1.0322\n",
      "267/295, train_loss: 0.0841, step time: 1.0390\n",
      "268/295, train_loss: 0.0922, step time: 1.0339\n",
      "269/295, train_loss: 0.4186, step time: 1.0877\n",
      "270/295, train_loss: 0.0444, step time: 1.1507\n",
      "271/295, train_loss: 0.3745, step time: 1.0454\n",
      "272/295, train_loss: 0.1236, step time: 1.0402\n",
      "273/295, train_loss: 0.1398, step time: 1.0330\n",
      "274/295, train_loss: 0.0897, step time: 1.0582\n",
      "275/295, train_loss: 0.0825, step time: 1.0435\n",
      "276/295, train_loss: 0.0987, step time: 1.0352\n",
      "277/295, train_loss: 0.0739, step time: 1.0454\n",
      "278/295, train_loss: 0.0937, step time: 1.0499\n",
      "279/295, train_loss: 0.0461, step time: 1.1023\n",
      "280/295, train_loss: 0.1115, step time: 1.0372\n",
      "281/295, train_loss: 0.0694, step time: 1.0373\n",
      "282/295, train_loss: 0.1020, step time: 1.0337\n",
      "283/295, train_loss: 0.1541, step time: 1.0504\n",
      "284/295, train_loss: 0.0587, step time: 1.0627\n",
      "285/295, train_loss: 0.1241, step time: 1.0584\n",
      "286/295, train_loss: 0.0323, step time: 1.0589\n",
      "287/295, train_loss: 0.0673, step time: 1.0667\n",
      "288/295, train_loss: 0.0611, step time: 1.0308\n",
      "289/295, train_loss: 0.1064, step time: 1.0292\n",
      "290/295, train_loss: 0.0560, step time: 1.0294\n",
      "291/295, train_loss: 0.1193, step time: 1.0302\n",
      "292/295, train_loss: 0.1677, step time: 1.0299\n",
      "293/295, train_loss: 0.0590, step time: 1.0304\n",
      "294/295, train_loss: 0.0647, step time: 1.0296\n",
      "295/295, train_loss: 0.1137, step time: 1.0297\n",
      "epoch 42 average loss: 0.1071\n",
      "current epoch: 42 current mean dice: 0.7817 tc: 0.7359 wt: 0.8479 et: 0.7653\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 42 is: 388.2998\n",
      "----------\n",
      "epoch 43/100\n",
      "1/295, train_loss: 0.3787, step time: 1.1664\n",
      "2/295, train_loss: 0.1361, step time: 1.0515\n",
      "3/295, train_loss: 0.0964, step time: 1.0675\n",
      "4/295, train_loss: 0.0736, step time: 1.1163\n",
      "5/295, train_loss: 0.3753, step time: 1.0531\n",
      "6/295, train_loss: 0.0800, step time: 1.0316\n",
      "7/295, train_loss: 0.0982, step time: 1.1251\n",
      "8/295, train_loss: 0.0569, step time: 1.0438\n",
      "9/295, train_loss: 0.0998, step time: 1.0477\n",
      "10/295, train_loss: 0.0913, step time: 1.0310\n",
      "11/295, train_loss: 0.1042, step time: 1.0435\n",
      "12/295, train_loss: 0.0404, step time: 1.0498\n",
      "13/295, train_loss: 0.0538, step time: 1.0394\n",
      "14/295, train_loss: 0.1675, step time: 1.0315\n",
      "15/295, train_loss: 0.0659, step time: 1.0422\n",
      "16/295, train_loss: 0.3959, step time: 1.0440\n",
      "17/295, train_loss: 0.1582, step time: 1.0543\n",
      "18/295, train_loss: 0.1318, step time: 1.0506\n",
      "19/295, train_loss: 0.0306, step time: 1.0513\n",
      "20/295, train_loss: 0.4103, step time: 1.0803\n",
      "21/295, train_loss: 0.0555, step time: 1.0380\n",
      "22/295, train_loss: 0.1452, step time: 1.0346\n",
      "23/295, train_loss: 0.0346, step time: 1.0352\n",
      "24/295, train_loss: 0.1091, step time: 1.0629\n",
      "25/295, train_loss: 0.1279, step time: 1.0374\n",
      "26/295, train_loss: 0.0521, step time: 1.0428\n",
      "27/295, train_loss: 0.0853, step time: 1.1619\n",
      "28/295, train_loss: 0.2322, step time: 1.0197\n",
      "29/295, train_loss: 0.1039, step time: 1.0446\n",
      "30/295, train_loss: 0.0386, step time: 1.0768\n",
      "31/295, train_loss: 0.4187, step time: 1.0610\n",
      "32/295, train_loss: 0.0696, step time: 1.0407\n",
      "33/295, train_loss: 0.0908, step time: 1.0366\n",
      "34/295, train_loss: 0.0714, step time: 1.0456\n",
      "35/295, train_loss: 0.0869, step time: 1.0368\n",
      "36/295, train_loss: 0.0716, step time: 1.0677\n",
      "37/295, train_loss: 0.0874, step time: 1.0480\n",
      "38/295, train_loss: 0.0818, step time: 1.0375\n",
      "39/295, train_loss: 0.1465, step time: 1.0495\n",
      "40/295, train_loss: 0.0455, step time: 1.0570\n",
      "41/295, train_loss: 0.0811, step time: 1.0321\n",
      "42/295, train_loss: 0.0505, step time: 1.0847\n",
      "43/295, train_loss: 0.3872, step time: 1.0717\n",
      "44/295, train_loss: 0.0410, step time: 1.0706\n",
      "45/295, train_loss: 0.0973, step time: 1.0656\n",
      "46/295, train_loss: 0.0688, step time: 1.0511\n",
      "47/295, train_loss: 0.0764, step time: 1.0629\n",
      "48/295, train_loss: 0.0785, step time: 1.0908\n",
      "49/295, train_loss: 0.0884, step time: 1.0723\n",
      "50/295, train_loss: 0.0942, step time: 1.0347\n",
      "51/295, train_loss: 0.0649, step time: 1.0524\n",
      "52/295, train_loss: 0.0799, step time: 1.0569\n",
      "53/295, train_loss: 0.0664, step time: 1.0408\n",
      "54/295, train_loss: 0.4020, step time: 1.0439\n",
      "55/295, train_loss: 0.0790, step time: 1.1419\n",
      "56/295, train_loss: 0.1288, step time: 1.0607\n",
      "57/295, train_loss: 0.0763, step time: 1.0473\n",
      "58/295, train_loss: 0.3956, step time: 1.0872\n",
      "59/295, train_loss: 0.0676, step time: 1.0377\n",
      "60/295, train_loss: 0.4177, step time: 1.1424\n",
      "61/295, train_loss: 0.0300, step time: 1.0342\n",
      "62/295, train_loss: 0.0589, step time: 1.0340\n",
      "63/295, train_loss: 0.4089, step time: 1.0433\n",
      "64/295, train_loss: 0.1567, step time: 1.0666\n",
      "65/295, train_loss: 0.0465, step time: 1.0414\n",
      "66/295, train_loss: 0.0785, step time: 1.0577\n",
      "67/295, train_loss: 0.0656, step time: 1.0485\n",
      "68/295, train_loss: 0.1040, step time: 1.0330\n",
      "69/295, train_loss: 0.0435, step time: 1.0474\n",
      "70/295, train_loss: 0.0577, step time: 1.1028\n",
      "71/295, train_loss: 0.3712, step time: 1.0300\n",
      "72/295, train_loss: 0.1003, step time: 1.0383\n",
      "73/295, train_loss: 0.0307, step time: 1.0885\n",
      "74/295, train_loss: 0.0474, step time: 1.1164\n",
      "75/295, train_loss: 0.1342, step time: 1.0401\n",
      "76/295, train_loss: 0.0404, step time: 1.0336\n",
      "77/295, train_loss: 0.0614, step time: 1.0390\n",
      "78/295, train_loss: 0.0585, step time: 1.0424\n",
      "79/295, train_loss: 0.0395, step time: 1.0386\n",
      "80/295, train_loss: 0.0852, step time: 1.0355\n",
      "81/295, train_loss: 0.1088, step time: 1.0665\n",
      "82/295, train_loss: 0.0523, step time: 1.0567\n",
      "83/295, train_loss: 0.0785, step time: 1.0414\n",
      "84/295, train_loss: 0.0941, step time: 1.1059\n",
      "85/295, train_loss: 0.1391, step time: 1.0758\n",
      "86/295, train_loss: 0.0881, step time: 1.0392\n",
      "87/295, train_loss: 0.0917, step time: 1.0478\n",
      "88/295, train_loss: 0.0520, step time: 1.0330\n",
      "89/295, train_loss: 0.1151, step time: 1.0446\n",
      "90/295, train_loss: 0.0684, step time: 1.1002\n",
      "91/295, train_loss: 0.0530, step time: 1.0380\n",
      "92/295, train_loss: 0.0523, step time: 1.0726\n",
      "93/295, train_loss: 0.0703, step time: 1.0498\n",
      "94/295, train_loss: 0.1001, step time: 1.0387\n",
      "95/295, train_loss: 0.0299, step time: 1.0336\n",
      "96/295, train_loss: 0.0619, step time: 1.0370\n",
      "97/295, train_loss: 0.0612, step time: 1.0724\n",
      "98/295, train_loss: 0.0564, step time: 1.1954\n",
      "99/295, train_loss: 0.0491, step time: 1.0322\n",
      "100/295, train_loss: 0.1565, step time: 1.0462\n",
      "101/295, train_loss: 0.0413, step time: 1.0642\n",
      "102/295, train_loss: 0.1244, step time: 1.0642\n",
      "103/295, train_loss: 0.0463, step time: 1.0307\n",
      "104/295, train_loss: 0.0605, step time: 1.0564\n",
      "105/295, train_loss: 0.0632, step time: 1.0365\n",
      "106/295, train_loss: 0.0352, step time: 1.0493\n",
      "107/295, train_loss: 0.0832, step time: 1.0639\n",
      "108/295, train_loss: 0.0349, step time: 1.0315\n",
      "109/295, train_loss: 0.0337, step time: 1.0408\n",
      "110/295, train_loss: 0.1399, step time: 1.0398\n",
      "111/295, train_loss: 0.0404, step time: 1.0413\n",
      "112/295, train_loss: 0.4912, step time: 1.0390\n",
      "113/295, train_loss: 0.0798, step time: 1.0385\n",
      "114/295, train_loss: 0.0983, step time: 1.0624\n",
      "115/295, train_loss: 0.2201, step time: 1.0403\n",
      "116/295, train_loss: 0.1173, step time: 1.0382\n",
      "117/295, train_loss: 0.1178, step time: 1.0359\n",
      "118/295, train_loss: 0.0732, step time: 1.0483\n",
      "119/295, train_loss: 0.3980, step time: 1.0881\n",
      "120/295, train_loss: 0.1323, step time: 1.0355\n",
      "121/295, train_loss: 0.0700, step time: 1.0357\n",
      "122/295, train_loss: 0.0330, step time: 1.0393\n",
      "123/295, train_loss: 0.1180, step time: 1.0464\n",
      "124/295, train_loss: 0.0484, step time: 1.0493\n",
      "125/295, train_loss: 0.0542, step time: 1.0385\n",
      "126/295, train_loss: 0.1947, step time: 1.0448\n",
      "127/295, train_loss: 0.1308, step time: 1.0521\n",
      "128/295, train_loss: 0.0698, step time: 1.0378\n",
      "129/295, train_loss: 0.0565, step time: 1.0402\n",
      "130/295, train_loss: 0.0444, step time: 1.0738\n",
      "131/295, train_loss: 0.3931, step time: 1.0432\n",
      "132/295, train_loss: 0.0885, step time: 1.0330\n",
      "133/295, train_loss: 0.2621, step time: 1.0399\n",
      "134/295, train_loss: 0.0654, step time: 1.0517\n",
      "135/295, train_loss: 0.1213, step time: 1.0621\n",
      "136/295, train_loss: 0.1484, step time: 1.0374\n",
      "137/295, train_loss: 0.0627, step time: 1.0463\n",
      "138/295, train_loss: 0.0899, step time: 1.0576\n",
      "139/295, train_loss: 0.0579, step time: 1.0582\n",
      "140/295, train_loss: 0.0587, step time: 1.0651\n",
      "141/295, train_loss: 0.0741, step time: 1.0383\n",
      "142/295, train_loss: 0.0912, step time: 1.0814\n",
      "143/295, train_loss: 0.0753, step time: 1.0855\n",
      "144/295, train_loss: 0.0599, step time: 1.0570\n",
      "145/295, train_loss: 0.1113, step time: 1.0309\n",
      "146/295, train_loss: 0.1236, step time: 1.0337\n",
      "147/295, train_loss: 0.0669, step time: 1.0334\n",
      "148/295, train_loss: 0.4069, step time: 1.0608\n",
      "149/295, train_loss: 0.0412, step time: 1.0503\n",
      "150/295, train_loss: 0.0468, step time: 1.0679\n",
      "151/295, train_loss: 0.0982, step time: 1.0438\n",
      "152/295, train_loss: 0.0644, step time: 1.0473\n",
      "153/295, train_loss: 0.1529, step time: 1.0435\n",
      "154/295, train_loss: 0.3075, step time: 1.0346\n",
      "155/295, train_loss: 0.0677, step time: 1.0350\n",
      "156/295, train_loss: 0.0779, step time: 1.0831\n",
      "157/295, train_loss: 0.0801, step time: 1.0689\n",
      "158/295, train_loss: 0.0639, step time: 1.0383\n",
      "159/295, train_loss: 0.0374, step time: 1.0559\n",
      "160/295, train_loss: 0.0547, step time: 1.0351\n",
      "161/295, train_loss: 0.0374, step time: 1.0581\n",
      "162/295, train_loss: 0.3988, step time: 1.0391\n",
      "163/295, train_loss: 0.0557, step time: 1.0393\n",
      "164/295, train_loss: 0.0992, step time: 1.0467\n",
      "165/295, train_loss: 0.0334, step time: 1.0380\n",
      "166/295, train_loss: 0.1635, step time: 1.0798\n",
      "167/295, train_loss: 0.0594, step time: 1.0420\n",
      "168/295, train_loss: 0.0597, step time: 1.0340\n",
      "169/295, train_loss: 0.1868, step time: 1.0433\n",
      "170/295, train_loss: 0.0639, step time: 1.1090\n",
      "171/295, train_loss: 0.3656, step time: 1.0415\n",
      "172/295, train_loss: 0.0995, step time: 1.0367\n",
      "173/295, train_loss: 0.3525, step time: 1.0425\n",
      "174/295, train_loss: 0.1793, step time: 1.0349\n",
      "175/295, train_loss: 0.0551, step time: 1.0679\n",
      "176/295, train_loss: 0.0459, step time: 1.0594\n",
      "177/295, train_loss: 0.0323, step time: 1.0362\n",
      "178/295, train_loss: 0.1836, step time: 1.0532\n",
      "179/295, train_loss: 0.0378, step time: 1.0361\n",
      "180/295, train_loss: 0.0788, step time: 1.0414\n",
      "181/295, train_loss: 0.0355, step time: 1.0424\n",
      "182/295, train_loss: 0.3759, step time: 1.0513\n",
      "183/295, train_loss: 0.0795, step time: 1.0478\n",
      "184/295, train_loss: 0.0402, step time: 1.0848\n",
      "185/295, train_loss: 0.0917, step time: 1.0339\n",
      "186/295, train_loss: 0.0868, step time: 1.0508\n",
      "187/295, train_loss: 0.1035, step time: 1.0388\n",
      "188/295, train_loss: 0.1002, step time: 1.0504\n",
      "189/295, train_loss: 0.0630, step time: 1.0328\n",
      "190/295, train_loss: 0.1716, step time: 1.0453\n",
      "191/295, train_loss: 0.1058, step time: 1.0602\n",
      "192/295, train_loss: 0.1157, step time: 1.0344\n",
      "193/295, train_loss: 0.0560, step time: 1.0421\n",
      "194/295, train_loss: 0.0638, step time: 1.0636\n",
      "195/295, train_loss: 0.0562, step time: 1.0937\n",
      "196/295, train_loss: 0.1458, step time: 1.0336\n",
      "197/295, train_loss: 0.0641, step time: 1.0610\n",
      "198/295, train_loss: 0.1006, step time: 1.0332\n",
      "199/295, train_loss: 0.0360, step time: 1.0560\n",
      "200/295, train_loss: 0.0382, step time: 1.1217\n",
      "201/295, train_loss: 0.1528, step time: 1.0738\n",
      "202/295, train_loss: 0.3520, step time: 1.0318\n",
      "203/295, train_loss: 0.1986, step time: 1.0519\n",
      "204/295, train_loss: 0.0500, step time: 1.0381\n",
      "205/295, train_loss: 0.1109, step time: 1.0417\n",
      "206/295, train_loss: 0.0566, step time: 1.0562\n",
      "207/295, train_loss: 0.0940, step time: 1.1118\n",
      "208/295, train_loss: 0.3833, step time: 1.0427\n",
      "209/295, train_loss: 0.0727, step time: 1.0439\n",
      "210/295, train_loss: 0.0795, step time: 1.0487\n",
      "211/295, train_loss: 0.0594, step time: 1.0450\n",
      "212/295, train_loss: 0.0494, step time: 1.0630\n",
      "213/295, train_loss: 0.0954, step time: 1.0597\n",
      "214/295, train_loss: 0.0624, step time: 1.0334\n",
      "215/295, train_loss: 0.0300, step time: 1.0515\n",
      "216/295, train_loss: 0.0674, step time: 1.0376\n",
      "217/295, train_loss: 0.0886, step time: 1.0333\n",
      "218/295, train_loss: 0.1119, step time: 1.0356\n",
      "219/295, train_loss: 0.1567, step time: 1.0504\n",
      "220/295, train_loss: 0.0565, step time: 1.0312\n",
      "221/295, train_loss: 0.1567, step time: 1.0342\n",
      "222/295, train_loss: 0.0494, step time: 1.0325\n",
      "223/295, train_loss: 0.0679, step time: 1.0421\n",
      "224/295, train_loss: 0.0960, step time: 1.0706\n",
      "225/295, train_loss: 0.0392, step time: 1.0417\n",
      "226/295, train_loss: 0.0537, step time: 1.0528\n",
      "227/295, train_loss: 0.1074, step time: 1.0428\n",
      "228/295, train_loss: 0.0554, step time: 1.0513\n",
      "229/295, train_loss: 0.0972, step time: 1.0684\n",
      "230/295, train_loss: 0.0235, step time: 1.0486\n",
      "231/295, train_loss: 0.0429, step time: 1.0365\n",
      "232/295, train_loss: 0.0490, step time: 1.0398\n",
      "233/295, train_loss: 0.1260, step time: 1.0615\n",
      "234/295, train_loss: 0.0958, step time: 1.0466\n",
      "235/295, train_loss: 0.0898, step time: 1.0592\n",
      "236/295, train_loss: 0.1636, step time: 1.0556\n",
      "237/295, train_loss: 0.0956, step time: 1.0537\n",
      "238/295, train_loss: 0.1386, step time: 1.0438\n",
      "239/295, train_loss: 0.0505, step time: 1.0903\n",
      "240/295, train_loss: 0.1432, step time: 1.0363\n",
      "241/295, train_loss: 0.0612, step time: 1.0501\n",
      "242/295, train_loss: 0.1287, step time: 1.0374\n",
      "243/295, train_loss: 0.0417, step time: 1.0403\n",
      "244/295, train_loss: 0.0760, step time: 1.0389\n",
      "245/295, train_loss: 0.0860, step time: 1.0378\n",
      "246/295, train_loss: 0.0593, step time: 1.0402\n",
      "247/295, train_loss: 0.0734, step time: 1.0547\n",
      "248/295, train_loss: 0.0584, step time: 1.0360\n",
      "249/295, train_loss: 0.0664, step time: 1.0453\n",
      "250/295, train_loss: 0.0572, step time: 1.0421\n",
      "251/295, train_loss: 0.0380, step time: 1.0463\n",
      "252/295, train_loss: 0.0382, step time: 1.0464\n",
      "253/295, train_loss: 0.0991, step time: 1.0813\n",
      "254/295, train_loss: 0.0592, step time: 1.0415\n",
      "255/295, train_loss: 0.3813, step time: 1.0382\n",
      "256/295, train_loss: 0.0340, step time: 1.0661\n",
      "257/295, train_loss: 0.0414, step time: 1.0433\n",
      "258/295, train_loss: 0.1214, step time: 1.0576\n",
      "259/295, train_loss: 0.0460, step time: 1.0889\n",
      "260/295, train_loss: 0.0379, step time: 1.0349\n",
      "261/295, train_loss: 0.0666, step time: 1.0465\n",
      "262/295, train_loss: 0.0375, step time: 1.0383\n",
      "263/295, train_loss: 0.0923, step time: 1.0388\n",
      "264/295, train_loss: 0.0814, step time: 1.0749\n",
      "265/295, train_loss: 0.1045, step time: 1.0636\n",
      "266/295, train_loss: 0.0468, step time: 1.0678\n",
      "267/295, train_loss: 0.1120, step time: 1.0443\n",
      "268/295, train_loss: 0.1181, step time: 1.0389\n",
      "269/295, train_loss: 0.1261, step time: 1.0420\n",
      "270/295, train_loss: 0.2054, step time: 1.0760\n",
      "271/295, train_loss: 0.0454, step time: 1.0822\n",
      "272/295, train_loss: 0.0385, step time: 1.0680\n",
      "273/295, train_loss: 0.0303, step time: 1.0428\n",
      "274/295, train_loss: 0.1008, step time: 1.0496\n",
      "275/295, train_loss: 0.0544, step time: 1.0474\n",
      "276/295, train_loss: 0.0780, step time: 1.0442\n",
      "277/295, train_loss: 0.1013, step time: 1.0427\n",
      "278/295, train_loss: 0.0674, step time: 1.0515\n",
      "279/295, train_loss: 0.0508, step time: 1.0416\n",
      "280/295, train_loss: 0.0446, step time: 1.0315\n",
      "281/295, train_loss: 0.0458, step time: 1.0423\n",
      "282/295, train_loss: 0.1779, step time: 1.0379\n",
      "283/295, train_loss: 0.1287, step time: 1.0499\n",
      "284/295, train_loss: 0.3915, step time: 1.0563\n",
      "285/295, train_loss: 0.0864, step time: 1.0589\n",
      "286/295, train_loss: 0.1394, step time: 1.0414\n",
      "287/295, train_loss: 0.0599, step time: 1.0464\n",
      "288/295, train_loss: 0.0570, step time: 1.0369\n",
      "289/295, train_loss: 0.0903, step time: 1.0281\n",
      "290/295, train_loss: 0.4157, step time: 1.0293\n",
      "291/295, train_loss: 0.0895, step time: 1.0293\n",
      "292/295, train_loss: 0.0884, step time: 1.0285\n",
      "293/295, train_loss: 0.0429, step time: 1.0292\n",
      "294/295, train_loss: 0.0830, step time: 1.0300\n",
      "295/295, train_loss: 0.0571, step time: 1.0289\n",
      "epoch 43 average loss: 0.1088\n",
      "current epoch: 43 current mean dice: 0.7616 tc: 0.7053 wt: 0.8369 et: 0.7465\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 43 is: 381.1835\n",
      "----------\n",
      "epoch 44/100\n",
      "1/295, train_loss: 0.3255, step time: 1.0773\n",
      "2/295, train_loss: 0.0994, step time: 1.1587\n",
      "3/295, train_loss: 0.0707, step time: 1.0732\n",
      "4/295, train_loss: 0.0472, step time: 1.0792\n",
      "5/295, train_loss: 0.1287, step time: 1.0394\n",
      "6/295, train_loss: 0.3743, step time: 1.0581\n",
      "7/295, train_loss: 0.0901, step time: 1.0456\n",
      "8/295, train_loss: 0.0414, step time: 1.0633\n",
      "9/295, train_loss: 0.0732, step time: 1.0320\n",
      "10/295, train_loss: 0.0397, step time: 1.0770\n",
      "11/295, train_loss: 0.0978, step time: 1.0332\n",
      "12/295, train_loss: 0.0754, step time: 1.0593\n",
      "13/295, train_loss: 0.1366, step time: 1.0598\n",
      "14/295, train_loss: 0.1256, step time: 1.0429\n",
      "15/295, train_loss: 0.0671, step time: 1.0493\n",
      "16/295, train_loss: 0.1090, step time: 1.0394\n",
      "17/295, train_loss: 0.0458, step time: 1.0430\n",
      "18/295, train_loss: 0.0829, step time: 1.0392\n",
      "19/295, train_loss: 0.1861, step time: 1.1174\n",
      "20/295, train_loss: 0.0678, step time: 1.0374\n",
      "21/295, train_loss: 0.0493, step time: 1.0534\n",
      "22/295, train_loss: 0.0602, step time: 1.0823\n",
      "23/295, train_loss: 0.0577, step time: 1.0350\n",
      "24/295, train_loss: 0.0362, step time: 1.0687\n",
      "25/295, train_loss: 0.3837, step time: 1.0615\n",
      "26/295, train_loss: 0.0960, step time: 1.0634\n",
      "27/295, train_loss: 0.0458, step time: 1.0676\n",
      "28/295, train_loss: 0.1255, step time: 1.0393\n",
      "29/295, train_loss: 0.0406, step time: 1.0444\n",
      "30/295, train_loss: 0.1138, step time: 1.0420\n",
      "31/295, train_loss: 0.0320, step time: 1.0597\n",
      "32/295, train_loss: 0.0553, step time: 1.1116\n",
      "33/295, train_loss: 0.1256, step time: 1.0361\n",
      "34/295, train_loss: 0.0351, step time: 1.0380\n",
      "35/295, train_loss: 0.0360, step time: 1.0400\n",
      "36/295, train_loss: 0.0701, step time: 1.0646\n",
      "37/295, train_loss: 0.0440, step time: 1.0468\n",
      "38/295, train_loss: 0.1057, step time: 1.0544\n",
      "39/295, train_loss: 0.1001, step time: 1.0342\n",
      "40/295, train_loss: 0.0527, step time: 1.0453\n",
      "41/295, train_loss: 0.3699, step time: 1.0402\n",
      "42/295, train_loss: 0.0720, step time: 1.0609\n",
      "43/295, train_loss: 0.0856, step time: 1.0362\n",
      "44/295, train_loss: 0.0360, step time: 1.0345\n",
      "45/295, train_loss: 0.1105, step time: 1.0369\n",
      "46/295, train_loss: 0.0687, step time: 1.0360\n",
      "47/295, train_loss: 0.0542, step time: 1.0383\n",
      "48/295, train_loss: 0.0899, step time: 1.0357\n",
      "49/295, train_loss: 0.0370, step time: 1.0432\n",
      "50/295, train_loss: 0.0807, step time: 1.0328\n",
      "51/295, train_loss: 0.0587, step time: 1.0361\n",
      "52/295, train_loss: 0.0623, step time: 1.0587\n",
      "53/295, train_loss: 0.0396, step time: 1.0437\n",
      "54/295, train_loss: 0.1397, step time: 1.0928\n",
      "55/295, train_loss: 0.0378, step time: 1.0520\n",
      "56/295, train_loss: 0.0940, step time: 1.0355\n",
      "57/295, train_loss: 0.1566, step time: 1.0491\n",
      "58/295, train_loss: 0.1153, step time: 1.0585\n",
      "59/295, train_loss: 0.0615, step time: 1.0319\n",
      "60/295, train_loss: 0.0395, step time: 1.0622\n",
      "61/295, train_loss: 0.0714, step time: 1.0332\n",
      "62/295, train_loss: 0.0727, step time: 1.0345\n",
      "63/295, train_loss: 0.4791, step time: 1.0690\n",
      "64/295, train_loss: 0.0549, step time: 1.0735\n",
      "65/295, train_loss: 0.0548, step time: 1.0403\n",
      "66/295, train_loss: 0.0669, step time: 1.0470\n",
      "67/295, train_loss: 0.3774, step time: 1.0468\n",
      "68/295, train_loss: 0.0827, step time: 1.0428\n",
      "69/295, train_loss: 0.0372, step time: 1.0639\n",
      "70/295, train_loss: 0.0650, step time: 1.0355\n",
      "71/295, train_loss: 0.0637, step time: 1.0400\n",
      "72/295, train_loss: 0.2088, step time: 1.1143\n",
      "73/295, train_loss: 0.0499, step time: 1.0307\n",
      "74/295, train_loss: 0.0555, step time: 1.0337\n",
      "75/295, train_loss: 0.0580, step time: 1.1020\n",
      "76/295, train_loss: 0.1235, step time: 1.0438\n",
      "77/295, train_loss: 0.1156, step time: 1.0479\n",
      "78/295, train_loss: 0.3956, step time: 1.0312\n",
      "79/295, train_loss: 0.3162, step time: 1.0391\n",
      "80/295, train_loss: 0.1078, step time: 1.0768\n",
      "81/295, train_loss: 0.0832, step time: 1.0731\n",
      "82/295, train_loss: 0.0819, step time: 1.0679\n",
      "83/295, train_loss: 0.1363, step time: 1.0526\n",
      "84/295, train_loss: 0.0755, step time: 1.0590\n",
      "85/295, train_loss: 0.1129, step time: 1.0331\n",
      "86/295, train_loss: 0.0961, step time: 1.0331\n",
      "87/295, train_loss: 0.0535, step time: 1.0370\n",
      "88/295, train_loss: 0.1051, step time: 1.0427\n",
      "89/295, train_loss: 0.0980, step time: 1.0384\n",
      "90/295, train_loss: 0.0677, step time: 1.0329\n",
      "91/295, train_loss: 0.0646, step time: 1.0409\n",
      "92/295, train_loss: 0.0957, step time: 1.0376\n",
      "93/295, train_loss: 0.1402, step time: 1.0405\n",
      "94/295, train_loss: 0.0543, step time: 1.0381\n",
      "95/295, train_loss: 0.0670, step time: 1.0337\n",
      "96/295, train_loss: 0.0449, step time: 1.0880\n",
      "97/295, train_loss: 0.1465, step time: 1.0652\n",
      "98/295, train_loss: 0.1103, step time: 1.0783\n",
      "99/295, train_loss: 0.0598, step time: 1.0335\n",
      "100/295, train_loss: 0.0946, step time: 1.0382\n",
      "101/295, train_loss: 0.3173, step time: 1.0381\n",
      "102/295, train_loss: 0.0329, step time: 1.0375\n",
      "103/295, train_loss: 0.0948, step time: 1.0375\n",
      "104/295, train_loss: 0.0451, step time: 1.0452\n",
      "105/295, train_loss: 0.1066, step time: 1.0404\n",
      "106/295, train_loss: 0.0893, step time: 1.0442\n",
      "107/295, train_loss: 0.0399, step time: 1.0462\n",
      "108/295, train_loss: 0.0515, step time: 1.1169\n",
      "109/295, train_loss: 0.3847, step time: 1.0413\n",
      "110/295, train_loss: 0.0388, step time: 1.0884\n",
      "111/295, train_loss: 0.0555, step time: 1.0693\n",
      "112/295, train_loss: 0.0502, step time: 1.0647\n",
      "113/295, train_loss: 0.3926, step time: 1.1157\n",
      "114/295, train_loss: 0.0505, step time: 1.0676\n",
      "115/295, train_loss: 0.0541, step time: 1.0649\n",
      "116/295, train_loss: 0.3844, step time: 1.0477\n",
      "117/295, train_loss: 0.0515, step time: 1.0424\n",
      "118/295, train_loss: 0.1185, step time: 1.0412\n",
      "119/295, train_loss: 0.0978, step time: 1.0349\n",
      "120/295, train_loss: 0.0306, step time: 1.0310\n",
      "121/295, train_loss: 0.0465, step time: 1.0398\n",
      "122/295, train_loss: 0.0517, step time: 1.0922\n",
      "123/295, train_loss: 0.3972, step time: 1.0374\n",
      "124/295, train_loss: 0.0615, step time: 1.0549\n",
      "125/295, train_loss: 0.0899, step time: 1.0451\n",
      "126/295, train_loss: 0.0385, step time: 1.0380\n",
      "127/295, train_loss: 0.0905, step time: 1.0611\n",
      "128/295, train_loss: 0.1246, step time: 1.0457\n",
      "129/295, train_loss: 0.1589, step time: 1.0521\n",
      "130/295, train_loss: 0.1399, step time: 1.0353\n",
      "131/295, train_loss: 0.3642, step time: 1.0386\n",
      "132/295, train_loss: 0.0392, step time: 1.0431\n",
      "133/295, train_loss: 0.0497, step time: 1.0342\n",
      "134/295, train_loss: 0.3846, step time: 1.0377\n",
      "135/295, train_loss: 0.0982, step time: 1.0372\n",
      "136/295, train_loss: 0.0895, step time: 1.0347\n",
      "137/295, train_loss: 0.0611, step time: 1.0447\n",
      "138/295, train_loss: 0.0817, step time: 1.0768\n",
      "139/295, train_loss: 0.0621, step time: 1.0942\n",
      "140/295, train_loss: 0.0586, step time: 1.0657\n",
      "141/295, train_loss: 0.0385, step time: 1.0462\n",
      "142/295, train_loss: 0.1521, step time: 1.0421\n",
      "143/295, train_loss: 0.3956, step time: 1.0459\n",
      "144/295, train_loss: 0.0334, step time: 1.0388\n",
      "145/295, train_loss: 0.0510, step time: 1.0384\n",
      "146/295, train_loss: 0.0293, step time: 1.0356\n",
      "147/295, train_loss: 0.0220, step time: 1.0368\n",
      "148/295, train_loss: 0.0457, step time: 1.0386\n",
      "149/295, train_loss: 0.1537, step time: 1.0340\n",
      "150/295, train_loss: 0.0684, step time: 1.0321\n",
      "151/295, train_loss: 0.0997, step time: 1.0559\n",
      "152/295, train_loss: 0.0377, step time: 1.0344\n",
      "153/295, train_loss: 0.0479, step time: 1.0632\n",
      "154/295, train_loss: 0.0735, step time: 1.0789\n",
      "155/295, train_loss: 0.0617, step time: 1.0351\n",
      "156/295, train_loss: 0.4189, step time: 1.0640\n",
      "157/295, train_loss: 0.0952, step time: 1.0833\n",
      "158/295, train_loss: 0.0367, step time: 1.0512\n",
      "159/295, train_loss: 0.0319, step time: 1.0329\n",
      "160/295, train_loss: 0.2014, step time: 1.0528\n",
      "161/295, train_loss: 0.1382, step time: 1.0360\n",
      "162/295, train_loss: 0.0827, step time: 1.0596\n",
      "163/295, train_loss: 0.0463, step time: 1.0561\n",
      "164/295, train_loss: 0.0499, step time: 1.0325\n",
      "165/295, train_loss: 0.0976, step time: 1.0336\n",
      "166/295, train_loss: 0.0778, step time: 1.0513\n",
      "167/295, train_loss: 0.1533, step time: 1.0508\n",
      "168/295, train_loss: 0.0646, step time: 1.0609\n",
      "169/295, train_loss: 0.1338, step time: 1.0604\n",
      "170/295, train_loss: 0.0682, step time: 1.1290\n",
      "171/295, train_loss: 0.0432, step time: 1.1105\n",
      "172/295, train_loss: 0.0757, step time: 1.0377\n",
      "173/295, train_loss: 0.0864, step time: 1.0467\n",
      "174/295, train_loss: 0.0612, step time: 1.0384\n",
      "175/295, train_loss: 0.4077, step time: 1.0311\n",
      "176/295, train_loss: 0.0400, step time: 1.0929\n",
      "177/295, train_loss: 0.4097, step time: 1.0314\n",
      "178/295, train_loss: 0.0552, step time: 1.0658\n",
      "179/295, train_loss: 0.0780, step time: 1.0547\n",
      "180/295, train_loss: 0.0926, step time: 1.0343\n",
      "181/295, train_loss: 0.1194, step time: 1.0375\n",
      "182/295, train_loss: 0.1045, step time: 1.0382\n",
      "183/295, train_loss: 0.0558, step time: 1.0895\n",
      "184/295, train_loss: 0.1844, step time: 1.0489\n",
      "185/295, train_loss: 0.0622, step time: 1.0403\n",
      "186/295, train_loss: 0.0782, step time: 1.0407\n",
      "187/295, train_loss: 0.1001, step time: 1.0618\n",
      "188/295, train_loss: 0.0548, step time: 1.0604\n",
      "189/295, train_loss: 0.1333, step time: 1.0361\n",
      "190/295, train_loss: 0.0806, step time: 1.0475\n",
      "191/295, train_loss: 0.0665, step time: 1.0530\n",
      "192/295, train_loss: 0.0876, step time: 1.0418\n",
      "193/295, train_loss: 0.0748, step time: 1.0355\n",
      "194/295, train_loss: 0.0712, step time: 1.0726\n",
      "195/295, train_loss: 0.0630, step time: 1.0587\n",
      "196/295, train_loss: 0.1463, step time: 1.0321\n",
      "197/295, train_loss: 0.0657, step time: 1.0421\n",
      "198/295, train_loss: 0.3903, step time: 1.0370\n",
      "199/295, train_loss: 0.1086, step time: 1.0425\n",
      "200/295, train_loss: 0.0596, step time: 1.0308\n",
      "201/295, train_loss: 0.1048, step time: 1.0400\n",
      "202/295, train_loss: 0.0697, step time: 1.0337\n",
      "203/295, train_loss: 0.4001, step time: 1.0379\n",
      "204/295, train_loss: 0.0711, step time: 1.0419\n",
      "205/295, train_loss: 0.0905, step time: 1.0346\n",
      "206/295, train_loss: 0.3954, step time: 1.0392\n",
      "207/295, train_loss: 0.0788, step time: 1.0400\n",
      "208/295, train_loss: 0.0501, step time: 1.0460\n",
      "209/295, train_loss: 0.0300, step time: 1.1267\n",
      "210/295, train_loss: 0.1587, step time: 1.0354\n",
      "211/295, train_loss: 0.0429, step time: 1.0441\n",
      "212/295, train_loss: 0.1091, step time: 1.0382\n",
      "213/295, train_loss: 0.1750, step time: 1.0344\n",
      "214/295, train_loss: 0.0541, step time: 1.0559\n",
      "215/295, train_loss: 0.0289, step time: 1.0445\n",
      "216/295, train_loss: 0.0446, step time: 1.0391\n",
      "217/295, train_loss: 0.0511, step time: 1.0695\n",
      "218/295, train_loss: 0.1212, step time: 1.0436\n",
      "219/295, train_loss: 0.0747, step time: 1.0386\n",
      "220/295, train_loss: 0.0482, step time: 1.0661\n",
      "221/295, train_loss: 0.1775, step time: 1.0542\n",
      "222/295, train_loss: 0.0479, step time: 1.0418\n",
      "223/295, train_loss: 0.0828, step time: 1.0319\n",
      "224/295, train_loss: 0.1290, step time: 1.0363\n",
      "225/295, train_loss: 0.3850, step time: 1.0371\n",
      "226/295, train_loss: 0.1344, step time: 1.0629\n",
      "227/295, train_loss: 0.1853, step time: 1.0383\n",
      "228/295, train_loss: 0.0861, step time: 1.0570\n",
      "229/295, train_loss: 0.0834, step time: 1.0394\n",
      "230/295, train_loss: 0.0460, step time: 1.0376\n",
      "231/295, train_loss: 0.0582, step time: 1.0356\n",
      "232/295, train_loss: 0.0690, step time: 1.0762\n",
      "233/295, train_loss: 0.0407, step time: 1.0381\n",
      "234/295, train_loss: 0.1244, step time: 1.0536\n",
      "235/295, train_loss: 0.0870, step time: 1.0580\n",
      "236/295, train_loss: 0.0487, step time: 1.0649\n",
      "237/295, train_loss: 0.0746, step time: 1.0415\n",
      "238/295, train_loss: 0.0897, step time: 1.0409\n",
      "239/295, train_loss: 0.1051, step time: 1.0687\n",
      "240/295, train_loss: 0.0815, step time: 1.0381\n",
      "241/295, train_loss: 0.4240, step time: 1.0546\n",
      "242/295, train_loss: 0.0539, step time: 1.0598\n",
      "243/295, train_loss: 0.0387, step time: 1.0594\n",
      "244/295, train_loss: 0.0899, step time: 1.0749\n",
      "245/295, train_loss: 0.1149, step time: 1.0676\n",
      "246/295, train_loss: 0.1273, step time: 1.0572\n",
      "247/295, train_loss: 0.1425, step time: 1.0847\n",
      "248/295, train_loss: 0.2294, step time: 1.0323\n",
      "249/295, train_loss: 0.0668, step time: 1.0629\n",
      "250/295, train_loss: 0.0776, step time: 1.0567\n",
      "251/295, train_loss: 0.1221, step time: 1.0356\n",
      "252/295, train_loss: 0.0708, step time: 1.0416\n",
      "253/295, train_loss: 0.0693, step time: 1.0375\n",
      "254/295, train_loss: 0.0653, step time: 1.0331\n",
      "255/295, train_loss: 0.0386, step time: 1.0383\n",
      "256/295, train_loss: 0.0311, step time: 1.0367\n",
      "257/295, train_loss: 0.1141, step time: 1.0687\n",
      "258/295, train_loss: 0.0562, step time: 1.0600\n",
      "259/295, train_loss: 0.1821, step time: 1.0504\n",
      "260/295, train_loss: 0.0429, step time: 1.0316\n",
      "261/295, train_loss: 0.1385, step time: 1.0351\n",
      "262/295, train_loss: 0.0994, step time: 1.0444\n",
      "263/295, train_loss: 0.0431, step time: 1.0924\n",
      "264/295, train_loss: 0.0877, step time: 1.0452\n",
      "265/295, train_loss: 0.0833, step time: 1.0593\n",
      "266/295, train_loss: 0.0668, step time: 1.0365\n",
      "267/295, train_loss: 0.0804, step time: 1.0571\n",
      "268/295, train_loss: 0.0828, step time: 1.0538\n",
      "269/295, train_loss: 0.0408, step time: 1.0628\n",
      "270/295, train_loss: 0.0362, step time: 1.0490\n",
      "271/295, train_loss: 0.0356, step time: 1.0387\n",
      "272/295, train_loss: 0.0748, step time: 1.0341\n",
      "273/295, train_loss: 0.1115, step time: 1.0413\n",
      "274/295, train_loss: 0.1052, step time: 1.0852\n",
      "275/295, train_loss: 0.0640, step time: 1.0769\n",
      "276/295, train_loss: 0.0981, step time: 1.0567\n",
      "277/295, train_loss: 0.1510, step time: 1.0447\n",
      "278/295, train_loss: 0.0594, step time: 1.1314\n",
      "279/295, train_loss: 0.0364, step time: 1.0378\n",
      "280/295, train_loss: 0.2248, step time: 1.0406\n",
      "281/295, train_loss: 0.1015, step time: 1.0503\n",
      "282/295, train_loss: 0.0671, step time: 1.1015\n",
      "283/295, train_loss: 0.1074, step time: 1.0790\n",
      "284/295, train_loss: 0.0453, step time: 1.0322\n",
      "285/295, train_loss: 0.1151, step time: 1.0357\n",
      "286/295, train_loss: 0.0981, step time: 1.0369\n",
      "287/295, train_loss: 0.0761, step time: 1.0342\n",
      "288/295, train_loss: 0.0310, step time: 1.0302\n",
      "289/295, train_loss: 0.0764, step time: 1.0296\n",
      "290/295, train_loss: 0.0645, step time: 1.0292\n",
      "291/295, train_loss: 0.1733, step time: 1.0308\n",
      "292/295, train_loss: 0.0818, step time: 1.0296\n",
      "293/295, train_loss: 0.0879, step time: 1.0288\n",
      "294/295, train_loss: 0.0558, step time: 1.0291\n",
      "295/295, train_loss: 0.4072, step time: 1.0288\n",
      "epoch 44 average loss: 0.1073\n",
      "current epoch: 44 current mean dice: 0.7952 tc: 0.7474 wt: 0.8631 et: 0.7799\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 44 is: 390.2988\n",
      "----------\n",
      "epoch 45/100\n",
      "1/295, train_loss: 0.0529, step time: 1.1811\n",
      "2/295, train_loss: 0.0666, step time: 1.1016\n",
      "3/295, train_loss: 0.0561, step time: 1.0813\n",
      "4/295, train_loss: 0.0884, step time: 1.0483\n",
      "5/295, train_loss: 0.0844, step time: 1.0322\n",
      "6/295, train_loss: 0.4022, step time: 1.0344\n",
      "7/295, train_loss: 0.0584, step time: 1.0920\n",
      "8/295, train_loss: 0.1449, step time: 1.1226\n",
      "9/295, train_loss: 0.0953, step time: 1.0719\n",
      "10/295, train_loss: 0.0833, step time: 1.0506\n",
      "11/295, train_loss: 0.0593, step time: 1.0386\n",
      "12/295, train_loss: 0.4799, step time: 1.0656\n",
      "13/295, train_loss: 0.0560, step time: 1.0400\n",
      "14/295, train_loss: 0.0320, step time: 1.0407\n",
      "15/295, train_loss: 0.0411, step time: 1.0637\n",
      "16/295, train_loss: 0.0631, step time: 1.0421\n",
      "17/295, train_loss: 0.0386, step time: 1.0689\n",
      "18/295, train_loss: 0.1541, step time: 1.1169\n",
      "19/295, train_loss: 0.0646, step time: 1.0463\n",
      "20/295, train_loss: 0.0887, step time: 1.0377\n",
      "21/295, train_loss: 0.0941, step time: 1.0442\n",
      "22/295, train_loss: 0.1079, step time: 1.0903\n",
      "23/295, train_loss: 0.0384, step time: 1.0339\n",
      "24/295, train_loss: 0.0973, step time: 1.0565\n",
      "25/295, train_loss: 0.0414, step time: 1.0975\n",
      "26/295, train_loss: 0.3971, step time: 1.0314\n",
      "27/295, train_loss: 0.1143, step time: 1.0479\n",
      "28/295, train_loss: 0.0458, step time: 1.0504\n",
      "29/295, train_loss: 0.0295, step time: 1.0344\n",
      "30/295, train_loss: 0.0744, step time: 1.0368\n",
      "31/295, train_loss: 0.0954, step time: 1.0351\n",
      "32/295, train_loss: 0.0886, step time: 1.0405\n",
      "33/295, train_loss: 0.1537, step time: 1.0387\n",
      "34/295, train_loss: 0.1452, step time: 1.0730\n",
      "35/295, train_loss: 0.0513, step time: 1.0430\n",
      "36/295, train_loss: 0.0837, step time: 1.0628\n",
      "37/295, train_loss: 0.0469, step time: 1.0512\n",
      "38/295, train_loss: 0.2240, step time: 1.0402\n",
      "39/295, train_loss: 0.1099, step time: 1.0879\n",
      "40/295, train_loss: 0.0927, step time: 1.0433\n",
      "41/295, train_loss: 0.0333, step time: 1.0473\n",
      "42/295, train_loss: 0.3816, step time: 1.0702\n",
      "43/295, train_loss: 0.0440, step time: 1.0604\n",
      "44/295, train_loss: 0.0796, step time: 1.0391\n",
      "45/295, train_loss: 0.1289, step time: 1.0571\n",
      "46/295, train_loss: 0.1349, step time: 1.0711\n",
      "47/295, train_loss: 0.0618, step time: 1.0556\n",
      "48/295, train_loss: 0.1154, step time: 1.0516\n",
      "49/295, train_loss: 0.0837, step time: 1.0596\n",
      "50/295, train_loss: 0.0613, step time: 1.0312\n",
      "51/295, train_loss: 0.0616, step time: 1.0352\n",
      "52/295, train_loss: 0.1832, step time: 1.0414\n",
      "53/295, train_loss: 0.0691, step time: 1.0641\n",
      "54/295, train_loss: 0.0366, step time: 1.0642\n",
      "55/295, train_loss: 0.0951, step time: 1.0666\n",
      "56/295, train_loss: 0.0316, step time: 1.0734\n",
      "57/295, train_loss: 0.0414, step time: 1.0311\n",
      "58/295, train_loss: 0.0434, step time: 1.0618\n",
      "59/295, train_loss: 0.3858, step time: 1.0316\n",
      "60/295, train_loss: 0.0907, step time: 1.0524\n",
      "61/295, train_loss: 0.0661, step time: 1.0562\n",
      "62/295, train_loss: 0.0576, step time: 1.0364\n",
      "63/295, train_loss: 0.1308, step time: 1.0393\n",
      "64/295, train_loss: 0.0520, step time: 1.0537\n",
      "65/295, train_loss: 0.1017, step time: 1.0610\n",
      "66/295, train_loss: 0.1158, step time: 1.0315\n",
      "67/295, train_loss: 0.0608, step time: 1.0526\n",
      "68/295, train_loss: 0.0797, step time: 1.1378\n",
      "69/295, train_loss: 0.3859, step time: 1.0498\n",
      "70/295, train_loss: 0.1321, step time: 1.0744\n",
      "71/295, train_loss: 0.0517, step time: 1.0666\n",
      "72/295, train_loss: 0.0783, step time: 1.0681\n",
      "73/295, train_loss: 0.0398, step time: 1.0405\n",
      "74/295, train_loss: 0.0434, step time: 1.0365\n",
      "75/295, train_loss: 0.0862, step time: 1.0347\n",
      "76/295, train_loss: 0.0350, step time: 1.0641\n",
      "77/295, train_loss: 0.1006, step time: 1.0535\n",
      "78/295, train_loss: 0.4022, step time: 1.0455\n",
      "79/295, train_loss: 0.0516, step time: 1.0364\n",
      "80/295, train_loss: 0.0457, step time: 1.0368\n",
      "81/295, train_loss: 0.0467, step time: 1.0473\n",
      "82/295, train_loss: 0.0416, step time: 1.0534\n",
      "83/295, train_loss: 0.0775, step time: 1.0509\n",
      "84/295, train_loss: 0.1238, step time: 1.0956\n",
      "85/295, train_loss: 0.1417, step time: 1.0654\n",
      "86/295, train_loss: 0.0901, step time: 1.0522\n",
      "87/295, train_loss: 0.0529, step time: 1.0305\n",
      "88/295, train_loss: 0.0541, step time: 1.0508\n",
      "89/295, train_loss: 0.0732, step time: 1.0589\n",
      "90/295, train_loss: 0.0534, step time: 1.1118\n",
      "91/295, train_loss: 0.0761, step time: 1.0359\n",
      "92/295, train_loss: 0.0544, step time: 1.0348\n",
      "93/295, train_loss: 0.1060, step time: 1.0673\n",
      "94/295, train_loss: 0.0808, step time: 1.0645\n",
      "95/295, train_loss: 0.0531, step time: 1.0730\n",
      "96/295, train_loss: 0.1291, step time: 1.0469\n",
      "97/295, train_loss: 0.3266, step time: 1.0304\n",
      "98/295, train_loss: 0.0530, step time: 1.0498\n",
      "99/295, train_loss: 0.3906, step time: 1.0437\n",
      "100/295, train_loss: 0.0301, step time: 1.0563\n",
      "101/295, train_loss: 0.0981, step time: 1.0324\n",
      "102/295, train_loss: 0.4036, step time: 1.0346\n",
      "103/295, train_loss: 0.0970, step time: 1.0562\n",
      "104/295, train_loss: 0.3744, step time: 1.0679\n",
      "105/295, train_loss: 0.0765, step time: 1.0349\n",
      "106/295, train_loss: 0.1638, step time: 1.0526\n",
      "107/295, train_loss: 0.0658, step time: 1.0417\n",
      "108/295, train_loss: 0.3740, step time: 1.0403\n",
      "109/295, train_loss: 0.0464, step time: 1.0549\n",
      "110/295, train_loss: 0.0669, step time: 1.0364\n",
      "111/295, train_loss: 0.1446, step time: 1.0554\n",
      "112/295, train_loss: 0.0652, step time: 1.0661\n",
      "113/295, train_loss: 0.0797, step time: 1.0802\n",
      "114/295, train_loss: 0.0468, step time: 1.0623\n",
      "115/295, train_loss: 0.0690, step time: 1.0629\n",
      "116/295, train_loss: 0.0726, step time: 1.0519\n",
      "117/295, train_loss: 0.1240, step time: 1.0609\n",
      "118/295, train_loss: 0.0686, step time: 1.0408\n",
      "119/295, train_loss: 0.0550, step time: 1.0956\n",
      "120/295, train_loss: 0.3935, step time: 1.0637\n",
      "121/295, train_loss: 0.0774, step time: 1.0469\n",
      "122/295, train_loss: 0.0507, step time: 1.0628\n",
      "123/295, train_loss: 0.0932, step time: 1.0426\n",
      "124/295, train_loss: 0.0333, step time: 1.1019\n",
      "125/295, train_loss: 0.1210, step time: 1.0781\n",
      "126/295, train_loss: 0.2032, step time: 1.0331\n",
      "127/295, train_loss: 0.0763, step time: 1.0395\n",
      "128/295, train_loss: 0.0326, step time: 1.0358\n",
      "129/295, train_loss: 0.0416, step time: 1.0510\n",
      "130/295, train_loss: 0.1347, step time: 1.0623\n",
      "131/295, train_loss: 0.0442, step time: 1.0327\n",
      "132/295, train_loss: 0.0635, step time: 1.0347\n",
      "133/295, train_loss: 0.1332, step time: 1.0350\n",
      "134/295, train_loss: 0.0688, step time: 1.0552\n",
      "135/295, train_loss: 0.0998, step time: 1.0628\n",
      "136/295, train_loss: 0.0957, step time: 1.0340\n",
      "137/295, train_loss: 0.0576, step time: 1.0517\n",
      "138/295, train_loss: 0.1330, step time: 1.0414\n",
      "139/295, train_loss: 0.0406, step time: 1.0404\n",
      "140/295, train_loss: 0.2323, step time: 1.0409\n",
      "141/295, train_loss: 0.1096, step time: 1.0709\n",
      "142/295, train_loss: 0.0661, step time: 1.0430\n",
      "143/295, train_loss: 0.0600, step time: 1.0706\n",
      "144/295, train_loss: 0.0790, step time: 1.0686\n",
      "145/295, train_loss: 0.0864, step time: 1.0644\n",
      "146/295, train_loss: 0.0910, step time: 1.0798\n",
      "147/295, train_loss: 0.1070, step time: 1.0323\n",
      "148/295, train_loss: 0.0993, step time: 1.0352\n",
      "149/295, train_loss: 0.0522, step time: 1.0470\n",
      "150/295, train_loss: 0.0656, step time: 1.0763\n",
      "151/295, train_loss: 0.0673, step time: 1.0402\n",
      "152/295, train_loss: 0.0934, step time: 1.0412\n",
      "153/295, train_loss: 0.4023, step time: 1.0457\n",
      "154/295, train_loss: 0.0410, step time: 1.0654\n",
      "155/295, train_loss: 0.0594, step time: 1.0393\n",
      "156/295, train_loss: 0.0775, step time: 1.0395\n",
      "157/295, train_loss: 0.0972, step time: 1.0363\n",
      "158/295, train_loss: 0.0491, step time: 1.0349\n",
      "159/295, train_loss: 0.1009, step time: 1.0427\n",
      "160/295, train_loss: 0.1523, step time: 1.0473\n",
      "161/295, train_loss: 0.1643, step time: 1.0784\n",
      "162/295, train_loss: 0.1120, step time: 1.0423\n",
      "163/295, train_loss: 0.0780, step time: 1.0554\n",
      "164/295, train_loss: 0.0406, step time: 1.0615\n",
      "165/295, train_loss: 0.3969, step time: 1.0399\n",
      "166/295, train_loss: 0.0460, step time: 1.0486\n",
      "167/295, train_loss: 0.0445, step time: 1.0368\n",
      "168/295, train_loss: 0.0717, step time: 1.0531\n",
      "169/295, train_loss: 0.1410, step time: 1.0497\n",
      "170/295, train_loss: 0.1204, step time: 1.0419\n",
      "171/295, train_loss: 0.0899, step time: 1.0506\n",
      "172/295, train_loss: 0.1323, step time: 1.0553\n",
      "173/295, train_loss: 0.1264, step time: 1.0446\n",
      "174/295, train_loss: 0.0871, step time: 1.0657\n",
      "175/295, train_loss: 0.0398, step time: 1.0446\n",
      "176/295, train_loss: 0.1675, step time: 1.0697\n",
      "177/295, train_loss: 0.0565, step time: 1.0424\n",
      "178/295, train_loss: 0.0907, step time: 1.0371\n",
      "179/295, train_loss: 0.0579, step time: 1.0509\n",
      "180/295, train_loss: 0.0838, step time: 1.1268\n",
      "181/295, train_loss: 0.0450, step time: 1.0330\n",
      "182/295, train_loss: 0.0407, step time: 1.0349\n",
      "183/295, train_loss: 0.0505, step time: 1.0401\n",
      "184/295, train_loss: 0.1042, step time: 1.0572\n",
      "185/295, train_loss: 0.1234, step time: 1.0328\n",
      "186/295, train_loss: 0.0496, step time: 1.0348\n",
      "187/295, train_loss: 0.0586, step time: 1.0512\n",
      "188/295, train_loss: 0.0443, step time: 1.0429\n",
      "189/295, train_loss: 0.0351, step time: 1.0367\n",
      "190/295, train_loss: 0.0787, step time: 1.0360\n",
      "191/295, train_loss: 0.1136, step time: 1.0443\n",
      "192/295, train_loss: 0.0670, step time: 1.0411\n",
      "193/295, train_loss: 0.1895, step time: 1.0718\n",
      "194/295, train_loss: 0.3812, step time: 1.0698\n",
      "195/295, train_loss: 0.1059, step time: 1.0438\n",
      "196/295, train_loss: 0.0593, step time: 1.0613\n",
      "197/295, train_loss: 0.0696, step time: 1.0942\n",
      "198/295, train_loss: 0.3752, step time: 1.0389\n",
      "199/295, train_loss: 0.1699, step time: 1.0455\n",
      "200/295, train_loss: 0.1378, step time: 1.0500\n",
      "201/295, train_loss: 0.0685, step time: 1.0467\n",
      "202/295, train_loss: 0.0640, step time: 1.0516\n",
      "203/295, train_loss: 0.3838, step time: 1.0389\n",
      "204/295, train_loss: 0.0525, step time: 1.0363\n",
      "205/295, train_loss: 0.1087, step time: 1.0833\n",
      "206/295, train_loss: 0.0911, step time: 1.0712\n",
      "207/295, train_loss: 0.1191, step time: 1.0747\n",
      "208/295, train_loss: 0.0772, step time: 1.0603\n",
      "209/295, train_loss: 0.0490, step time: 1.0413\n",
      "210/295, train_loss: 0.0336, step time: 1.0484\n",
      "211/295, train_loss: 0.0496, step time: 1.0388\n",
      "212/295, train_loss: 0.1007, step time: 1.0486\n",
      "213/295, train_loss: 0.0643, step time: 1.0320\n",
      "214/295, train_loss: 0.0539, step time: 1.0377\n",
      "215/295, train_loss: 0.1401, step time: 1.0386\n",
      "216/295, train_loss: 0.0450, step time: 1.0598\n",
      "217/295, train_loss: 0.0329, step time: 1.0897\n",
      "218/295, train_loss: 0.0353, step time: 1.1081\n",
      "219/295, train_loss: 0.0625, step time: 1.0398\n",
      "220/295, train_loss: 0.4137, step time: 1.0626\n",
      "221/295, train_loss: 0.0585, step time: 1.0478\n",
      "222/295, train_loss: 0.2475, step time: 1.0611\n",
      "223/295, train_loss: 0.0535, step time: 1.0583\n",
      "224/295, train_loss: 0.0407, step time: 1.0571\n",
      "225/295, train_loss: 0.0843, step time: 1.0476\n",
      "226/295, train_loss: 0.0611, step time: 1.0857\n",
      "227/295, train_loss: 0.0942, step time: 1.0326\n",
      "228/295, train_loss: 0.1079, step time: 1.0434\n",
      "229/295, train_loss: 0.0431, step time: 1.0406\n",
      "230/295, train_loss: 0.0604, step time: 1.0567\n",
      "231/295, train_loss: 0.0224, step time: 1.0673\n",
      "232/295, train_loss: 0.0559, step time: 1.0417\n",
      "233/295, train_loss: 0.0993, step time: 1.0700\n",
      "234/295, train_loss: 0.1339, step time: 1.0602\n",
      "235/295, train_loss: 0.1209, step time: 1.0394\n",
      "236/295, train_loss: 0.3132, step time: 1.0449\n",
      "237/295, train_loss: 0.0423, step time: 1.0520\n",
      "238/295, train_loss: 0.0491, step time: 1.0358\n",
      "239/295, train_loss: 0.0955, step time: 1.0626\n",
      "240/295, train_loss: 0.3745, step time: 1.0484\n",
      "241/295, train_loss: 0.4028, step time: 1.0684\n",
      "242/295, train_loss: 0.1228, step time: 1.0342\n",
      "243/295, train_loss: 0.0941, step time: 1.0484\n",
      "244/295, train_loss: 0.0333, step time: 1.0760\n",
      "245/295, train_loss: 0.0494, step time: 1.0650\n",
      "246/295, train_loss: 0.1039, step time: 1.0674\n",
      "247/295, train_loss: 0.0623, step time: 1.0658\n",
      "248/295, train_loss: 0.0308, step time: 1.0354\n",
      "249/295, train_loss: 0.0676, step time: 1.0598\n",
      "250/295, train_loss: 0.0971, step time: 1.0361\n",
      "251/295, train_loss: 0.1282, step time: 1.1105\n",
      "252/295, train_loss: 0.1405, step time: 1.0888\n",
      "253/295, train_loss: 0.1530, step time: 1.0681\n",
      "254/295, train_loss: 0.0902, step time: 1.0748\n",
      "255/295, train_loss: 0.0461, step time: 1.0443\n",
      "256/295, train_loss: 0.0709, step time: 1.0911\n",
      "257/295, train_loss: 0.0461, step time: 1.0406\n",
      "258/295, train_loss: 0.3990, step time: 1.0520\n",
      "259/295, train_loss: 0.0605, step time: 1.0381\n",
      "260/295, train_loss: 0.0565, step time: 1.0445\n",
      "261/295, train_loss: 0.0422, step time: 1.0404\n",
      "262/295, train_loss: 0.0944, step time: 1.0541\n",
      "263/295, train_loss: 0.0758, step time: 1.0748\n",
      "264/295, train_loss: 0.0475, step time: 1.0449\n",
      "265/295, train_loss: 0.0559, step time: 1.0351\n",
      "266/295, train_loss: 0.0612, step time: 1.0429\n",
      "267/295, train_loss: 0.1367, step time: 1.0436\n",
      "268/295, train_loss: 0.0383, step time: 1.0850\n",
      "269/295, train_loss: 0.0437, step time: 1.0690\n",
      "270/295, train_loss: 0.0587, step time: 1.0346\n",
      "271/295, train_loss: 0.0613, step time: 1.0524\n",
      "272/295, train_loss: 0.0716, step time: 1.0773\n",
      "273/295, train_loss: 0.0704, step time: 1.0400\n",
      "274/295, train_loss: 0.0803, step time: 1.0363\n",
      "275/295, train_loss: 0.1258, step time: 1.0502\n",
      "276/295, train_loss: 0.0933, step time: 1.0406\n",
      "277/295, train_loss: 0.0420, step time: 1.0417\n",
      "278/295, train_loss: 0.0737, step time: 1.0466\n",
      "279/295, train_loss: 0.0449, step time: 1.0432\n",
      "280/295, train_loss: 0.0651, step time: 1.0426\n",
      "281/295, train_loss: 0.0498, step time: 1.0385\n",
      "282/295, train_loss: 0.1494, step time: 1.0348\n",
      "283/295, train_loss: 0.0369, step time: 1.0417\n",
      "284/295, train_loss: 0.0532, step time: 1.0477\n",
      "285/295, train_loss: 0.0797, step time: 1.0409\n",
      "286/295, train_loss: 0.0713, step time: 1.0605\n",
      "287/295, train_loss: 0.0740, step time: 1.0376\n",
      "288/295, train_loss: 0.0339, step time: 1.0315\n",
      "289/295, train_loss: 0.1342, step time: 1.0295\n",
      "290/295, train_loss: 0.1033, step time: 1.0283\n",
      "291/295, train_loss: 0.1096, step time: 1.0283\n",
      "292/295, train_loss: 0.1568, step time: 1.0306\n",
      "293/295, train_loss: 0.1814, step time: 1.0283\n",
      "294/295, train_loss: 0.4105, step time: 1.0290\n",
      "295/295, train_loss: 0.1476, step time: 1.0291\n",
      "epoch 45 average loss: 0.1065\n",
      "current epoch: 45 current mean dice: 0.8037 tc: 0.7544 wt: 0.8706 et: 0.7921\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 45 is: 387.6284\n",
      "----------\n",
      "epoch 46/100\n",
      "1/295, train_loss: 0.0686, step time: 1.1273\n",
      "2/295, train_loss: 0.0679, step time: 1.0609\n",
      "3/295, train_loss: 0.0322, step time: 1.0729\n",
      "4/295, train_loss: 0.0862, step time: 1.0804\n",
      "5/295, train_loss: 0.0475, step time: 1.0623\n",
      "6/295, train_loss: 0.0908, step time: 1.0377\n",
      "7/295, train_loss: 0.0652, step time: 1.0701\n",
      "8/295, train_loss: 0.0993, step time: 1.0868\n",
      "9/295, train_loss: 0.3873, step time: 1.0526\n",
      "10/295, train_loss: 0.0545, step time: 1.0764\n",
      "11/295, train_loss: 0.1043, step time: 1.0397\n",
      "12/295, train_loss: 0.1457, step time: 1.0320\n",
      "13/295, train_loss: 0.0827, step time: 1.0898\n",
      "14/295, train_loss: 0.1521, step time: 1.0316\n",
      "15/295, train_loss: 0.1367, step time: 1.0353\n",
      "16/295, train_loss: 0.0688, step time: 1.0635\n",
      "17/295, train_loss: 0.1813, step time: 1.0333\n",
      "18/295, train_loss: 0.0615, step time: 1.0365\n",
      "19/295, train_loss: 0.0760, step time: 1.0372\n",
      "20/295, train_loss: 0.0496, step time: 1.0579\n",
      "21/295, train_loss: 0.0434, step time: 1.0704\n",
      "22/295, train_loss: 0.1035, step time: 1.0443\n",
      "23/295, train_loss: 0.3805, step time: 1.0496\n",
      "24/295, train_loss: 0.1263, step time: 1.0453\n",
      "25/295, train_loss: 0.0363, step time: 1.0598\n",
      "26/295, train_loss: 0.3040, step time: 1.0435\n",
      "27/295, train_loss: 0.1756, step time: 1.0317\n",
      "28/295, train_loss: 0.0676, step time: 1.0772\n",
      "29/295, train_loss: 0.0531, step time: 1.0452\n",
      "30/295, train_loss: 0.0874, step time: 1.1019\n",
      "31/295, train_loss: 0.0605, step time: 1.0324\n",
      "32/295, train_loss: 0.0565, step time: 1.0567\n",
      "33/295, train_loss: 0.1226, step time: 1.0326\n",
      "34/295, train_loss: 0.0691, step time: 1.0396\n",
      "35/295, train_loss: 0.0668, step time: 1.0321\n",
      "36/295, train_loss: 0.0871, step time: 1.0819\n",
      "37/295, train_loss: 0.0449, step time: 1.0570\n",
      "38/295, train_loss: 0.0869, step time: 1.0356\n",
      "39/295, train_loss: 0.0730, step time: 1.0696\n",
      "40/295, train_loss: 0.1498, step time: 1.0369\n",
      "41/295, train_loss: 0.0472, step time: 1.0312\n",
      "42/295, train_loss: 0.0616, step time: 1.0526\n",
      "43/295, train_loss: 0.1410, step time: 1.0329\n",
      "44/295, train_loss: 0.0678, step time: 1.0587\n",
      "45/295, train_loss: 0.4094, step time: 1.0341\n",
      "46/295, train_loss: 0.0642, step time: 1.0858\n",
      "47/295, train_loss: 0.0738, step time: 1.0383\n",
      "48/295, train_loss: 0.0591, step time: 1.0651\n",
      "49/295, train_loss: 0.0636, step time: 1.0726\n",
      "50/295, train_loss: 0.0562, step time: 1.0569\n",
      "51/295, train_loss: 0.1088, step time: 1.0423\n",
      "52/295, train_loss: 0.0852, step time: 1.0553\n",
      "53/295, train_loss: 0.0466, step time: 1.0337\n",
      "54/295, train_loss: 0.0462, step time: 1.0618\n",
      "55/295, train_loss: 0.1249, step time: 1.1289\n",
      "56/295, train_loss: 0.1020, step time: 1.0388\n",
      "57/295, train_loss: 0.0685, step time: 1.0333\n",
      "58/295, train_loss: 0.3900, step time: 1.0426\n",
      "59/295, train_loss: 0.0644, step time: 1.0653\n",
      "60/295, train_loss: 0.0456, step time: 1.0940\n",
      "61/295, train_loss: 0.0943, step time: 1.0346\n",
      "62/295, train_loss: 0.0809, step time: 1.0564\n",
      "63/295, train_loss: 0.0656, step time: 1.0689\n",
      "64/295, train_loss: 0.3913, step time: 1.0405\n",
      "65/295, train_loss: 0.0413, step time: 1.0458\n",
      "66/295, train_loss: 0.0616, step time: 1.0336\n",
      "67/295, train_loss: 0.0436, step time: 1.0371\n",
      "68/295, train_loss: 0.0321, step time: 1.0436\n",
      "69/295, train_loss: 0.0665, step time: 1.0640\n",
      "70/295, train_loss: 0.0804, step time: 1.0664\n",
      "71/295, train_loss: 0.1237, step time: 1.0454\n",
      "72/295, train_loss: 0.0702, step time: 1.0336\n",
      "73/295, train_loss: 0.0524, step time: 1.0460\n",
      "74/295, train_loss: 0.1122, step time: 1.0579\n",
      "75/295, train_loss: 0.0367, step time: 1.1027\n",
      "76/295, train_loss: 0.0567, step time: 1.1176\n",
      "77/295, train_loss: 0.0935, step time: 1.0466\n",
      "78/295, train_loss: 0.1784, step time: 1.0399\n",
      "79/295, train_loss: 0.1043, step time: 1.0372\n",
      "80/295, train_loss: 0.0385, step time: 1.0394\n",
      "81/295, train_loss: 0.0306, step time: 1.0420\n",
      "82/295, train_loss: 0.1350, step time: 1.0543\n",
      "83/295, train_loss: 0.0695, step time: 1.0514\n",
      "84/295, train_loss: 0.0502, step time: 1.0387\n",
      "85/295, train_loss: 0.1114, step time: 1.0318\n",
      "86/295, train_loss: 0.0688, step time: 1.0746\n",
      "87/295, train_loss: 0.0530, step time: 1.1197\n",
      "88/295, train_loss: 0.1198, step time: 1.0377\n",
      "89/295, train_loss: 0.0565, step time: 1.0392\n",
      "90/295, train_loss: 0.0545, step time: 1.0417\n",
      "91/295, train_loss: 0.1223, step time: 1.0478\n",
      "92/295, train_loss: 0.0508, step time: 1.0373\n",
      "93/295, train_loss: 0.0964, step time: 1.0420\n",
      "94/295, train_loss: 0.0613, step time: 1.1133\n",
      "95/295, train_loss: 0.0463, step time: 1.0508\n",
      "96/295, train_loss: 0.0479, step time: 1.0375\n",
      "97/295, train_loss: 0.0335, step time: 1.0437\n",
      "98/295, train_loss: 0.1063, step time: 1.1052\n",
      "99/295, train_loss: 0.1952, step time: 1.0476\n",
      "100/295, train_loss: 0.1602, step time: 1.0480\n",
      "101/295, train_loss: 0.0773, step time: 1.0566\n",
      "102/295, train_loss: 0.1059, step time: 1.0337\n",
      "103/295, train_loss: 0.1424, step time: 1.0475\n",
      "104/295, train_loss: 0.0724, step time: 1.0437\n",
      "105/295, train_loss: 0.3091, step time: 1.0742\n",
      "106/295, train_loss: 0.0451, step time: 1.0581\n",
      "107/295, train_loss: 0.0610, step time: 1.0416\n",
      "108/295, train_loss: 0.0501, step time: 1.0496\n",
      "109/295, train_loss: 0.1156, step time: 1.0438\n",
      "110/295, train_loss: 0.0626, step time: 1.0595\n",
      "111/295, train_loss: 0.0728, step time: 1.0506\n",
      "112/295, train_loss: 0.0769, step time: 1.0546\n",
      "113/295, train_loss: 0.0568, step time: 1.0383\n",
      "114/295, train_loss: 0.3760, step time: 1.0524\n",
      "115/295, train_loss: 0.0467, step time: 1.0420\n",
      "116/295, train_loss: 0.0919, step time: 1.0707\n",
      "117/295, train_loss: 0.1040, step time: 1.0663\n",
      "118/295, train_loss: 0.0569, step time: 1.0599\n",
      "119/295, train_loss: 0.0864, step time: 1.1090\n",
      "120/295, train_loss: 0.0733, step time: 1.0473\n",
      "121/295, train_loss: 0.0468, step time: 1.0325\n",
      "122/295, train_loss: 0.4724, step time: 1.0323\n",
      "123/295, train_loss: 0.0413, step time: 1.0434\n",
      "124/295, train_loss: 0.0698, step time: 1.0391\n",
      "125/295, train_loss: 0.4106, step time: 1.0723\n",
      "126/295, train_loss: 0.0606, step time: 1.0395\n",
      "127/295, train_loss: 0.0661, step time: 1.0571\n",
      "128/295, train_loss: 0.1002, step time: 1.0411\n",
      "129/295, train_loss: 0.0795, step time: 1.0830\n",
      "130/295, train_loss: 0.0454, step time: 1.0648\n",
      "131/295, train_loss: 0.0946, step time: 1.0545\n",
      "132/295, train_loss: 0.1667, step time: 1.0342\n",
      "133/295, train_loss: 0.3940, step time: 1.0548\n",
      "134/295, train_loss: 0.0851, step time: 1.0407\n",
      "135/295, train_loss: 0.1782, step time: 1.0486\n",
      "136/295, train_loss: 0.0487, step time: 1.0524\n",
      "137/295, train_loss: 0.0837, step time: 1.0397\n",
      "138/295, train_loss: 0.0799, step time: 1.0464\n",
      "139/295, train_loss: 0.0497, step time: 1.0741\n",
      "140/295, train_loss: 0.0725, step time: 1.1127\n",
      "141/295, train_loss: 0.0342, step time: 1.0368\n",
      "142/295, train_loss: 0.0802, step time: 1.0406\n",
      "143/295, train_loss: 0.0309, step time: 1.0410\n",
      "144/295, train_loss: 0.0411, step time: 1.0590\n",
      "145/295, train_loss: 0.0612, step time: 1.0455\n",
      "146/295, train_loss: 0.0539, step time: 1.0317\n",
      "147/295, train_loss: 0.1271, step time: 1.0320\n",
      "148/295, train_loss: 0.0996, step time: 1.0506\n",
      "149/295, train_loss: 0.0614, step time: 1.0577\n",
      "150/295, train_loss: 0.0809, step time: 1.0392\n",
      "151/295, train_loss: 0.3924, step time: 1.0390\n",
      "152/295, train_loss: 0.0405, step time: 1.0310\n",
      "153/295, train_loss: 0.0378, step time: 1.0374\n",
      "154/295, train_loss: 0.1246, step time: 1.0395\n",
      "155/295, train_loss: 0.1149, step time: 1.0534\n",
      "156/295, train_loss: 0.0469, step time: 1.0600\n",
      "157/295, train_loss: 0.0954, step time: 1.0354\n",
      "158/295, train_loss: 0.0379, step time: 1.0334\n",
      "159/295, train_loss: 0.3966, step time: 1.0354\n",
      "160/295, train_loss: 0.0893, step time: 1.1011\n",
      "161/295, train_loss: 0.0542, step time: 1.0376\n",
      "162/295, train_loss: 0.1000, step time: 1.0361\n",
      "163/295, train_loss: 0.0510, step time: 1.0346\n",
      "164/295, train_loss: 0.0671, step time: 1.0449\n",
      "165/295, train_loss: 0.0631, step time: 1.0310\n",
      "166/295, train_loss: 0.0615, step time: 1.0458\n",
      "167/295, train_loss: 0.1409, step time: 1.0400\n",
      "168/295, train_loss: 0.0891, step time: 1.0814\n",
      "169/295, train_loss: 0.0921, step time: 1.0328\n",
      "170/295, train_loss: 0.0299, step time: 1.0548\n",
      "171/295, train_loss: 0.0481, step time: 1.0513\n",
      "172/295, train_loss: 0.0511, step time: 1.0716\n",
      "173/295, train_loss: 0.0402, step time: 1.0427\n",
      "174/295, train_loss: 0.0489, step time: 1.0666\n",
      "175/295, train_loss: 0.1057, step time: 1.0627\n",
      "176/295, train_loss: 0.1038, step time: 1.0453\n",
      "177/295, train_loss: 0.3748, step time: 1.0415\n",
      "178/295, train_loss: 0.0418, step time: 1.0359\n",
      "179/295, train_loss: 0.0575, step time: 1.0456\n",
      "180/295, train_loss: 0.1733, step time: 1.0358\n",
      "181/295, train_loss: 0.0888, step time: 1.0415\n",
      "182/295, train_loss: 0.0630, step time: 1.0450\n",
      "183/295, train_loss: 0.0896, step time: 1.0370\n",
      "184/295, train_loss: 0.1239, step time: 1.0521\n",
      "185/295, train_loss: 0.0933, step time: 1.0417\n",
      "186/295, train_loss: 0.0511, step time: 1.0427\n",
      "187/295, train_loss: 0.0723, step time: 1.0456\n",
      "188/295, train_loss: 0.0933, step time: 1.0368\n",
      "189/295, train_loss: 0.0211, step time: 1.0362\n",
      "190/295, train_loss: 0.0899, step time: 1.0719\n",
      "191/295, train_loss: 0.0987, step time: 1.0996\n",
      "192/295, train_loss: 0.0515, step time: 1.0440\n",
      "193/295, train_loss: 0.0772, step time: 1.0591\n",
      "194/295, train_loss: 0.1000, step time: 1.0411\n",
      "195/295, train_loss: 0.0959, step time: 1.0382\n",
      "196/295, train_loss: 0.0469, step time: 1.0547\n",
      "197/295, train_loss: 0.1009, step time: 1.1148\n",
      "198/295, train_loss: 0.1070, step time: 1.0373\n",
      "199/295, train_loss: 0.0633, step time: 1.1410\n",
      "200/295, train_loss: 0.0712, step time: 1.0453\n",
      "201/295, train_loss: 0.0477, step time: 1.0473\n",
      "202/295, train_loss: 0.0377, step time: 1.0948\n",
      "203/295, train_loss: 0.0882, step time: 1.0349\n",
      "204/295, train_loss: 0.1148, step time: 1.0386\n",
      "205/295, train_loss: 0.3704, step time: 1.0396\n",
      "206/295, train_loss: 0.0931, step time: 1.0388\n",
      "207/295, train_loss: 0.4105, step time: 1.0498\n",
      "208/295, train_loss: 0.4049, step time: 1.0349\n",
      "209/295, train_loss: 0.0320, step time: 1.0753\n",
      "210/295, train_loss: 0.0495, step time: 1.0340\n",
      "211/295, train_loss: 0.1591, step time: 1.0373\n",
      "212/295, train_loss: 0.0375, step time: 1.0393\n",
      "213/295, train_loss: 0.0829, step time: 1.0406\n",
      "214/295, train_loss: 0.0680, step time: 1.0391\n",
      "215/295, train_loss: 0.0311, step time: 1.0352\n",
      "216/295, train_loss: 0.0733, step time: 1.0326\n",
      "217/295, train_loss: 0.1025, step time: 1.0346\n",
      "218/295, train_loss: 0.3228, step time: 1.0365\n",
      "219/295, train_loss: 0.0498, step time: 1.0351\n",
      "220/295, train_loss: 0.0925, step time: 1.0668\n",
      "221/295, train_loss: 0.0719, step time: 1.0387\n",
      "222/295, train_loss: 0.0476, step time: 1.0419\n",
      "223/295, train_loss: 0.0777, step time: 1.0495\n",
      "224/295, train_loss: 0.0799, step time: 1.0361\n",
      "225/295, train_loss: 0.0698, step time: 1.0412\n",
      "226/295, train_loss: 0.0379, step time: 1.0508\n",
      "227/295, train_loss: 0.0991, step time: 1.0378\n",
      "228/295, train_loss: 0.1183, step time: 1.0633\n",
      "229/295, train_loss: 0.0852, step time: 1.0375\n",
      "230/295, train_loss: 0.4053, step time: 1.0371\n",
      "231/295, train_loss: 0.0588, step time: 1.0671\n",
      "232/295, train_loss: 0.0865, step time: 1.1014\n",
      "233/295, train_loss: 0.0739, step time: 1.0358\n",
      "234/295, train_loss: 0.0650, step time: 1.0420\n",
      "235/295, train_loss: 0.0943, step time: 1.0410\n",
      "236/295, train_loss: 0.1090, step time: 1.0660\n",
      "237/295, train_loss: 0.0320, step time: 1.0521\n",
      "238/295, train_loss: 0.0909, step time: 1.0352\n",
      "239/295, train_loss: 0.0427, step time: 1.0385\n",
      "240/295, train_loss: 0.0519, step time: 1.0434\n",
      "241/295, train_loss: 0.3721, step time: 1.0513\n",
      "242/295, train_loss: 0.0327, step time: 1.0451\n",
      "243/295, train_loss: 0.0539, step time: 1.0407\n",
      "244/295, train_loss: 0.4098, step time: 1.0706\n",
      "245/295, train_loss: 0.0636, step time: 1.1401\n",
      "246/295, train_loss: 0.0918, step time: 1.0467\n",
      "247/295, train_loss: 0.4076, step time: 1.0328\n",
      "248/295, train_loss: 0.0966, step time: 1.0410\n",
      "249/295, train_loss: 0.0765, step time: 1.0405\n",
      "250/295, train_loss: 0.0537, step time: 1.0426\n",
      "251/295, train_loss: 0.1702, step time: 1.0461\n",
      "252/295, train_loss: 0.0501, step time: 1.0392\n",
      "253/295, train_loss: 0.0897, step time: 1.0520\n",
      "254/295, train_loss: 0.1879, step time: 1.0420\n",
      "255/295, train_loss: 0.0284, step time: 1.0595\n",
      "256/295, train_loss: 0.1266, step time: 1.0704\n",
      "257/295, train_loss: 0.0548, step time: 1.0438\n",
      "258/295, train_loss: 0.3971, step time: 1.0369\n",
      "259/295, train_loss: 0.2060, step time: 1.0316\n",
      "260/295, train_loss: 0.0826, step time: 1.0702\n",
      "261/295, train_loss: 0.1204, step time: 1.0450\n",
      "262/295, train_loss: 0.0579, step time: 1.0371\n",
      "263/295, train_loss: 0.0682, step time: 1.0373\n",
      "264/295, train_loss: 0.0414, step time: 1.0427\n",
      "265/295, train_loss: 0.0546, step time: 1.0756\n",
      "266/295, train_loss: 0.0778, step time: 1.0335\n",
      "267/295, train_loss: 0.1225, step time: 1.0457\n",
      "268/295, train_loss: 0.1068, step time: 1.0709\n",
      "269/295, train_loss: 0.1093, step time: 1.0514\n",
      "270/295, train_loss: 0.0877, step time: 1.0494\n",
      "271/295, train_loss: 0.0337, step time: 1.0344\n",
      "272/295, train_loss: 0.0448, step time: 1.0391\n",
      "273/295, train_loss: 0.1374, step time: 1.0764\n",
      "274/295, train_loss: 0.3669, step time: 1.0815\n",
      "275/295, train_loss: 0.0904, step time: 1.0452\n",
      "276/295, train_loss: 0.1734, step time: 1.0433\n",
      "277/295, train_loss: 0.1577, step time: 1.0680\n",
      "278/295, train_loss: 0.0362, step time: 1.0626\n",
      "279/295, train_loss: 0.0464, step time: 1.0553\n",
      "280/295, train_loss: 0.0607, step time: 1.0563\n",
      "281/295, train_loss: 0.0347, step time: 1.0370\n",
      "282/295, train_loss: 0.0482, step time: 1.0454\n",
      "283/295, train_loss: 0.0551, step time: 1.0414\n",
      "284/295, train_loss: 0.1115, step time: 1.0630\n",
      "285/295, train_loss: 0.0345, step time: 1.0455\n",
      "286/295, train_loss: 0.0974, step time: 1.0411\n",
      "287/295, train_loss: 0.1334, step time: 1.0366\n",
      "288/295, train_loss: 0.0638, step time: 1.0388\n",
      "289/295, train_loss: 0.0396, step time: 1.0289\n",
      "290/295, train_loss: 0.0341, step time: 1.0295\n",
      "291/295, train_loss: 0.0689, step time: 1.0294\n",
      "292/295, train_loss: 0.1289, step time: 1.0292\n",
      "293/295, train_loss: 0.1835, step time: 1.0289\n",
      "294/295, train_loss: 0.3929, step time: 1.0288\n",
      "295/295, train_loss: 0.0425, step time: 1.0292\n",
      "epoch 46 average loss: 0.1052\n",
      "current epoch: 46 current mean dice: 0.7682 tc: 0.7267 wt: 0.8309 et: 0.7515\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 46 is: 382.9367\n",
      "----------\n",
      "epoch 47/100\n",
      "1/295, train_loss: 0.0965, step time: 1.0586\n",
      "2/295, train_loss: 0.0397, step time: 1.0534\n",
      "3/295, train_loss: 0.0625, step time: 1.0794\n",
      "4/295, train_loss: 0.0787, step time: 1.1038\n",
      "5/295, train_loss: 0.0507, step time: 1.0979\n",
      "6/295, train_loss: 0.0607, step time: 1.0537\n",
      "7/295, train_loss: 0.3892, step time: 1.0987\n",
      "8/295, train_loss: 0.0389, step time: 1.0943\n",
      "9/295, train_loss: 0.4136, step time: 1.0404\n",
      "10/295, train_loss: 0.1590, step time: 1.0609\n",
      "11/295, train_loss: 0.0836, step time: 1.0307\n",
      "12/295, train_loss: 0.3943, step time: 1.0378\n",
      "13/295, train_loss: 0.0492, step time: 1.0626\n",
      "14/295, train_loss: 0.0963, step time: 1.0359\n",
      "15/295, train_loss: 0.1138, step time: 1.0332\n",
      "16/295, train_loss: 0.1138, step time: 1.0468\n",
      "17/295, train_loss: 0.0781, step time: 1.0447\n",
      "18/295, train_loss: 0.1171, step time: 1.0353\n",
      "19/295, train_loss: 0.0839, step time: 1.0669\n",
      "20/295, train_loss: 0.0446, step time: 1.0611\n",
      "21/295, train_loss: 0.0592, step time: 1.0743\n",
      "22/295, train_loss: 0.0715, step time: 1.0310\n",
      "23/295, train_loss: 0.0454, step time: 1.0354\n",
      "24/295, train_loss: 0.3104, step time: 1.0955\n",
      "25/295, train_loss: 0.0877, step time: 1.0522\n",
      "26/295, train_loss: 0.0471, step time: 1.0480\n",
      "27/295, train_loss: 0.0482, step time: 1.0512\n",
      "28/295, train_loss: 0.0541, step time: 1.0481\n",
      "29/295, train_loss: 0.3780, step time: 1.0393\n",
      "30/295, train_loss: 0.0476, step time: 1.0489\n",
      "31/295, train_loss: 0.0436, step time: 1.0725\n",
      "32/295, train_loss: 0.0674, step time: 1.0446\n",
      "33/295, train_loss: 0.0483, step time: 1.0354\n",
      "34/295, train_loss: 0.0750, step time: 1.0387\n",
      "35/295, train_loss: 0.0956, step time: 1.0459\n",
      "36/295, train_loss: 0.0521, step time: 1.0698\n",
      "37/295, train_loss: 0.0847, step time: 1.0514\n",
      "38/295, train_loss: 0.1039, step time: 1.0391\n",
      "39/295, train_loss: 0.0952, step time: 1.0640\n",
      "40/295, train_loss: 0.0391, step time: 1.0373\n",
      "41/295, train_loss: 0.3730, step time: 1.0486\n",
      "42/295, train_loss: 0.0304, step time: 1.0735\n",
      "43/295, train_loss: 0.4012, step time: 1.0436\n",
      "44/295, train_loss: 0.0354, step time: 1.0463\n",
      "45/295, train_loss: 0.0681, step time: 1.0395\n",
      "46/295, train_loss: 0.1574, step time: 1.0328\n",
      "47/295, train_loss: 0.0632, step time: 1.0384\n",
      "48/295, train_loss: 0.0535, step time: 1.0377\n",
      "49/295, train_loss: 0.0349, step time: 1.0401\n",
      "50/295, train_loss: 0.1083, step time: 1.0528\n",
      "51/295, train_loss: 0.4037, step time: 1.0531\n",
      "52/295, train_loss: 0.0718, step time: 1.0406\n",
      "53/295, train_loss: 0.0552, step time: 1.0707\n",
      "54/295, train_loss: 0.3912, step time: 1.0360\n",
      "55/295, train_loss: 0.0696, step time: 1.0997\n",
      "56/295, train_loss: 0.1514, step time: 1.0710\n",
      "57/295, train_loss: 0.1466, step time: 1.0635\n",
      "58/295, train_loss: 0.0792, step time: 1.0558\n",
      "59/295, train_loss: 0.0679, step time: 1.0572\n",
      "60/295, train_loss: 0.0500, step time: 1.0585\n",
      "61/295, train_loss: 0.0777, step time: 1.0503\n",
      "62/295, train_loss: 0.0428, step time: 1.0370\n",
      "63/295, train_loss: 0.0413, step time: 1.0311\n",
      "64/295, train_loss: 0.1161, step time: 1.0403\n",
      "65/295, train_loss: 0.0790, step time: 1.0961\n",
      "66/295, train_loss: 0.0400, step time: 1.0329\n",
      "67/295, train_loss: 0.0626, step time: 1.0310\n",
      "68/295, train_loss: 0.1205, step time: 1.0453\n",
      "69/295, train_loss: 0.0516, step time: 1.0409\n",
      "70/295, train_loss: 0.0577, step time: 1.0355\n",
      "71/295, train_loss: 0.0887, step time: 1.0362\n",
      "72/295, train_loss: 0.0788, step time: 1.0428\n",
      "73/295, train_loss: 0.3730, step time: 1.1183\n",
      "74/295, train_loss: 0.0529, step time: 1.0353\n",
      "75/295, train_loss: 0.0577, step time: 1.0572\n",
      "76/295, train_loss: 0.1744, step time: 1.0773\n",
      "77/295, train_loss: 0.1886, step time: 1.0343\n",
      "78/295, train_loss: 0.0683, step time: 1.0378\n",
      "79/295, train_loss: 0.3914, step time: 1.0420\n",
      "80/295, train_loss: 0.0687, step time: 1.0501\n",
      "81/295, train_loss: 0.0939, step time: 1.0758\n",
      "82/295, train_loss: 0.0611, step time: 1.1184\n",
      "83/295, train_loss: 0.0319, step time: 1.0592\n",
      "84/295, train_loss: 0.1287, step time: 1.0522\n",
      "85/295, train_loss: 0.0492, step time: 1.0610\n",
      "86/295, train_loss: 0.0314, step time: 1.0553\n",
      "87/295, train_loss: 0.0917, step time: 1.0607\n",
      "88/295, train_loss: 0.0523, step time: 1.0340\n",
      "89/295, train_loss: 0.0925, step time: 1.0315\n",
      "90/295, train_loss: 0.0653, step time: 1.0354\n",
      "91/295, train_loss: 0.0382, step time: 1.0697\n",
      "92/295, train_loss: 0.0889, step time: 1.0540\n",
      "93/295, train_loss: 0.0464, step time: 1.0440\n",
      "94/295, train_loss: 0.0626, step time: 1.0590\n",
      "95/295, train_loss: 0.0990, step time: 1.0324\n",
      "96/295, train_loss: 0.0972, step time: 1.0897\n",
      "97/295, train_loss: 0.0768, step time: 1.0367\n",
      "98/295, train_loss: 0.1048, step time: 1.0391\n",
      "99/295, train_loss: 0.0307, step time: 1.0575\n",
      "100/295, train_loss: 0.0703, step time: 1.0611\n",
      "101/295, train_loss: 0.0406, step time: 1.0401\n",
      "102/295, train_loss: 0.1025, step time: 1.0413\n",
      "103/295, train_loss: 0.1230, step time: 1.0471\n",
      "104/295, train_loss: 0.0364, step time: 1.0387\n",
      "105/295, train_loss: 0.0805, step time: 1.0716\n",
      "106/295, train_loss: 0.0376, step time: 1.0327\n",
      "107/295, train_loss: 0.1304, step time: 1.0318\n",
      "108/295, train_loss: 0.0926, step time: 1.0386\n",
      "109/295, train_loss: 0.0627, step time: 1.0760\n",
      "110/295, train_loss: 0.0902, step time: 1.0429\n",
      "111/295, train_loss: 0.4034, step time: 1.0468\n",
      "112/295, train_loss: 0.1213, step time: 1.0641\n",
      "113/295, train_loss: 0.0477, step time: 1.0535\n",
      "114/295, train_loss: 0.0356, step time: 1.0374\n",
      "115/295, train_loss: 0.0856, step time: 1.0369\n",
      "116/295, train_loss: 0.0487, step time: 1.0408\n",
      "117/295, train_loss: 0.1425, step time: 1.1300\n",
      "118/295, train_loss: 0.1073, step time: 1.0482\n",
      "119/295, train_loss: 0.1093, step time: 1.0789\n",
      "120/295, train_loss: 0.0572, step time: 1.0340\n",
      "121/295, train_loss: 0.0636, step time: 1.0435\n",
      "122/295, train_loss: 0.0663, step time: 1.0378\n",
      "123/295, train_loss: 0.0530, step time: 1.0393\n",
      "124/295, train_loss: 0.1689, step time: 1.0336\n",
      "125/295, train_loss: 0.0950, step time: 1.0392\n",
      "126/295, train_loss: 0.0783, step time: 1.0422\n",
      "127/295, train_loss: 0.0349, step time: 1.0384\n",
      "128/295, train_loss: 0.0514, step time: 1.0385\n",
      "129/295, train_loss: 0.1062, step time: 1.0368\n",
      "130/295, train_loss: 0.0688, step time: 1.0652\n",
      "131/295, train_loss: 0.0444, step time: 1.0469\n",
      "132/295, train_loss: 0.0818, step time: 1.0361\n",
      "133/295, train_loss: 0.0564, step time: 1.0925\n",
      "134/295, train_loss: 0.0491, step time: 1.0360\n",
      "135/295, train_loss: 0.0587, step time: 1.0499\n",
      "136/295, train_loss: 0.0936, step time: 1.0422\n",
      "137/295, train_loss: 0.1904, step time: 1.0367\n",
      "138/295, train_loss: 0.1276, step time: 1.0336\n",
      "139/295, train_loss: 0.0287, step time: 1.0535\n",
      "140/295, train_loss: 0.0574, step time: 1.0601\n",
      "141/295, train_loss: 0.0669, step time: 1.0667\n",
      "142/295, train_loss: 0.0731, step time: 1.0324\n",
      "143/295, train_loss: 0.0715, step time: 1.0552\n",
      "144/295, train_loss: 0.0668, step time: 1.0392\n",
      "145/295, train_loss: 0.4345, step time: 1.0325\n",
      "146/295, train_loss: 0.0540, step time: 1.0660\n",
      "147/295, train_loss: 0.0569, step time: 1.1228\n",
      "148/295, train_loss: 0.0467, step time: 1.0374\n",
      "149/295, train_loss: 0.3731, step time: 1.0326\n",
      "150/295, train_loss: 0.0391, step time: 1.0445\n",
      "151/295, train_loss: 0.0553, step time: 1.0377\n",
      "152/295, train_loss: 0.1186, step time: 1.0594\n",
      "153/295, train_loss: 0.1010, step time: 1.0553\n",
      "154/295, train_loss: 0.0360, step time: 1.0389\n",
      "155/295, train_loss: 0.0480, step time: 1.1040\n",
      "156/295, train_loss: 0.0433, step time: 1.0306\n",
      "157/295, train_loss: 0.3985, step time: 1.0351\n",
      "158/295, train_loss: 0.0384, step time: 1.0648\n",
      "159/295, train_loss: 0.0638, step time: 1.0716\n",
      "160/295, train_loss: 0.0615, step time: 1.0540\n",
      "161/295, train_loss: 0.4741, step time: 1.0544\n",
      "162/295, train_loss: 0.0680, step time: 1.0673\n",
      "163/295, train_loss: 0.1226, step time: 1.0383\n",
      "164/295, train_loss: 0.0381, step time: 1.0408\n",
      "165/295, train_loss: 0.0868, step time: 1.0740\n",
      "166/295, train_loss: 0.0789, step time: 1.0341\n",
      "167/295, train_loss: 0.0730, step time: 1.0667\n",
      "168/295, train_loss: 0.0763, step time: 1.0991\n",
      "169/295, train_loss: 0.0506, step time: 1.1004\n",
      "170/295, train_loss: 0.0891, step time: 1.0477\n",
      "171/295, train_loss: 0.1434, step time: 1.0491\n",
      "172/295, train_loss: 0.1102, step time: 1.0563\n",
      "173/295, train_loss: 0.1129, step time: 1.0895\n",
      "174/295, train_loss: 0.4158, step time: 1.0353\n",
      "175/295, train_loss: 0.0835, step time: 1.1132\n",
      "176/295, train_loss: 0.1188, step time: 1.0423\n",
      "177/295, train_loss: 0.0695, step time: 1.0748\n",
      "178/295, train_loss: 0.1495, step time: 1.1124\n",
      "179/295, train_loss: 0.0465, step time: 1.0320\n",
      "180/295, train_loss: 0.0326, step time: 1.0323\n",
      "181/295, train_loss: 0.0974, step time: 1.0367\n",
      "182/295, train_loss: 0.0291, step time: 1.0687\n",
      "183/295, train_loss: 0.1173, step time: 1.0436\n",
      "184/295, train_loss: 0.0805, step time: 1.0399\n",
      "185/295, train_loss: 0.0946, step time: 1.0360\n",
      "186/295, train_loss: 0.1177, step time: 1.0366\n",
      "187/295, train_loss: 0.0530, step time: 1.0451\n",
      "188/295, train_loss: 0.0812, step time: 1.0487\n",
      "189/295, train_loss: 0.0939, step time: 1.1456\n",
      "190/295, train_loss: 0.0454, step time: 1.0473\n",
      "191/295, train_loss: 0.0612, step time: 1.0444\n",
      "192/295, train_loss: 0.0361, step time: 1.0539\n",
      "193/295, train_loss: 0.0948, step time: 1.0482\n",
      "194/295, train_loss: 0.0625, step time: 1.0654\n",
      "195/295, train_loss: 0.0340, step time: 1.0373\n",
      "196/295, train_loss: 0.0451, step time: 1.0561\n",
      "197/295, train_loss: 0.0926, step time: 1.0310\n",
      "198/295, train_loss: 0.0388, step time: 1.1117\n",
      "199/295, train_loss: 0.0890, step time: 1.0428\n",
      "200/295, train_loss: 0.0637, step time: 1.0547\n",
      "201/295, train_loss: 0.1456, step time: 1.0321\n",
      "202/295, train_loss: 0.1414, step time: 1.0480\n",
      "203/295, train_loss: 0.1754, step time: 1.0519\n",
      "204/295, train_loss: 0.0700, step time: 1.0684\n",
      "205/295, train_loss: 0.0287, step time: 1.0380\n",
      "206/295, train_loss: 0.1129, step time: 1.0352\n",
      "207/295, train_loss: 0.0925, step time: 1.0814\n",
      "208/295, train_loss: 0.0835, step time: 1.0317\n",
      "209/295, train_loss: 0.1370, step time: 1.0580\n",
      "210/295, train_loss: 0.3922, step time: 1.0633\n",
      "211/295, train_loss: 0.1216, step time: 1.0655\n",
      "212/295, train_loss: 0.0367, step time: 1.0646\n",
      "213/295, train_loss: 0.0339, step time: 1.0448\n",
      "214/295, train_loss: 0.3633, step time: 1.0414\n",
      "215/295, train_loss: 0.0693, step time: 1.0734\n",
      "216/295, train_loss: 0.0613, step time: 1.0329\n",
      "217/295, train_loss: 0.1016, step time: 1.0442\n",
      "218/295, train_loss: 0.0587, step time: 1.0527\n",
      "219/295, train_loss: 0.0398, step time: 1.1376\n",
      "220/295, train_loss: 0.0221, step time: 1.0478\n",
      "221/295, train_loss: 0.0415, step time: 1.0321\n",
      "222/295, train_loss: 0.2090, step time: 1.0679\n",
      "223/295, train_loss: 0.0493, step time: 1.0956\n",
      "224/295, train_loss: 0.3774, step time: 1.0394\n",
      "225/295, train_loss: 0.1143, step time: 1.0579\n",
      "226/295, train_loss: 0.0916, step time: 1.0328\n",
      "227/295, train_loss: 0.0788, step time: 1.0711\n",
      "228/295, train_loss: 0.0509, step time: 1.0328\n",
      "229/295, train_loss: 0.0653, step time: 1.0366\n",
      "230/295, train_loss: 0.0521, step time: 1.1553\n",
      "231/295, train_loss: 0.3893, step time: 1.0554\n",
      "232/295, train_loss: 0.0659, step time: 1.0674\n",
      "233/295, train_loss: 0.0418, step time: 1.0717\n",
      "234/295, train_loss: 0.0559, step time: 1.0764\n",
      "235/295, train_loss: 0.0593, step time: 1.0340\n",
      "236/295, train_loss: 0.0906, step time: 1.0388\n",
      "237/295, train_loss: 0.2684, step time: 1.0374\n",
      "238/295, train_loss: 0.0400, step time: 1.0512\n",
      "239/295, train_loss: 0.0572, step time: 1.0550\n",
      "240/295, train_loss: 0.4091, step time: 1.0334\n",
      "241/295, train_loss: 0.1017, step time: 1.0329\n",
      "242/295, train_loss: 0.0961, step time: 1.0437\n",
      "243/295, train_loss: 0.1180, step time: 1.1019\n",
      "244/295, train_loss: 0.0821, step time: 1.0433\n",
      "245/295, train_loss: 0.1072, step time: 1.1131\n",
      "246/295, train_loss: 0.1110, step time: 1.0698\n",
      "247/295, train_loss: 0.1150, step time: 1.0336\n",
      "248/295, train_loss: 0.0530, step time: 1.0352\n",
      "249/295, train_loss: 0.1226, step time: 1.0554\n",
      "250/295, train_loss: 0.3077, step time: 1.0819\n",
      "251/295, train_loss: 0.0685, step time: 1.0435\n",
      "252/295, train_loss: 0.1269, step time: 1.0344\n",
      "253/295, train_loss: 0.0491, step time: 1.0343\n",
      "254/295, train_loss: 0.0516, step time: 1.0460\n",
      "255/295, train_loss: 0.0383, step time: 1.0519\n",
      "256/295, train_loss: 0.0314, step time: 1.0451\n",
      "257/295, train_loss: 0.1055, step time: 1.0421\n",
      "258/295, train_loss: 0.0281, step time: 1.0608\n",
      "259/295, train_loss: 0.0706, step time: 1.0587\n",
      "260/295, train_loss: 0.0443, step time: 1.0796\n",
      "261/295, train_loss: 0.0825, step time: 1.0384\n",
      "262/295, train_loss: 0.0583, step time: 1.0640\n",
      "263/295, train_loss: 0.0666, step time: 1.0342\n",
      "264/295, train_loss: 0.0332, step time: 1.0616\n",
      "265/295, train_loss: 0.0354, step time: 1.0473\n",
      "266/295, train_loss: 0.0539, step time: 1.0384\n",
      "267/295, train_loss: 0.0657, step time: 1.0393\n",
      "268/295, train_loss: 0.1328, step time: 1.0359\n",
      "269/295, train_loss: 0.0661, step time: 1.0467\n",
      "270/295, train_loss: 0.0639, step time: 1.1214\n",
      "271/295, train_loss: 0.0846, step time: 1.0381\n",
      "272/295, train_loss: 0.1337, step time: 1.0365\n",
      "273/295, train_loss: 0.1037, step time: 1.0355\n",
      "274/295, train_loss: 0.0772, step time: 1.0339\n",
      "275/295, train_loss: 0.0546, step time: 1.0392\n",
      "276/295, train_loss: 0.0383, step time: 1.0615\n",
      "277/295, train_loss: 0.0382, step time: 1.0415\n",
      "278/295, train_loss: 0.0613, step time: 1.0370\n",
      "279/295, train_loss: 0.1721, step time: 1.0437\n",
      "280/295, train_loss: 0.0831, step time: 1.0741\n",
      "281/295, train_loss: 0.0633, step time: 1.0425\n",
      "282/295, train_loss: 0.3765, step time: 1.0399\n",
      "283/295, train_loss: 0.0979, step time: 1.0509\n",
      "284/295, train_loss: 0.0864, step time: 1.0435\n",
      "285/295, train_loss: 0.0891, step time: 1.0385\n",
      "286/295, train_loss: 0.1230, step time: 1.0475\n",
      "287/295, train_loss: 0.0605, step time: 1.0477\n",
      "288/295, train_loss: 0.0550, step time: 1.0303\n",
      "289/295, train_loss: 0.0530, step time: 1.0305\n",
      "290/295, train_loss: 0.3311, step time: 1.0293\n",
      "291/295, train_loss: 0.1282, step time: 1.0288\n",
      "292/295, train_loss: 0.0436, step time: 1.0291\n",
      "293/295, train_loss: 0.0693, step time: 1.0283\n",
      "294/295, train_loss: 0.0701, step time: 1.0304\n",
      "295/295, train_loss: 0.1098, step time: 1.0298\n",
      "epoch 47 average loss: 0.1034\n",
      "current epoch: 47 current mean dice: 0.7926 tc: 0.7542 wt: 0.8682 et: 0.7624\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 47 is: 389.4984\n",
      "----------\n",
      "epoch 48/100\n",
      "1/295, train_loss: 0.0994, step time: 1.1475\n",
      "2/295, train_loss: 0.0803, step time: 1.1577\n",
      "3/295, train_loss: 0.1098, step time: 1.0464\n",
      "4/295, train_loss: 0.0592, step time: 1.0645\n",
      "5/295, train_loss: 0.0548, step time: 1.0830\n",
      "6/295, train_loss: 0.0666, step time: 1.0390\n",
      "7/295, train_loss: 0.0647, step time: 1.0342\n",
      "8/295, train_loss: 0.0430, step time: 1.0358\n",
      "9/295, train_loss: 0.0552, step time: 1.0363\n",
      "10/295, train_loss: 0.0302, step time: 1.0641\n",
      "11/295, train_loss: 0.1571, step time: 1.0449\n",
      "12/295, train_loss: 0.0297, step time: 1.0482\n",
      "13/295, train_loss: 0.0500, step time: 1.0320\n",
      "14/295, train_loss: 0.0737, step time: 1.0310\n",
      "15/295, train_loss: 0.0398, step time: 1.0392\n",
      "16/295, train_loss: 0.0489, step time: 1.0430\n",
      "17/295, train_loss: 0.0538, step time: 1.0750\n",
      "18/295, train_loss: 0.0458, step time: 1.0523\n",
      "19/295, train_loss: 0.1050, step time: 1.0457\n",
      "20/295, train_loss: 0.0667, step time: 1.0414\n",
      "21/295, train_loss: 0.0983, step time: 1.0689\n",
      "22/295, train_loss: 0.0349, step time: 1.0353\n",
      "23/295, train_loss: 0.0365, step time: 1.0387\n",
      "24/295, train_loss: 0.0895, step time: 1.0885\n",
      "25/295, train_loss: 0.1414, step time: 1.0319\n",
      "26/295, train_loss: 0.1009, step time: 1.0336\n",
      "27/295, train_loss: 0.0913, step time: 1.0529\n",
      "28/295, train_loss: 0.3878, step time: 1.0517\n",
      "29/295, train_loss: 0.0310, step time: 1.0726\n",
      "30/295, train_loss: 0.0850, step time: 1.0624\n",
      "31/295, train_loss: 0.0473, step time: 1.0386\n",
      "32/295, train_loss: 0.0949, step time: 1.0317\n",
      "33/295, train_loss: 0.0659, step time: 1.0309\n",
      "34/295, train_loss: 0.0453, step time: 1.0334\n",
      "35/295, train_loss: 0.0436, step time: 1.1053\n",
      "36/295, train_loss: 0.0881, step time: 1.0379\n",
      "37/295, train_loss: 0.0687, step time: 1.1284\n",
      "38/295, train_loss: 0.0610, step time: 1.0402\n",
      "39/295, train_loss: 0.0935, step time: 1.0374\n",
      "40/295, train_loss: 0.1601, step time: 1.0544\n",
      "41/295, train_loss: 0.0290, step time: 1.1650\n",
      "42/295, train_loss: 0.4142, step time: 1.0376\n",
      "43/295, train_loss: 0.0635, step time: 1.0389\n",
      "44/295, train_loss: 0.0825, step time: 1.0357\n",
      "45/295, train_loss: 0.0365, step time: 1.0491\n",
      "46/295, train_loss: 0.0500, step time: 1.0366\n",
      "47/295, train_loss: 0.1283, step time: 1.0437\n",
      "48/295, train_loss: 0.0667, step time: 1.0756\n",
      "49/295, train_loss: 0.0475, step time: 1.0525\n",
      "50/295, train_loss: 0.0435, step time: 1.0504\n",
      "51/295, train_loss: 0.0381, step time: 1.0333\n",
      "52/295, train_loss: 0.3981, step time: 1.0469\n",
      "53/295, train_loss: 0.0840, step time: 1.0761\n",
      "54/295, train_loss: 0.0348, step time: 1.0359\n",
      "55/295, train_loss: 0.0768, step time: 1.0460\n",
      "56/295, train_loss: 0.4191, step time: 1.0551\n",
      "57/295, train_loss: 0.3730, step time: 1.0407\n",
      "58/295, train_loss: 0.0546, step time: 1.0640\n",
      "59/295, train_loss: 0.0906, step time: 1.1112\n",
      "60/295, train_loss: 0.1328, step time: 1.0361\n",
      "61/295, train_loss: 0.0705, step time: 1.0380\n",
      "62/295, train_loss: 0.0721, step time: 1.0563\n",
      "63/295, train_loss: 0.0915, step time: 1.0792\n",
      "64/295, train_loss: 0.0433, step time: 1.0307\n",
      "65/295, train_loss: 0.0716, step time: 1.0400\n",
      "66/295, train_loss: 0.0387, step time: 1.0390\n",
      "67/295, train_loss: 0.0587, step time: 1.0351\n",
      "68/295, train_loss: 0.3993, step time: 1.0340\n",
      "69/295, train_loss: 0.0799, step time: 1.0353\n",
      "70/295, train_loss: 0.1036, step time: 1.0718\n",
      "71/295, train_loss: 0.0645, step time: 1.0391\n",
      "72/295, train_loss: 0.0873, step time: 1.0653\n",
      "73/295, train_loss: 0.0425, step time: 1.0627\n",
      "74/295, train_loss: 0.0635, step time: 1.0310\n",
      "75/295, train_loss: 0.3919, step time: 1.0358\n",
      "76/295, train_loss: 0.0526, step time: 1.0442\n",
      "77/295, train_loss: 0.1250, step time: 1.0788\n",
      "78/295, train_loss: 0.0887, step time: 1.0350\n",
      "79/295, train_loss: 0.3963, step time: 1.0378\n",
      "80/295, train_loss: 0.1278, step time: 1.0487\n",
      "81/295, train_loss: 0.0328, step time: 1.0396\n",
      "82/295, train_loss: 0.0336, step time: 1.0924\n",
      "83/295, train_loss: 0.0973, step time: 1.0695\n",
      "84/295, train_loss: 0.0901, step time: 1.0355\n",
      "85/295, train_loss: 0.1136, step time: 1.0383\n",
      "86/295, train_loss: 0.0400, step time: 1.1439\n",
      "87/295, train_loss: 0.0589, step time: 1.0442\n",
      "88/295, train_loss: 0.0370, step time: 1.0395\n",
      "89/295, train_loss: 0.1176, step time: 1.0427\n",
      "90/295, train_loss: 0.0469, step time: 1.0399\n",
      "91/295, train_loss: 0.0582, step time: 1.0448\n",
      "92/295, train_loss: 0.0841, step time: 1.0524\n",
      "93/295, train_loss: 0.1131, step time: 1.0553\n",
      "94/295, train_loss: 0.0446, step time: 1.0426\n",
      "95/295, train_loss: 0.1171, step time: 1.0942\n",
      "96/295, train_loss: 0.0779, step time: 1.0337\n",
      "97/295, train_loss: 0.0482, step time: 1.0451\n",
      "98/295, train_loss: 0.0416, step time: 1.0457\n",
      "99/295, train_loss: 0.0755, step time: 1.0333\n",
      "100/295, train_loss: 0.0343, step time: 1.0352\n",
      "101/295, train_loss: 0.0967, step time: 1.0789\n",
      "102/295, train_loss: 0.1379, step time: 1.0375\n",
      "103/295, train_loss: 0.1097, step time: 1.0453\n",
      "104/295, train_loss: 0.0452, step time: 1.0348\n",
      "105/295, train_loss: 0.0712, step time: 1.0438\n",
      "106/295, train_loss: 0.0594, step time: 1.0332\n",
      "107/295, train_loss: 0.1709, step time: 1.0313\n",
      "108/295, train_loss: 0.0842, step time: 1.0594\n",
      "109/295, train_loss: 0.0901, step time: 1.0538\n",
      "110/295, train_loss: 0.0951, step time: 1.0551\n",
      "111/295, train_loss: 0.3698, step time: 1.0474\n",
      "112/295, train_loss: 0.0820, step time: 1.0639\n",
      "113/295, train_loss: 0.0770, step time: 1.0902\n",
      "114/295, train_loss: 0.0617, step time: 1.0468\n",
      "115/295, train_loss: 0.1902, step time: 1.0778\n",
      "116/295, train_loss: 0.0523, step time: 1.0544\n",
      "117/295, train_loss: 0.0983, step time: 1.0566\n",
      "118/295, train_loss: 0.1703, step time: 1.0607\n",
      "119/295, train_loss: 0.0508, step time: 1.0942\n",
      "120/295, train_loss: 0.0876, step time: 1.0346\n",
      "121/295, train_loss: 0.0877, step time: 1.0422\n",
      "122/295, train_loss: 0.1072, step time: 1.0380\n",
      "123/295, train_loss: 0.1637, step time: 1.0765\n",
      "124/295, train_loss: 0.0876, step time: 1.0328\n",
      "125/295, train_loss: 0.0394, step time: 1.0523\n",
      "126/295, train_loss: 0.0506, step time: 1.0402\n",
      "127/295, train_loss: 0.0441, step time: 1.0849\n",
      "128/295, train_loss: 0.0457, step time: 1.0369\n",
      "129/295, train_loss: 0.0589, step time: 1.0356\n",
      "130/295, train_loss: 0.0647, step time: 1.0397\n",
      "131/295, train_loss: 0.0679, step time: 1.0383\n",
      "132/295, train_loss: 0.1074, step time: 1.0693\n",
      "133/295, train_loss: 0.0717, step time: 1.0512\n",
      "134/295, train_loss: 0.1104, step time: 1.0626\n",
      "135/295, train_loss: 0.0321, step time: 1.0328\n",
      "136/295, train_loss: 0.0857, step time: 1.0626\n",
      "137/295, train_loss: 0.0634, step time: 1.0369\n",
      "138/295, train_loss: 0.0505, step time: 1.0359\n",
      "139/295, train_loss: 0.0508, step time: 1.0503\n",
      "140/295, train_loss: 0.1679, step time: 1.0601\n",
      "141/295, train_loss: 0.1114, step time: 1.1638\n",
      "142/295, train_loss: 0.1323, step time: 1.0326\n",
      "143/295, train_loss: 0.0987, step time: 1.0331\n",
      "144/295, train_loss: 0.0700, step time: 1.0317\n",
      "145/295, train_loss: 0.1316, step time: 1.0411\n",
      "146/295, train_loss: 0.0919, step time: 1.0576\n",
      "147/295, train_loss: 0.0725, step time: 1.0636\n",
      "148/295, train_loss: 0.0524, step time: 1.0374\n",
      "149/295, train_loss: 0.0625, step time: 1.0372\n",
      "150/295, train_loss: 0.0843, step time: 1.0371\n",
      "151/295, train_loss: 0.0929, step time: 1.0554\n",
      "152/295, train_loss: 0.0694, step time: 1.0490\n",
      "153/295, train_loss: 0.0401, step time: 1.0478\n",
      "154/295, train_loss: 0.0230, step time: 1.0511\n",
      "155/295, train_loss: 0.0462, step time: 1.0551\n",
      "156/295, train_loss: 0.1249, step time: 1.0504\n",
      "157/295, train_loss: 0.4008, step time: 1.0567\n",
      "158/295, train_loss: 0.0785, step time: 1.0464\n",
      "159/295, train_loss: 0.0725, step time: 1.0329\n",
      "160/295, train_loss: 0.0594, step time: 1.0360\n",
      "161/295, train_loss: 0.1260, step time: 1.0464\n",
      "162/295, train_loss: 0.1167, step time: 1.0435\n",
      "163/295, train_loss: 0.0504, step time: 1.0344\n",
      "164/295, train_loss: 0.0822, step time: 1.0334\n",
      "165/295, train_loss: 0.2051, step time: 1.0411\n",
      "166/295, train_loss: 0.0877, step time: 1.0381\n",
      "167/295, train_loss: 0.0545, step time: 1.0369\n",
      "168/295, train_loss: 0.0571, step time: 1.0645\n",
      "169/295, train_loss: 0.0638, step time: 1.0387\n",
      "170/295, train_loss: 0.0546, step time: 1.0354\n",
      "171/295, train_loss: 0.1026, step time: 1.0336\n",
      "172/295, train_loss: 0.0535, step time: 1.0403\n",
      "173/295, train_loss: 0.0984, step time: 1.0364\n",
      "174/295, train_loss: 0.1175, step time: 1.0364\n",
      "175/295, train_loss: 0.4280, step time: 1.0923\n",
      "176/295, train_loss: 0.2551, step time: 1.0403\n",
      "177/295, train_loss: 0.0313, step time: 1.0369\n",
      "178/295, train_loss: 0.0415, step time: 1.0548\n",
      "179/295, train_loss: 0.0592, step time: 1.0395\n",
      "180/295, train_loss: 0.0454, step time: 1.0356\n",
      "181/295, train_loss: 0.0699, step time: 1.0324\n",
      "182/295, train_loss: 0.0365, step time: 1.0381\n",
      "183/295, train_loss: 0.0992, step time: 1.0347\n",
      "184/295, train_loss: 0.0766, step time: 1.0463\n",
      "185/295, train_loss: 0.1017, step time: 1.0390\n",
      "186/295, train_loss: 0.0485, step time: 1.0461\n",
      "187/295, train_loss: 0.0738, step time: 1.0367\n",
      "188/295, train_loss: 0.0447, step time: 1.0388\n",
      "189/295, train_loss: 0.1225, step time: 1.1060\n",
      "190/295, train_loss: 0.1250, step time: 1.0344\n",
      "191/295, train_loss: 0.0540, step time: 1.0383\n",
      "192/295, train_loss: 0.1104, step time: 1.0341\n",
      "193/295, train_loss: 0.1882, step time: 1.1336\n",
      "194/295, train_loss: 0.2719, step time: 1.0317\n",
      "195/295, train_loss: 0.1742, step time: 1.0577\n",
      "196/295, train_loss: 0.0700, step time: 1.0572\n",
      "197/295, train_loss: 0.0518, step time: 1.0401\n",
      "198/295, train_loss: 0.1015, step time: 1.0595\n",
      "199/295, train_loss: 0.0821, step time: 1.0437\n",
      "200/295, train_loss: 0.0625, step time: 1.0382\n",
      "201/295, train_loss: 0.1192, step time: 1.0432\n",
      "202/295, train_loss: 0.0401, step time: 1.0680\n",
      "203/295, train_loss: 0.0665, step time: 1.0422\n",
      "204/295, train_loss: 0.0820, step time: 1.0620\n",
      "205/295, train_loss: 0.0490, step time: 1.0854\n",
      "206/295, train_loss: 0.0621, step time: 1.0324\n",
      "207/295, train_loss: 0.0598, step time: 1.0321\n",
      "208/295, train_loss: 0.0466, step time: 1.0426\n",
      "209/295, train_loss: 0.1318, step time: 1.0610\n",
      "210/295, train_loss: 0.0956, step time: 1.0409\n",
      "211/295, train_loss: 0.0819, step time: 1.0465\n",
      "212/295, train_loss: 0.0627, step time: 1.0539\n",
      "213/295, train_loss: 0.0545, step time: 1.0710\n",
      "214/295, train_loss: 0.3790, step time: 1.0331\n",
      "215/295, train_loss: 0.0386, step time: 1.0402\n",
      "216/295, train_loss: 0.0941, step time: 1.0468\n",
      "217/295, train_loss: 0.3773, step time: 1.0414\n",
      "218/295, train_loss: 0.0308, step time: 1.0324\n",
      "219/295, train_loss: 0.0556, step time: 1.0542\n",
      "220/295, train_loss: 0.0364, step time: 1.0395\n",
      "221/295, train_loss: 0.0453, step time: 1.0338\n",
      "222/295, train_loss: 0.3376, step time: 1.0711\n",
      "223/295, train_loss: 0.1215, step time: 1.1163\n",
      "224/295, train_loss: 0.0619, step time: 1.0392\n",
      "225/295, train_loss: 0.0871, step time: 1.0467\n",
      "226/295, train_loss: 0.0608, step time: 1.0441\n",
      "227/295, train_loss: 0.0367, step time: 1.0465\n",
      "228/295, train_loss: 0.0420, step time: 1.0403\n",
      "229/295, train_loss: 0.3759, step time: 1.0584\n",
      "230/295, train_loss: 0.0758, step time: 1.0493\n",
      "231/295, train_loss: 0.0707, step time: 1.0472\n",
      "232/295, train_loss: 0.1315, step time: 1.0318\n",
      "233/295, train_loss: 0.0957, step time: 1.0357\n",
      "234/295, train_loss: 0.0407, step time: 1.0485\n",
      "235/295, train_loss: 0.3797, step time: 1.0361\n",
      "236/295, train_loss: 0.1025, step time: 1.0823\n",
      "237/295, train_loss: 0.0476, step time: 1.0349\n",
      "238/295, train_loss: 0.0649, step time: 1.1230\n",
      "239/295, train_loss: 0.0573, step time: 1.1250\n",
      "240/295, train_loss: 0.1142, step time: 1.0882\n",
      "241/295, train_loss: 0.1471, step time: 1.0692\n",
      "242/295, train_loss: 0.0656, step time: 1.0335\n",
      "243/295, train_loss: 0.3981, step time: 1.0489\n",
      "244/295, train_loss: 0.2578, step time: 1.0494\n",
      "245/295, train_loss: 0.3172, step time: 1.0434\n",
      "246/295, train_loss: 0.1444, step time: 1.0445\n",
      "247/295, train_loss: 0.0553, step time: 1.0425\n",
      "248/295, train_loss: 0.1009, step time: 1.0367\n",
      "249/295, train_loss: 0.1523, step time: 1.0371\n",
      "250/295, train_loss: 0.0489, step time: 1.0485\n",
      "251/295, train_loss: 0.0754, step time: 1.0400\n",
      "252/295, train_loss: 0.0493, step time: 1.0362\n",
      "253/295, train_loss: 0.1269, step time: 1.0425\n",
      "254/295, train_loss: 0.1091, step time: 1.0454\n",
      "255/295, train_loss: 0.1049, step time: 1.0729\n",
      "256/295, train_loss: 0.0588, step time: 1.0405\n",
      "257/295, train_loss: 0.0332, step time: 1.0334\n",
      "258/295, train_loss: 0.0680, step time: 1.0468\n",
      "259/295, train_loss: 0.1104, step time: 1.0562\n",
      "260/295, train_loss: 0.1716, step time: 1.0355\n",
      "261/295, train_loss: 0.0958, step time: 1.0361\n",
      "262/295, train_loss: 0.0311, step time: 1.0402\n",
      "263/295, train_loss: 0.0946, step time: 1.0353\n",
      "264/295, train_loss: 0.0676, step time: 1.0432\n",
      "265/295, train_loss: 0.4143, step time: 1.0368\n",
      "266/295, train_loss: 0.1234, step time: 1.0354\n",
      "267/295, train_loss: 0.0651, step time: 1.0482\n",
      "268/295, train_loss: 0.4919, step time: 1.0524\n",
      "269/295, train_loss: 0.0643, step time: 1.0499\n",
      "270/295, train_loss: 0.0439, step time: 1.0504\n",
      "271/295, train_loss: 0.1026, step time: 1.0320\n",
      "272/295, train_loss: 0.1012, step time: 1.1161\n",
      "273/295, train_loss: 0.0330, step time: 1.0330\n",
      "274/295, train_loss: 0.0435, step time: 1.0473\n",
      "275/295, train_loss: 0.0818, step time: 1.0443\n",
      "276/295, train_loss: 0.1820, step time: 1.0449\n",
      "277/295, train_loss: 0.3622, step time: 1.0575\n",
      "278/295, train_loss: 0.0895, step time: 1.0425\n",
      "279/295, train_loss: 0.4011, step time: 1.0667\n",
      "280/295, train_loss: 0.1516, step time: 1.0575\n",
      "281/295, train_loss: 0.1336, step time: 1.0351\n",
      "282/295, train_loss: 0.1019, step time: 1.0381\n",
      "283/295, train_loss: 0.4125, step time: 1.0346\n",
      "284/295, train_loss: 0.3811, step time: 1.0427\n",
      "285/295, train_loss: 0.0871, step time: 1.0732\n",
      "286/295, train_loss: 0.0408, step time: 1.0443\n",
      "287/295, train_loss: 0.1952, step time: 1.0561\n",
      "288/295, train_loss: 0.0485, step time: 1.0300\n",
      "289/295, train_loss: 0.0608, step time: 1.0295\n",
      "290/295, train_loss: 0.0817, step time: 1.0282\n",
      "291/295, train_loss: 0.0389, step time: 1.0292\n",
      "292/295, train_loss: 0.1620, step time: 1.0302\n",
      "293/295, train_loss: 0.0671, step time: 1.0299\n",
      "294/295, train_loss: 0.0786, step time: 1.0287\n",
      "295/295, train_loss: 0.0561, step time: 1.0350\n",
      "epoch 48 average loss: 0.1063\n",
      "current epoch: 48 current mean dice: 0.7667 tc: 0.7122 wt: 0.8453 et: 0.7450\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 48 is: 387.8172\n",
      "----------\n",
      "epoch 49/100\n",
      "1/295, train_loss: 0.1457, step time: 1.0845\n",
      "2/295, train_loss: 0.0528, step time: 1.1068\n",
      "3/295, train_loss: 0.0380, step time: 1.0536\n",
      "4/295, train_loss: 0.0560, step time: 1.0588\n",
      "5/295, train_loss: 0.0369, step time: 1.0302\n",
      "6/295, train_loss: 0.1183, step time: 1.0408\n",
      "7/295, train_loss: 0.4007, step time: 1.0531\n",
      "8/295, train_loss: 0.0631, step time: 1.0709\n",
      "9/295, train_loss: 0.0423, step time: 1.0335\n",
      "10/295, train_loss: 0.1693, step time: 1.0760\n",
      "11/295, train_loss: 0.1771, step time: 1.0647\n",
      "12/295, train_loss: 0.4949, step time: 1.0334\n",
      "13/295, train_loss: 0.0848, step time: 1.0395\n",
      "14/295, train_loss: 0.0679, step time: 1.0366\n",
      "15/295, train_loss: 0.1004, step time: 1.0420\n",
      "16/295, train_loss: 0.0609, step time: 1.0389\n",
      "17/295, train_loss: 0.0718, step time: 1.0600\n",
      "18/295, train_loss: 0.1080, step time: 1.0557\n",
      "19/295, train_loss: 0.2216, step time: 1.0515\n",
      "20/295, train_loss: 0.1088, step time: 1.0431\n",
      "21/295, train_loss: 0.0469, step time: 1.0863\n",
      "22/295, train_loss: 0.0492, step time: 1.0508\n",
      "23/295, train_loss: 0.0856, step time: 1.0963\n",
      "24/295, train_loss: 0.0322, step time: 1.1085\n",
      "25/295, train_loss: 0.0436, step time: 1.0769\n",
      "26/295, train_loss: 0.0970, step time: 1.0549\n",
      "27/295, train_loss: 0.3734, step time: 1.0916\n",
      "28/295, train_loss: 0.0699, step time: 1.0570\n",
      "29/295, train_loss: 0.0757, step time: 1.0699\n",
      "30/295, train_loss: 0.1228, step time: 1.0697\n",
      "31/295, train_loss: 0.4073, step time: 1.0293\n",
      "32/295, train_loss: 0.0637, step time: 1.0314\n",
      "33/295, train_loss: 0.0761, step time: 1.0663\n",
      "34/295, train_loss: 0.0517, step time: 1.0355\n",
      "35/295, train_loss: 0.3610, step time: 1.0745\n",
      "36/295, train_loss: 0.0544, step time: 1.0482\n",
      "37/295, train_loss: 0.1224, step time: 1.0430\n",
      "38/295, train_loss: 0.0819, step time: 1.0355\n",
      "39/295, train_loss: 0.1192, step time: 1.0369\n",
      "40/295, train_loss: 0.0557, step time: 1.0397\n",
      "41/295, train_loss: 0.0756, step time: 1.0394\n",
      "42/295, train_loss: 0.3870, step time: 1.0527\n",
      "43/295, train_loss: 0.0511, step time: 1.1111\n",
      "44/295, train_loss: 0.3791, step time: 1.0373\n",
      "45/295, train_loss: 0.0350, step time: 1.0667\n",
      "46/295, train_loss: 0.0640, step time: 1.0440\n",
      "47/295, train_loss: 0.0448, step time: 1.0338\n",
      "48/295, train_loss: 0.0609, step time: 1.0322\n",
      "49/295, train_loss: 0.0863, step time: 1.0408\n",
      "50/295, train_loss: 0.0451, step time: 1.0316\n",
      "51/295, train_loss: 0.0873, step time: 1.0553\n",
      "52/295, train_loss: 0.0532, step time: 1.0508\n",
      "53/295, train_loss: 0.1267, step time: 1.0397\n",
      "54/295, train_loss: 0.0704, step time: 1.0372\n",
      "55/295, train_loss: 0.1299, step time: 1.0521\n",
      "56/295, train_loss: 0.0885, step time: 1.1076\n",
      "57/295, train_loss: 0.0840, step time: 1.0478\n",
      "58/295, train_loss: 0.0588, step time: 1.0387\n",
      "59/295, train_loss: 0.0683, step time: 1.0618\n",
      "60/295, train_loss: 0.0403, step time: 1.0317\n",
      "61/295, train_loss: 0.0413, step time: 1.0411\n",
      "62/295, train_loss: 0.0622, step time: 1.0757\n",
      "63/295, train_loss: 0.0499, step time: 1.0432\n",
      "64/295, train_loss: 0.0556, step time: 1.0420\n",
      "65/295, train_loss: 0.4192, step time: 1.0318\n",
      "66/295, train_loss: 0.0659, step time: 1.0406\n",
      "67/295, train_loss: 0.0924, step time: 1.0408\n",
      "68/295, train_loss: 0.0977, step time: 1.0376\n",
      "69/295, train_loss: 0.1113, step time: 1.0355\n",
      "70/295, train_loss: 0.1168, step time: 1.0764\n",
      "71/295, train_loss: 0.0464, step time: 1.0537\n",
      "72/295, train_loss: 0.0575, step time: 1.0562\n",
      "73/295, train_loss: 0.0552, step time: 1.0527\n",
      "74/295, train_loss: 0.0431, step time: 1.0332\n",
      "75/295, train_loss: 0.1117, step time: 1.0454\n",
      "76/295, train_loss: 0.0561, step time: 1.0572\n",
      "77/295, train_loss: 0.0528, step time: 1.0409\n",
      "78/295, train_loss: 0.0504, step time: 1.0507\n",
      "79/295, train_loss: 0.3976, step time: 1.0406\n",
      "80/295, train_loss: 0.0646, step time: 1.0638\n",
      "81/295, train_loss: 0.0914, step time: 1.0829\n",
      "82/295, train_loss: 0.0853, step time: 1.0740\n",
      "83/295, train_loss: 0.1578, step time: 1.0670\n",
      "84/295, train_loss: 0.0307, step time: 1.0565\n",
      "85/295, train_loss: 0.1508, step time: 1.0418\n",
      "86/295, train_loss: 0.0465, step time: 1.0548\n",
      "87/295, train_loss: 0.1296, step time: 1.0321\n",
      "88/295, train_loss: 0.0328, step time: 1.0376\n",
      "89/295, train_loss: 0.0657, step time: 1.0337\n",
      "90/295, train_loss: 0.0393, step time: 1.0794\n",
      "91/295, train_loss: 0.0354, step time: 1.0403\n",
      "92/295, train_loss: 0.0294, step time: 1.1110\n",
      "93/295, train_loss: 0.0404, step time: 1.1781\n",
      "94/295, train_loss: 0.0751, step time: 1.0345\n",
      "95/295, train_loss: 0.1113, step time: 1.0392\n",
      "96/295, train_loss: 0.0788, step time: 1.0350\n",
      "97/295, train_loss: 0.0508, step time: 1.0540\n",
      "98/295, train_loss: 0.0397, step time: 1.0310\n",
      "99/295, train_loss: 0.1419, step time: 1.0425\n",
      "100/295, train_loss: 0.0467, step time: 1.0375\n",
      "101/295, train_loss: 0.1320, step time: 1.0809\n",
      "102/295, train_loss: 0.0882, step time: 1.0547\n",
      "103/295, train_loss: 0.0919, step time: 1.0668\n",
      "104/295, train_loss: 0.0511, step time: 1.0630\n",
      "105/295, train_loss: 0.0448, step time: 1.0358\n",
      "106/295, train_loss: 0.0387, step time: 1.0611\n",
      "107/295, train_loss: 0.0462, step time: 1.0789\n",
      "108/295, train_loss: 0.3381, step time: 1.0640\n",
      "109/295, train_loss: 0.0968, step time: 1.0340\n",
      "110/295, train_loss: 0.0873, step time: 1.0626\n",
      "111/295, train_loss: 0.0554, step time: 1.1247\n",
      "112/295, train_loss: 0.0301, step time: 1.0534\n",
      "113/295, train_loss: 0.0941, step time: 1.0496\n",
      "114/295, train_loss: 0.0824, step time: 1.0895\n",
      "115/295, train_loss: 0.0293, step time: 1.0452\n",
      "116/295, train_loss: 0.0404, step time: 1.0395\n",
      "117/295, train_loss: 0.1206, step time: 1.0477\n",
      "118/295, train_loss: 0.2145, step time: 1.0443\n",
      "119/295, train_loss: 0.1115, step time: 1.0321\n",
      "120/295, train_loss: 0.0964, step time: 1.0321\n",
      "121/295, train_loss: 0.0565, step time: 1.0478\n",
      "122/295, train_loss: 0.2284, step time: 1.0934\n",
      "123/295, train_loss: 0.0687, step time: 1.0345\n",
      "124/295, train_loss: 0.0540, step time: 1.0323\n",
      "125/295, train_loss: 0.3832, step time: 1.0396\n",
      "126/295, train_loss: 0.0308, step time: 1.0369\n",
      "127/295, train_loss: 0.0776, step time: 1.0341\n",
      "128/295, train_loss: 0.0389, step time: 1.0354\n",
      "129/295, train_loss: 0.2068, step time: 1.0372\n",
      "130/295, train_loss: 0.1182, step time: 1.0381\n",
      "131/295, train_loss: 0.0407, step time: 1.0323\n",
      "132/295, train_loss: 0.0902, step time: 1.0498\n",
      "133/295, train_loss: 0.1451, step time: 1.0359\n",
      "134/295, train_loss: 0.0716, step time: 1.0360\n",
      "135/295, train_loss: 0.0634, step time: 1.0315\n",
      "136/295, train_loss: 0.1101, step time: 1.0568\n",
      "137/295, train_loss: 0.0950, step time: 1.0362\n",
      "138/295, train_loss: 0.0620, step time: 1.0541\n",
      "139/295, train_loss: 0.0805, step time: 1.0417\n",
      "140/295, train_loss: 0.0752, step time: 1.0404\n",
      "141/295, train_loss: 0.1959, step time: 1.0360\n",
      "142/295, train_loss: 0.4304, step time: 1.0825\n",
      "143/295, train_loss: 0.1722, step time: 1.0587\n",
      "144/295, train_loss: 0.0307, step time: 1.0360\n",
      "145/295, train_loss: 0.1121, step time: 1.0500\n",
      "146/295, train_loss: 0.1124, step time: 1.0412\n",
      "147/295, train_loss: 0.0734, step time: 1.0336\n",
      "148/295, train_loss: 0.0528, step time: 1.0401\n",
      "149/295, train_loss: 0.1335, step time: 1.0498\n",
      "150/295, train_loss: 0.1523, step time: 1.0964\n",
      "151/295, train_loss: 0.0749, step time: 1.0314\n",
      "152/295, train_loss: 0.0971, step time: 1.0357\n",
      "153/295, train_loss: 0.0478, step time: 1.0934\n",
      "154/295, train_loss: 0.4107, step time: 1.0939\n",
      "155/295, train_loss: 0.0844, step time: 1.0595\n",
      "156/295, train_loss: 0.1008, step time: 1.0577\n",
      "157/295, train_loss: 0.1185, step time: 1.0538\n",
      "158/295, train_loss: 0.0504, step time: 1.0424\n",
      "159/295, train_loss: 0.0372, step time: 1.0599\n",
      "160/295, train_loss: 0.0660, step time: 1.0571\n",
      "161/295, train_loss: 0.0494, step time: 1.0384\n",
      "162/295, train_loss: 0.3778, step time: 1.0650\n",
      "163/295, train_loss: 0.0794, step time: 1.0346\n",
      "164/295, train_loss: 0.4014, step time: 1.0358\n",
      "165/295, train_loss: 0.0405, step time: 1.0556\n",
      "166/295, train_loss: 0.0479, step time: 1.1229\n",
      "167/295, train_loss: 0.0318, step time: 1.0404\n",
      "168/295, train_loss: 0.0679, step time: 1.0367\n",
      "169/295, train_loss: 0.0707, step time: 1.0373\n",
      "170/295, train_loss: 0.1023, step time: 1.0369\n",
      "171/295, train_loss: 0.0332, step time: 1.0365\n",
      "172/295, train_loss: 0.0853, step time: 1.0366\n",
      "173/295, train_loss: 0.0595, step time: 1.0323\n",
      "174/295, train_loss: 0.0664, step time: 1.0484\n",
      "175/295, train_loss: 0.0749, step time: 1.0488\n",
      "176/295, train_loss: 0.0804, step time: 1.0358\n",
      "177/295, train_loss: 0.0731, step time: 1.0377\n",
      "178/295, train_loss: 0.1190, step time: 1.0392\n",
      "179/295, train_loss: 0.3102, step time: 1.0542\n",
      "180/295, train_loss: 0.0790, step time: 1.0368\n",
      "181/295, train_loss: 0.0894, step time: 1.0324\n",
      "182/295, train_loss: 0.0689, step time: 1.0305\n",
      "183/295, train_loss: 0.0545, step time: 1.1206\n",
      "184/295, train_loss: 0.0345, step time: 1.0391\n",
      "185/295, train_loss: 0.0342, step time: 1.0377\n",
      "186/295, train_loss: 0.0499, step time: 1.0566\n",
      "187/295, train_loss: 0.0732, step time: 1.0418\n",
      "188/295, train_loss: 0.0345, step time: 1.0546\n",
      "189/295, train_loss: 0.1378, step time: 1.1021\n",
      "190/295, train_loss: 0.1252, step time: 1.0373\n",
      "191/295, train_loss: 0.0802, step time: 1.0701\n",
      "192/295, train_loss: 0.0290, step time: 1.0343\n",
      "193/295, train_loss: 0.0497, step time: 1.0362\n",
      "194/295, train_loss: 0.0377, step time: 1.0383\n",
      "195/295, train_loss: 0.0222, step time: 1.0589\n",
      "196/295, train_loss: 0.0926, step time: 1.0476\n",
      "197/295, train_loss: 0.1091, step time: 1.0382\n",
      "198/295, train_loss: 0.1562, step time: 1.0335\n",
      "199/295, train_loss: 0.2856, step time: 1.0630\n",
      "200/295, train_loss: 0.0464, step time: 1.0374\n",
      "201/295, train_loss: 0.0879, step time: 1.0459\n",
      "202/295, train_loss: 0.0851, step time: 1.0577\n",
      "203/295, train_loss: 0.0576, step time: 1.0343\n",
      "204/295, train_loss: 0.0424, step time: 1.0346\n",
      "205/295, train_loss: 0.0402, step time: 1.0445\n",
      "206/295, train_loss: 0.0428, step time: 1.0350\n",
      "207/295, train_loss: 0.1093, step time: 1.0563\n",
      "208/295, train_loss: 0.0714, step time: 1.0388\n",
      "209/295, train_loss: 0.0849, step time: 1.0486\n",
      "210/295, train_loss: 0.3851, step time: 1.0351\n",
      "211/295, train_loss: 0.3869, step time: 1.0596\n",
      "212/295, train_loss: 0.1036, step time: 1.0436\n",
      "213/295, train_loss: 0.1470, step time: 1.0527\n",
      "214/295, train_loss: 0.0491, step time: 1.0392\n",
      "215/295, train_loss: 0.0409, step time: 1.0719\n",
      "216/295, train_loss: 0.0650, step time: 1.0581\n",
      "217/295, train_loss: 0.0830, step time: 1.0568\n",
      "218/295, train_loss: 0.0675, step time: 1.0485\n",
      "219/295, train_loss: 0.0670, step time: 1.0458\n",
      "220/295, train_loss: 0.0752, step time: 1.0408\n",
      "221/295, train_loss: 0.0451, step time: 1.0919\n",
      "222/295, train_loss: 0.0840, step time: 1.0369\n",
      "223/295, train_loss: 0.1426, step time: 1.0411\n",
      "224/295, train_loss: 0.4028, step time: 1.0471\n",
      "225/295, train_loss: 0.0546, step time: 1.0377\n",
      "226/295, train_loss: 0.0924, step time: 1.0440\n",
      "227/295, train_loss: 0.0922, step time: 1.0368\n",
      "228/295, train_loss: 0.0864, step time: 1.0413\n",
      "229/295, train_loss: 0.1023, step time: 1.1001\n",
      "230/295, train_loss: 0.1035, step time: 1.0358\n",
      "231/295, train_loss: 0.0751, step time: 1.0407\n",
      "232/295, train_loss: 0.3732, step time: 1.0443\n",
      "233/295, train_loss: 0.0491, step time: 1.0627\n",
      "234/295, train_loss: 0.1173, step time: 1.0381\n",
      "235/295, train_loss: 0.0353, step time: 1.0509\n",
      "236/295, train_loss: 0.0812, step time: 1.0535\n",
      "237/295, train_loss: 0.1210, step time: 1.0454\n",
      "238/295, train_loss: 0.2243, step time: 1.0365\n",
      "239/295, train_loss: 0.0507, step time: 1.0608\n",
      "240/295, train_loss: 0.1371, step time: 1.0381\n",
      "241/295, train_loss: 0.0959, step time: 1.0410\n",
      "242/295, train_loss: 0.1050, step time: 1.0791\n",
      "243/295, train_loss: 0.4051, step time: 1.0348\n",
      "244/295, train_loss: 0.0555, step time: 1.0389\n",
      "245/295, train_loss: 0.0784, step time: 1.0847\n",
      "246/295, train_loss: 0.0532, step time: 1.0405\n",
      "247/295, train_loss: 0.0860, step time: 1.0336\n",
      "248/295, train_loss: 0.0398, step time: 1.1191\n",
      "249/295, train_loss: 0.0987, step time: 1.0540\n",
      "250/295, train_loss: 0.3956, step time: 1.0337\n",
      "251/295, train_loss: 0.1021, step time: 1.0326\n",
      "252/295, train_loss: 0.1070, step time: 1.0657\n",
      "253/295, train_loss: 0.1357, step time: 1.0417\n",
      "254/295, train_loss: 0.0629, step time: 1.0571\n",
      "255/295, train_loss: 0.0611, step time: 1.0319\n",
      "256/295, train_loss: 0.0370, step time: 1.0319\n",
      "257/295, train_loss: 0.4113, step time: 1.0476\n",
      "258/295, train_loss: 0.1109, step time: 1.0524\n",
      "259/295, train_loss: 0.0536, step time: 1.0326\n",
      "260/295, train_loss: 0.0536, step time: 1.0610\n",
      "261/295, train_loss: 0.1145, step time: 1.0931\n",
      "262/295, train_loss: 0.0439, step time: 1.0311\n",
      "263/295, train_loss: 0.0529, step time: 1.0341\n",
      "264/295, train_loss: 0.0649, step time: 1.0436\n",
      "265/295, train_loss: 0.0759, step time: 1.0391\n",
      "266/295, train_loss: 0.0460, step time: 1.0679\n",
      "267/295, train_loss: 0.0362, step time: 1.0561\n",
      "268/295, train_loss: 0.1366, step time: 1.0318\n",
      "269/295, train_loss: 0.0624, step time: 1.0693\n",
      "270/295, train_loss: 0.1853, step time: 1.0550\n",
      "271/295, train_loss: 0.1694, step time: 1.0680\n",
      "272/295, train_loss: 0.0816, step time: 1.0351\n",
      "273/295, train_loss: 0.1264, step time: 1.0411\n",
      "274/295, train_loss: 0.1155, step time: 1.0312\n",
      "275/295, train_loss: 0.0391, step time: 1.0331\n",
      "276/295, train_loss: 0.0916, step time: 1.0419\n",
      "277/295, train_loss: 0.0496, step time: 1.0332\n",
      "278/295, train_loss: 0.4132, step time: 1.0394\n",
      "279/295, train_loss: 0.0444, step time: 1.0397\n",
      "280/295, train_loss: 0.2143, step time: 1.0508\n",
      "281/295, train_loss: 0.1251, step time: 1.0614\n",
      "282/295, train_loss: 0.0729, step time: 1.0332\n",
      "283/295, train_loss: 0.0707, step time: 1.0935\n",
      "284/295, train_loss: 0.0701, step time: 1.0463\n",
      "285/295, train_loss: 0.0360, step time: 1.0336\n",
      "286/295, train_loss: 0.1357, step time: 1.0350\n",
      "287/295, train_loss: 0.0592, step time: 1.0379\n",
      "288/295, train_loss: 0.0693, step time: 1.0346\n",
      "289/295, train_loss: 0.0786, step time: 1.0277\n",
      "290/295, train_loss: 0.0642, step time: 1.0295\n",
      "291/295, train_loss: 0.0465, step time: 1.0283\n",
      "292/295, train_loss: 0.0367, step time: 1.0279\n",
      "293/295, train_loss: 0.1116, step time: 1.0289\n",
      "294/295, train_loss: 0.1012, step time: 1.0272\n",
      "295/295, train_loss: 0.1310, step time: 1.0285\n",
      "epoch 49 average loss: 0.1066\n",
      "current epoch: 49 current mean dice: 0.7950 tc: 0.7513 wt: 0.8555 et: 0.7856\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 49 is: 382.4762\n",
      "----------\n",
      "epoch 50/100\n",
      "1/295, train_loss: 0.1073, step time: 1.1268\n",
      "2/295, train_loss: 0.4897, step time: 1.0767\n",
      "3/295, train_loss: 0.1310, step time: 1.1223\n",
      "4/295, train_loss: 0.0484, step time: 1.1542\n",
      "5/295, train_loss: 0.1384, step time: 1.0341\n",
      "6/295, train_loss: 0.0763, step time: 1.0386\n",
      "7/295, train_loss: 0.0554, step time: 1.0654\n",
      "8/295, train_loss: 0.0562, step time: 1.0362\n",
      "9/295, train_loss: 0.0338, step time: 1.0314\n",
      "10/295, train_loss: 0.3893, step time: 1.0334\n",
      "11/295, train_loss: 0.0415, step time: 1.0580\n",
      "12/295, train_loss: 0.0330, step time: 1.0336\n",
      "13/295, train_loss: 0.1255, step time: 1.0516\n",
      "14/295, train_loss: 0.1447, step time: 1.0850\n",
      "15/295, train_loss: 0.1218, step time: 1.0313\n",
      "16/295, train_loss: 0.0878, step time: 1.0311\n",
      "17/295, train_loss: 0.0476, step time: 1.0389\n",
      "18/295, train_loss: 0.0343, step time: 1.0403\n",
      "19/295, train_loss: 0.1044, step time: 1.0568\n",
      "20/295, train_loss: 0.0550, step time: 1.0290\n",
      "21/295, train_loss: 0.0568, step time: 1.0378\n",
      "22/295, train_loss: 0.0357, step time: 1.0340\n",
      "23/295, train_loss: 0.0413, step time: 1.0568\n",
      "24/295, train_loss: 0.1601, step time: 1.0575\n",
      "25/295, train_loss: 0.4178, step time: 1.0464\n",
      "26/295, train_loss: 0.0679, step time: 1.0351\n",
      "27/295, train_loss: 0.0414, step time: 1.0474\n",
      "28/295, train_loss: 0.0876, step time: 1.0839\n",
      "29/295, train_loss: 0.0296, step time: 1.0647\n",
      "30/295, train_loss: 0.0793, step time: 1.0566\n",
      "31/295, train_loss: 0.0358, step time: 1.0286\n",
      "32/295, train_loss: 0.0494, step time: 1.0622\n",
      "33/295, train_loss: 0.0579, step time: 1.0572\n",
      "34/295, train_loss: 0.0685, step time: 1.0775\n",
      "35/295, train_loss: 0.1625, step time: 1.0320\n",
      "36/295, train_loss: 0.0779, step time: 1.0398\n",
      "37/295, train_loss: 0.1464, step time: 1.0838\n",
      "38/295, train_loss: 0.0586, step time: 1.0537\n",
      "39/295, train_loss: 0.0459, step time: 1.0371\n",
      "40/295, train_loss: 0.0643, step time: 1.0364\n",
      "41/295, train_loss: 0.0874, step time: 1.0590\n",
      "42/295, train_loss: 0.4020, step time: 1.0644\n",
      "43/295, train_loss: 0.0520, step time: 1.0631\n",
      "44/295, train_loss: 0.0360, step time: 1.0410\n",
      "45/295, train_loss: 0.0391, step time: 1.1219\n",
      "46/295, train_loss: 0.0981, step time: 1.0384\n",
      "47/295, train_loss: 0.1155, step time: 1.0339\n",
      "48/295, train_loss: 0.0283, step time: 1.0458\n",
      "49/295, train_loss: 0.0940, step time: 1.0503\n",
      "50/295, train_loss: 0.0887, step time: 1.0543\n",
      "51/295, train_loss: 0.0446, step time: 1.0385\n",
      "52/295, train_loss: 0.0649, step time: 1.0353\n",
      "53/295, train_loss: 0.1154, step time: 1.0372\n",
      "54/295, train_loss: 0.0501, step time: 1.0506\n",
      "55/295, train_loss: 0.0572, step time: 1.0411\n",
      "56/295, train_loss: 0.0884, step time: 1.0770\n",
      "57/295, train_loss: 0.0628, step time: 1.0586\n",
      "58/295, train_loss: 0.1073, step time: 1.0336\n",
      "59/295, train_loss: 0.3712, step time: 1.0512\n",
      "60/295, train_loss: 0.0518, step time: 1.1073\n",
      "61/295, train_loss: 0.0641, step time: 1.0368\n",
      "62/295, train_loss: 0.0768, step time: 1.0366\n",
      "63/295, train_loss: 0.0980, step time: 1.0728\n",
      "64/295, train_loss: 0.0885, step time: 1.0603\n",
      "65/295, train_loss: 0.0611, step time: 1.0540\n",
      "66/295, train_loss: 0.0531, step time: 1.0381\n",
      "67/295, train_loss: 0.0601, step time: 1.0823\n",
      "68/295, train_loss: 0.2075, step time: 1.0347\n",
      "69/295, train_loss: 0.0616, step time: 1.0707\n",
      "70/295, train_loss: 0.3791, step time: 1.0453\n",
      "71/295, train_loss: 0.0963, step time: 1.0831\n",
      "72/295, train_loss: 0.1368, step time: 1.0691\n",
      "73/295, train_loss: 0.0693, step time: 1.0318\n",
      "74/295, train_loss: 0.0370, step time: 1.0551\n",
      "75/295, train_loss: 0.1303, step time: 1.0465\n",
      "76/295, train_loss: 0.3912, step time: 1.0354\n",
      "77/295, train_loss: 0.0465, step time: 1.0414\n",
      "78/295, train_loss: 0.0565, step time: 1.0938\n",
      "79/295, train_loss: 0.0895, step time: 1.0322\n",
      "80/295, train_loss: 0.0952, step time: 1.0521\n",
      "81/295, train_loss: 0.0464, step time: 1.0386\n",
      "82/295, train_loss: 0.1234, step time: 1.0599\n",
      "83/295, train_loss: 0.0333, step time: 1.0434\n",
      "84/295, train_loss: 0.0353, step time: 1.0563\n",
      "85/295, train_loss: 0.0471, step time: 1.0309\n",
      "86/295, train_loss: 0.0981, step time: 1.0608\n",
      "87/295, train_loss: 0.0945, step time: 1.0408\n",
      "88/295, train_loss: 0.0932, step time: 1.0376\n",
      "89/295, train_loss: 0.3966, step time: 1.0538\n",
      "90/295, train_loss: 0.0522, step time: 1.0364\n",
      "91/295, train_loss: 0.3722, step time: 1.0503\n",
      "92/295, train_loss: 0.1436, step time: 1.0380\n",
      "93/295, train_loss: 0.0367, step time: 1.0731\n",
      "94/295, train_loss: 0.0330, step time: 1.0569\n",
      "95/295, train_loss: 0.3815, step time: 1.0534\n",
      "96/295, train_loss: 0.0483, step time: 1.0433\n",
      "97/295, train_loss: 0.1076, step time: 1.0365\n",
      "98/295, train_loss: 0.3756, step time: 1.0424\n",
      "99/295, train_loss: 0.0671, step time: 1.0413\n",
      "100/295, train_loss: 0.0606, step time: 1.0367\n",
      "101/295, train_loss: 0.0634, step time: 1.0384\n",
      "102/295, train_loss: 0.1741, step time: 1.0507\n",
      "103/295, train_loss: 0.0555, step time: 1.0507\n",
      "104/295, train_loss: 0.1201, step time: 1.0440\n",
      "105/295, train_loss: 0.0758, step time: 1.0343\n",
      "106/295, train_loss: 0.3715, step time: 1.0443\n",
      "107/295, train_loss: 0.1918, step time: 1.0568\n",
      "108/295, train_loss: 0.0912, step time: 1.0500\n",
      "109/295, train_loss: 0.0640, step time: 1.0426\n",
      "110/295, train_loss: 0.0852, step time: 1.0480\n",
      "111/295, train_loss: 0.0837, step time: 1.0446\n",
      "112/295, train_loss: 0.1222, step time: 1.0461\n",
      "113/295, train_loss: 0.0514, step time: 1.0535\n",
      "114/295, train_loss: 0.1442, step time: 1.0344\n",
      "115/295, train_loss: 0.0957, step time: 1.0376\n",
      "116/295, train_loss: 0.0452, step time: 1.0421\n",
      "117/295, train_loss: 0.1052, step time: 1.0480\n",
      "118/295, train_loss: 0.0888, step time: 1.0507\n",
      "119/295, train_loss: 0.0872, step time: 1.0544\n",
      "120/295, train_loss: 0.0947, step time: 1.0375\n",
      "121/295, train_loss: 0.0594, step time: 1.0425\n",
      "122/295, train_loss: 0.1044, step time: 1.0848\n",
      "123/295, train_loss: 0.0482, step time: 1.0359\n",
      "124/295, train_loss: 0.0431, step time: 1.0318\n",
      "125/295, train_loss: 0.1186, step time: 1.0319\n",
      "126/295, train_loss: 0.4032, step time: 1.0378\n",
      "127/295, train_loss: 0.0626, step time: 1.0553\n",
      "128/295, train_loss: 0.0649, step time: 1.0410\n",
      "129/295, train_loss: 0.0493, step time: 1.0804\n",
      "130/295, train_loss: 0.0907, step time: 1.0410\n",
      "131/295, train_loss: 0.0821, step time: 1.0532\n",
      "132/295, train_loss: 0.0500, step time: 1.0388\n",
      "133/295, train_loss: 0.3991, step time: 1.0453\n",
      "134/295, train_loss: 0.0416, step time: 1.0393\n",
      "135/295, train_loss: 0.0539, step time: 1.0458\n",
      "136/295, train_loss: 0.2543, step time: 1.0486\n",
      "137/295, train_loss: 0.0786, step time: 1.0408\n",
      "138/295, train_loss: 0.0336, step time: 1.0569\n",
      "139/295, train_loss: 0.0688, step time: 1.0987\n",
      "140/295, train_loss: 0.0413, step time: 1.0370\n",
      "141/295, train_loss: 0.0497, step time: 1.0768\n",
      "142/295, train_loss: 0.0864, step time: 1.0400\n",
      "143/295, train_loss: 0.0745, step time: 1.0527\n",
      "144/295, train_loss: 0.0730, step time: 1.0379\n",
      "145/295, train_loss: 0.3835, step time: 1.0440\n",
      "146/295, train_loss: 0.0404, step time: 1.0578\n",
      "147/295, train_loss: 0.0694, step time: 1.0512\n",
      "148/295, train_loss: 0.1471, step time: 1.0433\n",
      "149/295, train_loss: 0.0530, step time: 1.0451\n",
      "150/295, train_loss: 0.0996, step time: 1.0344\n",
      "151/295, train_loss: 0.0752, step time: 1.0373\n",
      "152/295, train_loss: 0.0697, step time: 1.0538\n",
      "153/295, train_loss: 0.0731, step time: 1.0671\n",
      "154/295, train_loss: 0.0929, step time: 1.0705\n",
      "155/295, train_loss: 0.0519, step time: 1.0312\n",
      "156/295, train_loss: 0.0841, step time: 1.0367\n",
      "157/295, train_loss: 0.1269, step time: 1.0345\n",
      "158/295, train_loss: 0.0830, step time: 1.0530\n",
      "159/295, train_loss: 0.0515, step time: 1.0390\n",
      "160/295, train_loss: 0.2391, step time: 1.0413\n",
      "161/295, train_loss: 0.0752, step time: 1.0577\n",
      "162/295, train_loss: 0.0681, step time: 1.0370\n",
      "163/295, train_loss: 0.0776, step time: 1.0312\n",
      "164/295, train_loss: 0.0391, step time: 1.0865\n",
      "165/295, train_loss: 0.0832, step time: 1.0777\n",
      "166/295, train_loss: 0.1125, step time: 1.0451\n",
      "167/295, train_loss: 0.3751, step time: 1.0504\n",
      "168/295, train_loss: 0.0872, step time: 1.0370\n",
      "169/295, train_loss: 0.1349, step time: 1.0322\n",
      "170/295, train_loss: 0.0478, step time: 1.0714\n",
      "171/295, train_loss: 0.0398, step time: 1.0548\n",
      "172/295, train_loss: 0.0296, step time: 1.0511\n",
      "173/295, train_loss: 0.0620, step time: 1.0572\n",
      "174/295, train_loss: 0.1480, step time: 1.0673\n",
      "175/295, train_loss: 0.0571, step time: 1.0538\n",
      "176/295, train_loss: 0.3641, step time: 1.0354\n",
      "177/295, train_loss: 0.0427, step time: 1.0488\n",
      "178/295, train_loss: 0.0820, step time: 1.0595\n",
      "179/295, train_loss: 0.0565, step time: 1.0351\n",
      "180/295, train_loss: 0.1183, step time: 1.0333\n",
      "181/295, train_loss: 0.0524, step time: 1.0369\n",
      "182/295, train_loss: 0.0686, step time: 1.0366\n",
      "183/295, train_loss: 0.1204, step time: 1.0311\n",
      "184/295, train_loss: 0.0306, step time: 1.0320\n",
      "185/295, train_loss: 0.0406, step time: 1.0309\n",
      "186/295, train_loss: 0.0553, step time: 1.0349\n",
      "187/295, train_loss: 0.0958, step time: 1.0361\n",
      "188/295, train_loss: 0.0319, step time: 1.0548\n",
      "189/295, train_loss: 0.1260, step time: 1.0357\n",
      "190/295, train_loss: 0.0467, step time: 1.0529\n",
      "191/295, train_loss: 0.1243, step time: 1.0628\n",
      "192/295, train_loss: 0.0219, step time: 1.0504\n",
      "193/295, train_loss: 0.0326, step time: 1.0366\n",
      "194/295, train_loss: 0.0764, step time: 1.0326\n",
      "195/295, train_loss: 0.0917, step time: 1.0663\n",
      "196/295, train_loss: 0.0975, step time: 1.0665\n",
      "197/295, train_loss: 0.1302, step time: 1.1055\n",
      "198/295, train_loss: 0.4135, step time: 1.0479\n",
      "199/295, train_loss: 0.0721, step time: 1.0386\n",
      "200/295, train_loss: 0.0784, step time: 1.0379\n",
      "201/295, train_loss: 0.0835, step time: 1.0488\n",
      "202/295, train_loss: 0.0616, step time: 1.0505\n",
      "203/295, train_loss: 0.0721, step time: 1.0769\n",
      "204/295, train_loss: 0.0328, step time: 1.0463\n",
      "205/295, train_loss: 0.0564, step time: 1.0449\n",
      "206/295, train_loss: 0.0705, step time: 1.0463\n",
      "207/295, train_loss: 0.0360, step time: 1.0482\n",
      "208/295, train_loss: 0.0909, step time: 1.0348\n",
      "209/295, train_loss: 0.3138, step time: 1.0507\n",
      "210/295, train_loss: 0.0564, step time: 1.0391\n",
      "211/295, train_loss: 0.0631, step time: 1.0458\n",
      "212/295, train_loss: 0.0551, step time: 1.1077\n",
      "213/295, train_loss: 0.0772, step time: 1.0625\n",
      "214/295, train_loss: 0.1977, step time: 1.0510\n",
      "215/295, train_loss: 0.0938, step time: 1.0349\n",
      "216/295, train_loss: 0.0490, step time: 1.0438\n",
      "217/295, train_loss: 0.1113, step time: 1.0366\n",
      "218/295, train_loss: 0.4032, step time: 1.0576\n",
      "219/295, train_loss: 0.0505, step time: 1.0351\n",
      "220/295, train_loss: 0.1083, step time: 1.0387\n",
      "221/295, train_loss: 0.1176, step time: 1.0480\n",
      "222/295, train_loss: 0.1236, step time: 1.0601\n",
      "223/295, train_loss: 0.0969, step time: 1.0933\n",
      "224/295, train_loss: 0.0496, step time: 1.0562\n",
      "225/295, train_loss: 0.0658, step time: 1.0299\n",
      "226/295, train_loss: 0.1125, step time: 1.0419\n",
      "227/295, train_loss: 0.0573, step time: 1.0540\n",
      "228/295, train_loss: 0.1610, step time: 1.0340\n",
      "229/295, train_loss: 0.0340, step time: 1.0551\n",
      "230/295, train_loss: 0.3800, step time: 1.0468\n",
      "231/295, train_loss: 0.0362, step time: 1.0347\n",
      "232/295, train_loss: 0.3929, step time: 1.0349\n",
      "233/295, train_loss: 0.4162, step time: 1.0387\n",
      "234/295, train_loss: 0.0725, step time: 1.0607\n",
      "235/295, train_loss: 0.1146, step time: 1.0319\n",
      "236/295, train_loss: 0.0825, step time: 1.0358\n",
      "237/295, train_loss: 0.0315, step time: 1.0392\n",
      "238/295, train_loss: 0.0566, step time: 1.0575\n",
      "239/295, train_loss: 0.0977, step time: 1.0358\n",
      "240/295, train_loss: 0.0453, step time: 1.0374\n",
      "241/295, train_loss: 0.0424, step time: 1.0753\n",
      "242/295, train_loss: 0.1362, step time: 1.0363\n",
      "243/295, train_loss: 0.0902, step time: 1.0354\n",
      "244/295, train_loss: 0.1225, step time: 1.0502\n",
      "245/295, train_loss: 0.1035, step time: 1.0596\n",
      "246/295, train_loss: 0.1211, step time: 1.0417\n",
      "247/295, train_loss: 0.1062, step time: 1.0464\n",
      "248/295, train_loss: 0.1096, step time: 1.0368\n",
      "249/295, train_loss: 0.0419, step time: 1.0349\n",
      "250/295, train_loss: 0.0803, step time: 1.0621\n",
      "251/295, train_loss: 0.0699, step time: 1.0402\n",
      "252/295, train_loss: 0.0635, step time: 1.0533\n",
      "253/295, train_loss: 0.1465, step time: 1.0765\n",
      "254/295, train_loss: 0.0533, step time: 1.0545\n",
      "255/295, train_loss: 0.0805, step time: 1.0321\n",
      "256/295, train_loss: 0.0906, step time: 1.0379\n",
      "257/295, train_loss: 0.0311, step time: 1.0353\n",
      "258/295, train_loss: 0.0367, step time: 1.0396\n",
      "259/295, train_loss: 0.3960, step time: 1.0527\n",
      "260/295, train_loss: 0.0393, step time: 1.0558\n",
      "261/295, train_loss: 0.0904, step time: 1.0337\n",
      "262/295, train_loss: 0.1514, step time: 1.0476\n",
      "263/295, train_loss: 0.3938, step time: 1.0516\n",
      "264/295, train_loss: 0.0727, step time: 1.1061\n",
      "265/295, train_loss: 0.0633, step time: 1.0394\n",
      "266/295, train_loss: 0.0650, step time: 1.0310\n",
      "267/295, train_loss: 0.0456, step time: 1.0497\n",
      "268/295, train_loss: 0.0719, step time: 1.0495\n",
      "269/295, train_loss: 0.0661, step time: 1.0581\n",
      "270/295, train_loss: 0.1065, step time: 1.0421\n",
      "271/295, train_loss: 0.0455, step time: 1.0356\n",
      "272/295, train_loss: 0.0295, step time: 1.0355\n",
      "273/295, train_loss: 0.0783, step time: 1.0436\n",
      "274/295, train_loss: 0.1332, step time: 1.0468\n",
      "275/295, train_loss: 0.1120, step time: 1.0406\n",
      "276/295, train_loss: 0.0704, step time: 1.0395\n",
      "277/295, train_loss: 0.0901, step time: 1.0423\n",
      "278/295, train_loss: 0.1318, step time: 1.0593\n",
      "279/295, train_loss: 0.0432, step time: 1.0531\n",
      "280/295, train_loss: 0.0699, step time: 1.0387\n",
      "281/295, train_loss: 0.0518, step time: 1.0430\n",
      "282/295, train_loss: 0.0370, step time: 1.0370\n",
      "283/295, train_loss: 0.0786, step time: 1.0296\n",
      "284/295, train_loss: 0.0911, step time: 1.1427\n",
      "285/295, train_loss: 0.1215, step time: 1.0364\n",
      "286/295, train_loss: 0.0773, step time: 1.0405\n",
      "287/295, train_loss: 0.0518, step time: 1.0375\n",
      "288/295, train_loss: 0.0695, step time: 1.0477\n",
      "289/295, train_loss: 0.0399, step time: 1.0286\n",
      "290/295, train_loss: 0.0449, step time: 1.0290\n",
      "291/295, train_loss: 0.0529, step time: 1.0290\n",
      "292/295, train_loss: 0.0516, step time: 1.0294\n",
      "293/295, train_loss: 0.0923, step time: 1.0281\n",
      "294/295, train_loss: 0.0603, step time: 1.0282\n",
      "295/295, train_loss: 0.0541, step time: 1.0277\n",
      "epoch 50 average loss: 0.1047\n",
      "current epoch: 50 current mean dice: 0.7944 tc: 0.7516 wt: 0.8569 et: 0.7807\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 50 is: 386.3707\n",
      "----------\n",
      "epoch 51/100\n",
      "1/295, train_loss: 0.3220, step time: 1.0688\n",
      "2/295, train_loss: 0.3824, step time: 1.0700\n",
      "3/295, train_loss: 0.0619, step time: 1.0551\n",
      "4/295, train_loss: 0.0773, step time: 1.0852\n",
      "5/295, train_loss: 0.0360, step time: 1.0549\n",
      "6/295, train_loss: 0.0784, step time: 1.0970\n",
      "7/295, train_loss: 0.0283, step time: 1.0432\n",
      "8/295, train_loss: 0.0299, step time: 1.0409\n",
      "9/295, train_loss: 0.0399, step time: 1.0499\n",
      "10/295, train_loss: 0.0537, step time: 1.0332\n",
      "11/295, train_loss: 0.0981, step time: 1.0332\n",
      "12/295, train_loss: 0.0328, step time: 1.0403\n",
      "13/295, train_loss: 0.3967, step time: 1.0357\n",
      "14/295, train_loss: 0.3706, step time: 1.0483\n",
      "15/295, train_loss: 0.0450, step time: 1.0933\n",
      "16/295, train_loss: 0.0342, step time: 1.0308\n",
      "17/295, train_loss: 0.0338, step time: 1.0577\n",
      "18/295, train_loss: 0.0454, step time: 1.0295\n",
      "19/295, train_loss: 0.1080, step time: 1.0345\n",
      "20/295, train_loss: 0.1666, step time: 1.0337\n",
      "21/295, train_loss: 0.0491, step time: 1.0563\n",
      "22/295, train_loss: 0.0758, step time: 1.0715\n",
      "23/295, train_loss: 0.0884, step time: 1.0312\n",
      "24/295, train_loss: 0.1296, step time: 1.0480\n",
      "25/295, train_loss: 0.0568, step time: 1.0585\n",
      "26/295, train_loss: 0.0304, step time: 1.0299\n",
      "27/295, train_loss: 0.0352, step time: 1.0324\n",
      "28/295, train_loss: 0.0766, step time: 1.0484\n",
      "29/295, train_loss: 0.0489, step time: 1.0289\n",
      "30/295, train_loss: 0.0861, step time: 1.0346\n",
      "31/295, train_loss: 0.1093, step time: 1.0395\n",
      "32/295, train_loss: 0.0308, step time: 1.0395\n",
      "33/295, train_loss: 0.3701, step time: 1.0401\n",
      "34/295, train_loss: 0.0305, step time: 1.0344\n",
      "35/295, train_loss: 0.0746, step time: 1.0437\n",
      "36/295, train_loss: 0.0751, step time: 1.0386\n",
      "37/295, train_loss: 0.0511, step time: 1.0567\n",
      "38/295, train_loss: 0.0472, step time: 1.0491\n",
      "39/295, train_loss: 0.0649, step time: 1.0450\n",
      "40/295, train_loss: 0.0469, step time: 1.0430\n",
      "41/295, train_loss: 0.0660, step time: 1.0347\n",
      "42/295, train_loss: 0.0794, step time: 1.0298\n",
      "43/295, train_loss: 0.0223, step time: 1.0463\n",
      "44/295, train_loss: 0.1102, step time: 1.0301\n",
      "45/295, train_loss: 0.0885, step time: 1.0447\n",
      "46/295, train_loss: 0.0639, step time: 1.0455\n",
      "47/295, train_loss: 0.0333, step time: 1.0362\n",
      "48/295, train_loss: 0.2115, step time: 1.0324\n",
      "49/295, train_loss: 0.0488, step time: 1.0748\n",
      "50/295, train_loss: 0.3941, step time: 1.0385\n",
      "51/295, train_loss: 0.4063, step time: 1.0333\n",
      "52/295, train_loss: 0.1060, step time: 1.0536\n",
      "53/295, train_loss: 0.1631, step time: 1.0570\n",
      "54/295, train_loss: 0.0688, step time: 1.0545\n",
      "55/295, train_loss: 0.0732, step time: 1.0312\n",
      "56/295, train_loss: 0.0625, step time: 1.0596\n",
      "57/295, train_loss: 0.0834, step time: 1.0640\n",
      "58/295, train_loss: 0.0638, step time: 1.0349\n",
      "59/295, train_loss: 0.0429, step time: 1.0342\n",
      "60/295, train_loss: 0.0692, step time: 1.0577\n",
      "61/295, train_loss: 0.0541, step time: 1.0396\n",
      "62/295, train_loss: 0.3723, step time: 1.0350\n",
      "63/295, train_loss: 0.0763, step time: 1.0333\n",
      "64/295, train_loss: 0.0860, step time: 1.0344\n",
      "65/295, train_loss: 0.0341, step time: 1.0816\n",
      "66/295, train_loss: 0.0780, step time: 1.0427\n",
      "67/295, train_loss: 0.0506, step time: 1.0353\n",
      "68/295, train_loss: 0.0386, step time: 1.1273\n",
      "69/295, train_loss: 0.0368, step time: 1.0403\n",
      "70/295, train_loss: 0.0439, step time: 1.0366\n",
      "71/295, train_loss: 0.0783, step time: 1.0616\n",
      "72/295, train_loss: 0.0463, step time: 1.1689\n",
      "73/295, train_loss: 0.0608, step time: 1.0380\n",
      "74/295, train_loss: 0.0872, step time: 1.0309\n",
      "75/295, train_loss: 0.4859, step time: 1.0381\n",
      "76/295, train_loss: 0.0593, step time: 1.0362\n",
      "77/295, train_loss: 0.1121, step time: 1.0308\n",
      "78/295, train_loss: 0.0680, step time: 1.0695\n",
      "79/295, train_loss: 0.0457, step time: 1.0513\n",
      "80/295, train_loss: 0.0453, step time: 1.0588\n",
      "81/295, train_loss: 0.0356, step time: 1.0651\n",
      "82/295, train_loss: 0.0550, step time: 1.0484\n",
      "83/295, train_loss: 0.0993, step time: 1.0387\n",
      "84/295, train_loss: 0.0977, step time: 1.0586\n",
      "85/295, train_loss: 0.0734, step time: 1.1079\n",
      "86/295, train_loss: 0.0513, step time: 1.0621\n",
      "87/295, train_loss: 0.0762, step time: 1.0534\n",
      "88/295, train_loss: 0.0864, step time: 1.0509\n",
      "89/295, train_loss: 0.0712, step time: 1.0903\n",
      "90/295, train_loss: 0.0785, step time: 1.0400\n",
      "91/295, train_loss: 0.0763, step time: 1.0318\n",
      "92/295, train_loss: 0.0388, step time: 1.0387\n",
      "93/295, train_loss: 0.0888, step time: 1.0434\n",
      "94/295, train_loss: 0.0897, step time: 1.1208\n",
      "95/295, train_loss: 0.0508, step time: 1.0329\n",
      "96/295, train_loss: 0.0277, step time: 1.0614\n",
      "97/295, train_loss: 0.0803, step time: 1.0582\n",
      "98/295, train_loss: 0.0887, step time: 1.0485\n",
      "99/295, train_loss: 0.0780, step time: 1.0337\n",
      "100/295, train_loss: 0.3999, step time: 1.0576\n",
      "101/295, train_loss: 0.1047, step time: 1.0366\n",
      "102/295, train_loss: 0.0560, step time: 1.0381\n",
      "103/295, train_loss: 0.0627, step time: 1.0310\n",
      "104/295, train_loss: 0.0996, step time: 1.0356\n",
      "105/295, train_loss: 0.0836, step time: 1.0558\n",
      "106/295, train_loss: 0.0962, step time: 1.0340\n",
      "107/295, train_loss: 0.0580, step time: 1.0621\n",
      "108/295, train_loss: 0.0327, step time: 1.0532\n",
      "109/295, train_loss: 0.0436, step time: 1.0315\n",
      "110/295, train_loss: 0.0896, step time: 1.0391\n",
      "111/295, train_loss: 0.0303, step time: 1.0828\n",
      "112/295, train_loss: 0.1012, step time: 1.0540\n",
      "113/295, train_loss: 0.1644, step time: 1.0523\n",
      "114/295, train_loss: 0.0574, step time: 1.0310\n",
      "115/295, train_loss: 0.1212, step time: 1.0321\n",
      "116/295, train_loss: 0.0417, step time: 1.0560\n",
      "117/295, train_loss: 0.0617, step time: 1.0337\n",
      "118/295, train_loss: 0.0890, step time: 1.1029\n",
      "119/295, train_loss: 0.0624, step time: 1.0733\n",
      "120/295, train_loss: 0.1212, step time: 1.0354\n",
      "121/295, train_loss: 0.1147, step time: 1.0426\n",
      "122/295, train_loss: 0.4300, step time: 1.0511\n",
      "123/295, train_loss: 0.0940, step time: 1.0613\n",
      "124/295, train_loss: 0.0562, step time: 1.0585\n",
      "125/295, train_loss: 0.0826, step time: 1.0426\n",
      "126/295, train_loss: 0.0702, step time: 1.0412\n",
      "127/295, train_loss: 0.0878, step time: 1.0603\n",
      "128/295, train_loss: 0.0634, step time: 1.0373\n",
      "129/295, train_loss: 0.1189, step time: 1.0330\n",
      "130/295, train_loss: 0.1290, step time: 1.0524\n",
      "131/295, train_loss: 0.0531, step time: 1.0563\n",
      "132/295, train_loss: 0.1759, step time: 1.0414\n",
      "133/295, train_loss: 0.0918, step time: 1.0649\n",
      "134/295, train_loss: 0.1021, step time: 1.0902\n",
      "135/295, train_loss: 0.0504, step time: 1.1027\n",
      "136/295, train_loss: 0.0678, step time: 1.0346\n",
      "137/295, train_loss: 0.0523, step time: 1.0390\n",
      "138/295, train_loss: 0.0462, step time: 1.0405\n",
      "139/295, train_loss: 0.3928, step time: 1.0405\n",
      "140/295, train_loss: 0.0864, step time: 1.0417\n",
      "141/295, train_loss: 0.0935, step time: 1.0411\n",
      "142/295, train_loss: 0.3896, step time: 1.0568\n",
      "143/295, train_loss: 0.0401, step time: 1.0560\n",
      "144/295, train_loss: 0.1050, step time: 1.0449\n",
      "145/295, train_loss: 0.1176, step time: 1.0340\n",
      "146/295, train_loss: 0.0487, step time: 1.0590\n",
      "147/295, train_loss: 0.0494, step time: 1.0319\n",
      "148/295, train_loss: 0.0885, step time: 1.0785\n",
      "149/295, train_loss: 0.0627, step time: 1.0393\n",
      "150/295, train_loss: 0.0612, step time: 1.0375\n",
      "151/295, train_loss: 0.0308, step time: 1.0874\n",
      "152/295, train_loss: 0.0657, step time: 1.0434\n",
      "153/295, train_loss: 0.3919, step time: 1.0466\n",
      "154/295, train_loss: 0.1053, step time: 1.0348\n",
      "155/295, train_loss: 0.1334, step time: 1.0341\n",
      "156/295, train_loss: 0.0803, step time: 1.0364\n",
      "157/295, train_loss: 0.3699, step time: 1.0347\n",
      "158/295, train_loss: 0.0665, step time: 1.0413\n",
      "159/295, train_loss: 0.0531, step time: 1.0365\n",
      "160/295, train_loss: 0.0635, step time: 1.0319\n",
      "161/295, train_loss: 0.1212, step time: 1.0331\n",
      "162/295, train_loss: 0.0991, step time: 1.0420\n",
      "163/295, train_loss: 0.0785, step time: 1.1056\n",
      "164/295, train_loss: 0.0737, step time: 1.0335\n",
      "165/295, train_loss: 0.0356, step time: 1.1155\n",
      "166/295, train_loss: 0.1263, step time: 1.0357\n",
      "167/295, train_loss: 0.1522, step time: 1.0369\n",
      "168/295, train_loss: 0.0760, step time: 1.0789\n",
      "169/295, train_loss: 0.0508, step time: 1.0451\n",
      "170/295, train_loss: 0.1002, step time: 1.0528\n",
      "171/295, train_loss: 0.0810, step time: 1.1067\n",
      "172/295, train_loss: 0.0642, step time: 1.0353\n",
      "173/295, train_loss: 0.1047, step time: 1.0369\n",
      "174/295, train_loss: 0.0648, step time: 1.0391\n",
      "175/295, train_loss: 0.0679, step time: 1.0505\n",
      "176/295, train_loss: 0.0779, step time: 1.0722\n",
      "177/295, train_loss: 0.0370, step time: 1.0360\n",
      "178/295, train_loss: 0.0401, step time: 1.0632\n",
      "179/295, train_loss: 0.1209, step time: 1.0516\n",
      "180/295, train_loss: 0.1265, step time: 1.0439\n",
      "181/295, train_loss: 0.0893, step time: 1.0491\n",
      "182/295, train_loss: 0.4010, step time: 1.0398\n",
      "183/295, train_loss: 0.0421, step time: 1.0407\n",
      "184/295, train_loss: 0.0638, step time: 1.0422\n",
      "185/295, train_loss: 0.0456, step time: 1.0414\n",
      "186/295, train_loss: 0.0792, step time: 1.0959\n",
      "187/295, train_loss: 0.0527, step time: 1.0440\n",
      "188/295, train_loss: 0.0556, step time: 1.0354\n",
      "189/295, train_loss: 0.0359, step time: 1.0497\n",
      "190/295, train_loss: 0.0287, step time: 1.0394\n",
      "191/295, train_loss: 0.1239, step time: 1.0805\n",
      "192/295, train_loss: 0.0615, step time: 1.0495\n",
      "193/295, train_loss: 0.0442, step time: 1.0423\n",
      "194/295, train_loss: 0.0706, step time: 1.0374\n",
      "195/295, train_loss: 0.0981, step time: 1.0317\n",
      "196/295, train_loss: 0.1285, step time: 1.0534\n",
      "197/295, train_loss: 0.1090, step time: 1.0471\n",
      "198/295, train_loss: 0.0598, step time: 1.0417\n",
      "199/295, train_loss: 0.1074, step time: 1.0419\n",
      "200/295, train_loss: 0.0489, step time: 1.0339\n",
      "201/295, train_loss: 0.0726, step time: 1.0350\n",
      "202/295, train_loss: 0.3725, step time: 1.0344\n",
      "203/295, train_loss: 0.1746, step time: 1.0597\n",
      "204/295, train_loss: 0.3987, step time: 1.0442\n",
      "205/295, train_loss: 0.1186, step time: 1.0520\n",
      "206/295, train_loss: 0.1350, step time: 1.0396\n",
      "207/295, train_loss: 0.0471, step time: 1.0334\n",
      "208/295, train_loss: 0.0662, step time: 1.0411\n",
      "209/295, train_loss: 0.0330, step time: 1.0348\n",
      "210/295, train_loss: 0.0940, step time: 1.0366\n",
      "211/295, train_loss: 0.1272, step time: 1.0379\n",
      "212/295, train_loss: 0.0620, step time: 1.0485\n",
      "213/295, train_loss: 0.0907, step time: 1.0553\n",
      "214/295, train_loss: 0.0392, step time: 1.0351\n",
      "215/295, train_loss: 0.0551, step time: 1.0574\n",
      "216/295, train_loss: 0.1048, step time: 1.0377\n",
      "217/295, train_loss: 0.0608, step time: 1.0335\n",
      "218/295, train_loss: 0.0627, step time: 1.0516\n",
      "219/295, train_loss: 0.3929, step time: 1.0367\n",
      "220/295, train_loss: 0.3615, step time: 1.0420\n",
      "221/295, train_loss: 0.0579, step time: 1.0402\n",
      "222/295, train_loss: 0.1842, step time: 1.0436\n",
      "223/295, train_loss: 0.1281, step time: 1.0668\n",
      "224/295, train_loss: 0.0469, step time: 1.0636\n",
      "225/295, train_loss: 0.0372, step time: 1.0436\n",
      "226/295, train_loss: 0.0603, step time: 1.0780\n",
      "227/295, train_loss: 0.0642, step time: 1.0316\n",
      "228/295, train_loss: 0.0640, step time: 1.0363\n",
      "229/295, train_loss: 0.0794, step time: 1.0367\n",
      "230/295, train_loss: 0.0673, step time: 1.0684\n",
      "231/295, train_loss: 0.1884, step time: 1.0394\n",
      "232/295, train_loss: 0.0964, step time: 1.0322\n",
      "233/295, train_loss: 0.0617, step time: 1.0333\n",
      "234/295, train_loss: 0.1009, step time: 1.0654\n",
      "235/295, train_loss: 0.0361, step time: 1.0336\n",
      "236/295, train_loss: 0.0476, step time: 1.0334\n",
      "237/295, train_loss: 0.4212, step time: 1.0376\n",
      "238/295, train_loss: 0.0366, step time: 1.0312\n",
      "239/295, train_loss: 0.0402, step time: 1.0353\n",
      "240/295, train_loss: 0.0447, step time: 1.0538\n",
      "241/295, train_loss: 0.1645, step time: 1.0373\n",
      "242/295, train_loss: 0.0338, step time: 1.0418\n",
      "243/295, train_loss: 0.0942, step time: 1.0358\n",
      "244/295, train_loss: 0.0623, step time: 1.0696\n",
      "245/295, train_loss: 0.0733, step time: 1.0610\n",
      "246/295, train_loss: 0.1160, step time: 1.0422\n",
      "247/295, train_loss: 0.0731, step time: 1.0462\n",
      "248/295, train_loss: 0.0410, step time: 1.0514\n",
      "249/295, train_loss: 0.0437, step time: 1.0359\n",
      "250/295, train_loss: 0.1302, step time: 1.0328\n",
      "251/295, train_loss: 0.0437, step time: 1.0367\n",
      "252/295, train_loss: 0.1361, step time: 1.0351\n",
      "253/295, train_loss: 0.0383, step time: 1.0382\n",
      "254/295, train_loss: 0.0474, step time: 1.0354\n",
      "255/295, train_loss: 0.0441, step time: 1.0338\n",
      "256/295, train_loss: 0.0701, step time: 1.0580\n",
      "257/295, train_loss: 0.0481, step time: 1.0369\n",
      "258/295, train_loss: 0.4193, step time: 1.0497\n",
      "259/295, train_loss: 0.0595, step time: 1.0323\n",
      "260/295, train_loss: 0.0266, step time: 1.0398\n",
      "261/295, train_loss: 0.0381, step time: 1.0402\n",
      "262/295, train_loss: 0.1079, step time: 1.0343\n",
      "263/295, train_loss: 0.1070, step time: 1.0339\n",
      "264/295, train_loss: 0.1046, step time: 1.0390\n",
      "265/295, train_loss: 0.0554, step time: 1.0407\n",
      "266/295, train_loss: 0.1345, step time: 1.0351\n",
      "267/295, train_loss: 0.1358, step time: 1.0384\n",
      "268/295, train_loss: 0.0495, step time: 1.0392\n",
      "269/295, train_loss: 0.0878, step time: 1.0400\n",
      "270/295, train_loss: 0.0534, step time: 1.0465\n",
      "271/295, train_loss: 0.1935, step time: 1.0372\n",
      "272/295, train_loss: 0.0504, step time: 1.0339\n",
      "273/295, train_loss: 0.0413, step time: 1.0368\n",
      "274/295, train_loss: 0.0800, step time: 1.0491\n",
      "275/295, train_loss: 0.1170, step time: 1.1153\n",
      "276/295, train_loss: 0.1347, step time: 1.0353\n",
      "277/295, train_loss: 0.3973, step time: 1.0376\n",
      "278/295, train_loss: 0.0430, step time: 1.0540\n",
      "279/295, train_loss: 0.2741, step time: 1.0336\n",
      "280/295, train_loss: 0.0793, step time: 1.0363\n",
      "281/295, train_loss: 0.0544, step time: 1.0599\n",
      "282/295, train_loss: 0.1129, step time: 1.0447\n",
      "283/295, train_loss: 0.0440, step time: 1.0491\n",
      "284/295, train_loss: 0.0344, step time: 1.0407\n",
      "285/295, train_loss: 0.0904, step time: 1.0718\n",
      "286/295, train_loss: 0.0652, step time: 1.0912\n",
      "287/295, train_loss: 0.1083, step time: 1.0317\n",
      "288/295, train_loss: 0.0543, step time: 1.0296\n",
      "289/295, train_loss: 0.0499, step time: 1.0291\n",
      "290/295, train_loss: 0.1321, step time: 1.0276\n",
      "291/295, train_loss: 0.0456, step time: 1.0297\n",
      "292/295, train_loss: 0.0664, step time: 1.0290\n",
      "293/295, train_loss: 0.0704, step time: 1.0282\n",
      "294/295, train_loss: 0.0983, step time: 1.0286\n",
      "295/295, train_loss: 0.1079, step time: 1.0286\n",
      "epoch 51 average loss: 0.1007\n",
      "current epoch: 51 current mean dice: 0.7849 tc: 0.7401 wt: 0.8543 et: 0.7626\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 51 is: 385.0328\n",
      "----------\n",
      "epoch 52/100\n",
      "1/295, train_loss: 0.0418, step time: 1.2559\n",
      "2/295, train_loss: 0.0405, step time: 1.1039\n",
      "3/295, train_loss: 0.1255, step time: 1.0336\n",
      "4/295, train_loss: 0.3595, step time: 1.0418\n",
      "5/295, train_loss: 0.0860, step time: 1.0528\n",
      "6/295, train_loss: 0.0289, step time: 1.0382\n",
      "7/295, train_loss: 0.0478, step time: 1.0772\n",
      "8/295, train_loss: 0.0825, step time: 1.0871\n",
      "9/295, train_loss: 0.0815, step time: 1.0549\n",
      "10/295, train_loss: 0.1476, step time: 1.0397\n",
      "11/295, train_loss: 0.0653, step time: 1.0586\n",
      "12/295, train_loss: 0.3862, step time: 1.0318\n",
      "13/295, train_loss: 0.0303, step time: 1.0295\n",
      "14/295, train_loss: 0.0796, step time: 1.0302\n",
      "15/295, train_loss: 0.0437, step time: 1.0389\n",
      "16/295, train_loss: 0.0468, step time: 1.0424\n",
      "17/295, train_loss: 0.0905, step time: 1.0326\n",
      "18/295, train_loss: 0.0458, step time: 1.0984\n",
      "19/295, train_loss: 0.0632, step time: 1.0695\n",
      "20/295, train_loss: 0.0634, step time: 1.0312\n",
      "21/295, train_loss: 0.1047, step time: 1.0680\n",
      "22/295, train_loss: 0.0489, step time: 1.0826\n",
      "23/295, train_loss: 0.0734, step time: 1.0406\n",
      "24/295, train_loss: 0.0603, step time: 1.0797\n",
      "25/295, train_loss: 0.0978, step time: 1.0294\n",
      "26/295, train_loss: 0.0397, step time: 1.0308\n",
      "27/295, train_loss: 0.0284, step time: 1.0299\n",
      "28/295, train_loss: 0.0882, step time: 1.0684\n",
      "29/295, train_loss: 0.0702, step time: 1.0373\n",
      "30/295, train_loss: 0.0455, step time: 1.0488\n",
      "31/295, train_loss: 0.0421, step time: 1.0436\n",
      "32/295, train_loss: 0.1457, step time: 1.0408\n",
      "33/295, train_loss: 0.0373, step time: 1.0742\n",
      "34/295, train_loss: 0.0893, step time: 1.0938\n",
      "35/295, train_loss: 0.1245, step time: 1.0742\n",
      "36/295, train_loss: 0.1421, step time: 1.0496\n",
      "37/295, train_loss: 0.0498, step time: 1.0483\n",
      "38/295, train_loss: 0.0846, step time: 1.0311\n",
      "39/295, train_loss: 0.0413, step time: 1.0361\n",
      "40/295, train_loss: 0.0897, step time: 1.0435\n",
      "41/295, train_loss: 0.1102, step time: 1.0508\n",
      "42/295, train_loss: 0.0331, step time: 1.0498\n",
      "43/295, train_loss: 0.0482, step time: 1.0390\n",
      "44/295, train_loss: 0.0408, step time: 1.0370\n",
      "45/295, train_loss: 0.0687, step time: 1.0378\n",
      "46/295, train_loss: 0.1329, step time: 1.0538\n",
      "47/295, train_loss: 0.1135, step time: 1.0387\n",
      "48/295, train_loss: 0.3868, step time: 1.0964\n",
      "49/295, train_loss: 0.0837, step time: 1.0384\n",
      "50/295, train_loss: 0.1173, step time: 1.0523\n",
      "51/295, train_loss: 0.0320, step time: 1.0486\n",
      "52/295, train_loss: 0.0467, step time: 1.0497\n",
      "53/295, train_loss: 0.0619, step time: 1.0315\n",
      "54/295, train_loss: 0.1007, step time: 1.0331\n",
      "55/295, train_loss: 0.0345, step time: 1.0710\n",
      "56/295, train_loss: 0.0488, step time: 1.0433\n",
      "57/295, train_loss: 0.0420, step time: 1.0477\n",
      "58/295, train_loss: 0.1305, step time: 1.0748\n",
      "59/295, train_loss: 0.0759, step time: 1.0864\n",
      "60/295, train_loss: 0.0380, step time: 1.0299\n",
      "61/295, train_loss: 0.0596, step time: 1.0393\n",
      "62/295, train_loss: 0.3940, step time: 1.0383\n",
      "63/295, train_loss: 0.1082, step time: 1.0441\n",
      "64/295, train_loss: 0.3995, step time: 1.0521\n",
      "65/295, train_loss: 0.1528, step time: 1.0301\n",
      "66/295, train_loss: 0.0729, step time: 1.0362\n",
      "67/295, train_loss: 0.1150, step time: 1.0667\n",
      "68/295, train_loss: 0.0545, step time: 1.0377\n",
      "69/295, train_loss: 0.2750, step time: 1.0395\n",
      "70/295, train_loss: 0.0329, step time: 1.0416\n",
      "71/295, train_loss: 0.0520, step time: 1.0399\n",
      "72/295, train_loss: 0.0952, step time: 1.0371\n",
      "73/295, train_loss: 0.0439, step time: 1.0386\n",
      "74/295, train_loss: 0.0553, step time: 1.0532\n",
      "75/295, train_loss: 0.0389, step time: 1.0531\n",
      "76/295, train_loss: 0.1572, step time: 1.0557\n",
      "77/295, train_loss: 0.1042, step time: 1.0354\n",
      "78/295, train_loss: 0.0971, step time: 1.0773\n",
      "79/295, train_loss: 0.0655, step time: 1.0414\n",
      "80/295, train_loss: 0.0383, step time: 1.0343\n",
      "81/295, train_loss: 0.1239, step time: 1.0689\n",
      "82/295, train_loss: 0.0771, step time: 1.1293\n",
      "83/295, train_loss: 0.1173, step time: 1.0809\n",
      "84/295, train_loss: 0.1005, step time: 1.0367\n",
      "85/295, train_loss: 0.0686, step time: 1.0419\n",
      "86/295, train_loss: 0.0573, step time: 1.0563\n",
      "87/295, train_loss: 0.1465, step time: 1.0874\n",
      "88/295, train_loss: 0.1207, step time: 1.0591\n",
      "89/295, train_loss: 0.4184, step time: 1.0449\n",
      "90/295, train_loss: 0.0547, step time: 1.0369\n",
      "91/295, train_loss: 0.0436, step time: 1.0335\n",
      "92/295, train_loss: 0.0319, step time: 1.0361\n",
      "93/295, train_loss: 0.0578, step time: 1.0482\n",
      "94/295, train_loss: 0.2835, step time: 1.0450\n",
      "95/295, train_loss: 0.0290, step time: 1.0300\n",
      "96/295, train_loss: 0.1003, step time: 1.0358\n",
      "97/295, train_loss: 0.0885, step time: 1.0373\n",
      "98/295, train_loss: 0.0532, step time: 1.0368\n",
      "99/295, train_loss: 0.0937, step time: 1.0460\n",
      "100/295, train_loss: 0.0534, step time: 1.0369\n",
      "101/295, train_loss: 0.0391, step time: 1.0515\n",
      "102/295, train_loss: 0.3907, step time: 1.0535\n",
      "103/295, train_loss: 0.0573, step time: 1.0539\n",
      "104/295, train_loss: 0.3800, step time: 1.0408\n",
      "105/295, train_loss: 0.0346, step time: 1.1286\n",
      "106/295, train_loss: 0.0677, step time: 1.0762\n",
      "107/295, train_loss: 0.1198, step time: 1.0633\n",
      "108/295, train_loss: 0.1142, step time: 1.0674\n",
      "109/295, train_loss: 0.0492, step time: 1.0490\n",
      "110/295, train_loss: 0.0850, step time: 1.0371\n",
      "111/295, train_loss: 0.1016, step time: 1.0373\n",
      "112/295, train_loss: 0.3720, step time: 1.0579\n",
      "113/295, train_loss: 0.0624, step time: 1.0631\n",
      "114/295, train_loss: 0.0611, step time: 1.0303\n",
      "115/295, train_loss: 0.0945, step time: 1.0329\n",
      "116/295, train_loss: 0.0673, step time: 1.0706\n",
      "117/295, train_loss: 0.0787, step time: 1.0415\n",
      "118/295, train_loss: 0.0646, step time: 1.0397\n",
      "119/295, train_loss: 0.0993, step time: 1.0325\n",
      "120/295, train_loss: 0.0851, step time: 1.0404\n",
      "121/295, train_loss: 0.0375, step time: 1.0346\n",
      "122/295, train_loss: 0.1061, step time: 1.0555\n",
      "123/295, train_loss: 0.1121, step time: 1.0393\n",
      "124/295, train_loss: 0.0761, step time: 1.0469\n",
      "125/295, train_loss: 0.0345, step time: 1.0552\n",
      "126/295, train_loss: 0.0749, step time: 1.0366\n",
      "127/295, train_loss: 0.0378, step time: 1.0353\n",
      "128/295, train_loss: 0.0854, step time: 1.0363\n",
      "129/295, train_loss: 0.0791, step time: 1.0395\n",
      "130/295, train_loss: 0.0573, step time: 1.0373\n",
      "131/295, train_loss: 0.0675, step time: 1.0355\n",
      "132/295, train_loss: 0.0449, step time: 1.0608\n",
      "133/295, train_loss: 0.0644, step time: 1.0647\n",
      "134/295, train_loss: 0.0541, step time: 1.0463\n",
      "135/295, train_loss: 0.3957, step time: 1.0375\n",
      "136/295, train_loss: 0.1166, step time: 1.0368\n",
      "137/295, train_loss: 0.0433, step time: 1.0349\n",
      "138/295, train_loss: 0.0597, step time: 1.0467\n",
      "139/295, train_loss: 0.0410, step time: 1.0675\n",
      "140/295, train_loss: 0.1707, step time: 1.0352\n",
      "141/295, train_loss: 0.3944, step time: 1.0616\n",
      "142/295, train_loss: 0.0311, step time: 1.0906\n",
      "143/295, train_loss: 0.1088, step time: 1.0353\n",
      "144/295, train_loss: 0.0524, step time: 1.0493\n",
      "145/295, train_loss: 0.0489, step time: 1.0514\n",
      "146/295, train_loss: 0.0673, step time: 1.0758\n",
      "147/295, train_loss: 0.0757, step time: 1.0812\n",
      "148/295, train_loss: 0.0288, step time: 1.0608\n",
      "149/295, train_loss: 0.0717, step time: 1.0359\n",
      "150/295, train_loss: 0.0511, step time: 1.0586\n",
      "151/295, train_loss: 0.0638, step time: 1.0417\n",
      "152/295, train_loss: 0.0750, step time: 1.0704\n",
      "153/295, train_loss: 0.0800, step time: 1.0800\n",
      "154/295, train_loss: 0.1373, step time: 1.0638\n",
      "155/295, train_loss: 0.0906, step time: 1.0415\n",
      "156/295, train_loss: 0.0328, step time: 1.0382\n",
      "157/295, train_loss: 0.0522, step time: 1.0415\n",
      "158/295, train_loss: 0.0312, step time: 1.0663\n",
      "159/295, train_loss: 0.0737, step time: 1.0800\n",
      "160/295, train_loss: 0.3692, step time: 1.0963\n",
      "161/295, train_loss: 0.0725, step time: 1.0417\n",
      "162/295, train_loss: 0.1049, step time: 1.0467\n",
      "163/295, train_loss: 0.0504, step time: 1.0362\n",
      "164/295, train_loss: 0.0543, step time: 1.0391\n",
      "165/295, train_loss: 0.0606, step time: 1.0449\n",
      "166/295, train_loss: 0.0981, step time: 1.0377\n",
      "167/295, train_loss: 0.1643, step time: 1.0038\n",
      "168/295, train_loss: 0.0886, step time: 1.0391\n",
      "169/295, train_loss: 0.0983, step time: 1.0508\n",
      "170/295, train_loss: 0.0498, step time: 1.0373\n",
      "171/295, train_loss: 0.0471, step time: 1.0331\n",
      "172/295, train_loss: 0.1494, step time: 1.0654\n",
      "173/295, train_loss: 0.0776, step time: 1.0357\n",
      "174/295, train_loss: 0.0574, step time: 1.0376\n",
      "175/295, train_loss: 0.0333, step time: 1.0431\n",
      "176/295, train_loss: 0.0589, step time: 1.0451\n",
      "177/295, train_loss: 0.0774, step time: 1.0733\n",
      "178/295, train_loss: 0.0521, step time: 1.0683\n",
      "179/295, train_loss: 0.0220, step time: 1.0308\n",
      "180/295, train_loss: 0.0957, step time: 1.0345\n",
      "181/295, train_loss: 0.0547, step time: 1.0467\n",
      "182/295, train_loss: 0.0687, step time: 1.0609\n",
      "183/295, train_loss: 0.4718, step time: 1.0460\n",
      "184/295, train_loss: 0.0657, step time: 1.0407\n",
      "185/295, train_loss: 0.0399, step time: 1.0378\n",
      "186/295, train_loss: 0.0960, step time: 1.0379\n",
      "187/295, train_loss: 0.1166, step time: 1.0380\n",
      "188/295, train_loss: 0.0881, step time: 1.0601\n",
      "189/295, train_loss: 0.3718, step time: 1.1356\n",
      "190/295, train_loss: 0.0777, step time: 1.0415\n",
      "191/295, train_loss: 0.1337, step time: 1.0327\n",
      "192/295, train_loss: 0.0732, step time: 1.0403\n",
      "193/295, train_loss: 0.4028, step time: 1.0367\n",
      "194/295, train_loss: 0.0626, step time: 1.0427\n",
      "195/295, train_loss: 0.0435, step time: 1.0759\n",
      "196/295, train_loss: 0.0665, step time: 1.0381\n",
      "197/295, train_loss: 0.0603, step time: 1.0321\n",
      "198/295, train_loss: 0.0591, step time: 1.0341\n",
      "199/295, train_loss: 0.0359, step time: 1.0346\n",
      "200/295, train_loss: 0.4013, step time: 1.0733\n",
      "201/295, train_loss: 0.0655, step time: 1.0420\n",
      "202/295, train_loss: 0.1160, step time: 1.0344\n",
      "203/295, train_loss: 0.0466, step time: 1.0610\n",
      "204/295, train_loss: 0.1136, step time: 1.0559\n",
      "205/295, train_loss: 0.1249, step time: 1.0318\n",
      "206/295, train_loss: 0.0445, step time: 1.0382\n",
      "207/295, train_loss: 0.0368, step time: 1.0399\n",
      "208/295, train_loss: 0.0803, step time: 1.0626\n",
      "209/295, train_loss: 0.0701, step time: 1.0817\n",
      "210/295, train_loss: 0.0505, step time: 1.0564\n",
      "211/295, train_loss: 0.0744, step time: 1.0528\n",
      "212/295, train_loss: 0.0462, step time: 1.0385\n",
      "213/295, train_loss: 0.0915, step time: 1.0378\n",
      "214/295, train_loss: 0.0774, step time: 1.0435\n",
      "215/295, train_loss: 0.0728, step time: 1.0436\n",
      "216/295, train_loss: 0.0361, step time: 1.0566\n",
      "217/295, train_loss: 0.1085, step time: 1.0532\n",
      "218/295, train_loss: 0.4077, step time: 1.0424\n",
      "219/295, train_loss: 0.0716, step time: 1.0328\n",
      "220/295, train_loss: 0.1589, step time: 1.0398\n",
      "221/295, train_loss: 0.0367, step time: 1.0591\n",
      "222/295, train_loss: 0.3917, step time: 1.0565\n",
      "223/295, train_loss: 0.0340, step time: 1.0577\n",
      "224/295, train_loss: 0.0607, step time: 1.0827\n",
      "225/295, train_loss: 0.0868, step time: 1.0375\n",
      "226/295, train_loss: 0.1005, step time: 1.0496\n",
      "227/295, train_loss: 0.0859, step time: 1.0499\n",
      "228/295, train_loss: 0.3758, step time: 1.0511\n",
      "229/295, train_loss: 0.1111, step time: 1.0389\n",
      "230/295, train_loss: 0.0886, step time: 1.0324\n",
      "231/295, train_loss: 0.0276, step time: 1.0550\n",
      "232/295, train_loss: 0.0311, step time: 1.0665\n",
      "233/295, train_loss: 0.0522, step time: 1.0518\n",
      "234/295, train_loss: 0.1934, step time: 1.0571\n",
      "235/295, train_loss: 0.0433, step time: 1.0835\n",
      "236/295, train_loss: 0.1026, step time: 1.0406\n",
      "237/295, train_loss: 0.1907, step time: 1.0385\n",
      "238/295, train_loss: 0.0476, step time: 1.0496\n",
      "239/295, train_loss: 0.1236, step time: 1.0729\n",
      "240/295, train_loss: 0.0540, step time: 1.0451\n",
      "241/295, train_loss: 0.1178, step time: 1.0351\n",
      "242/295, train_loss: 0.0616, step time: 1.0818\n",
      "243/295, train_loss: 0.0539, step time: 1.0591\n",
      "244/295, train_loss: 0.0639, step time: 1.0469\n",
      "245/295, train_loss: 0.1110, step time: 1.0445\n",
      "246/295, train_loss: 0.0575, step time: 1.0368\n",
      "247/295, train_loss: 0.0812, step time: 1.0414\n",
      "248/295, train_loss: 0.0322, step time: 1.1141\n",
      "249/295, train_loss: 0.0374, step time: 1.0388\n",
      "250/295, train_loss: 0.3701, step time: 1.1017\n",
      "251/295, train_loss: 0.0443, step time: 1.0552\n",
      "252/295, train_loss: 0.0517, step time: 1.0330\n",
      "253/295, train_loss: 0.0635, step time: 1.0344\n",
      "254/295, train_loss: 0.0547, step time: 1.0403\n",
      "255/295, train_loss: 0.0645, step time: 1.0718\n",
      "256/295, train_loss: 0.0896, step time: 1.0514\n",
      "257/295, train_loss: 0.0714, step time: 1.1271\n",
      "258/295, train_loss: 0.0742, step time: 1.0392\n",
      "259/295, train_loss: 0.3875, step time: 1.0378\n",
      "260/295, train_loss: 0.0427, step time: 1.0353\n",
      "261/295, train_loss: 0.0596, step time: 1.0462\n",
      "262/295, train_loss: 0.0917, step time: 1.0744\n",
      "263/295, train_loss: 0.0580, step time: 1.0358\n",
      "264/295, train_loss: 0.0904, step time: 1.0573\n",
      "265/295, train_loss: 0.0678, step time: 1.0344\n",
      "266/295, train_loss: 0.0680, step time: 1.0412\n",
      "267/295, train_loss: 0.0732, step time: 1.0328\n",
      "268/295, train_loss: 0.0367, step time: 1.0404\n",
      "269/295, train_loss: 0.0373, step time: 1.0364\n",
      "270/295, train_loss: 0.0843, step time: 1.0784\n",
      "271/295, train_loss: 0.0959, step time: 1.0356\n",
      "272/295, train_loss: 0.0355, step time: 1.0658\n",
      "273/295, train_loss: 0.1667, step time: 1.0423\n",
      "274/295, train_loss: 0.0416, step time: 1.0672\n",
      "275/295, train_loss: 0.0649, step time: 1.0705\n",
      "276/295, train_loss: 0.0782, step time: 1.0613\n",
      "277/295, train_loss: 0.1154, step time: 1.0321\n",
      "278/295, train_loss: 0.0783, step time: 1.0395\n",
      "279/295, train_loss: 0.0385, step time: 1.0466\n",
      "280/295, train_loss: 0.0443, step time: 1.0366\n",
      "281/295, train_loss: 0.0947, step time: 1.0389\n",
      "282/295, train_loss: 0.1285, step time: 1.0389\n",
      "283/295, train_loss: 0.0591, step time: 1.0797\n",
      "284/295, train_loss: 0.0506, step time: 1.1062\n",
      "285/295, train_loss: 0.0513, step time: 1.0352\n",
      "286/295, train_loss: 0.0864, step time: 1.0403\n",
      "287/295, train_loss: 0.0878, step time: 1.0470\n",
      "288/295, train_loss: 0.0311, step time: 1.0520\n",
      "289/295, train_loss: 0.1441, step time: 1.0291\n",
      "290/295, train_loss: 0.3936, step time: 1.0297\n",
      "291/295, train_loss: 0.0984, step time: 1.0292\n",
      "292/295, train_loss: 0.0478, step time: 1.0297\n",
      "293/295, train_loss: 0.0589, step time: 1.0318\n",
      "294/295, train_loss: 0.0580, step time: 1.0291\n",
      "295/295, train_loss: 0.0528, step time: 1.0289\n",
      "epoch 52 average loss: 0.0986\n",
      "current epoch: 52 current mean dice: 0.7796 tc: 0.7366 wt: 0.8465 et: 0.7576\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 52 is: 382.8171\n",
      "----------\n",
      "epoch 53/100\n",
      "1/295, train_loss: 0.0402, step time: 1.1411\n",
      "2/295, train_loss: 0.0651, step time: 1.1093\n",
      "3/295, train_loss: 0.0759, step time: 1.1166\n",
      "4/295, train_loss: 0.0217, step time: 1.0725\n",
      "5/295, train_loss: 0.3895, step time: 1.0358\n",
      "6/295, train_loss: 0.3909, step time: 1.0469\n",
      "7/295, train_loss: 0.0549, step time: 1.0339\n",
      "8/295, train_loss: 0.1140, step time: 1.0339\n",
      "9/295, train_loss: 0.0463, step time: 1.0432\n",
      "10/295, train_loss: 0.0509, step time: 1.0436\n",
      "11/295, train_loss: 0.0704, step time: 1.0565\n",
      "12/295, train_loss: 0.0673, step time: 1.0531\n",
      "13/295, train_loss: 0.0534, step time: 1.0589\n",
      "14/295, train_loss: 0.0449, step time: 1.0581\n",
      "15/295, train_loss: 0.0398, step time: 1.0310\n",
      "16/295, train_loss: 0.0383, step time: 1.0961\n",
      "17/295, train_loss: 0.0732, step time: 1.0349\n",
      "18/295, train_loss: 0.0466, step time: 1.0385\n",
      "19/295, train_loss: 0.1088, step time: 1.0408\n",
      "20/295, train_loss: 0.0326, step time: 1.1090\n",
      "21/295, train_loss: 0.0707, step time: 1.0391\n",
      "22/295, train_loss: 0.1098, step time: 1.0535\n",
      "23/295, train_loss: 0.4111, step time: 1.0390\n",
      "24/295, train_loss: 0.0982, step time: 1.0389\n",
      "25/295, train_loss: 0.0606, step time: 1.0819\n",
      "26/295, train_loss: 0.0816, step time: 1.0410\n",
      "27/295, train_loss: 0.1224, step time: 1.0349\n",
      "28/295, train_loss: 0.3888, step time: 1.0347\n",
      "29/295, train_loss: 0.0943, step time: 1.0714\n",
      "30/295, train_loss: 0.0601, step time: 1.0997\n",
      "31/295, train_loss: 0.0598, step time: 1.1201\n",
      "32/295, train_loss: 0.0892, step time: 1.0829\n",
      "33/295, train_loss: 0.0867, step time: 1.0397\n",
      "34/295, train_loss: 0.0682, step time: 1.0580\n",
      "35/295, train_loss: 0.1026, step time: 1.0656\n",
      "36/295, train_loss: 0.0717, step time: 1.0333\n",
      "37/295, train_loss: 0.0670, step time: 1.0327\n",
      "38/295, train_loss: 0.0520, step time: 1.0365\n",
      "39/295, train_loss: 0.0436, step time: 1.0394\n",
      "40/295, train_loss: 0.0399, step time: 1.0505\n",
      "41/295, train_loss: 0.0376, step time: 1.0511\n",
      "42/295, train_loss: 0.0613, step time: 1.0445\n",
      "43/295, train_loss: 0.0840, step time: 1.0313\n",
      "44/295, train_loss: 0.0526, step time: 1.0436\n",
      "45/295, train_loss: 0.0495, step time: 1.0335\n",
      "46/295, train_loss: 0.0631, step time: 1.0371\n",
      "47/295, train_loss: 0.0386, step time: 1.0371\n",
      "48/295, train_loss: 0.0368, step time: 1.0401\n",
      "49/295, train_loss: 0.0504, step time: 1.0578\n",
      "50/295, train_loss: 0.0491, step time: 1.0613\n",
      "51/295, train_loss: 0.1048, step time: 1.0387\n",
      "52/295, train_loss: 0.0487, step time: 1.1155\n",
      "53/295, train_loss: 0.3753, step time: 1.0350\n",
      "54/295, train_loss: 0.0912, step time: 1.0528\n",
      "55/295, train_loss: 0.0478, step time: 1.0585\n",
      "56/295, train_loss: 0.0639, step time: 1.0462\n",
      "57/295, train_loss: 0.0460, step time: 1.0435\n",
      "58/295, train_loss: 0.1498, step time: 1.0603\n",
      "59/295, train_loss: 0.1521, step time: 1.0340\n",
      "60/295, train_loss: 0.0412, step time: 1.0431\n",
      "61/295, train_loss: 0.0450, step time: 1.0454\n",
      "62/295, train_loss: 0.3615, step time: 1.0920\n",
      "63/295, train_loss: 0.0348, step time: 1.0490\n",
      "64/295, train_loss: 0.0749, step time: 1.0344\n",
      "65/295, train_loss: 0.0393, step time: 1.0577\n",
      "66/295, train_loss: 0.0815, step time: 1.0531\n",
      "67/295, train_loss: 0.0756, step time: 1.0422\n",
      "68/295, train_loss: 0.0751, step time: 1.0609\n",
      "69/295, train_loss: 0.0688, step time: 1.0402\n",
      "70/295, train_loss: 0.0452, step time: 1.0391\n",
      "71/295, train_loss: 0.0587, step time: 1.0342\n",
      "72/295, train_loss: 0.0317, step time: 1.0502\n",
      "73/295, train_loss: 0.1021, step time: 1.0446\n",
      "74/295, train_loss: 0.3698, step time: 1.0355\n",
      "75/295, train_loss: 0.1842, step time: 1.0429\n",
      "76/295, train_loss: 0.0365, step time: 1.0969\n",
      "77/295, train_loss: 0.0502, step time: 1.0322\n",
      "78/295, train_loss: 0.0319, step time: 1.0634\n",
      "79/295, train_loss: 0.0618, step time: 1.0651\n",
      "80/295, train_loss: 0.0341, step time: 1.0378\n",
      "81/295, train_loss: 0.0564, step time: 1.0598\n",
      "82/295, train_loss: 0.0329, step time: 1.0329\n",
      "83/295, train_loss: 0.0394, step time: 1.0351\n",
      "84/295, train_loss: 0.0494, step time: 1.0695\n",
      "85/295, train_loss: 0.0853, step time: 1.0626\n",
      "86/295, train_loss: 0.0687, step time: 1.0536\n",
      "87/295, train_loss: 0.0411, step time: 1.0335\n",
      "88/295, train_loss: 0.0749, step time: 1.0404\n",
      "89/295, train_loss: 0.0296, step time: 1.0788\n",
      "90/295, train_loss: 0.3975, step time: 1.0368\n",
      "91/295, train_loss: 0.1247, step time: 1.0376\n",
      "92/295, train_loss: 0.0337, step time: 1.0362\n",
      "93/295, train_loss: 0.3991, step time: 1.0417\n",
      "94/295, train_loss: 0.0307, step time: 1.0833\n",
      "95/295, train_loss: 0.3926, step time: 1.0331\n",
      "96/295, train_loss: 0.0592, step time: 1.0703\n",
      "97/295, train_loss: 0.0570, step time: 1.0401\n",
      "98/295, train_loss: 0.0408, step time: 1.0372\n",
      "99/295, train_loss: 0.1000, step time: 1.0429\n",
      "100/295, train_loss: 0.3796, step time: 1.0594\n",
      "101/295, train_loss: 0.0640, step time: 1.0344\n",
      "102/295, train_loss: 0.0804, step time: 1.0368\n",
      "103/295, train_loss: 0.0278, step time: 1.0333\n",
      "104/295, train_loss: 0.0938, step time: 1.0429\n",
      "105/295, train_loss: 0.1124, step time: 1.0484\n",
      "106/295, train_loss: 0.0442, step time: 1.0423\n",
      "107/295, train_loss: 0.0850, step time: 1.1016\n",
      "108/295, train_loss: 0.0773, step time: 1.0536\n",
      "109/295, train_loss: 0.3879, step time: 1.0566\n",
      "110/295, train_loss: 0.1578, step time: 1.0549\n",
      "111/295, train_loss: 0.0498, step time: 1.0774\n",
      "112/295, train_loss: 0.1434, step time: 1.0393\n",
      "113/295, train_loss: 0.0457, step time: 1.0510\n",
      "114/295, train_loss: 0.0545, step time: 1.0384\n",
      "115/295, train_loss: 0.0863, step time: 1.0570\n",
      "116/295, train_loss: 0.0278, step time: 1.0619\n",
      "117/295, train_loss: 0.1080, step time: 1.0673\n",
      "118/295, train_loss: 0.0613, step time: 1.0527\n",
      "119/295, train_loss: 0.0954, step time: 1.0377\n",
      "120/295, train_loss: 0.0923, step time: 1.0482\n",
      "121/295, train_loss: 0.0521, step time: 1.0459\n",
      "122/295, train_loss: 0.0939, step time: 1.0833\n",
      "123/295, train_loss: 0.0626, step time: 1.0681\n",
      "124/295, train_loss: 0.0869, step time: 1.0497\n",
      "125/295, train_loss: 0.3837, step time: 1.0339\n",
      "126/295, train_loss: 0.0875, step time: 1.0665\n",
      "127/295, train_loss: 0.0556, step time: 1.0503\n",
      "128/295, train_loss: 0.0286, step time: 1.0368\n",
      "129/295, train_loss: 0.0327, step time: 1.0540\n",
      "130/295, train_loss: 0.0649, step time: 1.0454\n",
      "131/295, train_loss: 0.0622, step time: 1.0370\n",
      "132/295, train_loss: 0.3949, step time: 1.0491\n",
      "133/295, train_loss: 0.0651, step time: 1.0962\n",
      "134/295, train_loss: 0.1415, step time: 1.0515\n",
      "135/295, train_loss: 0.0311, step time: 1.0571\n",
      "136/295, train_loss: 0.0300, step time: 1.0937\n",
      "137/295, train_loss: 0.0555, step time: 1.0374\n",
      "138/295, train_loss: 0.3707, step time: 1.0349\n",
      "139/295, train_loss: 0.1084, step time: 1.0351\n",
      "140/295, train_loss: 0.1169, step time: 1.0471\n",
      "141/295, train_loss: 0.0464, step time: 1.1242\n",
      "142/295, train_loss: 0.1439, step time: 1.1080\n",
      "143/295, train_loss: 0.0329, step time: 1.0387\n",
      "144/295, train_loss: 0.1000, step time: 1.0428\n",
      "145/295, train_loss: 0.0302, step time: 1.0360\n",
      "146/295, train_loss: 0.0339, step time: 1.0456\n",
      "147/295, train_loss: 0.0904, step time: 1.0444\n",
      "148/295, train_loss: 0.0635, step time: 1.0412\n",
      "149/295, train_loss: 0.0521, step time: 1.0593\n",
      "150/295, train_loss: 0.0364, step time: 1.0365\n",
      "151/295, train_loss: 0.1149, step time: 1.0571\n",
      "152/295, train_loss: 0.0603, step time: 1.0428\n",
      "153/295, train_loss: 0.0745, step time: 1.0868\n",
      "154/295, train_loss: 0.0532, step time: 1.0342\n",
      "155/295, train_loss: 0.0994, step time: 1.0356\n",
      "156/295, train_loss: 0.0627, step time: 1.0380\n",
      "157/295, train_loss: 0.1500, step time: 1.0419\n",
      "158/295, train_loss: 0.0914, step time: 1.1083\n",
      "159/295, train_loss: 0.0745, step time: 1.0417\n",
      "160/295, train_loss: 0.1189, step time: 1.0392\n",
      "161/295, train_loss: 0.0694, step time: 1.0427\n",
      "162/295, train_loss: 0.0727, step time: 1.0406\n",
      "163/295, train_loss: 0.0572, step time: 1.0417\n",
      "164/295, train_loss: 0.0346, step time: 1.0425\n",
      "165/295, train_loss: 0.0624, step time: 1.0346\n",
      "166/295, train_loss: 0.0470, step time: 1.0345\n",
      "167/295, train_loss: 0.0822, step time: 1.0369\n",
      "168/295, train_loss: 0.0909, step time: 1.0331\n",
      "169/295, train_loss: 0.0715, step time: 1.0534\n",
      "170/295, train_loss: 0.0887, step time: 1.0438\n",
      "171/295, train_loss: 0.0774, step time: 1.0644\n",
      "172/295, train_loss: 0.2884, step time: 1.0570\n",
      "173/295, train_loss: 0.0435, step time: 1.0593\n",
      "174/295, train_loss: 0.0914, step time: 1.0330\n",
      "175/295, train_loss: 0.1009, step time: 1.0596\n",
      "176/295, train_loss: 0.0452, step time: 1.0362\n",
      "177/295, train_loss: 0.3762, step time: 1.0390\n",
      "178/295, train_loss: 0.1026, step time: 1.0334\n",
      "179/295, train_loss: 0.0704, step time: 1.0374\n",
      "180/295, train_loss: 0.0372, step time: 1.0331\n",
      "181/295, train_loss: 0.0939, step time: 1.0672\n",
      "182/295, train_loss: 0.1131, step time: 1.0385\n",
      "183/295, train_loss: 0.4741, step time: 1.0374\n",
      "184/295, train_loss: 0.0490, step time: 1.0421\n",
      "185/295, train_loss: 0.0895, step time: 1.0976\n",
      "186/295, train_loss: 0.0859, step time: 1.0447\n",
      "187/295, train_loss: 0.0567, step time: 1.1413\n",
      "188/295, train_loss: 0.0702, step time: 1.0681\n",
      "189/295, train_loss: 0.0650, step time: 1.0428\n",
      "190/295, train_loss: 0.0343, step time: 1.0829\n",
      "191/295, train_loss: 0.1242, step time: 1.1158\n",
      "192/295, train_loss: 0.0607, step time: 1.0377\n",
      "193/295, train_loss: 0.0428, step time: 1.0318\n",
      "194/295, train_loss: 0.0843, step time: 1.0394\n",
      "195/295, train_loss: 0.0624, step time: 1.0560\n",
      "196/295, train_loss: 0.1532, step time: 1.0384\n",
      "197/295, train_loss: 0.0286, step time: 1.0998\n",
      "198/295, train_loss: 0.0604, step time: 1.0498\n",
      "199/295, train_loss: 0.3912, step time: 1.0599\n",
      "200/295, train_loss: 0.0952, step time: 1.0431\n",
      "201/295, train_loss: 0.0762, step time: 1.0888\n",
      "202/295, train_loss: 0.0748, step time: 1.0367\n",
      "203/295, train_loss: 0.0359, step time: 1.0376\n",
      "204/295, train_loss: 0.1443, step time: 1.0361\n",
      "205/295, train_loss: 0.0878, step time: 1.0421\n",
      "206/295, train_loss: 0.0455, step time: 1.0430\n",
      "207/295, train_loss: 0.1099, step time: 1.0589\n",
      "208/295, train_loss: 0.0705, step time: 1.0357\n",
      "209/295, train_loss: 0.0734, step time: 1.0337\n",
      "210/295, train_loss: 0.0756, step time: 1.0320\n",
      "211/295, train_loss: 0.1345, step time: 1.0381\n",
      "212/295, train_loss: 0.0851, step time: 1.0599\n",
      "213/295, train_loss: 0.0692, step time: 1.0573\n",
      "214/295, train_loss: 0.1076, step time: 1.0458\n",
      "215/295, train_loss: 0.0860, step time: 1.0546\n",
      "216/295, train_loss: 0.0350, step time: 1.0328\n",
      "217/295, train_loss: 0.1126, step time: 1.0485\n",
      "218/295, train_loss: 0.2707, step time: 1.0396\n",
      "219/295, train_loss: 0.3702, step time: 1.0376\n",
      "220/295, train_loss: 0.0443, step time: 1.0439\n",
      "221/295, train_loss: 0.0396, step time: 1.0618\n",
      "222/295, train_loss: 0.0458, step time: 1.0333\n",
      "223/295, train_loss: 0.0977, step time: 1.0641\n",
      "224/295, train_loss: 0.0520, step time: 1.0393\n",
      "225/295, train_loss: 0.1853, step time: 1.0802\n",
      "226/295, train_loss: 0.0867, step time: 1.0443\n",
      "227/295, train_loss: 0.0424, step time: 1.0408\n",
      "228/295, train_loss: 0.0631, step time: 1.0358\n",
      "229/295, train_loss: 0.0390, step time: 1.0372\n",
      "230/295, train_loss: 0.0695, step time: 1.0620\n",
      "231/295, train_loss: 0.1162, step time: 1.0527\n",
      "232/295, train_loss: 0.0388, step time: 1.0585\n",
      "233/295, train_loss: 0.0586, step time: 1.0334\n",
      "234/295, train_loss: 0.1278, step time: 1.0348\n",
      "235/295, train_loss: 0.0449, step time: 1.0365\n",
      "236/295, train_loss: 0.0505, step time: 1.0570\n",
      "237/295, train_loss: 0.0498, step time: 1.0367\n",
      "238/295, train_loss: 0.0912, step time: 1.0520\n",
      "239/295, train_loss: 0.0920, step time: 1.0358\n",
      "240/295, train_loss: 0.0505, step time: 1.0554\n",
      "241/295, train_loss: 0.0605, step time: 1.0370\n",
      "242/295, train_loss: 0.0517, step time: 1.0330\n",
      "243/295, train_loss: 0.3725, step time: 1.0616\n",
      "244/295, train_loss: 0.1397, step time: 1.0484\n",
      "245/295, train_loss: 0.0985, step time: 1.0649\n",
      "246/295, train_loss: 0.0944, step time: 1.0680\n",
      "247/295, train_loss: 0.0496, step time: 1.0453\n",
      "248/295, train_loss: 0.0783, step time: 1.0445\n",
      "249/295, train_loss: 0.1249, step time: 1.0691\n",
      "250/295, train_loss: 0.1090, step time: 1.0852\n",
      "251/295, train_loss: 0.0844, step time: 1.0623\n",
      "252/295, train_loss: 0.0536, step time: 1.0337\n",
      "253/295, train_loss: 0.0666, step time: 1.0423\n",
      "254/295, train_loss: 0.0968, step time: 1.0704\n",
      "255/295, train_loss: 0.1287, step time: 1.0503\n",
      "256/295, train_loss: 0.0472, step time: 1.0320\n",
      "257/295, train_loss: 0.3932, step time: 1.0413\n",
      "258/295, train_loss: 0.0830, step time: 1.0530\n",
      "259/295, train_loss: 0.1024, step time: 1.0382\n",
      "260/295, train_loss: 0.0447, step time: 1.0523\n",
      "261/295, train_loss: 0.1038, step time: 1.0654\n",
      "262/295, train_loss: 0.1915, step time: 1.0529\n",
      "263/295, train_loss: 0.0768, step time: 1.0504\n",
      "264/295, train_loss: 0.0280, step time: 1.0604\n",
      "265/295, train_loss: 0.0478, step time: 1.0586\n",
      "266/295, train_loss: 0.0711, step time: 1.0335\n",
      "267/295, train_loss: 0.1077, step time: 1.0851\n",
      "268/295, train_loss: 0.3950, step time: 1.0511\n",
      "269/295, train_loss: 0.0435, step time: 1.0669\n",
      "270/295, train_loss: 0.0661, step time: 1.0538\n",
      "271/295, train_loss: 0.0623, step time: 1.0376\n",
      "272/295, train_loss: 0.1195, step time: 1.0723\n",
      "273/295, train_loss: 0.0937, step time: 1.0677\n",
      "274/295, train_loss: 0.0602, step time: 1.0340\n",
      "275/295, train_loss: 0.1105, step time: 1.0333\n",
      "276/295, train_loss: 0.1243, step time: 1.0347\n",
      "277/295, train_loss: 0.0396, step time: 1.0436\n",
      "278/295, train_loss: 0.0748, step time: 1.0644\n",
      "279/295, train_loss: 0.0778, step time: 1.0614\n",
      "280/295, train_loss: 0.0438, step time: 1.0533\n",
      "281/295, train_loss: 0.0621, step time: 1.0338\n",
      "282/295, train_loss: 0.0844, step time: 1.0722\n",
      "283/295, train_loss: 0.0510, step time: 1.0717\n",
      "284/295, train_loss: 0.1321, step time: 1.0756\n",
      "285/295, train_loss: 0.0527, step time: 1.0452\n",
      "286/295, train_loss: 0.0692, step time: 1.0767\n",
      "287/295, train_loss: 0.0546, step time: 1.0349\n",
      "288/295, train_loss: 0.0386, step time: 1.0331\n",
      "289/295, train_loss: 0.0840, step time: 1.0299\n",
      "290/295, train_loss: 0.0343, step time: 1.0295\n",
      "291/295, train_loss: 0.0565, step time: 1.0295\n",
      "292/295, train_loss: 0.0508, step time: 1.0296\n",
      "293/295, train_loss: 0.0620, step time: 1.0303\n",
      "294/295, train_loss: 0.0692, step time: 1.0292\n",
      "295/295, train_loss: 0.0550, step time: 1.0285\n",
      "epoch 53 average loss: 0.0970\n",
      "current epoch: 53 current mean dice: 0.7854 tc: 0.7465 wt: 0.8464 et: 0.7708\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 53 is: 387.2005\n",
      "----------\n",
      "epoch 54/100\n",
      "1/295, train_loss: 0.0799, step time: 1.1663\n",
      "2/295, train_loss: 0.0386, step time: 1.0640\n",
      "3/295, train_loss: 0.0317, step time: 1.0423\n",
      "4/295, train_loss: 0.3833, step time: 1.0564\n",
      "5/295, train_loss: 0.0765, step time: 1.0916\n",
      "6/295, train_loss: 0.1305, step time: 1.0300\n",
      "7/295, train_loss: 0.0679, step time: 1.0361\n",
      "8/295, train_loss: 0.0419, step time: 1.0585\n",
      "9/295, train_loss: 0.0730, step time: 1.0945\n",
      "10/295, train_loss: 0.0399, step time: 1.0801\n",
      "11/295, train_loss: 0.0460, step time: 1.0328\n",
      "12/295, train_loss: 0.0461, step time: 1.0434\n",
      "13/295, train_loss: 0.1262, step time: 1.0308\n",
      "14/295, train_loss: 0.0370, step time: 1.0526\n",
      "15/295, train_loss: 0.0695, step time: 1.0323\n",
      "16/295, train_loss: 0.0457, step time: 1.0315\n",
      "17/295, train_loss: 0.0593, step time: 1.0502\n",
      "18/295, train_loss: 0.0723, step time: 1.0369\n",
      "19/295, train_loss: 0.0324, step time: 1.0778\n",
      "20/295, train_loss: 0.0747, step time: 1.0701\n",
      "21/295, train_loss: 0.0372, step time: 1.0517\n",
      "22/295, train_loss: 0.0952, step time: 1.0310\n",
      "23/295, train_loss: 0.1071, step time: 1.1025\n",
      "24/295, train_loss: 0.0413, step time: 1.0312\n",
      "25/295, train_loss: 0.0864, step time: 1.0346\n",
      "26/295, train_loss: 0.0491, step time: 1.0374\n",
      "27/295, train_loss: 0.0510, step time: 1.0332\n",
      "28/295, train_loss: 0.0631, step time: 1.0391\n",
      "29/295, train_loss: 0.1562, step time: 1.0834\n",
      "30/295, train_loss: 0.0842, step time: 1.0604\n",
      "31/295, train_loss: 0.1019, step time: 1.0287\n",
      "32/295, train_loss: 0.0623, step time: 1.0292\n",
      "33/295, train_loss: 0.0459, step time: 1.0341\n",
      "34/295, train_loss: 0.0571, step time: 1.0419\n",
      "35/295, train_loss: 0.1484, step time: 1.0511\n",
      "36/295, train_loss: 0.3693, step time: 1.0329\n",
      "37/295, train_loss: 0.0897, step time: 1.0305\n",
      "38/295, train_loss: 0.3725, step time: 1.0385\n",
      "39/295, train_loss: 0.1200, step time: 1.0357\n",
      "40/295, train_loss: 0.0623, step time: 1.0294\n",
      "41/295, train_loss: 0.1054, step time: 1.0342\n",
      "42/295, train_loss: 0.0377, step time: 1.0419\n",
      "43/295, train_loss: 0.0795, step time: 1.0528\n",
      "44/295, train_loss: 0.0791, step time: 1.0449\n",
      "45/295, train_loss: 0.0627, step time: 1.0500\n",
      "46/295, train_loss: 0.3946, step time: 1.0331\n",
      "47/295, train_loss: 0.0853, step time: 1.1031\n",
      "48/295, train_loss: 0.0589, step time: 1.0405\n",
      "49/295, train_loss: 0.0636, step time: 1.0332\n",
      "50/295, train_loss: 0.0625, step time: 1.0565\n",
      "51/295, train_loss: 0.0558, step time: 1.0485\n",
      "52/295, train_loss: 0.0418, step time: 1.0374\n",
      "53/295, train_loss: 0.1240, step time: 1.0478\n",
      "54/295, train_loss: 0.0613, step time: 1.0916\n",
      "55/295, train_loss: 0.0321, step time: 1.0504\n",
      "56/295, train_loss: 0.0946, step time: 1.0371\n",
      "57/295, train_loss: 0.1468, step time: 1.1128\n",
      "58/295, train_loss: 0.3907, step time: 1.0964\n",
      "59/295, train_loss: 0.0922, step time: 1.0800\n",
      "60/295, train_loss: 0.0522, step time: 1.0357\n",
      "61/295, train_loss: 0.0480, step time: 1.0570\n",
      "62/295, train_loss: 0.0752, step time: 1.0682\n",
      "63/295, train_loss: 0.0489, step time: 1.0600\n",
      "64/295, train_loss: 0.0663, step time: 1.0379\n",
      "65/295, train_loss: 0.1104, step time: 1.0465\n",
      "66/295, train_loss: 0.0625, step time: 1.0393\n",
      "67/295, train_loss: 0.0908, step time: 1.0568\n",
      "68/295, train_loss: 0.1301, step time: 1.0501\n",
      "69/295, train_loss: 0.0878, step time: 1.0398\n",
      "70/295, train_loss: 0.0996, step time: 1.0602\n",
      "71/295, train_loss: 0.0704, step time: 1.0408\n",
      "72/295, train_loss: 0.0442, step time: 1.0305\n",
      "73/295, train_loss: 0.0473, step time: 1.0355\n",
      "74/295, train_loss: 0.1221, step time: 1.0486\n",
      "75/295, train_loss: 0.0520, step time: 1.0957\n",
      "76/295, train_loss: 0.0732, step time: 1.0389\n",
      "77/295, train_loss: 0.0359, step time: 1.0402\n",
      "78/295, train_loss: 0.0533, step time: 1.1300\n",
      "79/295, train_loss: 0.0830, step time: 1.0389\n",
      "80/295, train_loss: 0.0748, step time: 1.0422\n",
      "81/295, train_loss: 0.0774, step time: 1.1010\n",
      "82/295, train_loss: 0.1468, step time: 1.0339\n",
      "83/295, train_loss: 0.0474, step time: 1.0601\n",
      "84/295, train_loss: 0.3837, step time: 1.0554\n",
      "85/295, train_loss: 0.0286, step time: 1.0499\n",
      "86/295, train_loss: 0.0585, step time: 1.0765\n",
      "87/295, train_loss: 0.0377, step time: 1.0999\n",
      "88/295, train_loss: 0.0322, step time: 1.0324\n",
      "89/295, train_loss: 0.0311, step time: 1.0406\n",
      "90/295, train_loss: 0.3901, step time: 1.0358\n",
      "91/295, train_loss: 0.0670, step time: 1.0403\n",
      "92/295, train_loss: 0.1499, step time: 1.0369\n",
      "93/295, train_loss: 0.3617, step time: 1.0378\n",
      "94/295, train_loss: 0.0613, step time: 1.0388\n",
      "95/295, train_loss: 0.0779, step time: 1.0418\n",
      "96/295, train_loss: 0.0957, step time: 1.0432\n",
      "97/295, train_loss: 0.0588, step time: 1.0704\n",
      "98/295, train_loss: 0.0661, step time: 1.0357\n",
      "99/295, train_loss: 0.0844, step time: 1.0548\n",
      "100/295, train_loss: 0.0366, step time: 1.0655\n",
      "101/295, train_loss: 0.0672, step time: 1.0342\n",
      "102/295, train_loss: 0.0419, step time: 1.0740\n",
      "103/295, train_loss: 0.0403, step time: 1.0449\n",
      "104/295, train_loss: 0.0489, step time: 1.0337\n",
      "105/295, train_loss: 0.0476, step time: 1.0496\n",
      "106/295, train_loss: 0.1459, step time: 1.0810\n",
      "107/295, train_loss: 0.0343, step time: 1.0330\n",
      "108/295, train_loss: 0.0626, step time: 1.1288\n",
      "109/295, train_loss: 0.0280, step time: 1.0300\n",
      "110/295, train_loss: 0.0737, step time: 1.0574\n",
      "111/295, train_loss: 0.1600, step time: 1.0573\n",
      "112/295, train_loss: 0.0802, step time: 1.0407\n",
      "113/295, train_loss: 0.0728, step time: 1.0383\n",
      "114/295, train_loss: 0.0269, step time: 1.0661\n",
      "115/295, train_loss: 0.0366, step time: 1.0528\n",
      "116/295, train_loss: 0.0678, step time: 1.0341\n",
      "117/295, train_loss: 0.1096, step time: 1.0536\n",
      "118/295, train_loss: 0.0404, step time: 1.0517\n",
      "119/295, train_loss: 0.2723, step time: 1.0368\n",
      "120/295, train_loss: 0.4100, step time: 1.0604\n",
      "121/295, train_loss: 0.0321, step time: 1.0416\n",
      "122/295, train_loss: 0.0492, step time: 1.0330\n",
      "123/295, train_loss: 0.0678, step time: 1.0456\n",
      "124/295, train_loss: 0.0381, step time: 1.1207\n",
      "125/295, train_loss: 0.1139, step time: 1.0353\n",
      "126/295, train_loss: 0.0451, step time: 1.0311\n",
      "127/295, train_loss: 0.0999, step time: 1.0737\n",
      "128/295, train_loss: 0.1168, step time: 1.0357\n",
      "129/295, train_loss: 0.0935, step time: 1.0434\n",
      "130/295, train_loss: 0.0622, step time: 1.0997\n",
      "131/295, train_loss: 0.0863, step time: 1.0474\n",
      "132/295, train_loss: 0.0633, step time: 1.0781\n",
      "133/295, train_loss: 0.0380, step time: 1.0482\n",
      "134/295, train_loss: 0.1211, step time: 1.0301\n",
      "135/295, train_loss: 0.0860, step time: 1.0369\n",
      "136/295, train_loss: 0.0911, step time: 1.0344\n",
      "137/295, train_loss: 0.3978, step time: 1.0570\n",
      "138/295, train_loss: 0.0754, step time: 1.0450\n",
      "139/295, train_loss: 0.0639, step time: 1.0437\n",
      "140/295, train_loss: 0.1062, step time: 1.0730\n",
      "141/295, train_loss: 0.1693, step time: 1.0516\n",
      "142/295, train_loss: 0.0295, step time: 1.0606\n",
      "143/295, train_loss: 0.0796, step time: 1.0700\n",
      "144/295, train_loss: 0.0290, step time: 1.0370\n",
      "145/295, train_loss: 0.0584, step time: 1.1189\n",
      "146/295, train_loss: 0.1247, step time: 1.0393\n",
      "147/295, train_loss: 0.0639, step time: 1.0802\n",
      "148/295, train_loss: 0.3699, step time: 1.0325\n",
      "149/295, train_loss: 0.1398, step time: 1.0340\n",
      "150/295, train_loss: 0.0448, step time: 1.0360\n",
      "151/295, train_loss: 0.0790, step time: 1.0403\n",
      "152/295, train_loss: 0.0815, step time: 1.0786\n",
      "153/295, train_loss: 0.1085, step time: 1.0333\n",
      "154/295, train_loss: 0.0465, step time: 1.0393\n",
      "155/295, train_loss: 0.0825, step time: 1.0570\n",
      "156/295, train_loss: 0.0890, step time: 1.0458\n",
      "157/295, train_loss: 0.0423, step time: 1.0715\n",
      "158/295, train_loss: 0.0974, step time: 1.0430\n",
      "159/295, train_loss: 0.0856, step time: 1.0749\n",
      "160/295, train_loss: 0.0555, step time: 1.0428\n",
      "161/295, train_loss: 0.0883, step time: 1.0364\n",
      "162/295, train_loss: 0.3696, step time: 1.0871\n",
      "163/295, train_loss: 0.0847, step time: 1.0486\n",
      "164/295, train_loss: 0.0384, step time: 1.0445\n",
      "165/295, train_loss: 0.0713, step time: 1.0981\n",
      "166/295, train_loss: 0.0725, step time: 1.0417\n",
      "167/295, train_loss: 0.0552, step time: 1.0332\n",
      "168/295, train_loss: 0.3967, step time: 1.0446\n",
      "169/295, train_loss: 0.1038, step time: 1.0471\n",
      "170/295, train_loss: 0.0865, step time: 1.0355\n",
      "171/295, train_loss: 0.0894, step time: 1.0334\n",
      "172/295, train_loss: 0.0475, step time: 1.0703\n",
      "173/295, train_loss: 0.0495, step time: 1.0829\n",
      "174/295, train_loss: 0.0343, step time: 1.0616\n",
      "175/295, train_loss: 0.0627, step time: 1.0862\n",
      "176/295, train_loss: 0.0466, step time: 1.0340\n",
      "177/295, train_loss: 0.0481, step time: 1.0422\n",
      "178/295, train_loss: 0.0950, step time: 1.0486\n",
      "179/295, train_loss: 0.0333, step time: 1.1281\n",
      "180/295, train_loss: 0.0865, step time: 1.0623\n",
      "181/295, train_loss: 0.0637, step time: 1.1364\n",
      "182/295, train_loss: 0.0487, step time: 1.0376\n",
      "183/295, train_loss: 0.0585, step time: 1.0582\n",
      "184/295, train_loss: 0.0867, step time: 1.0599\n",
      "185/295, train_loss: 0.0729, step time: 1.0456\n",
      "186/295, train_loss: 0.0597, step time: 1.0375\n",
      "187/295, train_loss: 0.0353, step time: 1.0322\n",
      "188/295, train_loss: 0.0344, step time: 1.1164\n",
      "189/295, train_loss: 0.0653, step time: 1.0405\n",
      "190/295, train_loss: 0.0397, step time: 1.0440\n",
      "191/295, train_loss: 0.0579, step time: 1.0403\n",
      "192/295, train_loss: 0.1295, step time: 1.0543\n",
      "193/295, train_loss: 0.0840, step time: 1.0388\n",
      "194/295, train_loss: 0.0211, step time: 1.0981\n",
      "195/295, train_loss: 0.0349, step time: 1.0376\n",
      "196/295, train_loss: 0.0445, step time: 1.0661\n",
      "197/295, train_loss: 0.0516, step time: 1.0769\n",
      "198/295, train_loss: 0.0582, step time: 1.0364\n",
      "199/295, train_loss: 0.0907, step time: 1.0498\n",
      "200/295, train_loss: 0.0906, step time: 1.0771\n",
      "201/295, train_loss: 0.0437, step time: 1.0379\n",
      "202/295, train_loss: 0.1099, step time: 1.0413\n",
      "203/295, train_loss: 0.4682, step time: 1.0798\n",
      "204/295, train_loss: 0.0710, step time: 1.0491\n",
      "205/295, train_loss: 0.0981, step time: 1.0479\n",
      "206/295, train_loss: 0.0582, step time: 1.0418\n",
      "207/295, train_loss: 0.0384, step time: 1.0809\n",
      "208/295, train_loss: 0.3944, step time: 1.0798\n",
      "209/295, train_loss: 0.1052, step time: 1.0359\n",
      "210/295, train_loss: 0.1333, step time: 1.0416\n",
      "211/295, train_loss: 0.0493, step time: 1.0388\n",
      "212/295, train_loss: 0.3701, step time: 1.0333\n",
      "213/295, train_loss: 0.1109, step time: 1.0566\n",
      "214/295, train_loss: 0.3806, step time: 1.1469\n",
      "215/295, train_loss: 0.2169, step time: 1.0375\n",
      "216/295, train_loss: 0.3949, step time: 1.0400\n",
      "217/295, train_loss: 0.1101, step time: 1.0523\n",
      "218/295, train_loss: 0.2943, step time: 1.0433\n",
      "219/295, train_loss: 0.0623, step time: 1.0962\n",
      "220/295, train_loss: 0.0286, step time: 1.0322\n",
      "221/295, train_loss: 0.0570, step time: 1.0406\n",
      "222/295, train_loss: 0.0298, step time: 1.0397\n",
      "223/295, train_loss: 0.0706, step time: 1.0406\n",
      "224/295, train_loss: 0.0558, step time: 1.0324\n",
      "225/295, train_loss: 0.0673, step time: 1.0356\n",
      "226/295, train_loss: 0.0380, step time: 1.0377\n",
      "227/295, train_loss: 0.0763, step time: 1.0535\n",
      "228/295, train_loss: 0.0300, step time: 1.0651\n",
      "229/295, train_loss: 0.1227, step time: 1.0826\n",
      "230/295, train_loss: 0.0750, step time: 1.0351\n",
      "231/295, train_loss: 0.0851, step time: 1.0566\n",
      "232/295, train_loss: 0.1014, step time: 1.0456\n",
      "233/295, train_loss: 0.0361, step time: 1.0627\n",
      "234/295, train_loss: 0.0937, step time: 1.0414\n",
      "235/295, train_loss: 0.0958, step time: 1.0579\n",
      "236/295, train_loss: 0.0754, step time: 1.0500\n",
      "237/295, train_loss: 0.0385, step time: 1.0541\n",
      "238/295, train_loss: 0.0577, step time: 1.0493\n",
      "239/295, train_loss: 0.0656, step time: 1.0565\n",
      "240/295, train_loss: 0.0541, step time: 1.1102\n",
      "241/295, train_loss: 0.1474, step time: 1.0566\n",
      "242/295, train_loss: 0.1896, step time: 1.0347\n",
      "243/295, train_loss: 0.1019, step time: 1.0497\n",
      "244/295, train_loss: 0.0780, step time: 1.0398\n",
      "245/295, train_loss: 0.1159, step time: 1.0561\n",
      "246/295, train_loss: 0.0869, step time: 1.0605\n",
      "247/295, train_loss: 0.0508, step time: 1.0422\n",
      "248/295, train_loss: 0.0457, step time: 1.1232\n",
      "249/295, train_loss: 0.0499, step time: 1.0348\n",
      "250/295, train_loss: 0.0665, step time: 1.0394\n",
      "251/295, train_loss: 0.0725, step time: 1.1478\n",
      "252/295, train_loss: 0.3942, step time: 1.0378\n",
      "253/295, train_loss: 0.1028, step time: 1.0569\n",
      "254/295, train_loss: 0.0509, step time: 1.0369\n",
      "255/295, train_loss: 0.1436, step time: 1.0357\n",
      "256/295, train_loss: 0.0883, step time: 1.0377\n",
      "257/295, train_loss: 0.0661, step time: 1.0418\n",
      "258/295, train_loss: 0.0796, step time: 1.0358\n",
      "259/295, train_loss: 0.0933, step time: 1.0754\n",
      "260/295, train_loss: 0.0944, step time: 1.0357\n",
      "261/295, train_loss: 0.0817, step time: 1.0334\n",
      "262/295, train_loss: 0.1867, step time: 1.0442\n",
      "263/295, train_loss: 0.0731, step time: 1.0510\n",
      "264/295, train_loss: 0.0696, step time: 1.0479\n",
      "265/295, train_loss: 0.0434, step time: 1.1073\n",
      "266/295, train_loss: 0.0381, step time: 1.0363\n",
      "267/295, train_loss: 0.3936, step time: 1.0593\n",
      "268/295, train_loss: 0.1058, step time: 1.0508\n",
      "269/295, train_loss: 0.0581, step time: 1.0387\n",
      "270/295, train_loss: 0.0546, step time: 1.0371\n",
      "271/295, train_loss: 0.0360, step time: 1.0492\n",
      "272/295, train_loss: 0.0593, step time: 1.0551\n",
      "273/295, train_loss: 0.0544, step time: 1.0542\n",
      "274/295, train_loss: 0.0664, step time: 1.0373\n",
      "275/295, train_loss: 0.0477, step time: 1.0347\n",
      "276/295, train_loss: 0.0406, step time: 1.0712\n",
      "277/295, train_loss: 0.1292, step time: 1.0418\n",
      "278/295, train_loss: 0.1112, step time: 1.0433\n",
      "279/295, train_loss: 0.0304, step time: 1.0329\n",
      "280/295, train_loss: 0.0833, step time: 1.0373\n",
      "281/295, train_loss: 0.0507, step time: 1.0375\n",
      "282/295, train_loss: 0.0414, step time: 1.0982\n",
      "283/295, train_loss: 0.0520, step time: 1.0623\n",
      "284/295, train_loss: 0.3907, step time: 1.0363\n",
      "285/295, train_loss: 0.4009, step time: 1.0350\n",
      "286/295, train_loss: 0.0344, step time: 1.0396\n",
      "287/295, train_loss: 0.1063, step time: 1.0466\n",
      "288/295, train_loss: 0.0675, step time: 1.0463\n",
      "289/295, train_loss: 0.0818, step time: 1.0384\n",
      "290/295, train_loss: 0.1188, step time: 1.0294\n",
      "291/295, train_loss: 0.0953, step time: 1.0292\n",
      "292/295, train_loss: 0.0347, step time: 1.0289\n",
      "293/295, train_loss: 0.0367, step time: 1.0295\n",
      "294/295, train_loss: 0.0545, step time: 1.0294\n",
      "295/295, train_loss: 0.0389, step time: 1.0289\n",
      "epoch 54 average loss: 0.0976\n",
      "current epoch: 54 current mean dice: 0.7850 tc: 0.7455 wt: 0.8498 et: 0.7603\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 54 is: 392.3889\n",
      "----------\n",
      "epoch 55/100\n",
      "1/295, train_loss: 0.3926, step time: 1.1645\n",
      "2/295, train_loss: 0.0384, step time: 1.0533\n",
      "3/295, train_loss: 0.1521, step time: 1.0575\n",
      "4/295, train_loss: 0.0282, step time: 1.0980\n",
      "5/295, train_loss: 0.0532, step time: 1.0922\n",
      "6/295, train_loss: 0.0590, step time: 1.0438\n",
      "7/295, train_loss: 0.0958, step time: 1.0411\n",
      "8/295, train_loss: 0.0881, step time: 1.0424\n",
      "9/295, train_loss: 0.0467, step time: 1.0302\n",
      "10/295, train_loss: 0.0985, step time: 1.0401\n",
      "11/295, train_loss: 0.0877, step time: 1.0683\n",
      "12/295, train_loss: 0.0957, step time: 1.0634\n",
      "13/295, train_loss: 0.0407, step time: 1.0416\n",
      "14/295, train_loss: 0.0956, step time: 1.0371\n",
      "15/295, train_loss: 0.0401, step time: 1.0350\n",
      "16/295, train_loss: 0.0447, step time: 1.0581\n",
      "17/295, train_loss: 0.0340, step time: 1.1255\n",
      "18/295, train_loss: 0.0489, step time: 1.0429\n",
      "19/295, train_loss: 0.0622, step time: 1.0314\n",
      "20/295, train_loss: 0.0695, step time: 1.0514\n",
      "21/295, train_loss: 0.0360, step time: 1.0315\n",
      "22/295, train_loss: 0.0533, step time: 1.0605\n",
      "23/295, train_loss: 0.0688, step time: 1.0345\n",
      "24/295, train_loss: 0.0938, step time: 1.1053\n",
      "25/295, train_loss: 0.0369, step time: 1.0392\n",
      "26/295, train_loss: 0.0420, step time: 1.0344\n",
      "27/295, train_loss: 0.3900, step time: 1.0388\n",
      "28/295, train_loss: 0.0271, step time: 1.0381\n",
      "29/295, train_loss: 0.3929, step time: 1.0613\n",
      "30/295, train_loss: 0.3853, step time: 1.0632\n",
      "31/295, train_loss: 0.0940, step time: 1.0296\n",
      "32/295, train_loss: 0.0964, step time: 1.0292\n",
      "33/295, train_loss: 0.0498, step time: 1.0364\n",
      "34/295, train_loss: 0.3663, step time: 1.0305\n",
      "35/295, train_loss: 0.0777, step time: 1.1361\n",
      "36/295, train_loss: 0.0891, step time: 1.0457\n",
      "37/295, train_loss: 0.2395, step time: 1.0935\n",
      "38/295, train_loss: 0.0477, step time: 1.0624\n",
      "39/295, train_loss: 0.0461, step time: 1.0505\n",
      "40/295, train_loss: 0.0305, step time: 1.0323\n",
      "41/295, train_loss: 0.0735, step time: 1.0464\n",
      "42/295, train_loss: 0.0636, step time: 1.0606\n",
      "43/295, train_loss: 0.0768, step time: 1.0510\n",
      "44/295, train_loss: 0.3885, step time: 1.0309\n",
      "45/295, train_loss: 0.0802, step time: 1.0360\n",
      "46/295, train_loss: 0.3921, step time: 1.0396\n",
      "47/295, train_loss: 0.0563, step time: 1.0628\n",
      "48/295, train_loss: 0.0731, step time: 1.0314\n",
      "49/295, train_loss: 0.0642, step time: 1.0314\n",
      "50/295, train_loss: 0.0511, step time: 1.0320\n",
      "51/295, train_loss: 0.0792, step time: 1.0422\n",
      "52/295, train_loss: 0.1138, step time: 1.0338\n",
      "53/295, train_loss: 0.0721, step time: 1.0386\n",
      "54/295, train_loss: 0.1549, step time: 1.0720\n",
      "55/295, train_loss: 0.3980, step time: 1.0490\n",
      "56/295, train_loss: 0.0496, step time: 1.0550\n",
      "57/295, train_loss: 0.0579, step time: 1.0536\n",
      "58/295, train_loss: 0.1156, step time: 1.0589\n",
      "59/295, train_loss: 0.0832, step time: 1.0413\n",
      "60/295, train_loss: 0.0589, step time: 1.0544\n",
      "61/295, train_loss: 0.0490, step time: 1.0590\n",
      "62/295, train_loss: 0.0856, step time: 1.0324\n",
      "63/295, train_loss: 0.0421, step time: 1.0363\n",
      "64/295, train_loss: 0.4109, step time: 1.0648\n",
      "65/295, train_loss: 0.0370, step time: 1.0430\n",
      "66/295, train_loss: 0.0967, step time: 1.0394\n",
      "67/295, train_loss: 0.0993, step time: 1.0508\n",
      "68/295, train_loss: 0.0684, step time: 1.1585\n",
      "69/295, train_loss: 0.0691, step time: 1.0621\n",
      "70/295, train_loss: 0.0512, step time: 1.0903\n",
      "71/295, train_loss: 0.0431, step time: 1.0405\n",
      "72/295, train_loss: 0.0568, step time: 1.0999\n",
      "73/295, train_loss: 0.0563, step time: 1.0374\n",
      "74/295, train_loss: 0.1283, step time: 1.0756\n",
      "75/295, train_loss: 0.0541, step time: 1.0331\n",
      "76/295, train_loss: 0.0367, step time: 1.0354\n",
      "77/295, train_loss: 0.0621, step time: 1.0666\n",
      "78/295, train_loss: 0.0721, step time: 1.0452\n",
      "79/295, train_loss: 0.0321, step time: 1.0882\n",
      "80/295, train_loss: 0.0651, step time: 1.0306\n",
      "81/295, train_loss: 0.0663, step time: 1.0317\n",
      "82/295, train_loss: 0.0302, step time: 1.0594\n",
      "83/295, train_loss: 0.3676, step time: 1.0524\n",
      "84/295, train_loss: 0.0454, step time: 1.0417\n",
      "85/295, train_loss: 0.0492, step time: 1.0611\n",
      "86/295, train_loss: 0.0588, step time: 1.0653\n",
      "87/295, train_loss: 0.0301, step time: 1.0306\n",
      "88/295, train_loss: 0.0437, step time: 1.0309\n",
      "89/295, train_loss: 0.0952, step time: 1.0351\n",
      "90/295, train_loss: 0.0499, step time: 1.1376\n",
      "91/295, train_loss: 0.0611, step time: 1.1335\n",
      "92/295, train_loss: 0.0549, step time: 1.0405\n",
      "93/295, train_loss: 0.4710, step time: 1.0515\n",
      "94/295, train_loss: 0.0977, step time: 1.0373\n",
      "95/295, train_loss: 0.0830, step time: 1.0407\n",
      "96/295, train_loss: 0.1031, step time: 1.0305\n",
      "97/295, train_loss: 0.0896, step time: 1.0349\n",
      "98/295, train_loss: 0.3593, step time: 1.0342\n",
      "99/295, train_loss: 0.1438, step time: 1.0396\n",
      "100/295, train_loss: 0.3703, step time: 1.0619\n",
      "101/295, train_loss: 0.0755, step time: 1.0517\n",
      "102/295, train_loss: 0.0691, step time: 1.0489\n",
      "103/295, train_loss: 0.0878, step time: 1.0523\n",
      "104/295, train_loss: 0.0883, step time: 1.0330\n",
      "105/295, train_loss: 0.0368, step time: 1.0314\n",
      "106/295, train_loss: 0.1153, step time: 1.0565\n",
      "107/295, train_loss: 0.0340, step time: 1.0369\n",
      "108/295, train_loss: 0.0538, step time: 1.0372\n",
      "109/295, train_loss: 0.0361, step time: 1.0438\n",
      "110/295, train_loss: 0.1016, step time: 1.0530\n",
      "111/295, train_loss: 0.3741, step time: 1.0334\n",
      "112/295, train_loss: 0.0548, step time: 1.0632\n",
      "113/295, train_loss: 0.0907, step time: 1.0402\n",
      "114/295, train_loss: 0.0396, step time: 1.0632\n",
      "115/295, train_loss: 0.0356, step time: 1.0512\n",
      "116/295, train_loss: 0.0480, step time: 1.0318\n",
      "117/295, train_loss: 0.0574, step time: 1.0447\n",
      "118/295, train_loss: 0.0660, step time: 1.0565\n",
      "119/295, train_loss: 0.1484, step time: 1.0343\n",
      "120/295, train_loss: 0.0291, step time: 1.0488\n",
      "121/295, train_loss: 0.0933, step time: 1.0498\n",
      "122/295, train_loss: 0.0429, step time: 1.0455\n",
      "123/295, train_loss: 0.0762, step time: 1.0512\n",
      "124/295, train_loss: 0.0647, step time: 1.0450\n",
      "125/295, train_loss: 0.0353, step time: 1.0337\n",
      "126/295, train_loss: 0.1613, step time: 1.0605\n",
      "127/295, train_loss: 0.0210, step time: 1.0410\n",
      "128/295, train_loss: 0.1228, step time: 1.0338\n",
      "129/295, train_loss: 0.0617, step time: 1.0373\n",
      "130/295, train_loss: 0.0751, step time: 1.0612\n",
      "131/295, train_loss: 0.1361, step time: 1.0322\n",
      "132/295, train_loss: 0.0908, step time: 1.0376\n",
      "133/295, train_loss: 0.1217, step time: 1.0726\n",
      "134/295, train_loss: 0.0423, step time: 1.0812\n",
      "135/295, train_loss: 0.0581, step time: 1.0363\n",
      "136/295, train_loss: 0.0565, step time: 1.0360\n",
      "137/295, train_loss: 0.0577, step time: 1.0975\n",
      "138/295, train_loss: 0.0354, step time: 1.0480\n",
      "139/295, train_loss: 0.1086, step time: 1.0719\n",
      "140/295, train_loss: 0.0496, step time: 1.0447\n",
      "141/295, train_loss: 0.0894, step time: 1.0446\n",
      "142/295, train_loss: 0.0353, step time: 1.0575\n",
      "143/295, train_loss: 0.0653, step time: 1.0755\n",
      "144/295, train_loss: 0.0505, step time: 1.0559\n",
      "145/295, train_loss: 0.0345, step time: 1.0898\n",
      "146/295, train_loss: 0.0364, step time: 1.0710\n",
      "147/295, train_loss: 0.0571, step time: 1.0398\n",
      "148/295, train_loss: 0.0396, step time: 1.0447\n",
      "149/295, train_loss: 0.0311, step time: 1.0692\n",
      "150/295, train_loss: 0.0953, step time: 1.0791\n",
      "151/295, train_loss: 0.0782, step time: 1.0302\n",
      "152/295, train_loss: 0.0351, step time: 1.0396\n",
      "153/295, train_loss: 0.1232, step time: 1.0483\n",
      "154/295, train_loss: 0.0294, step time: 1.0320\n",
      "155/295, train_loss: 0.0436, step time: 1.0729\n",
      "156/295, train_loss: 0.0454, step time: 1.0362\n",
      "157/295, train_loss: 0.0884, step time: 1.0342\n",
      "158/295, train_loss: 0.0855, step time: 1.1073\n",
      "159/295, train_loss: 0.1068, step time: 1.0312\n",
      "160/295, train_loss: 0.1242, step time: 1.0718\n",
      "161/295, train_loss: 0.1052, step time: 1.0326\n",
      "162/295, train_loss: 0.0484, step time: 1.0428\n",
      "163/295, train_loss: 0.3699, step time: 1.0359\n",
      "164/295, train_loss: 0.1212, step time: 1.0407\n",
      "165/295, train_loss: 0.0737, step time: 1.0972\n",
      "166/295, train_loss: 0.0618, step time: 1.0320\n",
      "167/295, train_loss: 0.0521, step time: 1.0325\n",
      "168/295, train_loss: 0.0732, step time: 1.0962\n",
      "169/295, train_loss: 0.0920, step time: 1.0391\n",
      "170/295, train_loss: 0.0851, step time: 1.0590\n",
      "171/295, train_loss: 0.0738, step time: 1.0395\n",
      "172/295, train_loss: 0.0603, step time: 1.0385\n",
      "173/295, train_loss: 0.0883, step time: 1.0350\n",
      "174/295, train_loss: 0.0983, step time: 1.0604\n",
      "175/295, train_loss: 0.0388, step time: 1.0742\n",
      "176/295, train_loss: 0.0327, step time: 1.0523\n",
      "177/295, train_loss: 0.1528, step time: 1.0487\n",
      "178/295, train_loss: 0.0483, step time: 1.0706\n",
      "179/295, train_loss: 0.0665, step time: 1.0352\n",
      "180/295, train_loss: 0.0337, step time: 1.0485\n",
      "181/295, train_loss: 0.1176, step time: 1.0587\n",
      "182/295, train_loss: 0.0914, step time: 1.0449\n",
      "183/295, train_loss: 0.0801, step time: 1.0458\n",
      "184/295, train_loss: 0.0893, step time: 1.0433\n",
      "185/295, train_loss: 0.0759, step time: 1.0638\n",
      "186/295, train_loss: 0.0420, step time: 1.0712\n",
      "187/295, train_loss: 0.0779, step time: 1.0343\n",
      "188/295, train_loss: 0.0664, step time: 1.0486\n",
      "189/295, train_loss: 0.0803, step time: 1.0522\n",
      "190/295, train_loss: 0.0753, step time: 1.0387\n",
      "191/295, train_loss: 0.0604, step time: 1.0311\n",
      "192/295, train_loss: 0.0523, step time: 1.0590\n",
      "193/295, train_loss: 0.1429, step time: 1.1285\n",
      "194/295, train_loss: 0.3832, step time: 1.0342\n",
      "195/295, train_loss: 0.1882, step time: 1.0455\n",
      "196/295, train_loss: 0.0731, step time: 1.0333\n",
      "197/295, train_loss: 0.0838, step time: 1.0392\n",
      "198/295, train_loss: 0.0600, step time: 1.0359\n",
      "199/295, train_loss: 0.0916, step time: 1.0501\n",
      "200/295, train_loss: 0.1185, step time: 1.0648\n",
      "201/295, train_loss: 0.0373, step time: 1.0441\n",
      "202/295, train_loss: 0.0472, step time: 1.0362\n",
      "203/295, train_loss: 0.0624, step time: 1.0363\n",
      "204/295, train_loss: 0.0544, step time: 1.0633\n",
      "205/295, train_loss: 0.0476, step time: 1.0820\n",
      "206/295, train_loss: 0.0544, step time: 1.0462\n",
      "207/295, train_loss: 0.2726, step time: 1.0684\n",
      "208/295, train_loss: 0.0965, step time: 1.0432\n",
      "209/295, train_loss: 0.1229, step time: 1.0363\n",
      "210/295, train_loss: 0.1091, step time: 1.0496\n",
      "211/295, train_loss: 0.3871, step time: 1.0390\n",
      "212/295, train_loss: 0.1174, step time: 1.0506\n",
      "213/295, train_loss: 0.0676, step time: 1.0411\n",
      "214/295, train_loss: 0.0753, step time: 1.0376\n",
      "215/295, train_loss: 0.1143, step time: 1.0313\n",
      "216/295, train_loss: 0.0412, step time: 1.0319\n",
      "217/295, train_loss: 0.0597, step time: 1.0662\n",
      "218/295, train_loss: 0.3668, step time: 1.0648\n",
      "219/295, train_loss: 0.0716, step time: 1.0422\n",
      "220/295, train_loss: 0.0882, step time: 1.0427\n",
      "221/295, train_loss: 0.0722, step time: 1.0374\n",
      "222/295, train_loss: 0.0383, step time: 1.0353\n",
      "223/295, train_loss: 0.0889, step time: 1.0411\n",
      "224/295, train_loss: 0.0387, step time: 1.0517\n",
      "225/295, train_loss: 0.0737, step time: 1.0571\n",
      "226/295, train_loss: 0.0371, step time: 1.0347\n",
      "227/295, train_loss: 0.1843, step time: 1.0434\n",
      "228/295, train_loss: 0.0505, step time: 1.0445\n",
      "229/295, train_loss: 0.0879, step time: 1.0621\n",
      "230/295, train_loss: 0.0423, step time: 1.0525\n",
      "231/295, train_loss: 0.1195, step time: 1.0613\n",
      "232/295, train_loss: 0.4081, step time: 1.0453\n",
      "233/295, train_loss: 0.0901, step time: 1.0669\n",
      "234/295, train_loss: 0.0377, step time: 1.0346\n",
      "235/295, train_loss: 0.0502, step time: 1.0477\n",
      "236/295, train_loss: 0.0797, step time: 1.0614\n",
      "237/295, train_loss: 0.0643, step time: 1.0420\n",
      "238/295, train_loss: 0.0601, step time: 1.0410\n",
      "239/295, train_loss: 0.0601, step time: 1.0550\n",
      "240/295, train_loss: 0.0940, step time: 1.0337\n",
      "241/295, train_loss: 0.0474, step time: 1.0342\n",
      "242/295, train_loss: 0.0785, step time: 1.0405\n",
      "243/295, train_loss: 0.0303, step time: 1.0621\n",
      "244/295, train_loss: 0.0455, step time: 1.0493\n",
      "245/295, train_loss: 0.0674, step time: 1.0395\n",
      "246/295, train_loss: 0.0540, step time: 1.0771\n",
      "247/295, train_loss: 0.1371, step time: 1.0526\n",
      "248/295, train_loss: 0.0704, step time: 1.0565\n",
      "249/295, train_loss: 0.1483, step time: 1.0582\n",
      "250/295, train_loss: 0.0583, step time: 1.0497\n",
      "251/295, train_loss: 0.0523, step time: 1.0424\n",
      "252/295, train_loss: 0.0591, step time: 1.0341\n",
      "253/295, train_loss: 0.1139, step time: 1.0770\n",
      "254/295, train_loss: 0.0690, step time: 1.0412\n",
      "255/295, train_loss: 0.0492, step time: 1.0467\n",
      "256/295, train_loss: 0.0895, step time: 1.0430\n",
      "257/295, train_loss: 0.0427, step time: 1.0439\n",
      "258/295, train_loss: 0.0661, step time: 1.0458\n",
      "259/295, train_loss: 0.0374, step time: 1.0843\n",
      "260/295, train_loss: 0.0824, step time: 1.0539\n",
      "261/295, train_loss: 0.0755, step time: 1.0360\n",
      "262/295, train_loss: 0.0877, step time: 1.0581\n",
      "263/295, train_loss: 0.1033, step time: 1.0384\n",
      "264/295, train_loss: 0.0680, step time: 1.0355\n",
      "265/295, train_loss: 0.0404, step time: 1.0471\n",
      "266/295, train_loss: 0.0290, step time: 1.0487\n",
      "267/295, train_loss: 0.3883, step time: 1.0821\n",
      "268/295, train_loss: 0.0332, step time: 1.0322\n",
      "269/295, train_loss: 0.3996, step time: 1.0346\n",
      "270/295, train_loss: 0.0414, step time: 1.0321\n",
      "271/295, train_loss: 0.0555, step time: 1.0380\n",
      "272/295, train_loss: 0.0734, step time: 1.0624\n",
      "273/295, train_loss: 0.0698, step time: 1.0486\n",
      "274/295, train_loss: 0.3778, step time: 1.0786\n",
      "275/295, train_loss: 0.0698, step time: 1.0445\n",
      "276/295, train_loss: 0.0291, step time: 1.0455\n",
      "277/295, train_loss: 0.0557, step time: 1.0783\n",
      "278/295, train_loss: 0.0593, step time: 1.0512\n",
      "279/295, train_loss: 0.0321, step time: 1.0381\n",
      "280/295, train_loss: 0.1036, step time: 1.0336\n",
      "281/295, train_loss: 0.1853, step time: 1.0507\n",
      "282/295, train_loss: 0.0781, step time: 1.0415\n",
      "283/295, train_loss: 0.0281, step time: 1.0356\n",
      "284/295, train_loss: 0.0467, step time: 1.0340\n",
      "285/295, train_loss: 0.0857, step time: 1.1409\n",
      "286/295, train_loss: 0.0908, step time: 1.0913\n",
      "287/295, train_loss: 0.0632, step time: 1.0360\n",
      "288/295, train_loss: 0.0337, step time: 1.0349\n",
      "289/295, train_loss: 0.0445, step time: 1.0288\n",
      "290/295, train_loss: 0.1207, step time: 1.0289\n",
      "291/295, train_loss: 0.0408, step time: 1.0291\n",
      "292/295, train_loss: 0.1128, step time: 1.0283\n",
      "293/295, train_loss: 0.1002, step time: 1.0300\n",
      "294/295, train_loss: 0.0876, step time: 1.0282\n",
      "295/295, train_loss: 0.0631, step time: 1.0289\n",
      "epoch 55 average loss: 0.0957\n",
      "current epoch: 55 current mean dice: 0.7912 tc: 0.7502 wt: 0.8590 et: 0.7717\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 55 is: 381.3503\n",
      "----------\n",
      "epoch 56/100\n",
      "1/295, train_loss: 0.0960, step time: 1.1354\n",
      "2/295, train_loss: 0.0762, step time: 1.1027\n",
      "3/295, train_loss: 0.0550, step time: 1.0555\n",
      "4/295, train_loss: 0.0441, step time: 1.0650\n",
      "5/295, train_loss: 0.0556, step time: 1.0541\n",
      "6/295, train_loss: 0.1221, step time: 1.0568\n",
      "7/295, train_loss: 0.3797, step time: 1.0437\n",
      "8/295, train_loss: 0.0621, step time: 1.0683\n",
      "9/295, train_loss: 0.0670, step time: 1.0497\n",
      "10/295, train_loss: 0.0561, step time: 1.0327\n",
      "11/295, train_loss: 0.0516, step time: 1.0380\n",
      "12/295, train_loss: 0.0452, step time: 1.0783\n",
      "13/295, train_loss: 0.0730, step time: 1.1217\n",
      "14/295, train_loss: 0.0989, step time: 1.0391\n",
      "15/295, train_loss: 0.0605, step time: 1.0326\n",
      "16/295, train_loss: 0.0415, step time: 1.0347\n",
      "17/295, train_loss: 0.1129, step time: 1.0507\n",
      "18/295, train_loss: 0.4095, step time: 1.1236\n",
      "19/295, train_loss: 0.0575, step time: 1.0414\n",
      "20/295, train_loss: 0.0552, step time: 1.0406\n",
      "21/295, train_loss: 0.0685, step time: 1.0373\n",
      "22/295, train_loss: 0.3895, step time: 1.0657\n",
      "23/295, train_loss: 0.0964, step time: 1.0711\n",
      "24/295, train_loss: 0.0414, step time: 1.0294\n",
      "25/295, train_loss: 0.3900, step time: 1.0483\n",
      "26/295, train_loss: 0.1492, step time: 1.0300\n",
      "27/295, train_loss: 0.0310, step time: 1.0583\n",
      "28/295, train_loss: 0.0635, step time: 1.1337\n",
      "29/295, train_loss: 0.1097, step time: 1.0416\n",
      "30/295, train_loss: 0.0663, step time: 1.0346\n",
      "31/295, train_loss: 0.0391, step time: 1.0978\n",
      "32/295, train_loss: 0.1045, step time: 1.0459\n",
      "33/295, train_loss: 0.0763, step time: 1.0313\n",
      "34/295, train_loss: 0.0422, step time: 1.0372\n",
      "35/295, train_loss: 0.0606, step time: 1.0375\n",
      "36/295, train_loss: 0.0725, step time: 1.0573\n",
      "37/295, train_loss: 0.0801, step time: 1.0463\n",
      "38/295, train_loss: 0.0992, step time: 1.0333\n",
      "39/295, train_loss: 0.0695, step time: 1.0476\n",
      "40/295, train_loss: 0.0299, step time: 1.0365\n",
      "41/295, train_loss: 0.0747, step time: 1.0405\n",
      "42/295, train_loss: 0.1117, step time: 1.0370\n",
      "43/295, train_loss: 0.0869, step time: 1.1084\n",
      "44/295, train_loss: 0.0571, step time: 1.0327\n",
      "45/295, train_loss: 0.0634, step time: 1.0376\n",
      "46/295, train_loss: 0.4129, step time: 1.0330\n",
      "47/295, train_loss: 0.0919, step time: 1.0335\n",
      "48/295, train_loss: 0.0972, step time: 1.1176\n",
      "49/295, train_loss: 0.0371, step time: 1.0825\n",
      "50/295, train_loss: 0.0764, step time: 1.0509\n",
      "51/295, train_loss: 0.0554, step time: 1.0670\n",
      "52/295, train_loss: 0.0738, step time: 1.0652\n",
      "53/295, train_loss: 0.0476, step time: 1.0328\n",
      "54/295, train_loss: 0.0283, step time: 1.0332\n",
      "55/295, train_loss: 0.0977, step time: 1.0554\n",
      "56/295, train_loss: 0.3611, step time: 1.0376\n",
      "57/295, train_loss: 0.0665, step time: 1.0631\n",
      "58/295, train_loss: 0.0965, step time: 1.0844\n",
      "59/295, train_loss: 0.0798, step time: 1.0362\n",
      "60/295, train_loss: 0.0351, step time: 1.0505\n",
      "61/295, train_loss: 0.1361, step time: 1.0324\n",
      "62/295, train_loss: 0.1129, step time: 1.0384\n",
      "63/295, train_loss: 0.0634, step time: 1.0375\n",
      "64/295, train_loss: 0.0538, step time: 1.0443\n",
      "65/295, train_loss: 0.0889, step time: 1.0371\n",
      "66/295, train_loss: 0.0591, step time: 1.0454\n",
      "67/295, train_loss: 0.1186, step time: 1.0870\n",
      "68/295, train_loss: 0.0360, step time: 1.0387\n",
      "69/295, train_loss: 0.4125, step time: 1.0466\n",
      "70/295, train_loss: 0.0398, step time: 1.0354\n",
      "71/295, train_loss: 0.0386, step time: 1.0470\n",
      "72/295, train_loss: 0.0329, step time: 1.0638\n",
      "73/295, train_loss: 0.1261, step time: 1.0618\n",
      "74/295, train_loss: 0.0966, step time: 1.0782\n",
      "75/295, train_loss: 0.0288, step time: 1.1091\n",
      "76/295, train_loss: 0.0462, step time: 1.0343\n",
      "77/295, train_loss: 0.0686, step time: 1.0402\n",
      "78/295, train_loss: 0.0620, step time: 1.0627\n",
      "79/295, train_loss: 0.0637, step time: 1.1408\n",
      "80/295, train_loss: 0.1182, step time: 1.0375\n",
      "81/295, train_loss: 0.0685, step time: 1.0505\n",
      "82/295, train_loss: 0.0840, step time: 1.0736\n",
      "83/295, train_loss: 0.0271, step time: 1.0455\n",
      "84/295, train_loss: 0.0872, step time: 1.0376\n",
      "85/295, train_loss: 0.0750, step time: 1.0458\n",
      "86/295, train_loss: 0.0353, step time: 1.0441\n",
      "87/295, train_loss: 0.0586, step time: 1.0959\n",
      "88/295, train_loss: 0.0279, step time: 1.0421\n",
      "89/295, train_loss: 0.1188, step time: 1.0624\n",
      "90/295, train_loss: 0.0928, step time: 1.0540\n",
      "91/295, train_loss: 0.0565, step time: 1.0352\n",
      "92/295, train_loss: 0.0841, step time: 1.0357\n",
      "93/295, train_loss: 0.0582, step time: 1.0384\n",
      "94/295, train_loss: 0.0382, step time: 1.0420\n",
      "95/295, train_loss: 0.0634, step time: 1.0566\n",
      "96/295, train_loss: 0.1223, step time: 1.0311\n",
      "97/295, train_loss: 0.0904, step time: 1.0604\n",
      "98/295, train_loss: 0.4341, step time: 1.0594\n",
      "99/295, train_loss: 0.1457, step time: 1.0340\n",
      "100/295, train_loss: 0.0586, step time: 1.0345\n",
      "101/295, train_loss: 0.3912, step time: 1.0321\n",
      "102/295, train_loss: 0.0307, step time: 1.0343\n",
      "103/295, train_loss: 0.0397, step time: 1.0382\n",
      "104/295, train_loss: 0.3720, step time: 1.0434\n",
      "105/295, train_loss: 0.2413, step time: 1.0492\n",
      "106/295, train_loss: 0.0440, step time: 1.0335\n",
      "107/295, train_loss: 0.0692, step time: 1.0411\n",
      "108/295, train_loss: 0.1547, step time: 1.0472\n",
      "109/295, train_loss: 0.0568, step time: 1.0556\n",
      "110/295, train_loss: 0.0518, step time: 1.0730\n",
      "111/295, train_loss: 0.1279, step time: 1.0339\n",
      "112/295, train_loss: 0.0562, step time: 1.0555\n",
      "113/295, train_loss: 0.0537, step time: 1.0390\n",
      "114/295, train_loss: 0.0538, step time: 1.1065\n",
      "115/295, train_loss: 0.0482, step time: 1.0400\n",
      "116/295, train_loss: 0.0806, step time: 1.0374\n",
      "117/295, train_loss: 0.0739, step time: 1.0475\n",
      "118/295, train_loss: 0.1549, step time: 1.0817\n",
      "119/295, train_loss: 0.0683, step time: 1.0511\n",
      "120/295, train_loss: 0.3941, step time: 1.0583\n",
      "121/295, train_loss: 0.0918, step time: 1.0558\n",
      "122/295, train_loss: 0.0822, step time: 1.0403\n",
      "123/295, train_loss: 0.0403, step time: 1.0525\n",
      "124/295, train_loss: 0.0368, step time: 1.0353\n",
      "125/295, train_loss: 0.0604, step time: 1.0406\n",
      "126/295, train_loss: 0.4094, step time: 1.0340\n",
      "127/295, train_loss: 0.0972, step time: 1.0636\n",
      "128/295, train_loss: 0.0461, step time: 1.0511\n",
      "129/295, train_loss: 0.0779, step time: 1.0370\n",
      "130/295, train_loss: 0.0878, step time: 1.0333\n",
      "131/295, train_loss: 0.0849, step time: 1.0536\n",
      "132/295, train_loss: 0.0945, step time: 1.0604\n",
      "133/295, train_loss: 0.0886, step time: 1.0309\n",
      "134/295, train_loss: 0.0708, step time: 1.0569\n",
      "135/295, train_loss: 0.1106, step time: 1.0367\n",
      "136/295, train_loss: 0.0466, step time: 1.0358\n",
      "137/295, train_loss: 0.3857, step time: 1.0583\n",
      "138/295, train_loss: 0.3686, step time: 1.0395\n",
      "139/295, train_loss: 0.0552, step time: 1.0362\n",
      "140/295, train_loss: 0.1813, step time: 1.0369\n",
      "141/295, train_loss: 0.0351, step time: 1.0572\n",
      "142/295, train_loss: 0.0508, step time: 1.0410\n",
      "143/295, train_loss: 0.1335, step time: 1.0789\n",
      "144/295, train_loss: 0.0941, step time: 1.0889\n",
      "145/295, train_loss: 0.0925, step time: 1.0762\n",
      "146/295, train_loss: 0.1117, step time: 1.0605\n",
      "147/295, train_loss: 0.0370, step time: 1.0649\n",
      "148/295, train_loss: 0.0958, step time: 1.0391\n",
      "149/295, train_loss: 0.0458, step time: 1.0667\n",
      "150/295, train_loss: 0.0888, step time: 1.0442\n",
      "151/295, train_loss: 0.1264, step time: 1.0578\n",
      "152/295, train_loss: 0.0488, step time: 1.0444\n",
      "153/295, train_loss: 0.0752, step time: 1.1105\n",
      "154/295, train_loss: 0.0828, step time: 1.0296\n",
      "155/295, train_loss: 0.0351, step time: 1.0314\n",
      "156/295, train_loss: 0.0270, step time: 1.0395\n",
      "157/295, train_loss: 0.0398, step time: 1.0791\n",
      "158/295, train_loss: 0.0531, step time: 1.0350\n",
      "159/295, train_loss: 0.0789, step time: 1.0361\n",
      "160/295, train_loss: 0.0652, step time: 1.0484\n",
      "161/295, train_loss: 0.2264, step time: 1.0802\n",
      "162/295, train_loss: 0.0446, step time: 1.0345\n",
      "163/295, train_loss: 0.0974, step time: 1.0544\n",
      "164/295, train_loss: 0.0924, step time: 1.0884\n",
      "165/295, train_loss: 0.1048, step time: 1.0638\n",
      "166/295, train_loss: 0.0374, step time: 1.0345\n",
      "167/295, train_loss: 0.0512, step time: 1.0764\n",
      "168/295, train_loss: 0.0847, step time: 1.0912\n",
      "169/295, train_loss: 0.0459, step time: 1.0318\n",
      "170/295, train_loss: 0.0356, step time: 1.0346\n",
      "171/295, train_loss: 0.1249, step time: 1.0467\n",
      "172/295, train_loss: 0.0485, step time: 1.0496\n",
      "173/295, train_loss: 0.0583, step time: 1.0342\n",
      "174/295, train_loss: 0.0459, step time: 1.0425\n",
      "175/295, train_loss: 0.0737, step time: 1.0379\n",
      "176/295, train_loss: 0.4650, step time: 1.0524\n",
      "177/295, train_loss: 0.0748, step time: 1.0325\n",
      "178/295, train_loss: 0.0467, step time: 1.0628\n",
      "179/295, train_loss: 0.0775, step time: 1.0415\n",
      "180/295, train_loss: 0.0568, step time: 1.0660\n",
      "181/295, train_loss: 0.0366, step time: 1.0382\n",
      "182/295, train_loss: 0.0737, step time: 1.0344\n",
      "183/295, train_loss: 0.0726, step time: 1.0360\n",
      "184/295, train_loss: 0.0435, step time: 1.0399\n",
      "185/295, train_loss: 0.0361, step time: 1.0338\n",
      "186/295, train_loss: 0.0287, step time: 1.0440\n",
      "187/295, train_loss: 0.0356, step time: 1.0525\n",
      "188/295, train_loss: 0.0267, step time: 1.0331\n",
      "189/295, train_loss: 0.1436, step time: 1.0412\n",
      "190/295, train_loss: 0.0334, step time: 1.0414\n",
      "191/295, train_loss: 0.0657, step time: 1.0338\n",
      "192/295, train_loss: 0.0947, step time: 1.0639\n",
      "193/295, train_loss: 0.0993, step time: 1.0490\n",
      "194/295, train_loss: 0.0492, step time: 1.0362\n",
      "195/295, train_loss: 0.0625, step time: 1.0412\n",
      "196/295, train_loss: 0.0879, step time: 1.0465\n",
      "197/295, train_loss: 0.0513, step time: 1.0402\n",
      "198/295, train_loss: 0.0410, step time: 1.0708\n",
      "199/295, train_loss: 0.0491, step time: 1.0420\n",
      "200/295, train_loss: 0.0915, step time: 1.0455\n",
      "201/295, train_loss: 0.1459, step time: 1.0409\n",
      "202/295, train_loss: 0.0394, step time: 1.0316\n",
      "203/295, train_loss: 0.0515, step time: 1.0545\n",
      "204/295, train_loss: 0.0978, step time: 1.0476\n",
      "205/295, train_loss: 0.0456, step time: 1.0710\n",
      "206/295, train_loss: 0.0614, step time: 1.0612\n",
      "207/295, train_loss: 0.1046, step time: 1.0861\n",
      "208/295, train_loss: 0.0467, step time: 1.0431\n",
      "209/295, train_loss: 0.0691, step time: 1.0773\n",
      "210/295, train_loss: 0.0514, step time: 1.0424\n",
      "211/295, train_loss: 0.0331, step time: 1.0475\n",
      "212/295, train_loss: 0.0995, step time: 1.0950\n",
      "213/295, train_loss: 0.3687, step time: 1.0491\n",
      "214/295, train_loss: 0.0543, step time: 1.1006\n",
      "215/295, train_loss: 0.0947, step time: 1.0352\n",
      "216/295, train_loss: 0.0476, step time: 1.0376\n",
      "217/295, train_loss: 0.3708, step time: 1.0618\n",
      "218/295, train_loss: 0.0689, step time: 1.0611\n",
      "219/295, train_loss: 0.0785, step time: 1.0394\n",
      "220/295, train_loss: 0.0768, step time: 1.0881\n",
      "221/295, train_loss: 0.0328, step time: 1.0578\n",
      "222/295, train_loss: 0.1132, step time: 1.0808\n",
      "223/295, train_loss: 0.0366, step time: 1.0342\n",
      "224/295, train_loss: 0.0644, step time: 1.0605\n",
      "225/295, train_loss: 0.1416, step time: 1.0419\n",
      "226/295, train_loss: 0.3953, step time: 1.0764\n",
      "227/295, train_loss: 0.0741, step time: 1.0420\n",
      "228/295, train_loss: 0.0356, step time: 1.0836\n",
      "229/295, train_loss: 0.0715, step time: 1.0489\n",
      "230/295, train_loss: 0.3697, step time: 1.0488\n",
      "231/295, train_loss: 0.0378, step time: 1.0365\n",
      "232/295, train_loss: 0.1766, step time: 1.0387\n",
      "233/295, train_loss: 0.0299, step time: 1.0361\n",
      "234/295, train_loss: 0.0920, step time: 1.0972\n",
      "235/295, train_loss: 0.0850, step time: 1.0337\n",
      "236/295, train_loss: 0.0409, step time: 1.0342\n",
      "237/295, train_loss: 0.0224, step time: 1.0591\n",
      "238/295, train_loss: 0.0683, step time: 1.0510\n",
      "239/295, train_loss: 0.1422, step time: 1.0391\n",
      "240/295, train_loss: 0.0927, step time: 1.0388\n",
      "241/295, train_loss: 0.0925, step time: 1.0360\n",
      "242/295, train_loss: 0.0780, step time: 1.0376\n",
      "243/295, train_loss: 0.0568, step time: 1.0362\n",
      "244/295, train_loss: 0.0615, step time: 1.0355\n",
      "245/295, train_loss: 0.1307, step time: 1.0328\n",
      "246/295, train_loss: 0.0305, step time: 1.0395\n",
      "247/295, train_loss: 0.0447, step time: 1.0461\n",
      "248/295, train_loss: 0.0736, step time: 1.0821\n",
      "249/295, train_loss: 0.0604, step time: 1.0446\n",
      "250/295, train_loss: 0.0647, step time: 1.0363\n",
      "251/295, train_loss: 0.0615, step time: 1.0393\n",
      "252/295, train_loss: 0.0430, step time: 1.0403\n",
      "253/295, train_loss: 0.0967, step time: 1.0603\n",
      "254/295, train_loss: 0.0671, step time: 1.0343\n",
      "255/295, train_loss: 0.0331, step time: 1.0633\n",
      "256/295, train_loss: 0.0452, step time: 1.0696\n",
      "257/295, train_loss: 0.1037, step time: 1.0481\n",
      "258/295, train_loss: 0.0534, step time: 1.0479\n",
      "259/295, train_loss: 0.1035, step time: 1.0713\n",
      "260/295, train_loss: 0.1274, step time: 1.0466\n",
      "261/295, train_loss: 0.0530, step time: 1.0561\n",
      "262/295, train_loss: 0.1095, step time: 1.0346\n",
      "263/295, train_loss: 0.0275, step time: 1.0785\n",
      "264/295, train_loss: 0.0607, step time: 1.0422\n",
      "265/295, train_loss: 0.0727, step time: 1.0485\n",
      "266/295, train_loss: 0.0717, step time: 1.0325\n",
      "267/295, train_loss: 0.3904, step time: 1.0336\n",
      "268/295, train_loss: 0.0919, step time: 1.0411\n",
      "269/295, train_loss: 0.0518, step time: 1.0922\n",
      "270/295, train_loss: 0.0493, step time: 1.0371\n",
      "271/295, train_loss: 0.0486, step time: 1.0340\n",
      "272/295, train_loss: 0.0837, step time: 1.0439\n",
      "273/295, train_loss: 0.0908, step time: 1.0522\n",
      "274/295, train_loss: 0.0458, step time: 1.0396\n",
      "275/295, train_loss: 0.0824, step time: 1.0382\n",
      "276/295, train_loss: 0.1107, step time: 1.0357\n",
      "277/295, train_loss: 0.0372, step time: 1.0326\n",
      "278/295, train_loss: 0.0855, step time: 1.0389\n",
      "279/295, train_loss: 0.0417, step time: 1.0685\n",
      "280/295, train_loss: 0.0502, step time: 1.0823\n",
      "281/295, train_loss: 0.0336, step time: 1.0520\n",
      "282/295, train_loss: 0.3787, step time: 1.0339\n",
      "283/295, train_loss: 0.0674, step time: 1.0611\n",
      "284/295, train_loss: 0.0626, step time: 1.0886\n",
      "285/295, train_loss: 0.3953, step time: 1.0539\n",
      "286/295, train_loss: 0.1328, step time: 1.0496\n",
      "287/295, train_loss: 0.0839, step time: 1.0472\n",
      "288/295, train_loss: 0.0581, step time: 1.0321\n",
      "289/295, train_loss: 0.1046, step time: 1.0291\n",
      "290/295, train_loss: 0.0656, step time: 1.0305\n",
      "291/295, train_loss: 0.0754, step time: 1.0295\n",
      "292/295, train_loss: 0.0318, step time: 1.0293\n",
      "293/295, train_loss: 0.0411, step time: 1.0286\n",
      "294/295, train_loss: 0.0975, step time: 1.0296\n",
      "295/295, train_loss: 0.0387, step time: 1.0289\n",
      "epoch 56 average loss: 0.0958\n",
      "current epoch: 56 current mean dice: 0.7762 tc: 0.7307 wt: 0.8409 et: 0.7630\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 56 is: 386.9033\n",
      "----------\n",
      "epoch 57/100\n",
      "1/295, train_loss: 0.0899, step time: 1.1141\n",
      "2/295, train_loss: 0.0407, step time: 1.1154\n",
      "3/295, train_loss: 0.0513, step time: 1.0565\n",
      "4/295, train_loss: 0.0841, step time: 1.0916\n",
      "5/295, train_loss: 0.0996, step time: 1.0655\n",
      "6/295, train_loss: 0.1517, step time: 1.0504\n",
      "7/295, train_loss: 0.0664, step time: 1.0601\n",
      "8/295, train_loss: 0.0478, step time: 1.0969\n",
      "9/295, train_loss: 0.0422, step time: 1.0345\n",
      "10/295, train_loss: 0.0951, step time: 1.0720\n",
      "11/295, train_loss: 0.0973, step time: 1.0498\n",
      "12/295, train_loss: 0.0866, step time: 1.0348\n",
      "13/295, train_loss: 0.1003, step time: 1.0403\n",
      "14/295, train_loss: 0.0549, step time: 1.0434\n",
      "15/295, train_loss: 0.3928, step time: 1.0494\n",
      "16/295, train_loss: 0.0916, step time: 1.0363\n",
      "17/295, train_loss: 0.3978, step time: 1.0319\n",
      "18/295, train_loss: 0.0899, step time: 1.0424\n",
      "19/295, train_loss: 0.0672, step time: 1.0708\n",
      "20/295, train_loss: 0.0849, step time: 1.1111\n",
      "21/295, train_loss: 0.0346, step time: 1.0347\n",
      "22/295, train_loss: 0.0478, step time: 1.0388\n",
      "23/295, train_loss: 0.0350, step time: 1.0539\n",
      "24/295, train_loss: 0.0781, step time: 1.0294\n",
      "25/295, train_loss: 0.0421, step time: 1.0649\n",
      "26/295, train_loss: 0.0557, step time: 1.0662\n",
      "27/295, train_loss: 0.1367, step time: 1.0445\n",
      "28/295, train_loss: 0.3922, step time: 1.0527\n",
      "29/295, train_loss: 0.0614, step time: 1.0657\n",
      "30/295, train_loss: 0.0825, step time: 1.0584\n",
      "31/295, train_loss: 0.0426, step time: 1.0365\n",
      "32/295, train_loss: 0.1226, step time: 1.0413\n",
      "33/295, train_loss: 0.0538, step time: 1.0317\n",
      "34/295, train_loss: 0.0923, step time: 1.0443\n",
      "35/295, train_loss: 0.3741, step time: 1.0412\n",
      "36/295, train_loss: 0.0332, step time: 1.0388\n",
      "37/295, train_loss: 0.3671, step time: 1.0522\n",
      "38/295, train_loss: 0.0706, step time: 1.0777\n",
      "39/295, train_loss: 0.0597, step time: 1.0443\n",
      "40/295, train_loss: 0.0275, step time: 1.0528\n",
      "41/295, train_loss: 0.0949, step time: 1.0537\n",
      "42/295, train_loss: 0.1251, step time: 1.0553\n",
      "43/295, train_loss: 0.0360, step time: 1.0387\n",
      "44/295, train_loss: 0.1279, step time: 1.0473\n",
      "45/295, train_loss: 0.0496, step time: 1.0429\n",
      "46/295, train_loss: 0.0560, step time: 1.0634\n",
      "47/295, train_loss: 0.0426, step time: 1.0349\n",
      "48/295, train_loss: 0.0853, step time: 1.0622\n",
      "49/295, train_loss: 0.1644, step time: 1.1340\n",
      "50/295, train_loss: 0.0948, step time: 1.0498\n",
      "51/295, train_loss: 0.0829, step time: 1.0312\n",
      "52/295, train_loss: 0.0732, step time: 1.0432\n",
      "53/295, train_loss: 0.0550, step time: 1.0332\n",
      "54/295, train_loss: 0.0639, step time: 1.0440\n",
      "55/295, train_loss: 0.0567, step time: 1.0616\n",
      "56/295, train_loss: 0.0751, step time: 1.0369\n",
      "57/295, train_loss: 0.0425, step time: 1.0535\n",
      "58/295, train_loss: 0.0511, step time: 1.0719\n",
      "59/295, train_loss: 0.0866, step time: 1.0410\n",
      "60/295, train_loss: 0.0463, step time: 1.0318\n",
      "61/295, train_loss: 0.0793, step time: 1.0464\n",
      "62/295, train_loss: 0.0885, step time: 1.0589\n",
      "63/295, train_loss: 0.0909, step time: 1.0332\n",
      "64/295, train_loss: 0.0853, step time: 1.0333\n",
      "65/295, train_loss: 0.0707, step time: 1.0405\n",
      "66/295, train_loss: 0.1953, step time: 1.0954\n",
      "67/295, train_loss: 0.0358, step time: 1.0387\n",
      "68/295, train_loss: 0.0688, step time: 1.0397\n",
      "69/295, train_loss: 0.0479, step time: 1.0322\n",
      "70/295, train_loss: 0.4676, step time: 1.0438\n",
      "71/295, train_loss: 0.2200, step time: 1.0571\n",
      "72/295, train_loss: 0.0630, step time: 1.0463\n",
      "73/295, train_loss: 0.0500, step time: 1.0423\n",
      "74/295, train_loss: 0.0761, step time: 1.0468\n",
      "75/295, train_loss: 0.0553, step time: 1.0319\n",
      "76/295, train_loss: 0.0630, step time: 1.0387\n",
      "77/295, train_loss: 0.0378, step time: 1.0437\n",
      "78/295, train_loss: 0.0461, step time: 1.0478\n",
      "79/295, train_loss: 0.0702, step time: 1.0646\n",
      "80/295, train_loss: 0.1294, step time: 1.0324\n",
      "81/295, train_loss: 0.0745, step time: 1.0412\n",
      "82/295, train_loss: 0.4052, step time: 1.0499\n",
      "83/295, train_loss: 0.0609, step time: 1.0385\n",
      "84/295, train_loss: 0.1458, step time: 1.0401\n",
      "85/295, train_loss: 0.1905, step time: 1.0338\n",
      "86/295, train_loss: 0.3683, step time: 1.0375\n",
      "87/295, train_loss: 0.3697, step time: 1.0470\n",
      "88/295, train_loss: 0.0600, step time: 1.0464\n",
      "89/295, train_loss: 0.0553, step time: 1.0335\n",
      "90/295, train_loss: 0.0431, step time: 1.0514\n",
      "91/295, train_loss: 0.0292, step time: 1.1105\n",
      "92/295, train_loss: 0.0593, step time: 1.0394\n",
      "93/295, train_loss: 0.0935, step time: 1.0653\n",
      "94/295, train_loss: 0.0606, step time: 1.0685\n",
      "95/295, train_loss: 0.0382, step time: 1.0454\n",
      "96/295, train_loss: 0.1408, step time: 1.0571\n",
      "97/295, train_loss: 0.1244, step time: 1.0524\n",
      "98/295, train_loss: 0.0585, step time: 1.0382\n",
      "99/295, train_loss: 0.0636, step time: 1.0379\n",
      "100/295, train_loss: 0.0319, step time: 1.0671\n",
      "101/295, train_loss: 0.0348, step time: 1.0602\n",
      "102/295, train_loss: 0.0392, step time: 1.0391\n",
      "103/295, train_loss: 0.0730, step time: 1.0517\n",
      "104/295, train_loss: 0.0320, step time: 1.0401\n",
      "105/295, train_loss: 0.0871, step time: 1.0683\n",
      "106/295, train_loss: 0.0468, step time: 1.0621\n",
      "107/295, train_loss: 0.0593, step time: 1.0385\n",
      "108/295, train_loss: 0.4023, step time: 1.0415\n",
      "109/295, train_loss: 0.0903, step time: 1.0309\n",
      "110/295, train_loss: 0.0400, step time: 1.0479\n",
      "111/295, train_loss: 0.4610, step time: 1.0475\n",
      "112/295, train_loss: 0.0678, step time: 1.0352\n",
      "113/295, train_loss: 0.0759, step time: 1.0552\n",
      "114/295, train_loss: 0.0305, step time: 1.0361\n",
      "115/295, train_loss: 0.1061, step time: 1.0475\n",
      "116/295, train_loss: 0.0277, step time: 1.0744\n",
      "117/295, train_loss: 0.2407, step time: 1.0612\n",
      "118/295, train_loss: 0.0597, step time: 1.0713\n",
      "119/295, train_loss: 0.0450, step time: 1.0517\n",
      "120/295, train_loss: 0.1154, step time: 1.0319\n",
      "121/295, train_loss: 0.0544, step time: 1.0319\n",
      "122/295, train_loss: 0.3636, step time: 1.0345\n",
      "123/295, train_loss: 0.0935, step time: 1.0412\n",
      "124/295, train_loss: 0.3941, step time: 1.0594\n",
      "125/295, train_loss: 0.0963, step time: 1.0315\n",
      "126/295, train_loss: 0.0289, step time: 1.0657\n",
      "127/295, train_loss: 0.0871, step time: 1.0420\n",
      "128/295, train_loss: 0.0501, step time: 1.0325\n",
      "129/295, train_loss: 0.3953, step time: 1.0442\n",
      "130/295, train_loss: 0.0928, step time: 1.0746\n",
      "131/295, train_loss: 0.0774, step time: 1.0342\n",
      "132/295, train_loss: 0.0338, step time: 1.0553\n",
      "133/295, train_loss: 0.0557, step time: 1.0295\n",
      "134/295, train_loss: 0.0549, step time: 1.0359\n",
      "135/295, train_loss: 0.0991, step time: 1.0346\n",
      "136/295, train_loss: 0.1519, step time: 1.0489\n",
      "137/295, train_loss: 0.0750, step time: 1.0621\n",
      "138/295, train_loss: 0.0742, step time: 1.0515\n",
      "139/295, train_loss: 0.0623, step time: 1.0320\n",
      "140/295, train_loss: 0.0515, step time: 1.0309\n",
      "141/295, train_loss: 0.1064, step time: 1.0376\n",
      "142/295, train_loss: 0.0734, step time: 1.0328\n",
      "143/295, train_loss: 0.0550, step time: 1.0374\n",
      "144/295, train_loss: 0.0606, step time: 1.0486\n",
      "145/295, train_loss: 0.0286, step time: 1.0373\n",
      "146/295, train_loss: 0.0560, step time: 1.0367\n",
      "147/295, train_loss: 0.0582, step time: 1.0589\n",
      "148/295, train_loss: 0.1025, step time: 1.0403\n",
      "149/295, train_loss: 0.0617, step time: 1.0382\n",
      "150/295, train_loss: 0.0333, step time: 1.0402\n",
      "151/295, train_loss: 0.2429, step time: 1.0537\n",
      "152/295, train_loss: 0.1022, step time: 1.0488\n",
      "153/295, train_loss: 0.0476, step time: 1.0374\n",
      "154/295, train_loss: 0.0521, step time: 1.0331\n",
      "155/295, train_loss: 0.1187, step time: 1.0447\n",
      "156/295, train_loss: 0.0526, step time: 1.0368\n",
      "157/295, train_loss: 0.0808, step time: 1.0611\n",
      "158/295, train_loss: 0.0726, step time: 1.0482\n",
      "159/295, train_loss: 0.0663, step time: 1.0345\n",
      "160/295, train_loss: 0.0934, step time: 1.0455\n",
      "161/295, train_loss: 0.1538, step time: 1.0503\n",
      "162/295, train_loss: 0.3667, step time: 1.0734\n",
      "163/295, train_loss: 0.1132, step time: 1.0380\n",
      "164/295, train_loss: 0.0952, step time: 1.0740\n",
      "165/295, train_loss: 0.0661, step time: 1.0439\n",
      "166/295, train_loss: 0.1294, step time: 1.0396\n",
      "167/295, train_loss: 0.1355, step time: 1.0425\n",
      "168/295, train_loss: 0.3745, step time: 1.0641\n",
      "169/295, train_loss: 0.0657, step time: 1.0354\n",
      "170/295, train_loss: 0.0803, step time: 1.0347\n",
      "171/295, train_loss: 0.0756, step time: 1.0377\n",
      "172/295, train_loss: 0.0533, step time: 1.0346\n",
      "173/295, train_loss: 0.0536, step time: 1.0610\n",
      "174/295, train_loss: 0.0395, step time: 1.0736\n",
      "175/295, train_loss: 0.0389, step time: 1.0462\n",
      "176/295, train_loss: 0.0387, step time: 1.0597\n",
      "177/295, train_loss: 0.0979, step time: 1.0342\n",
      "178/295, train_loss: 0.0403, step time: 1.0373\n",
      "179/295, train_loss: 0.0955, step time: 1.0446\n",
      "180/295, train_loss: 0.0706, step time: 1.0640\n",
      "181/295, train_loss: 0.0527, step time: 1.0567\n",
      "182/295, train_loss: 0.0367, step time: 1.0331\n",
      "183/295, train_loss: 0.0449, step time: 1.0376\n",
      "184/295, train_loss: 0.0736, step time: 1.0355\n",
      "185/295, train_loss: 0.0321, step time: 1.0562\n",
      "186/295, train_loss: 0.0486, step time: 1.0353\n",
      "187/295, train_loss: 0.0318, step time: 1.0369\n",
      "188/295, train_loss: 0.0702, step time: 1.0455\n",
      "189/295, train_loss: 0.0556, step time: 1.0392\n",
      "190/295, train_loss: 0.0575, step time: 1.0339\n",
      "191/295, train_loss: 0.0656, step time: 1.0406\n",
      "192/295, train_loss: 0.0225, step time: 1.0376\n",
      "193/295, train_loss: 0.0350, step time: 1.0748\n",
      "194/295, train_loss: 0.0853, step time: 1.0693\n",
      "195/295, train_loss: 0.0824, step time: 1.0504\n",
      "196/295, train_loss: 0.0643, step time: 1.0710\n",
      "197/295, train_loss: 0.0994, step time: 1.0387\n",
      "198/295, train_loss: 0.0886, step time: 1.0452\n",
      "199/295, train_loss: 0.0382, step time: 1.0901\n",
      "200/295, train_loss: 0.1175, step time: 1.0404\n",
      "201/295, train_loss: 0.3734, step time: 1.0450\n",
      "202/295, train_loss: 0.0297, step time: 1.1358\n",
      "203/295, train_loss: 0.0375, step time: 1.0347\n",
      "204/295, train_loss: 0.1109, step time: 1.0314\n",
      "205/295, train_loss: 0.0334, step time: 1.0434\n",
      "206/295, train_loss: 0.1088, step time: 1.0424\n",
      "207/295, train_loss: 0.0483, step time: 1.0438\n",
      "208/295, train_loss: 0.3892, step time: 1.0539\n",
      "209/295, train_loss: 0.0536, step time: 1.0386\n",
      "210/295, train_loss: 0.0366, step time: 1.0421\n",
      "211/295, train_loss: 0.0646, step time: 1.0352\n",
      "212/295, train_loss: 0.0519, step time: 1.0451\n",
      "213/295, train_loss: 0.3895, step time: 1.0474\n",
      "214/295, train_loss: 0.0838, step time: 1.0330\n",
      "215/295, train_loss: 0.0509, step time: 1.0326\n",
      "216/295, train_loss: 0.0848, step time: 1.0372\n",
      "217/295, train_loss: 0.0612, step time: 1.0419\n",
      "218/295, train_loss: 0.0462, step time: 1.0365\n",
      "219/295, train_loss: 0.0906, step time: 1.0338\n",
      "220/295, train_loss: 0.1604, step time: 1.0441\n",
      "221/295, train_loss: 0.0267, step time: 1.0313\n",
      "222/295, train_loss: 0.1660, step time: 1.0386\n",
      "223/295, train_loss: 0.0824, step time: 1.0448\n",
      "224/295, train_loss: 0.0476, step time: 1.0323\n",
      "225/295, train_loss: 0.1115, step time: 1.0409\n",
      "226/295, train_loss: 0.0776, step time: 1.0353\n",
      "227/295, train_loss: 0.3775, step time: 1.0563\n",
      "228/295, train_loss: 0.0715, step time: 1.0600\n",
      "229/295, train_loss: 0.0908, step time: 1.0515\n",
      "230/295, train_loss: 0.1009, step time: 1.0390\n",
      "231/295, train_loss: 0.0644, step time: 1.0365\n",
      "232/295, train_loss: 0.0474, step time: 1.0433\n",
      "233/295, train_loss: 0.0312, step time: 1.0371\n",
      "234/295, train_loss: 0.0349, step time: 1.0424\n",
      "235/295, train_loss: 0.0405, step time: 1.0681\n",
      "236/295, train_loss: 0.0890, step time: 1.0352\n",
      "237/295, train_loss: 0.0370, step time: 1.0568\n",
      "238/295, train_loss: 0.0396, step time: 1.0335\n",
      "239/295, train_loss: 0.1107, step time: 1.0418\n",
      "240/295, train_loss: 0.0528, step time: 1.0400\n",
      "241/295, train_loss: 0.0767, step time: 1.0319\n",
      "242/295, train_loss: 0.0724, step time: 1.0580\n",
      "243/295, train_loss: 0.1003, step time: 1.0588\n",
      "244/295, train_loss: 0.0895, step time: 1.0516\n",
      "245/295, train_loss: 0.4091, step time: 1.0401\n",
      "246/295, train_loss: 0.0584, step time: 1.0607\n",
      "247/295, train_loss: 0.0368, step time: 1.0653\n",
      "248/295, train_loss: 0.0316, step time: 1.0404\n",
      "249/295, train_loss: 0.0681, step time: 1.0589\n",
      "250/295, train_loss: 0.0589, step time: 1.0352\n",
      "251/295, train_loss: 0.0414, step time: 1.0423\n",
      "252/295, train_loss: 0.1207, step time: 1.0563\n",
      "253/295, train_loss: 0.0763, step time: 1.0658\n",
      "254/295, train_loss: 0.1232, step time: 1.0353\n",
      "255/295, train_loss: 0.0770, step time: 1.0446\n",
      "256/295, train_loss: 0.1173, step time: 1.0466\n",
      "257/295, train_loss: 0.0758, step time: 1.0748\n",
      "258/295, train_loss: 0.0637, step time: 1.0563\n",
      "259/295, train_loss: 0.0606, step time: 1.0370\n",
      "260/295, train_loss: 0.0761, step time: 1.0349\n",
      "261/295, train_loss: 0.0660, step time: 1.0414\n",
      "262/295, train_loss: 0.0297, step time: 1.1289\n",
      "263/295, train_loss: 0.0905, step time: 1.0959\n",
      "264/295, train_loss: 0.1066, step time: 1.0327\n",
      "265/295, train_loss: 0.0729, step time: 1.0380\n",
      "266/295, train_loss: 0.0460, step time: 1.0493\n",
      "267/295, train_loss: 0.0611, step time: 1.0351\n",
      "268/295, train_loss: 0.0399, step time: 1.0372\n",
      "269/295, train_loss: 0.0464, step time: 1.0391\n",
      "270/295, train_loss: 0.0672, step time: 1.0428\n",
      "271/295, train_loss: 0.0967, step time: 1.0421\n",
      "272/295, train_loss: 0.0445, step time: 1.0422\n",
      "273/295, train_loss: 0.0495, step time: 1.0331\n",
      "274/295, train_loss: 0.1092, step time: 1.0446\n",
      "275/295, train_loss: 0.0353, step time: 1.0301\n",
      "276/295, train_loss: 0.3865, step time: 1.0341\n",
      "277/295, train_loss: 0.0568, step time: 1.0365\n",
      "278/295, train_loss: 0.1207, step time: 1.0379\n",
      "279/295, train_loss: 0.0537, step time: 1.0442\n",
      "280/295, train_loss: 0.0587, step time: 1.0463\n",
      "281/295, train_loss: 0.0534, step time: 1.0334\n",
      "282/295, train_loss: 0.0869, step time: 1.0370\n",
      "283/295, train_loss: 0.0360, step time: 1.0368\n",
      "284/295, train_loss: 0.0410, step time: 1.0387\n",
      "285/295, train_loss: 0.0897, step time: 1.0414\n",
      "286/295, train_loss: 0.0342, step time: 1.0628\n",
      "287/295, train_loss: 0.0326, step time: 1.0644\n",
      "288/295, train_loss: 0.0465, step time: 1.0292\n",
      "289/295, train_loss: 0.0786, step time: 1.0305\n",
      "290/295, train_loss: 0.0875, step time: 1.0290\n",
      "291/295, train_loss: 0.0404, step time: 1.0284\n",
      "292/295, train_loss: 0.0770, step time: 1.0293\n",
      "293/295, train_loss: 0.0569, step time: 1.0286\n",
      "294/295, train_loss: 0.0446, step time: 1.0280\n",
      "295/295, train_loss: 0.1574, step time: 1.0280\n",
      "epoch 57 average loss: 0.0962\n",
      "current epoch: 57 current mean dice: 0.7812 tc: 0.7354 wt: 0.8485 et: 0.7647\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 57 is: 382.5545\n",
      "----------\n",
      "epoch 58/100\n",
      "1/295, train_loss: 0.0700, step time: 1.1486\n",
      "2/295, train_loss: 0.3894, step time: 1.0780\n",
      "3/295, train_loss: 0.3670, step time: 1.1683\n",
      "4/295, train_loss: 0.0364, step time: 1.0500\n",
      "5/295, train_loss: 0.0713, step time: 1.0535\n",
      "6/295, train_loss: 0.0665, step time: 1.0962\n",
      "7/295, train_loss: 0.0320, step time: 1.0468\n",
      "8/295, train_loss: 0.1057, step time: 1.0450\n",
      "9/295, train_loss: 0.0827, step time: 1.0379\n",
      "10/295, train_loss: 0.0549, step time: 1.0609\n",
      "11/295, train_loss: 0.0813, step time: 1.0507\n",
      "12/295, train_loss: 0.3957, step time: 1.0429\n",
      "13/295, train_loss: 0.0279, step time: 1.0387\n",
      "14/295, train_loss: 0.0794, step time: 1.0636\n",
      "15/295, train_loss: 0.0467, step time: 1.0360\n",
      "16/295, train_loss: 0.0854, step time: 1.0470\n",
      "17/295, train_loss: 0.3687, step time: 1.0642\n",
      "18/295, train_loss: 0.0509, step time: 1.0727\n",
      "19/295, train_loss: 0.0563, step time: 1.0469\n",
      "20/295, train_loss: 0.0506, step time: 1.0355\n",
      "21/295, train_loss: 0.0543, step time: 1.0400\n",
      "22/295, train_loss: 0.0410, step time: 1.0370\n",
      "23/295, train_loss: 0.0290, step time: 1.0477\n",
      "24/295, train_loss: 0.0465, step time: 1.0573\n",
      "25/295, train_loss: 0.3791, step time: 1.0629\n",
      "26/295, train_loss: 0.0492, step time: 1.0700\n",
      "27/295, train_loss: 0.0577, step time: 1.0334\n",
      "28/295, train_loss: 0.0460, step time: 1.0554\n",
      "29/295, train_loss: 0.0376, step time: 1.0451\n",
      "30/295, train_loss: 0.0773, step time: 1.0627\n",
      "31/295, train_loss: 0.0537, step time: 1.0402\n",
      "32/295, train_loss: 0.0726, step time: 1.0309\n",
      "33/295, train_loss: 0.0340, step time: 1.0413\n",
      "34/295, train_loss: 0.1246, step time: 1.0470\n",
      "35/295, train_loss: 0.1370, step time: 1.0406\n",
      "36/295, train_loss: 0.0971, step time: 1.0445\n",
      "37/295, train_loss: 0.0461, step time: 1.0569\n",
      "38/295, train_loss: 0.0626, step time: 1.0398\n",
      "39/295, train_loss: 0.0465, step time: 1.0344\n",
      "40/295, train_loss: 0.1038, step time: 1.1081\n",
      "41/295, train_loss: 0.0609, step time: 1.0335\n",
      "42/295, train_loss: 0.0412, step time: 1.0399\n",
      "43/295, train_loss: 0.3950, step time: 1.0770\n",
      "44/295, train_loss: 0.0863, step time: 1.0346\n",
      "45/295, train_loss: 0.0366, step time: 1.0551\n",
      "46/295, train_loss: 0.0627, step time: 1.0339\n",
      "47/295, train_loss: 0.0664, step time: 1.0309\n",
      "48/295, train_loss: 0.0611, step time: 1.0972\n",
      "49/295, train_loss: 0.1093, step time: 1.0325\n",
      "50/295, train_loss: 0.0606, step time: 1.0343\n",
      "51/295, train_loss: 0.0741, step time: 1.0302\n",
      "52/295, train_loss: 0.0832, step time: 1.0503\n",
      "53/295, train_loss: 0.0310, step time: 1.0365\n",
      "54/295, train_loss: 0.0718, step time: 1.0546\n",
      "55/295, train_loss: 0.0434, step time: 1.0578\n",
      "56/295, train_loss: 0.0292, step time: 1.0411\n",
      "57/295, train_loss: 0.0441, step time: 1.0335\n",
      "58/295, train_loss: 0.0410, step time: 1.0388\n",
      "59/295, train_loss: 0.1073, step time: 1.0957\n",
      "60/295, train_loss: 0.0769, step time: 1.1247\n",
      "61/295, train_loss: 0.0879, step time: 1.0519\n",
      "62/295, train_loss: 0.3858, step time: 1.0337\n",
      "63/295, train_loss: 0.0839, step time: 1.0476\n",
      "64/295, train_loss: 0.0841, step time: 1.1394\n",
      "65/295, train_loss: 0.0837, step time: 1.0438\n",
      "66/295, train_loss: 0.0382, step time: 1.0585\n",
      "67/295, train_loss: 0.0290, step time: 1.0529\n",
      "68/295, train_loss: 0.3881, step time: 1.0299\n",
      "69/295, train_loss: 0.0411, step time: 1.0436\n",
      "70/295, train_loss: 0.1190, step time: 1.0383\n",
      "71/295, train_loss: 0.0420, step time: 1.0296\n",
      "72/295, train_loss: 0.0341, step time: 1.0371\n",
      "73/295, train_loss: 0.3651, step time: 1.0384\n",
      "74/295, train_loss: 0.0765, step time: 1.0424\n",
      "75/295, train_loss: 0.0339, step time: 1.0368\n",
      "76/295, train_loss: 0.0590, step time: 1.0365\n",
      "77/295, train_loss: 0.0520, step time: 1.0328\n",
      "78/295, train_loss: 0.0514, step time: 1.0335\n",
      "79/295, train_loss: 0.0741, step time: 1.0444\n",
      "80/295, train_loss: 0.0611, step time: 1.0316\n",
      "81/295, train_loss: 0.0580, step time: 1.0370\n",
      "82/295, train_loss: 0.0298, step time: 1.0388\n",
      "83/295, train_loss: 0.0707, step time: 1.0934\n",
      "84/295, train_loss: 0.0927, step time: 1.0552\n",
      "85/295, train_loss: 0.0643, step time: 1.0629\n",
      "86/295, train_loss: 0.0879, step time: 1.0349\n",
      "87/295, train_loss: 0.0802, step time: 1.0503\n",
      "88/295, train_loss: 0.0398, step time: 1.0762\n",
      "89/295, train_loss: 0.0457, step time: 1.0434\n",
      "90/295, train_loss: 0.0511, step time: 1.0595\n",
      "91/295, train_loss: 0.0994, step time: 1.0446\n",
      "92/295, train_loss: 0.0600, step time: 1.0522\n",
      "93/295, train_loss: 0.1028, step time: 1.0300\n",
      "94/295, train_loss: 0.0922, step time: 1.0328\n",
      "95/295, train_loss: 0.0475, step time: 1.0438\n",
      "96/295, train_loss: 0.0467, step time: 1.0344\n",
      "97/295, train_loss: 0.0599, step time: 1.0352\n",
      "98/295, train_loss: 0.0647, step time: 1.0400\n",
      "99/295, train_loss: 0.3742, step time: 1.0389\n",
      "100/295, train_loss: 0.0687, step time: 1.0560\n",
      "101/295, train_loss: 0.0432, step time: 1.0474\n",
      "102/295, train_loss: 0.0545, step time: 1.0475\n",
      "103/295, train_loss: 0.0566, step time: 1.0489\n",
      "104/295, train_loss: 0.0577, step time: 1.0382\n",
      "105/295, train_loss: 0.0214, step time: 1.0388\n",
      "106/295, train_loss: 0.0757, step time: 1.0335\n",
      "107/295, train_loss: 0.1445, step time: 1.0473\n",
      "108/295, train_loss: 0.1448, step time: 1.0538\n",
      "109/295, train_loss: 0.1192, step time: 1.0580\n",
      "110/295, train_loss: 0.0825, step time: 1.0315\n",
      "111/295, train_loss: 0.1039, step time: 1.0410\n",
      "112/295, train_loss: 0.0973, step time: 1.0375\n",
      "113/295, train_loss: 0.2338, step time: 1.0463\n",
      "114/295, train_loss: 0.0336, step time: 1.0603\n",
      "115/295, train_loss: 0.0513, step time: 1.0448\n",
      "116/295, train_loss: 0.1207, step time: 1.0549\n",
      "117/295, train_loss: 0.0826, step time: 1.0484\n",
      "118/295, train_loss: 0.1389, step time: 1.0495\n",
      "119/295, train_loss: 0.1459, step time: 1.0390\n",
      "120/295, train_loss: 0.0436, step time: 1.1298\n",
      "121/295, train_loss: 0.0907, step time: 1.0319\n",
      "122/295, train_loss: 0.0317, step time: 1.0377\n",
      "123/295, train_loss: 0.0846, step time: 1.0388\n",
      "124/295, train_loss: 0.0859, step time: 1.0434\n",
      "125/295, train_loss: 0.0909, step time: 1.0342\n",
      "126/295, train_loss: 0.0576, step time: 1.0965\n",
      "127/295, train_loss: 0.0747, step time: 1.0377\n",
      "128/295, train_loss: 0.0602, step time: 1.0599\n",
      "129/295, train_loss: 0.1088, step time: 1.0441\n",
      "130/295, train_loss: 0.4103, step time: 1.0324\n",
      "131/295, train_loss: 0.0831, step time: 1.0336\n",
      "132/295, train_loss: 0.0372, step time: 1.0318\n",
      "133/295, train_loss: 0.0937, step time: 1.0351\n",
      "134/295, train_loss: 0.0595, step time: 1.0386\n",
      "135/295, train_loss: 0.0395, step time: 1.0327\n",
      "136/295, train_loss: 0.0366, step time: 1.1039\n",
      "137/295, train_loss: 0.0949, step time: 1.0661\n",
      "138/295, train_loss: 0.1083, step time: 1.0578\n",
      "139/295, train_loss: 0.0565, step time: 1.0815\n",
      "140/295, train_loss: 0.1675, step time: 1.0374\n",
      "141/295, train_loss: 0.0619, step time: 1.0487\n",
      "142/295, train_loss: 0.0985, step time: 1.2281\n",
      "143/295, train_loss: 0.0885, step time: 1.0541\n",
      "144/295, train_loss: 0.0646, step time: 1.0388\n",
      "145/295, train_loss: 0.0787, step time: 1.0659\n",
      "146/295, train_loss: 0.0489, step time: 1.0472\n",
      "147/295, train_loss: 0.0415, step time: 1.0683\n",
      "148/295, train_loss: 0.0642, step time: 1.0378\n",
      "149/295, train_loss: 0.1162, step time: 1.0344\n",
      "150/295, train_loss: 0.0558, step time: 1.0373\n",
      "151/295, train_loss: 0.0339, step time: 1.0585\n",
      "152/295, train_loss: 0.0566, step time: 1.0782\n",
      "153/295, train_loss: 0.0322, step time: 1.0488\n",
      "154/295, train_loss: 0.0901, step time: 1.0409\n",
      "155/295, train_loss: 0.0806, step time: 1.0759\n",
      "156/295, train_loss: 0.0466, step time: 1.0417\n",
      "157/295, train_loss: 0.3671, step time: 1.0914\n",
      "158/295, train_loss: 0.0807, step time: 1.0870\n",
      "159/295, train_loss: 0.1435, step time: 1.0367\n",
      "160/295, train_loss: 0.1722, step time: 1.0353\n",
      "161/295, train_loss: 0.4629, step time: 1.0391\n",
      "162/295, train_loss: 0.0458, step time: 1.0441\n",
      "163/295, train_loss: 0.0671, step time: 1.0455\n",
      "164/295, train_loss: 0.0505, step time: 1.0560\n",
      "165/295, train_loss: 0.0725, step time: 1.0355\n",
      "166/295, train_loss: 0.1181, step time: 1.0386\n",
      "167/295, train_loss: 0.0749, step time: 1.0393\n",
      "168/295, train_loss: 0.0490, step time: 1.0611\n",
      "169/295, train_loss: 0.0899, step time: 1.0358\n",
      "170/295, train_loss: 0.1106, step time: 1.0419\n",
      "171/295, train_loss: 0.0909, step time: 1.0905\n",
      "172/295, train_loss: 0.0582, step time: 1.0593\n",
      "173/295, train_loss: 0.0361, step time: 1.1082\n",
      "174/295, train_loss: 0.0926, step time: 1.0464\n",
      "175/295, train_loss: 0.2672, step time: 1.0361\n",
      "176/295, train_loss: 0.3796, step time: 1.0355\n",
      "177/295, train_loss: 0.1159, step time: 1.0354\n",
      "178/295, train_loss: 0.0673, step time: 1.0390\n",
      "179/295, train_loss: 0.0567, step time: 1.1202\n",
      "180/295, train_loss: 0.0974, step time: 1.0385\n",
      "181/295, train_loss: 0.0821, step time: 1.0375\n",
      "182/295, train_loss: 0.1329, step time: 1.0383\n",
      "183/295, train_loss: 0.0382, step time: 1.0618\n",
      "184/295, train_loss: 0.0585, step time: 1.0381\n",
      "185/295, train_loss: 0.0360, step time: 1.0961\n",
      "186/295, train_loss: 0.0363, step time: 1.0450\n",
      "187/295, train_loss: 0.0277, step time: 1.0496\n",
      "188/295, train_loss: 0.0340, step time: 1.0343\n",
      "189/295, train_loss: 0.1164, step time: 1.0343\n",
      "190/295, train_loss: 0.0634, step time: 1.0454\n",
      "191/295, train_loss: 0.1104, step time: 1.0626\n",
      "192/295, train_loss: 0.0681, step time: 1.0616\n",
      "193/295, train_loss: 0.0500, step time: 1.0401\n",
      "194/295, train_loss: 0.0309, step time: 1.0386\n",
      "195/295, train_loss: 0.0478, step time: 1.0437\n",
      "196/295, train_loss: 0.0274, step time: 1.0584\n",
      "197/295, train_loss: 0.0446, step time: 1.0361\n",
      "198/295, train_loss: 0.3882, step time: 1.0368\n",
      "199/295, train_loss: 0.0618, step time: 1.0427\n",
      "200/295, train_loss: 0.0500, step time: 1.1016\n",
      "201/295, train_loss: 0.0346, step time: 1.0890\n",
      "202/295, train_loss: 0.1139, step time: 1.0408\n",
      "203/295, train_loss: 0.1019, step time: 1.0435\n",
      "204/295, train_loss: 0.0891, step time: 1.0390\n",
      "205/295, train_loss: 0.0605, step time: 1.0366\n",
      "206/295, train_loss: 0.0385, step time: 1.0364\n",
      "207/295, train_loss: 0.0329, step time: 1.0414\n",
      "208/295, train_loss: 0.0522, step time: 1.0417\n",
      "209/295, train_loss: 0.3687, step time: 1.0348\n",
      "210/295, train_loss: 0.1011, step time: 1.0510\n",
      "211/295, train_loss: 0.0592, step time: 1.0467\n",
      "212/295, train_loss: 0.0934, step time: 1.0330\n",
      "213/295, train_loss: 0.0314, step time: 1.0421\n",
      "214/295, train_loss: 0.0364, step time: 1.0330\n",
      "215/295, train_loss: 0.0581, step time: 1.0387\n",
      "216/295, train_loss: 0.0314, step time: 1.0379\n",
      "217/295, train_loss: 0.1130, step time: 1.0349\n",
      "218/295, train_loss: 0.0756, step time: 1.0354\n",
      "219/295, train_loss: 0.0318, step time: 1.0345\n",
      "220/295, train_loss: 0.3840, step time: 1.0630\n",
      "221/295, train_loss: 0.0899, step time: 1.0681\n",
      "222/295, train_loss: 0.0496, step time: 1.0452\n",
      "223/295, train_loss: 0.0758, step time: 1.0525\n",
      "224/295, train_loss: 0.0880, step time: 1.0431\n",
      "225/295, train_loss: 0.1231, step time: 1.0434\n",
      "226/295, train_loss: 0.0710, step time: 1.0309\n",
      "227/295, train_loss: 0.0672, step time: 1.0350\n",
      "228/295, train_loss: 0.0849, step time: 1.0358\n",
      "229/295, train_loss: 0.0416, step time: 1.0411\n",
      "230/295, train_loss: 0.0887, step time: 1.0434\n",
      "231/295, train_loss: 0.0398, step time: 1.0440\n",
      "232/295, train_loss: 0.0339, step time: 1.0453\n",
      "233/295, train_loss: 0.0408, step time: 1.0358\n",
      "234/295, train_loss: 0.1267, step time: 1.0368\n",
      "235/295, train_loss: 0.2031, step time: 1.0455\n",
      "236/295, train_loss: 0.1491, step time: 1.0558\n",
      "237/295, train_loss: 0.0553, step time: 1.0387\n",
      "238/295, train_loss: 0.0656, step time: 1.0462\n",
      "239/295, train_loss: 0.0747, step time: 1.0888\n",
      "240/295, train_loss: 0.0559, step time: 1.0440\n",
      "241/295, train_loss: 0.0934, step time: 1.0570\n",
      "242/295, train_loss: 0.0589, step time: 1.0451\n",
      "243/295, train_loss: 0.0496, step time: 1.0550\n",
      "244/295, train_loss: 0.0365, step time: 1.0457\n",
      "245/295, train_loss: 0.0797, step time: 1.0695\n",
      "246/295, train_loss: 0.0677, step time: 1.0372\n",
      "247/295, train_loss: 0.1177, step time: 1.0375\n",
      "248/295, train_loss: 0.2469, step time: 1.0381\n",
      "249/295, train_loss: 0.0335, step time: 1.0397\n",
      "250/295, train_loss: 0.0598, step time: 1.0521\n",
      "251/295, train_loss: 0.0920, step time: 1.0488\n",
      "252/295, train_loss: 0.1067, step time: 1.0387\n",
      "253/295, train_loss: 0.0598, step time: 1.0478\n",
      "254/295, train_loss: 0.4022, step time: 1.0325\n",
      "255/295, train_loss: 0.0361, step time: 1.0392\n",
      "256/295, train_loss: 0.0706, step time: 1.0728\n",
      "257/295, train_loss: 0.0670, step time: 1.0387\n",
      "258/295, train_loss: 0.0529, step time: 1.0531\n",
      "259/295, train_loss: 0.1159, step time: 1.0420\n",
      "260/295, train_loss: 0.0639, step time: 1.0485\n",
      "261/295, train_loss: 0.0955, step time: 1.0735\n",
      "262/295, train_loss: 0.0784, step time: 1.0373\n",
      "263/295, train_loss: 0.0416, step time: 1.0561\n",
      "264/295, train_loss: 0.0381, step time: 1.1217\n",
      "265/295, train_loss: 0.0790, step time: 1.0366\n",
      "266/295, train_loss: 0.0534, step time: 1.0419\n",
      "267/295, train_loss: 0.1542, step time: 1.0455\n",
      "268/295, train_loss: 0.3592, step time: 1.0381\n",
      "269/295, train_loss: 0.0586, step time: 1.0691\n",
      "270/295, train_loss: 0.1318, step time: 1.1260\n",
      "271/295, train_loss: 0.3908, step time: 1.0381\n",
      "272/295, train_loss: 0.0432, step time: 1.0398\n",
      "273/295, train_loss: 0.3882, step time: 1.0370\n",
      "274/295, train_loss: 0.0348, step time: 1.0414\n",
      "275/295, train_loss: 0.0489, step time: 1.0347\n",
      "276/295, train_loss: 0.0526, step time: 1.0394\n",
      "277/295, train_loss: 0.0543, step time: 1.0381\n",
      "278/295, train_loss: 0.0808, step time: 1.0418\n",
      "279/295, train_loss: 0.0704, step time: 1.0616\n",
      "280/295, train_loss: 0.1976, step time: 1.0558\n",
      "281/295, train_loss: 0.0406, step time: 1.0357\n",
      "282/295, train_loss: 0.0877, step time: 1.0581\n",
      "283/295, train_loss: 0.0558, step time: 1.0365\n",
      "284/295, train_loss: 0.0416, step time: 1.0463\n",
      "285/295, train_loss: 0.0302, step time: 1.0592\n",
      "286/295, train_loss: 0.0874, step time: 1.0453\n",
      "287/295, train_loss: 0.0819, step time: 1.0479\n",
      "288/295, train_loss: 0.0545, step time: 1.0349\n",
      "289/295, train_loss: 0.0612, step time: 1.0307\n",
      "290/295, train_loss: 0.3909, step time: 1.0294\n",
      "291/295, train_loss: 0.0440, step time: 1.0294\n",
      "292/295, train_loss: 0.1069, step time: 1.0298\n",
      "293/295, train_loss: 0.1142, step time: 1.0303\n",
      "294/295, train_loss: 0.1680, step time: 1.0298\n",
      "295/295, train_loss: 0.0767, step time: 1.0287\n",
      "epoch 58 average loss: 0.0960\n",
      "current epoch: 58 current mean dice: 0.7666 tc: 0.7259 wt: 0.8205 et: 0.7616\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 58 is: 382.0684\n",
      "----------\n",
      "epoch 59/100\n",
      "1/295, train_loss: 0.0816, step time: 1.0951\n",
      "2/295, train_loss: 0.0444, step time: 1.0698\n",
      "3/295, train_loss: 0.0628, step time: 1.0541\n",
      "4/295, train_loss: 0.0795, step time: 1.1263\n",
      "5/295, train_loss: 0.0785, step time: 1.0562\n",
      "6/295, train_loss: 0.0443, step time: 1.0500\n",
      "7/295, train_loss: 0.0636, step time: 1.0671\n",
      "8/295, train_loss: 0.0569, step time: 1.0439\n",
      "9/295, train_loss: 0.0305, step time: 1.0477\n",
      "10/295, train_loss: 0.0393, step time: 1.0397\n",
      "11/295, train_loss: 0.0318, step time: 1.0417\n",
      "12/295, train_loss: 0.0901, step time: 1.0727\n",
      "13/295, train_loss: 0.0809, step time: 1.0353\n",
      "14/295, train_loss: 0.0431, step time: 1.0425\n",
      "15/295, train_loss: 0.1027, step time: 1.0306\n",
      "16/295, train_loss: 0.3929, step time: 1.0380\n",
      "17/295, train_loss: 0.0814, step time: 1.0401\n",
      "18/295, train_loss: 0.1203, step time: 1.0531\n",
      "19/295, train_loss: 0.1665, step time: 1.0335\n",
      "20/295, train_loss: 0.0760, step time: 1.0418\n",
      "21/295, train_loss: 0.0439, step time: 1.0369\n",
      "22/295, train_loss: 0.0652, step time: 1.0394\n",
      "23/295, train_loss: 0.0526, step time: 1.0404\n",
      "24/295, train_loss: 0.3697, step time: 1.0355\n",
      "25/295, train_loss: 0.1100, step time: 1.0415\n",
      "26/295, train_loss: 0.0589, step time: 1.0700\n",
      "27/295, train_loss: 0.0957, step time: 1.0631\n",
      "28/295, train_loss: 0.0636, step time: 1.1151\n",
      "29/295, train_loss: 0.1661, step time: 1.0510\n",
      "30/295, train_loss: 0.1232, step time: 1.0373\n",
      "31/295, train_loss: 0.0312, step time: 1.0477\n",
      "32/295, train_loss: 0.0315, step time: 1.1001\n",
      "33/295, train_loss: 0.1049, step time: 1.0769\n",
      "34/295, train_loss: 0.0434, step time: 1.0470\n",
      "35/295, train_loss: 0.0522, step time: 1.0309\n",
      "36/295, train_loss: 0.1155, step time: 1.0427\n",
      "37/295, train_loss: 0.0471, step time: 1.0492\n",
      "38/295, train_loss: 0.0691, step time: 1.1065\n",
      "39/295, train_loss: 0.0463, step time: 1.0397\n",
      "40/295, train_loss: 0.0891, step time: 1.0592\n",
      "41/295, train_loss: 0.0492, step time: 1.0574\n",
      "42/295, train_loss: 0.3915, step time: 1.0365\n",
      "43/295, train_loss: 0.0483, step time: 1.0384\n",
      "44/295, train_loss: 0.0720, step time: 1.1619\n",
      "45/295, train_loss: 0.0757, step time: 1.0515\n",
      "46/295, train_loss: 0.3875, step time: 1.0869\n",
      "47/295, train_loss: 0.3848, step time: 1.0334\n",
      "48/295, train_loss: 0.0429, step time: 1.1205\n",
      "49/295, train_loss: 0.3741, step time: 1.0407\n",
      "50/295, train_loss: 0.0509, step time: 1.0325\n",
      "51/295, train_loss: 0.0600, step time: 1.1050\n",
      "52/295, train_loss: 0.0871, step time: 1.0778\n",
      "53/295, train_loss: 0.3932, step time: 1.0359\n",
      "54/295, train_loss: 0.0636, step time: 1.0400\n",
      "55/295, train_loss: 0.2648, step time: 1.0551\n",
      "56/295, train_loss: 0.1615, step time: 1.0487\n",
      "57/295, train_loss: 0.0638, step time: 1.0941\n",
      "58/295, train_loss: 0.1012, step time: 1.0332\n",
      "59/295, train_loss: 0.0749, step time: 1.0421\n",
      "60/295, train_loss: 0.1209, step time: 1.0564\n",
      "61/295, train_loss: 0.0858, step time: 1.0478\n",
      "62/295, train_loss: 0.0839, step time: 1.0432\n",
      "63/295, train_loss: 0.0409, step time: 1.0358\n",
      "64/295, train_loss: 0.0637, step time: 1.0576\n",
      "65/295, train_loss: 0.0434, step time: 1.0386\n",
      "66/295, train_loss: 0.0907, step time: 1.0676\n",
      "67/295, train_loss: 0.1356, step time: 1.0702\n",
      "68/295, train_loss: 0.0677, step time: 1.0309\n",
      "69/295, train_loss: 0.0530, step time: 1.0318\n",
      "70/295, train_loss: 0.0771, step time: 1.0374\n",
      "71/295, train_loss: 0.0639, step time: 1.0535\n",
      "72/295, train_loss: 0.0571, step time: 1.0553\n",
      "73/295, train_loss: 0.0711, step time: 1.1075\n",
      "74/295, train_loss: 0.0880, step time: 1.0591\n",
      "75/295, train_loss: 0.0938, step time: 1.0385\n",
      "76/295, train_loss: 0.3595, step time: 1.0532\n",
      "77/295, train_loss: 0.0634, step time: 1.0343\n",
      "78/295, train_loss: 0.1165, step time: 1.0404\n",
      "79/295, train_loss: 0.0467, step time: 1.0400\n",
      "80/295, train_loss: 0.3802, step time: 1.0527\n",
      "81/295, train_loss: 0.0867, step time: 1.0564\n",
      "82/295, train_loss: 0.0528, step time: 1.0345\n",
      "83/295, train_loss: 0.0565, step time: 1.0533\n",
      "84/295, train_loss: 0.0675, step time: 1.0753\n",
      "85/295, train_loss: 0.4070, step time: 1.0604\n",
      "86/295, train_loss: 0.0400, step time: 1.0870\n",
      "87/295, train_loss: 0.0300, step time: 1.0371\n",
      "88/295, train_loss: 0.1011, step time: 1.0316\n",
      "89/295, train_loss: 0.1069, step time: 1.0308\n",
      "90/295, train_loss: 0.0343, step time: 1.0467\n",
      "91/295, train_loss: 0.0444, step time: 1.0974\n",
      "92/295, train_loss: 0.0377, step time: 1.0509\n",
      "93/295, train_loss: 0.0360, step time: 1.0343\n",
      "94/295, train_loss: 0.0739, step time: 1.1696\n",
      "95/295, train_loss: 0.1149, step time: 1.0325\n",
      "96/295, train_loss: 0.0768, step time: 1.0316\n",
      "97/295, train_loss: 0.0970, step time: 1.0387\n",
      "98/295, train_loss: 0.1307, step time: 1.0890\n",
      "99/295, train_loss: 0.3864, step time: 1.0343\n",
      "100/295, train_loss: 0.0727, step time: 1.0615\n",
      "101/295, train_loss: 0.0828, step time: 1.0472\n",
      "102/295, train_loss: 0.0296, step time: 1.0690\n",
      "103/295, train_loss: 0.0776, step time: 1.0366\n",
      "104/295, train_loss: 0.0878, step time: 1.0448\n",
      "105/295, train_loss: 0.0444, step time: 1.0569\n",
      "106/295, train_loss: 0.0337, step time: 1.0496\n",
      "107/295, train_loss: 0.0502, step time: 1.0428\n",
      "108/295, train_loss: 0.0832, step time: 1.0481\n",
      "109/295, train_loss: 0.0341, step time: 1.0472\n",
      "110/295, train_loss: 0.0590, step time: 1.0402\n",
      "111/295, train_loss: 0.0476, step time: 1.0381\n",
      "112/295, train_loss: 0.0519, step time: 1.0477\n",
      "113/295, train_loss: 0.0340, step time: 1.0533\n",
      "114/295, train_loss: 0.0566, step time: 1.0661\n",
      "115/295, train_loss: 0.0306, step time: 1.0327\n",
      "116/295, train_loss: 0.0943, step time: 1.0792\n",
      "117/295, train_loss: 0.0459, step time: 1.0377\n",
      "118/295, train_loss: 0.1632, step time: 1.0417\n",
      "119/295, train_loss: 0.3894, step time: 1.0598\n",
      "120/295, train_loss: 0.0617, step time: 1.0357\n",
      "121/295, train_loss: 0.3650, step time: 1.0400\n",
      "122/295, train_loss: 0.0406, step time: 1.0362\n",
      "123/295, train_loss: 0.0640, step time: 1.0380\n",
      "124/295, train_loss: 0.0562, step time: 1.0738\n",
      "125/295, train_loss: 0.4082, step time: 1.0383\n",
      "126/295, train_loss: 0.0596, step time: 1.0356\n",
      "127/295, train_loss: 0.0762, step time: 1.0435\n",
      "128/295, train_loss: 0.0510, step time: 1.0483\n",
      "129/295, train_loss: 0.0268, step time: 1.0559\n",
      "130/295, train_loss: 0.0489, step time: 1.0376\n",
      "131/295, train_loss: 0.0449, step time: 1.0347\n",
      "132/295, train_loss: 0.3866, step time: 1.0522\n",
      "133/295, train_loss: 0.0490, step time: 1.0952\n",
      "134/295, train_loss: 0.0682, step time: 1.0517\n",
      "135/295, train_loss: 0.0534, step time: 1.0416\n",
      "136/295, train_loss: 0.0566, step time: 1.0449\n",
      "137/295, train_loss: 0.0743, step time: 1.0648\n",
      "138/295, train_loss: 0.0326, step time: 1.0495\n",
      "139/295, train_loss: 0.0724, step time: 1.0520\n",
      "140/295, train_loss: 0.0293, step time: 1.0759\n",
      "141/295, train_loss: 0.1136, step time: 1.0938\n",
      "142/295, train_loss: 0.0874, step time: 1.0429\n",
      "143/295, train_loss: 0.1345, step time: 1.0998\n",
      "144/295, train_loss: 0.0589, step time: 1.0334\n",
      "145/295, train_loss: 0.0645, step time: 1.0625\n",
      "146/295, train_loss: 0.0824, step time: 1.0837\n",
      "147/295, train_loss: 0.0870, step time: 1.0390\n",
      "148/295, train_loss: 0.1007, step time: 1.0407\n",
      "149/295, train_loss: 0.0473, step time: 1.0474\n",
      "150/295, train_loss: 0.0371, step time: 1.0473\n",
      "151/295, train_loss: 0.0535, step time: 1.1249\n",
      "152/295, train_loss: 0.0358, step time: 1.0908\n",
      "153/295, train_loss: 0.0396, step time: 1.1094\n",
      "154/295, train_loss: 0.0988, step time: 1.0355\n",
      "155/295, train_loss: 0.0830, step time: 1.0382\n",
      "156/295, train_loss: 0.0375, step time: 1.0742\n",
      "157/295, train_loss: 0.3849, step time: 1.0381\n",
      "158/295, train_loss: 0.0326, step time: 1.1013\n",
      "159/295, train_loss: 0.0428, step time: 1.0453\n",
      "160/295, train_loss: 0.0536, step time: 1.0558\n",
      "161/295, train_loss: 0.0337, step time: 1.0367\n",
      "162/295, train_loss: 0.0862, step time: 1.0446\n",
      "163/295, train_loss: 0.0374, step time: 1.0687\n",
      "164/295, train_loss: 0.0429, step time: 1.0661\n",
      "165/295, train_loss: 0.0677, step time: 1.0459\n",
      "166/295, train_loss: 0.0588, step time: 1.0469\n",
      "167/295, train_loss: 0.0488, step time: 1.0359\n",
      "168/295, train_loss: 0.0617, step time: 1.0668\n",
      "169/295, train_loss: 0.0293, step time: 1.0490\n",
      "170/295, train_loss: 0.0257, step time: 1.0566\n",
      "171/295, train_loss: 0.4628, step time: 1.0384\n",
      "172/295, train_loss: 0.1099, step time: 1.0456\n",
      "173/295, train_loss: 0.0325, step time: 1.0463\n",
      "174/295, train_loss: 0.0390, step time: 1.0356\n",
      "175/295, train_loss: 0.0557, step time: 1.0392\n",
      "176/295, train_loss: 0.0972, step time: 1.0392\n",
      "177/295, train_loss: 0.0927, step time: 1.0517\n",
      "178/295, train_loss: 0.0430, step time: 1.0368\n",
      "179/295, train_loss: 0.0854, step time: 1.0459\n",
      "180/295, train_loss: 0.1124, step time: 1.0464\n",
      "181/295, train_loss: 0.1589, step time: 1.0701\n",
      "182/295, train_loss: 0.0391, step time: 1.0629\n",
      "183/295, train_loss: 0.0621, step time: 1.0638\n",
      "184/295, train_loss: 0.0359, step time: 1.0566\n",
      "185/295, train_loss: 0.0655, step time: 1.0388\n",
      "186/295, train_loss: 0.1140, step time: 1.0471\n",
      "187/295, train_loss: 0.0675, step time: 1.0671\n",
      "188/295, train_loss: 0.1198, step time: 1.0596\n",
      "189/295, train_loss: 0.0604, step time: 1.0822\n",
      "190/295, train_loss: 0.0575, step time: 1.0394\n",
      "191/295, train_loss: 0.0885, step time: 1.0329\n",
      "192/295, train_loss: 0.0729, step time: 1.0394\n",
      "193/295, train_loss: 0.0599, step time: 1.0550\n",
      "194/295, train_loss: 0.1427, step time: 1.0434\n",
      "195/295, train_loss: 0.0472, step time: 1.0650\n",
      "196/295, train_loss: 0.0324, step time: 1.0387\n",
      "197/295, train_loss: 0.0582, step time: 1.0491\n",
      "198/295, train_loss: 0.0404, step time: 1.0452\n",
      "199/295, train_loss: 0.0599, step time: 1.0408\n",
      "200/295, train_loss: 0.0848, step time: 1.0359\n",
      "201/295, train_loss: 0.0883, step time: 1.0423\n",
      "202/295, train_loss: 0.0451, step time: 1.0471\n",
      "203/295, train_loss: 0.0789, step time: 1.1176\n",
      "204/295, train_loss: 0.0278, step time: 1.0335\n",
      "205/295, train_loss: 0.0320, step time: 1.0373\n",
      "206/295, train_loss: 0.0910, step time: 1.0784\n",
      "207/295, train_loss: 0.0781, step time: 1.0447\n",
      "208/295, train_loss: 0.0605, step time: 1.0407\n",
      "209/295, train_loss: 0.0717, step time: 1.0414\n",
      "210/295, train_loss: 0.0752, step time: 1.0708\n",
      "211/295, train_loss: 0.1648, step time: 1.0384\n",
      "212/295, train_loss: 0.0364, step time: 1.0366\n",
      "213/295, train_loss: 0.0998, step time: 1.0498\n",
      "214/295, train_loss: 0.0645, step time: 1.0346\n",
      "215/295, train_loss: 0.1174, step time: 1.0711\n",
      "216/295, train_loss: 0.0735, step time: 1.0328\n",
      "217/295, train_loss: 0.3679, step time: 1.0744\n",
      "218/295, train_loss: 0.0683, step time: 1.0494\n",
      "219/295, train_loss: 0.0558, step time: 1.0701\n",
      "220/295, train_loss: 0.0312, step time: 1.0416\n",
      "221/295, train_loss: 0.0542, step time: 1.0422\n",
      "222/295, train_loss: 0.0654, step time: 1.0562\n",
      "223/295, train_loss: 0.0745, step time: 1.0902\n",
      "224/295, train_loss: 0.0558, step time: 1.0335\n",
      "225/295, train_loss: 0.0738, step time: 1.0389\n",
      "226/295, train_loss: 0.0514, step time: 1.0498\n",
      "227/295, train_loss: 0.1038, step time: 1.0377\n",
      "228/295, train_loss: 0.0410, step time: 1.0361\n",
      "229/295, train_loss: 0.0287, step time: 1.0472\n",
      "230/295, train_loss: 0.3892, step time: 1.0941\n",
      "231/295, train_loss: 0.1233, step time: 1.0595\n",
      "232/295, train_loss: 0.0303, step time: 1.0431\n",
      "233/295, train_loss: 0.1181, step time: 1.0621\n",
      "234/295, train_loss: 0.0990, step time: 1.0624\n",
      "235/295, train_loss: 0.0476, step time: 1.0351\n",
      "236/295, train_loss: 0.0363, step time: 1.0363\n",
      "237/295, train_loss: 0.0845, step time: 1.0416\n",
      "238/295, train_loss: 0.0842, step time: 1.0541\n",
      "239/295, train_loss: 0.0637, step time: 1.0624\n",
      "240/295, train_loss: 0.0576, step time: 1.0720\n",
      "241/295, train_loss: 0.0742, step time: 1.0440\n",
      "242/295, train_loss: 0.0991, step time: 1.0626\n",
      "243/295, train_loss: 0.0416, step time: 1.0341\n",
      "244/295, train_loss: 0.0338, step time: 1.0353\n",
      "245/295, train_loss: 0.1124, step time: 1.0617\n",
      "246/295, train_loss: 0.0491, step time: 1.0454\n",
      "247/295, train_loss: 0.0728, step time: 1.0354\n",
      "248/295, train_loss: 0.0418, step time: 1.0468\n",
      "249/295, train_loss: 0.0858, step time: 1.0403\n",
      "250/295, train_loss: 0.0848, step time: 1.0518\n",
      "251/295, train_loss: 0.0422, step time: 1.0489\n",
      "252/295, train_loss: 0.0329, step time: 1.0341\n",
      "253/295, train_loss: 0.1418, step time: 1.0367\n",
      "254/295, train_loss: 0.1065, step time: 1.0675\n",
      "255/295, train_loss: 0.0544, step time: 1.0581\n",
      "256/295, train_loss: 0.0522, step time: 1.1342\n",
      "257/295, train_loss: 0.1056, step time: 1.0760\n",
      "258/295, train_loss: 0.1016, step time: 1.0424\n",
      "259/295, train_loss: 0.3691, step time: 1.0436\n",
      "260/295, train_loss: 0.0334, step time: 1.0385\n",
      "261/295, train_loss: 0.1071, step time: 1.0552\n",
      "262/295, train_loss: 0.1046, step time: 1.0558\n",
      "263/295, train_loss: 0.0886, step time: 1.0340\n",
      "264/295, train_loss: 0.0608, step time: 1.0687\n",
      "265/295, train_loss: 0.1157, step time: 1.0432\n",
      "266/295, train_loss: 0.0444, step time: 1.0472\n",
      "267/295, train_loss: 0.0885, step time: 1.0488\n",
      "268/295, train_loss: 0.0267, step time: 1.0434\n",
      "269/295, train_loss: 0.4099, step time: 1.0666\n",
      "270/295, train_loss: 0.2268, step time: 1.0361\n",
      "271/295, train_loss: 0.1049, step time: 1.0639\n",
      "272/295, train_loss: 0.1326, step time: 1.0572\n",
      "273/295, train_loss: 0.0515, step time: 1.0396\n",
      "274/295, train_loss: 0.0372, step time: 1.0397\n",
      "275/295, train_loss: 0.1129, step time: 1.0420\n",
      "276/295, train_loss: 0.1422, step time: 1.0492\n",
      "277/295, train_loss: 0.0758, step time: 1.0380\n",
      "278/295, train_loss: 0.0460, step time: 1.0462\n",
      "279/295, train_loss: 0.0415, step time: 1.0607\n",
      "280/295, train_loss: 0.1117, step time: 1.0772\n",
      "281/295, train_loss: 0.0607, step time: 1.0571\n",
      "282/295, train_loss: 0.0279, step time: 1.0466\n",
      "283/295, train_loss: 0.0332, step time: 1.0488\n",
      "284/295, train_loss: 0.1154, step time: 1.1203\n",
      "285/295, train_loss: 0.0413, step time: 1.0325\n",
      "286/295, train_loss: 0.0646, step time: 1.0367\n",
      "287/295, train_loss: 0.1102, step time: 1.0351\n",
      "288/295, train_loss: 0.0569, step time: 1.0308\n",
      "289/295, train_loss: 0.0830, step time: 1.0298\n",
      "290/295, train_loss: 0.0603, step time: 1.0306\n",
      "291/295, train_loss: 0.0817, step time: 1.0289\n",
      "292/295, train_loss: 0.3682, step time: 1.0296\n",
      "293/295, train_loss: 0.1566, step time: 1.0302\n",
      "294/295, train_loss: 0.1330, step time: 1.0315\n",
      "295/295, train_loss: 0.0904, step time: 1.0297\n",
      "epoch 59 average loss: 0.0952\n",
      "current epoch: 59 current mean dice: 0.8004 tc: 0.7617 wt: 0.8588 et: 0.7863\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 59 is: 389.8751\n",
      "----------\n",
      "epoch 60/100\n",
      "1/295, train_loss: 0.0454, step time: 1.1538\n",
      "2/295, train_loss: 0.0689, step time: 1.0920\n",
      "3/295, train_loss: 0.0923, step time: 1.0982\n",
      "4/295, train_loss: 0.0367, step time: 1.0791\n",
      "5/295, train_loss: 0.1043, step time: 1.0542\n",
      "6/295, train_loss: 0.0349, step time: 1.0867\n",
      "7/295, train_loss: 0.0953, step time: 1.0438\n",
      "8/295, train_loss: 0.0671, step time: 1.0671\n",
      "9/295, train_loss: 0.0420, step time: 1.0321\n",
      "10/295, train_loss: 0.0855, step time: 1.0351\n",
      "11/295, train_loss: 0.0475, step time: 1.0766\n",
      "12/295, train_loss: 0.0726, step time: 1.0395\n",
      "13/295, train_loss: 0.0480, step time: 1.0396\n",
      "14/295, train_loss: 0.0353, step time: 1.0847\n",
      "15/295, train_loss: 0.0864, step time: 1.0744\n",
      "16/295, train_loss: 0.1386, step time: 1.0375\n",
      "17/295, train_loss: 0.0261, step time: 1.0578\n",
      "18/295, train_loss: 0.0745, step time: 1.0758\n",
      "19/295, train_loss: 0.0913, step time: 1.0369\n",
      "20/295, train_loss: 0.0617, step time: 1.0699\n",
      "21/295, train_loss: 0.0959, step time: 1.0404\n",
      "22/295, train_loss: 0.4073, step time: 1.0322\n",
      "23/295, train_loss: 0.1369, step time: 1.1081\n",
      "24/295, train_loss: 0.1273, step time: 1.0604\n",
      "25/295, train_loss: 0.1087, step time: 1.0366\n",
      "26/295, train_loss: 0.0634, step time: 1.0716\n",
      "27/295, train_loss: 0.0468, step time: 1.0476\n",
      "28/295, train_loss: 0.0313, step time: 1.0500\n",
      "29/295, train_loss: 0.0634, step time: 1.0537\n",
      "30/295, train_loss: 0.0647, step time: 1.0462\n",
      "31/295, train_loss: 0.2671, step time: 1.0364\n",
      "32/295, train_loss: 0.0537, step time: 1.1240\n",
      "33/295, train_loss: 0.0298, step time: 1.0492\n",
      "34/295, train_loss: 0.0884, step time: 1.0338\n",
      "35/295, train_loss: 0.0777, step time: 1.0317\n",
      "36/295, train_loss: 0.0454, step time: 1.0528\n",
      "37/295, train_loss: 0.1006, step time: 1.0398\n",
      "38/295, train_loss: 0.0937, step time: 1.0486\n",
      "39/295, train_loss: 0.0318, step time: 1.0494\n",
      "40/295, train_loss: 0.0698, step time: 1.0370\n",
      "41/295, train_loss: 0.0471, step time: 1.0359\n",
      "42/295, train_loss: 0.0877, step time: 1.0586\n",
      "43/295, train_loss: 0.0539, step time: 1.1066\n",
      "44/295, train_loss: 0.1212, step time: 1.0337\n",
      "45/295, train_loss: 0.1481, step time: 1.0347\n",
      "46/295, train_loss: 0.0626, step time: 1.0457\n",
      "47/295, train_loss: 0.0937, step time: 1.1134\n",
      "48/295, train_loss: 0.0400, step time: 1.0484\n",
      "49/295, train_loss: 0.4156, step time: 1.0515\n",
      "50/295, train_loss: 0.0566, step time: 1.0406\n",
      "51/295, train_loss: 0.0351, step time: 1.0421\n",
      "52/295, train_loss: 0.0312, step time: 1.0707\n",
      "53/295, train_loss: 0.0396, step time: 1.1330\n",
      "54/295, train_loss: 0.0737, step time: 1.0386\n",
      "55/295, train_loss: 0.1247, step time: 1.0397\n",
      "56/295, train_loss: 0.0574, step time: 1.0578\n",
      "57/295, train_loss: 0.0526, step time: 1.1177\n",
      "58/295, train_loss: 0.0514, step time: 1.0738\n",
      "59/295, train_loss: 0.3869, step time: 1.0724\n",
      "60/295, train_loss: 0.0813, step time: 1.0405\n",
      "61/295, train_loss: 0.1009, step time: 1.0714\n",
      "62/295, train_loss: 0.0825, step time: 1.0813\n",
      "63/295, train_loss: 0.0551, step time: 1.0386\n",
      "64/295, train_loss: 0.0396, step time: 1.0822\n",
      "65/295, train_loss: 0.0617, step time: 1.0359\n",
      "66/295, train_loss: 0.3911, step time: 1.0385\n",
      "67/295, train_loss: 0.0593, step time: 1.0554\n",
      "68/295, train_loss: 0.2253, step time: 1.0502\n",
      "69/295, train_loss: 0.0559, step time: 1.0634\n",
      "70/295, train_loss: 0.3850, step time: 1.0495\n",
      "71/295, train_loss: 0.0548, step time: 1.0380\n",
      "72/295, train_loss: 0.0351, step time: 1.0755\n",
      "73/295, train_loss: 0.0421, step time: 1.0864\n",
      "74/295, train_loss: 0.0452, step time: 1.0346\n",
      "75/295, train_loss: 0.0531, step time: 1.0382\n",
      "76/295, train_loss: 0.0848, step time: 1.0402\n",
      "77/295, train_loss: 0.0353, step time: 1.0361\n",
      "78/295, train_loss: 0.3994, step time: 1.1166\n",
      "79/295, train_loss: 0.0757, step time: 1.0362\n",
      "80/295, train_loss: 0.0465, step time: 1.0548\n",
      "81/295, train_loss: 0.0461, step time: 1.0452\n",
      "82/295, train_loss: 0.0356, step time: 1.0441\n",
      "83/295, train_loss: 0.0423, step time: 1.0689\n",
      "84/295, train_loss: 0.0890, step time: 1.0504\n",
      "85/295, train_loss: 0.0604, step time: 1.0371\n",
      "86/295, train_loss: 0.1058, step time: 1.0563\n",
      "87/295, train_loss: 0.0445, step time: 1.0406\n",
      "88/295, train_loss: 0.0412, step time: 1.0393\n",
      "89/295, train_loss: 0.0541, step time: 1.0826\n",
      "90/295, train_loss: 0.0723, step time: 1.0698\n",
      "91/295, train_loss: 0.0328, step time: 1.0827\n",
      "92/295, train_loss: 0.0656, step time: 1.0440\n",
      "93/295, train_loss: 0.1054, step time: 1.0335\n",
      "94/295, train_loss: 0.0625, step time: 1.0485\n",
      "95/295, train_loss: 0.0675, step time: 1.0350\n",
      "96/295, train_loss: 0.1035, step time: 1.0429\n",
      "97/295, train_loss: 0.0435, step time: 1.0594\n",
      "98/295, train_loss: 0.0520, step time: 1.0533\n",
      "99/295, train_loss: 0.0384, step time: 1.0555\n",
      "100/295, train_loss: 0.1007, step time: 1.0534\n",
      "101/295, train_loss: 0.3873, step time: 1.0359\n",
      "102/295, train_loss: 0.1180, step time: 1.0402\n",
      "103/295, train_loss: 0.0389, step time: 1.0614\n",
      "104/295, train_loss: 0.0747, step time: 1.1159\n",
      "105/295, train_loss: 0.0325, step time: 1.0385\n",
      "106/295, train_loss: 0.0356, step time: 1.0378\n",
      "107/295, train_loss: 0.0760, step time: 1.0426\n",
      "108/295, train_loss: 0.0940, step time: 1.0320\n",
      "109/295, train_loss: 0.0281, step time: 1.0381\n",
      "110/295, train_loss: 0.0662, step time: 1.0512\n",
      "111/295, train_loss: 0.0588, step time: 1.0886\n",
      "112/295, train_loss: 0.0725, step time: 1.0624\n",
      "113/295, train_loss: 0.0824, step time: 1.0559\n",
      "114/295, train_loss: 0.0939, step time: 1.0431\n",
      "115/295, train_loss: 0.0342, step time: 1.0647\n",
      "116/295, train_loss: 0.1125, step time: 1.0477\n",
      "117/295, train_loss: 0.0516, step time: 1.0375\n",
      "118/295, train_loss: 0.0963, step time: 1.0408\n",
      "119/295, train_loss: 0.1030, step time: 1.0431\n",
      "120/295, train_loss: 0.0620, step time: 1.0449\n",
      "121/295, train_loss: 0.1160, step time: 1.0383\n",
      "122/295, train_loss: 0.0870, step time: 1.0349\n",
      "123/295, train_loss: 0.0684, step time: 1.0330\n",
      "124/295, train_loss: 0.0306, step time: 1.0417\n",
      "125/295, train_loss: 0.0432, step time: 1.0576\n",
      "126/295, train_loss: 0.0521, step time: 1.0355\n",
      "127/295, train_loss: 0.1064, step time: 1.0349\n",
      "128/295, train_loss: 0.0650, step time: 1.0515\n",
      "129/295, train_loss: 0.0584, step time: 1.1238\n",
      "130/295, train_loss: 0.0279, step time: 1.0513\n",
      "131/295, train_loss: 0.0759, step time: 1.0596\n",
      "132/295, train_loss: 0.0327, step time: 1.0735\n",
      "133/295, train_loss: 0.0580, step time: 1.0420\n",
      "134/295, train_loss: 0.0839, step time: 1.0422\n",
      "135/295, train_loss: 0.0912, step time: 1.0324\n",
      "136/295, train_loss: 0.0336, step time: 1.0372\n",
      "137/295, train_loss: 0.3749, step time: 1.0445\n",
      "138/295, train_loss: 0.4642, step time: 1.0681\n",
      "139/295, train_loss: 0.0844, step time: 1.0377\n",
      "140/295, train_loss: 0.0309, step time: 1.0553\n",
      "141/295, train_loss: 0.1118, step time: 1.0853\n",
      "142/295, train_loss: 0.0860, step time: 1.0721\n",
      "143/295, train_loss: 0.0447, step time: 1.0729\n",
      "144/295, train_loss: 0.1217, step time: 1.0368\n",
      "145/295, train_loss: 0.0274, step time: 1.0423\n",
      "146/295, train_loss: 0.0616, step time: 1.1537\n",
      "147/295, train_loss: 0.0895, step time: 1.0367\n",
      "148/295, train_loss: 0.0415, step time: 1.0481\n",
      "149/295, train_loss: 0.0472, step time: 1.0550\n",
      "150/295, train_loss: 0.0518, step time: 1.0630\n",
      "151/295, train_loss: 0.0320, step time: 1.0382\n",
      "152/295, train_loss: 0.3743, step time: 1.0393\n",
      "153/295, train_loss: 0.0483, step time: 1.0419\n",
      "154/295, train_loss: 0.3742, step time: 1.0394\n",
      "155/295, train_loss: 0.1337, step time: 1.0444\n",
      "156/295, train_loss: 0.1305, step time: 1.0413\n",
      "157/295, train_loss: 0.1104, step time: 1.0809\n",
      "158/295, train_loss: 0.0536, step time: 1.0574\n",
      "159/295, train_loss: 0.1527, step time: 1.0406\n",
      "160/295, train_loss: 0.0373, step time: 1.0697\n",
      "161/295, train_loss: 0.0766, step time: 1.0414\n",
      "162/295, train_loss: 0.0920, step time: 1.0430\n",
      "163/295, train_loss: 0.0751, step time: 1.0658\n",
      "164/295, train_loss: 0.0702, step time: 1.0647\n",
      "165/295, train_loss: 0.0840, step time: 1.0439\n",
      "166/295, train_loss: 0.0448, step time: 1.0378\n",
      "167/295, train_loss: 0.0245, step time: 1.0653\n",
      "168/295, train_loss: 0.0355, step time: 1.0791\n",
      "169/295, train_loss: 0.0988, step time: 1.0825\n",
      "170/295, train_loss: 0.0941, step time: 1.0635\n",
      "171/295, train_loss: 0.0542, step time: 1.0439\n",
      "172/295, train_loss: 0.0277, step time: 1.0763\n",
      "173/295, train_loss: 0.0867, step time: 1.0636\n",
      "174/295, train_loss: 0.0904, step time: 1.0417\n",
      "175/295, train_loss: 0.0970, step time: 1.0392\n",
      "176/295, train_loss: 0.0727, step time: 1.0545\n",
      "177/295, train_loss: 0.0497, step time: 1.0396\n",
      "178/295, train_loss: 0.0309, step time: 1.0471\n",
      "179/295, train_loss: 0.0580, step time: 1.0384\n",
      "180/295, train_loss: 0.1019, step time: 1.0322\n",
      "181/295, train_loss: 0.4086, step time: 1.0522\n",
      "182/295, train_loss: 0.3912, step time: 1.0374\n",
      "183/295, train_loss: 0.1025, step time: 1.0657\n",
      "184/295, train_loss: 0.0385, step time: 1.0390\n",
      "185/295, train_loss: 0.0761, step time: 1.0394\n",
      "186/295, train_loss: 0.1171, step time: 1.0490\n",
      "187/295, train_loss: 0.0403, step time: 1.0380\n",
      "188/295, train_loss: 0.3678, step time: 1.0336\n",
      "189/295, train_loss: 0.0438, step time: 1.0333\n",
      "190/295, train_loss: 0.0463, step time: 1.0348\n",
      "191/295, train_loss: 0.1172, step time: 1.0627\n",
      "192/295, train_loss: 0.0872, step time: 1.0439\n",
      "193/295, train_loss: 0.0646, step time: 1.0374\n",
      "194/295, train_loss: 0.0578, step time: 1.0503\n",
      "195/295, train_loss: 0.3599, step time: 1.0443\n",
      "196/295, train_loss: 0.0632, step time: 1.0557\n",
      "197/295, train_loss: 0.3786, step time: 1.0436\n",
      "198/295, train_loss: 0.1067, step time: 1.0443\n",
      "199/295, train_loss: 0.0287, step time: 1.0515\n",
      "200/295, train_loss: 0.0481, step time: 1.0606\n",
      "201/295, train_loss: 0.3683, step time: 1.0414\n",
      "202/295, train_loss: 0.0490, step time: 1.0404\n",
      "203/295, train_loss: 0.0822, step time: 1.0556\n",
      "204/295, train_loss: 0.0735, step time: 1.0478\n",
      "205/295, train_loss: 0.0855, step time: 1.0364\n",
      "206/295, train_loss: 0.0397, step time: 1.0495\n",
      "207/295, train_loss: 0.0843, step time: 1.0386\n",
      "208/295, train_loss: 0.0555, step time: 1.0387\n",
      "209/295, train_loss: 0.0292, step time: 1.0422\n",
      "210/295, train_loss: 0.0311, step time: 1.0321\n",
      "211/295, train_loss: 0.0597, step time: 1.0370\n",
      "212/295, train_loss: 0.0598, step time: 1.0360\n",
      "213/295, train_loss: 0.0649, step time: 1.0345\n",
      "214/295, train_loss: 0.0457, step time: 1.0633\n",
      "215/295, train_loss: 0.2105, step time: 1.0573\n",
      "216/295, train_loss: 0.3680, step time: 1.0360\n",
      "217/295, train_loss: 0.0945, step time: 1.0376\n",
      "218/295, train_loss: 0.0656, step time: 1.0596\n",
      "219/295, train_loss: 0.0472, step time: 1.0363\n",
      "220/295, train_loss: 0.0692, step time: 1.0499\n",
      "221/295, train_loss: 0.0933, step time: 1.0547\n",
      "222/295, train_loss: 0.0560, step time: 1.0349\n",
      "223/295, train_loss: 0.0834, step time: 1.0404\n",
      "224/295, train_loss: 0.0966, step time: 1.0387\n",
      "225/295, train_loss: 0.0679, step time: 1.0634\n",
      "226/295, train_loss: 0.0434, step time: 1.0473\n",
      "227/295, train_loss: 0.0589, step time: 1.1035\n",
      "228/295, train_loss: 0.0602, step time: 1.0362\n",
      "229/295, train_loss: 0.0378, step time: 1.0455\n",
      "230/295, train_loss: 0.0384, step time: 1.0541\n",
      "231/295, train_loss: 0.0786, step time: 1.0463\n",
      "232/295, train_loss: 0.0634, step time: 1.0397\n",
      "233/295, train_loss: 0.0234, step time: 1.0544\n",
      "234/295, train_loss: 0.1211, step time: 1.0502\n",
      "235/295, train_loss: 0.1983, step time: 1.0344\n",
      "236/295, train_loss: 0.0418, step time: 1.0420\n",
      "237/295, train_loss: 0.0688, step time: 1.0616\n",
      "238/295, train_loss: 0.0300, step time: 1.0389\n",
      "239/295, train_loss: 0.1146, step time: 1.0366\n",
      "240/295, train_loss: 0.0320, step time: 1.0424\n",
      "241/295, train_loss: 0.0541, step time: 1.0579\n",
      "242/295, train_loss: 0.1061, step time: 1.0394\n",
      "243/295, train_loss: 0.0836, step time: 1.0594\n",
      "244/295, train_loss: 0.0528, step time: 1.0340\n",
      "245/295, train_loss: 0.0340, step time: 1.0392\n",
      "246/295, train_loss: 0.0491, step time: 1.0646\n",
      "247/295, train_loss: 0.0646, step time: 1.0923\n",
      "248/295, train_loss: 0.0614, step time: 1.0316\n",
      "249/295, train_loss: 0.0861, step time: 1.0489\n",
      "250/295, train_loss: 0.0446, step time: 1.0414\n",
      "251/295, train_loss: 0.0479, step time: 1.0460\n",
      "252/295, train_loss: 0.0789, step time: 1.0747\n",
      "253/295, train_loss: 0.0627, step time: 1.0463\n",
      "254/295, train_loss: 0.0873, step time: 1.1039\n",
      "255/295, train_loss: 0.0586, step time: 1.0646\n",
      "256/295, train_loss: 0.0520, step time: 1.0915\n",
      "257/295, train_loss: 0.0668, step time: 1.0356\n",
      "258/295, train_loss: 0.0571, step time: 1.0632\n",
      "259/295, train_loss: 0.1361, step time: 1.0442\n",
      "260/295, train_loss: 0.3713, step time: 1.0464\n",
      "261/295, train_loss: 0.0611, step time: 1.0466\n",
      "262/295, train_loss: 0.1021, step time: 1.0360\n",
      "263/295, train_loss: 0.0681, step time: 1.0497\n",
      "264/295, train_loss: 0.0353, step time: 1.0910\n",
      "265/295, train_loss: 0.0910, step time: 1.0593\n",
      "266/295, train_loss: 0.0952, step time: 1.0324\n",
      "267/295, train_loss: 0.0567, step time: 1.0424\n",
      "268/295, train_loss: 0.0420, step time: 1.0395\n",
      "269/295, train_loss: 0.0418, step time: 1.0392\n",
      "270/295, train_loss: 0.0413, step time: 1.0348\n",
      "271/295, train_loss: 0.3900, step time: 1.0596\n",
      "272/295, train_loss: 0.0698, step time: 1.0396\n",
      "273/295, train_loss: 0.1482, step time: 1.0376\n",
      "274/295, train_loss: 0.3891, step time: 1.0400\n",
      "275/295, train_loss: 0.0554, step time: 1.0515\n",
      "276/295, train_loss: 0.0443, step time: 1.0392\n",
      "277/295, train_loss: 0.0555, step time: 1.1133\n",
      "278/295, train_loss: 0.0733, step time: 1.0356\n",
      "279/295, train_loss: 0.1018, step time: 1.0400\n",
      "280/295, train_loss: 0.0345, step time: 1.0569\n",
      "281/295, train_loss: 0.0598, step time: 1.0364\n",
      "282/295, train_loss: 0.0326, step time: 1.0421\n",
      "283/295, train_loss: 0.0584, step time: 1.0731\n",
      "284/295, train_loss: 0.1173, step time: 1.0640\n",
      "285/295, train_loss: 0.3884, step time: 1.0570\n",
      "286/295, train_loss: 0.0435, step time: 1.0433\n",
      "287/295, train_loss: 0.0749, step time: 1.0502\n",
      "288/295, train_loss: 0.0378, step time: 1.0648\n",
      "289/295, train_loss: 0.0710, step time: 1.0311\n",
      "290/295, train_loss: 0.0499, step time: 1.0298\n",
      "291/295, train_loss: 0.0842, step time: 1.0294\n",
      "292/295, train_loss: 0.0557, step time: 1.0300\n",
      "293/295, train_loss: 0.0287, step time: 1.0287\n",
      "294/295, train_loss: 0.0849, step time: 1.0315\n",
      "295/295, train_loss: 0.0470, step time: 1.0299\n",
      "epoch 60 average loss: 0.0930\n",
      "current epoch: 60 current mean dice: 0.7717 tc: 0.7325 wt: 0.8347 et: 0.7525\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 60 is: 387.7501\n",
      "----------\n",
      "epoch 61/100\n",
      "1/295, train_loss: 0.1001, step time: 1.1701\n",
      "2/295, train_loss: 0.0752, step time: 1.1610\n",
      "3/295, train_loss: 0.0469, step time: 1.2307\n",
      "4/295, train_loss: 0.0463, step time: 1.0576\n",
      "5/295, train_loss: 0.0311, step time: 1.0632\n",
      "6/295, train_loss: 0.0721, step time: 1.0721\n",
      "7/295, train_loss: 0.0333, step time: 1.0428\n",
      "8/295, train_loss: 0.3994, step time: 1.0536\n",
      "9/295, train_loss: 0.0549, step time: 1.0518\n",
      "10/295, train_loss: 0.1034, step time: 1.0304\n",
      "11/295, train_loss: 0.0716, step time: 1.0403\n",
      "12/295, train_loss: 0.0841, step time: 1.0408\n",
      "13/295, train_loss: 0.0751, step time: 1.0331\n",
      "14/295, train_loss: 0.0696, step time: 1.0632\n",
      "15/295, train_loss: 0.0891, step time: 1.0337\n",
      "16/295, train_loss: 0.0986, step time: 1.0306\n",
      "17/295, train_loss: 0.0683, step time: 1.0376\n",
      "18/295, train_loss: 0.0276, step time: 1.0387\n",
      "19/295, train_loss: 0.3885, step time: 1.0324\n",
      "20/295, train_loss: 0.0656, step time: 1.0303\n",
      "21/295, train_loss: 0.0716, step time: 1.0323\n",
      "22/295, train_loss: 0.0584, step time: 1.0761\n",
      "23/295, train_loss: 0.0539, step time: 1.0734\n",
      "24/295, train_loss: 0.0439, step time: 1.0397\n",
      "25/295, train_loss: 0.0308, step time: 1.0321\n",
      "26/295, train_loss: 0.1168, step time: 1.0414\n",
      "27/295, train_loss: 0.0860, step time: 1.0378\n",
      "28/295, train_loss: 0.0521, step time: 1.0481\n",
      "29/295, train_loss: 0.0400, step time: 1.1649\n",
      "30/295, train_loss: 0.0339, step time: 1.0646\n",
      "31/295, train_loss: 0.0334, step time: 1.0574\n",
      "32/295, train_loss: 0.0857, step time: 1.0394\n",
      "33/295, train_loss: 0.0301, step time: 1.0393\n",
      "34/295, train_loss: 0.0648, step time: 1.0408\n",
      "35/295, train_loss: 0.3844, step time: 1.0903\n",
      "36/295, train_loss: 0.0699, step time: 1.0410\n",
      "37/295, train_loss: 0.0880, step time: 1.0332\n",
      "38/295, train_loss: 0.0358, step time: 1.0353\n",
      "39/295, train_loss: 0.0654, step time: 1.0605\n",
      "40/295, train_loss: 0.0341, step time: 1.0394\n",
      "41/295, train_loss: 0.0624, step time: 1.0525\n",
      "42/295, train_loss: 0.0274, step time: 1.0550\n",
      "43/295, train_loss: 0.0871, step time: 1.0461\n",
      "44/295, train_loss: 0.0375, step time: 1.0403\n",
      "45/295, train_loss: 0.0856, step time: 1.1138\n",
      "46/295, train_loss: 0.0485, step time: 1.0398\n",
      "47/295, train_loss: 0.0827, step time: 1.0659\n",
      "48/295, train_loss: 0.0898, step time: 1.0408\n",
      "49/295, train_loss: 0.0958, step time: 1.0794\n",
      "50/295, train_loss: 0.0893, step time: 1.0406\n",
      "51/295, train_loss: 0.0272, step time: 1.0330\n",
      "52/295, train_loss: 0.0314, step time: 1.0341\n",
      "53/295, train_loss: 0.0323, step time: 1.0422\n",
      "54/295, train_loss: 0.3756, step time: 1.0320\n",
      "55/295, train_loss: 0.0803, step time: 1.0379\n",
      "56/295, train_loss: 0.0364, step time: 1.0539\n",
      "57/295, train_loss: 0.0578, step time: 1.0642\n",
      "58/295, train_loss: 0.0514, step time: 1.0482\n",
      "59/295, train_loss: 0.0460, step time: 1.0505\n",
      "60/295, train_loss: 0.0598, step time: 1.1112\n",
      "61/295, train_loss: 0.0447, step time: 1.0307\n",
      "62/295, train_loss: 0.0679, step time: 1.0402\n",
      "63/295, train_loss: 0.0301, step time: 1.0416\n",
      "64/295, train_loss: 0.0424, step time: 1.0333\n",
      "65/295, train_loss: 0.0516, step time: 1.0345\n",
      "66/295, train_loss: 0.0955, step time: 1.0318\n",
      "67/295, train_loss: 0.0748, step time: 1.0888\n",
      "68/295, train_loss: 0.0396, step time: 1.0333\n",
      "69/295, train_loss: 0.0668, step time: 1.0509\n",
      "70/295, train_loss: 0.0423, step time: 1.0644\n",
      "71/295, train_loss: 0.0858, step time: 1.0476\n",
      "72/295, train_loss: 0.0705, step time: 1.0638\n",
      "73/295, train_loss: 0.0582, step time: 1.0308\n",
      "74/295, train_loss: 0.0680, step time: 1.0420\n",
      "75/295, train_loss: 0.3845, step time: 1.0499\n",
      "76/295, train_loss: 0.0554, step time: 1.0382\n",
      "77/295, train_loss: 0.0475, step time: 1.0375\n",
      "78/295, train_loss: 0.0443, step time: 1.0577\n",
      "79/295, train_loss: 0.0472, step time: 1.0748\n",
      "80/295, train_loss: 0.0605, step time: 1.0371\n",
      "81/295, train_loss: 0.0988, step time: 1.0438\n",
      "82/295, train_loss: 0.3959, step time: 1.0407\n",
      "83/295, train_loss: 0.0471, step time: 1.0890\n",
      "84/295, train_loss: 0.1069, step time: 1.0356\n",
      "85/295, train_loss: 0.0728, step time: 1.0841\n",
      "86/295, train_loss: 0.0589, step time: 1.0341\n",
      "87/295, train_loss: 0.0311, step time: 1.0489\n",
      "88/295, train_loss: 0.0294, step time: 1.0792\n",
      "89/295, train_loss: 0.1025, step time: 1.0342\n",
      "90/295, train_loss: 0.0462, step time: 1.0564\n",
      "91/295, train_loss: 0.0548, step time: 1.0581\n",
      "92/295, train_loss: 0.0823, step time: 1.0385\n",
      "93/295, train_loss: 0.1083, step time: 1.1004\n",
      "94/295, train_loss: 0.0454, step time: 1.0428\n",
      "95/295, train_loss: 0.0360, step time: 1.0440\n",
      "96/295, train_loss: 0.1100, step time: 1.0551\n",
      "97/295, train_loss: 0.1040, step time: 1.0497\n",
      "98/295, train_loss: 0.0643, step time: 1.0465\n",
      "99/295, train_loss: 0.0447, step time: 1.0386\n",
      "100/295, train_loss: 0.0586, step time: 1.0346\n",
      "101/295, train_loss: 0.0427, step time: 1.0485\n",
      "102/295, train_loss: 0.3662, step time: 1.0771\n",
      "103/295, train_loss: 0.0723, step time: 1.0362\n",
      "104/295, train_loss: 0.0532, step time: 1.0357\n",
      "105/295, train_loss: 0.0360, step time: 1.0459\n",
      "106/295, train_loss: 0.1379, step time: 1.0620\n",
      "107/295, train_loss: 0.0612, step time: 1.0419\n",
      "108/295, train_loss: 0.0395, step time: 1.0639\n",
      "109/295, train_loss: 0.0536, step time: 1.0540\n",
      "110/295, train_loss: 0.0698, step time: 1.0724\n",
      "111/295, train_loss: 0.0322, step time: 1.0395\n",
      "112/295, train_loss: 0.0983, step time: 1.0382\n",
      "113/295, train_loss: 0.1171, step time: 1.0495\n",
      "114/295, train_loss: 0.0552, step time: 1.0389\n",
      "115/295, train_loss: 0.0776, step time: 1.0401\n",
      "116/295, train_loss: 0.0588, step time: 1.0491\n",
      "117/295, train_loss: 0.0265, step time: 1.0679\n",
      "118/295, train_loss: 0.0760, step time: 1.0369\n",
      "119/295, train_loss: 0.2378, step time: 1.0319\n",
      "120/295, train_loss: 0.0691, step time: 1.0348\n",
      "121/295, train_loss: 0.0398, step time: 1.1038\n",
      "122/295, train_loss: 0.0769, step time: 1.1353\n",
      "123/295, train_loss: 0.0288, step time: 1.0394\n",
      "124/295, train_loss: 0.0602, step time: 1.0630\n",
      "125/295, train_loss: 0.0558, step time: 1.0372\n",
      "126/295, train_loss: 0.4596, step time: 1.0596\n",
      "127/295, train_loss: 0.0833, step time: 1.0784\n",
      "128/295, train_loss: 0.1078, step time: 1.0593\n",
      "129/295, train_loss: 0.0434, step time: 1.0474\n",
      "130/295, train_loss: 0.0883, step time: 1.0426\n",
      "131/295, train_loss: 0.0433, step time: 1.0353\n",
      "132/295, train_loss: 0.3698, step time: 1.0363\n",
      "133/295, train_loss: 0.0813, step time: 1.1095\n",
      "134/295, train_loss: 0.3648, step time: 1.0549\n",
      "135/295, train_loss: 0.0848, step time: 1.0327\n",
      "136/295, train_loss: 0.0300, step time: 1.0580\n",
      "137/295, train_loss: 0.0513, step time: 1.0736\n",
      "138/295, train_loss: 0.0477, step time: 1.0621\n",
      "139/295, train_loss: 0.1025, step time: 1.0559\n",
      "140/295, train_loss: 0.0899, step time: 1.0718\n",
      "141/295, train_loss: 0.1389, step time: 1.0578\n",
      "142/295, train_loss: 0.0457, step time: 1.0672\n",
      "143/295, train_loss: 0.0758, step time: 1.0668\n",
      "144/295, train_loss: 0.0889, step time: 1.0728\n",
      "145/295, train_loss: 0.0388, step time: 1.0394\n",
      "146/295, train_loss: 0.0367, step time: 1.0322\n",
      "147/295, train_loss: 0.0734, step time: 1.0577\n",
      "148/295, train_loss: 0.3868, step time: 1.0342\n",
      "149/295, train_loss: 0.0336, step time: 1.0627\n",
      "150/295, train_loss: 0.3868, step time: 1.0441\n",
      "151/295, train_loss: 0.3916, step time: 1.0533\n",
      "152/295, train_loss: 0.0639, step time: 1.0318\n",
      "153/295, train_loss: 0.1163, step time: 1.0370\n",
      "154/295, train_loss: 0.0261, step time: 1.0437\n",
      "155/295, train_loss: 0.0579, step time: 1.0492\n",
      "156/295, train_loss: 0.1104, step time: 1.0548\n",
      "157/295, train_loss: 0.0660, step time: 1.0314\n",
      "158/295, train_loss: 0.1988, step time: 1.0785\n",
      "159/295, train_loss: 0.0712, step time: 1.0394\n",
      "160/295, train_loss: 0.0559, step time: 1.0401\n",
      "161/295, train_loss: 0.0553, step time: 1.1175\n",
      "162/295, train_loss: 0.1488, step time: 1.0401\n",
      "163/295, train_loss: 0.0982, step time: 1.0406\n",
      "164/295, train_loss: 0.0536, step time: 1.0567\n",
      "165/295, train_loss: 0.0442, step time: 1.0608\n",
      "166/295, train_loss: 0.1080, step time: 1.0387\n",
      "167/295, train_loss: 0.0452, step time: 1.0401\n",
      "168/295, train_loss: 0.0822, step time: 1.0422\n",
      "169/295, train_loss: 0.3705, step time: 1.1203\n",
      "170/295, train_loss: 0.0818, step time: 1.0720\n",
      "171/295, train_loss: 0.0867, step time: 1.0477\n",
      "172/295, train_loss: 0.0843, step time: 1.0718\n",
      "173/295, train_loss: 0.0501, step time: 1.0490\n",
      "174/295, train_loss: 0.0362, step time: 1.0361\n",
      "175/295, train_loss: 0.0927, step time: 1.1268\n",
      "176/295, train_loss: 0.1126, step time: 1.0358\n",
      "177/295, train_loss: 0.1458, step time: 1.0504\n",
      "178/295, train_loss: 0.0485, step time: 1.1873\n",
      "179/295, train_loss: 0.0207, step time: 1.0328\n",
      "180/295, train_loss: 0.0387, step time: 1.0445\n",
      "181/295, train_loss: 0.0340, step time: 1.0450\n",
      "182/295, train_loss: 0.0675, step time: 1.0659\n",
      "183/295, train_loss: 0.0749, step time: 1.0348\n",
      "184/295, train_loss: 0.0333, step time: 1.0477\n",
      "185/295, train_loss: 0.1275, step time: 1.0409\n",
      "186/295, train_loss: 0.0824, step time: 1.1296\n",
      "187/295, train_loss: 0.1107, step time: 1.0389\n",
      "188/295, train_loss: 0.1144, step time: 1.0416\n",
      "189/295, train_loss: 0.0971, step time: 1.0426\n",
      "190/295, train_loss: 0.0631, step time: 1.0362\n",
      "191/295, train_loss: 0.0419, step time: 1.0604\n",
      "192/295, train_loss: 0.0359, step time: 1.0520\n",
      "193/295, train_loss: 0.0651, step time: 1.1226\n",
      "194/295, train_loss: 0.0664, step time: 1.0632\n",
      "195/295, train_loss: 0.0528, step time: 1.0416\n",
      "196/295, train_loss: 0.2073, step time: 1.0341\n",
      "197/295, train_loss: 0.1051, step time: 1.0559\n",
      "198/295, train_loss: 0.0359, step time: 1.0346\n",
      "199/295, train_loss: 0.0810, step time: 1.0377\n",
      "200/295, train_loss: 0.0599, step time: 1.0376\n",
      "201/295, train_loss: 0.0467, step time: 1.0359\n",
      "202/295, train_loss: 0.0649, step time: 1.0375\n",
      "203/295, train_loss: 0.0406, step time: 1.0822\n",
      "204/295, train_loss: 0.0549, step time: 1.0374\n",
      "205/295, train_loss: 0.3720, step time: 1.0566\n",
      "206/295, train_loss: 0.0680, step time: 1.0384\n",
      "207/295, train_loss: 0.3901, step time: 1.0370\n",
      "208/295, train_loss: 0.0299, step time: 1.0556\n",
      "209/295, train_loss: 0.0816, step time: 1.0328\n",
      "210/295, train_loss: 0.0735, step time: 1.0559\n",
      "211/295, train_loss: 0.0352, step time: 1.0592\n",
      "212/295, train_loss: 0.0616, step time: 1.1200\n",
      "213/295, train_loss: 0.0521, step time: 1.0608\n",
      "214/295, train_loss: 0.0518, step time: 1.0466\n",
      "215/295, train_loss: 0.3654, step time: 1.0360\n",
      "216/295, train_loss: 0.0848, step time: 1.0980\n",
      "217/295, train_loss: 0.0330, step time: 1.0416\n",
      "218/295, train_loss: 0.0774, step time: 1.0632\n",
      "219/295, train_loss: 0.0876, step time: 1.0797\n",
      "220/295, train_loss: 0.0375, step time: 1.0404\n",
      "221/295, train_loss: 0.0601, step time: 1.0409\n",
      "222/295, train_loss: 0.1279, step time: 1.0596\n",
      "223/295, train_loss: 0.1111, step time: 1.0381\n",
      "224/295, train_loss: 0.0677, step time: 1.0421\n",
      "225/295, train_loss: 0.0620, step time: 1.0888\n",
      "226/295, train_loss: 0.0574, step time: 1.0405\n",
      "227/295, train_loss: 0.1225, step time: 1.0693\n",
      "228/295, train_loss: 0.0557, step time: 1.0824\n",
      "229/295, train_loss: 0.3584, step time: 1.1586\n",
      "230/295, train_loss: 0.0292, step time: 1.0513\n",
      "231/295, train_loss: 0.0614, step time: 1.0743\n",
      "232/295, train_loss: 0.0460, step time: 1.0444\n",
      "233/295, train_loss: 0.0628, step time: 1.0369\n",
      "234/295, train_loss: 0.0693, step time: 1.0423\n",
      "235/295, train_loss: 0.0413, step time: 1.0696\n",
      "236/295, train_loss: 0.0410, step time: 1.0429\n",
      "237/295, train_loss: 0.0374, step time: 1.0488\n",
      "238/295, train_loss: 0.0579, step time: 1.0688\n",
      "239/295, train_loss: 0.0528, step time: 1.0503\n",
      "240/295, train_loss: 0.1057, step time: 1.0658\n",
      "241/295, train_loss: 0.0964, step time: 1.0802\n",
      "242/295, train_loss: 0.0867, step time: 1.0400\n",
      "243/295, train_loss: 0.4064, step time: 1.0337\n",
      "244/295, train_loss: 0.0689, step time: 1.0355\n",
      "245/295, train_loss: 0.0778, step time: 1.0634\n",
      "246/295, train_loss: 0.0348, step time: 1.0407\n",
      "247/295, train_loss: 0.0454, step time: 1.1483\n",
      "248/295, train_loss: 0.0873, step time: 1.0527\n",
      "249/295, train_loss: 0.1241, step time: 1.0412\n",
      "250/295, train_loss: 0.4063, step time: 1.0378\n",
      "251/295, train_loss: 0.0552, step time: 1.0368\n",
      "252/295, train_loss: 0.0527, step time: 1.0363\n",
      "253/295, train_loss: 0.1320, step time: 1.0457\n",
      "254/295, train_loss: 0.0395, step time: 1.0374\n",
      "255/295, train_loss: 0.0838, step time: 1.0345\n",
      "256/295, train_loss: 0.0639, step time: 1.1329\n",
      "257/295, train_loss: 0.0416, step time: 1.0700\n",
      "258/295, train_loss: 0.0582, step time: 1.0396\n",
      "259/295, train_loss: 0.0541, step time: 1.0887\n",
      "260/295, train_loss: 0.1139, step time: 1.0433\n",
      "261/295, train_loss: 0.0876, step time: 1.0643\n",
      "262/295, train_loss: 0.0416, step time: 1.0535\n",
      "263/295, train_loss: 0.0771, step time: 1.0540\n",
      "264/295, train_loss: 0.0342, step time: 1.0430\n",
      "265/295, train_loss: 0.0763, step time: 1.0326\n",
      "266/295, train_loss: 0.0395, step time: 1.0670\n",
      "267/295, train_loss: 0.0378, step time: 1.0546\n",
      "268/295, train_loss: 0.0610, step time: 1.0585\n",
      "269/295, train_loss: 0.0473, step time: 1.0596\n",
      "270/295, train_loss: 0.0269, step time: 1.0405\n",
      "271/295, train_loss: 0.1465, step time: 1.0821\n",
      "272/295, train_loss: 0.0395, step time: 1.0494\n",
      "273/295, train_loss: 0.0311, step time: 1.0472\n",
      "274/295, train_loss: 0.0504, step time: 1.1187\n",
      "275/295, train_loss: 0.1020, step time: 1.0720\n",
      "276/295, train_loss: 0.3879, step time: 1.0786\n",
      "277/295, train_loss: 0.0459, step time: 1.0403\n",
      "278/295, train_loss: 0.0865, step time: 1.0576\n",
      "279/295, train_loss: 0.0858, step time: 1.0376\n",
      "280/295, train_loss: 0.0295, step time: 1.0406\n",
      "281/295, train_loss: 0.0631, step time: 1.0457\n",
      "282/295, train_loss: 0.0539, step time: 1.0351\n",
      "283/295, train_loss: 0.0424, step time: 1.0386\n",
      "284/295, train_loss: 0.0845, step time: 1.0424\n",
      "285/295, train_loss: 0.1227, step time: 1.0649\n",
      "286/295, train_loss: 0.1056, step time: 1.1297\n",
      "287/295, train_loss: 0.0409, step time: 1.0603\n",
      "288/295, train_loss: 0.1102, step time: 1.0319\n",
      "289/295, train_loss: 0.0908, step time: 1.0299\n",
      "290/295, train_loss: 0.0808, step time: 1.0300\n",
      "291/295, train_loss: 0.1032, step time: 1.0316\n",
      "292/295, train_loss: 0.0483, step time: 1.0301\n",
      "293/295, train_loss: 0.0917, step time: 1.0298\n",
      "294/295, train_loss: 0.0612, step time: 1.0300\n",
      "295/295, train_loss: 0.3748, step time: 1.0309\n",
      "epoch 61 average loss: 0.0913\n",
      "current epoch: 61 current mean dice: 0.7929 tc: 0.7549 wt: 0.8535 et: 0.7762\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 61 is: 393.3568\n",
      "----------\n",
      "epoch 62/100\n",
      "1/295, train_loss: 0.1060, step time: 1.0976\n",
      "2/295, train_loss: 0.3912, step time: 1.0359\n",
      "3/295, train_loss: 0.0581, step time: 1.0377\n",
      "4/295, train_loss: 0.0758, step time: 1.0845\n",
      "5/295, train_loss: 0.0553, step time: 1.1065\n",
      "6/295, train_loss: 0.0300, step time: 1.0629\n",
      "7/295, train_loss: 0.0890, step time: 1.0357\n",
      "8/295, train_loss: 0.0544, step time: 1.0364\n",
      "9/295, train_loss: 0.0287, step time: 1.0506\n",
      "10/295, train_loss: 0.0579, step time: 1.0512\n",
      "11/295, train_loss: 0.0557, step time: 1.0796\n",
      "12/295, train_loss: 0.0643, step time: 1.0646\n",
      "13/295, train_loss: 0.3803, step time: 1.0394\n",
      "14/295, train_loss: 0.0657, step time: 1.0433\n",
      "15/295, train_loss: 0.0621, step time: 1.0517\n",
      "16/295, train_loss: 0.0375, step time: 1.0706\n",
      "17/295, train_loss: 0.0294, step time: 1.0318\n",
      "18/295, train_loss: 0.0514, step time: 1.0347\n",
      "19/295, train_loss: 0.0454, step time: 1.0423\n",
      "20/295, train_loss: 0.0715, step time: 1.0384\n",
      "21/295, train_loss: 0.0408, step time: 1.0479\n",
      "22/295, train_loss: 0.0508, step time: 1.1254\n",
      "23/295, train_loss: 0.0545, step time: 1.0582\n",
      "24/295, train_loss: 0.1114, step time: 1.0566\n",
      "25/295, train_loss: 0.1581, step time: 1.0388\n",
      "26/295, train_loss: 0.0881, step time: 1.0435\n",
      "27/295, train_loss: 0.0907, step time: 1.0343\n",
      "28/295, train_loss: 0.0772, step time: 1.0388\n",
      "29/295, train_loss: 0.0378, step time: 1.0426\n",
      "30/295, train_loss: 0.0664, step time: 1.0362\n",
      "31/295, train_loss: 0.0602, step time: 1.0450\n",
      "32/295, train_loss: 0.0305, step time: 1.0506\n",
      "33/295, train_loss: 0.3676, step time: 1.0366\n",
      "34/295, train_loss: 0.0396, step time: 1.0574\n",
      "35/295, train_loss: 0.0551, step time: 1.0414\n",
      "36/295, train_loss: 0.0580, step time: 1.0309\n",
      "37/295, train_loss: 0.0720, step time: 1.0438\n",
      "38/295, train_loss: 0.0639, step time: 1.0327\n",
      "39/295, train_loss: 0.0703, step time: 1.0372\n",
      "40/295, train_loss: 0.0575, step time: 1.0308\n",
      "41/295, train_loss: 0.0545, step time: 1.0798\n",
      "42/295, train_loss: 0.0648, step time: 1.0330\n",
      "43/295, train_loss: 0.0802, step time: 1.0442\n",
      "44/295, train_loss: 0.1201, step time: 1.0531\n",
      "45/295, train_loss: 0.0585, step time: 1.0352\n",
      "46/295, train_loss: 0.0880, step time: 1.0607\n",
      "47/295, train_loss: 0.0481, step time: 1.0879\n",
      "48/295, train_loss: 0.0299, step time: 1.0437\n",
      "49/295, train_loss: 0.0867, step time: 1.0331\n",
      "50/295, train_loss: 0.0691, step time: 1.0324\n",
      "51/295, train_loss: 0.0566, step time: 1.0302\n",
      "52/295, train_loss: 0.0304, step time: 1.0411\n",
      "53/295, train_loss: 0.0655, step time: 1.0695\n",
      "54/295, train_loss: 0.0319, step time: 1.0996\n",
      "55/295, train_loss: 0.0351, step time: 1.0618\n",
      "56/295, train_loss: 0.0459, step time: 1.0444\n",
      "57/295, train_loss: 0.0487, step time: 1.1324\n",
      "58/295, train_loss: 0.4110, step time: 1.0503\n",
      "59/295, train_loss: 0.0384, step time: 1.0491\n",
      "60/295, train_loss: 0.1122, step time: 1.0371\n",
      "61/295, train_loss: 0.0902, step time: 1.0617\n",
      "62/295, train_loss: 0.0338, step time: 1.0404\n",
      "63/295, train_loss: 0.0484, step time: 1.0803\n",
      "64/295, train_loss: 0.0527, step time: 1.0388\n",
      "65/295, train_loss: 0.0898, step time: 1.0368\n",
      "66/295, train_loss: 0.0894, step time: 1.0463\n",
      "67/295, train_loss: 0.0421, step time: 1.0676\n",
      "68/295, train_loss: 0.0813, step time: 1.0494\n",
      "69/295, train_loss: 0.0750, step time: 1.0420\n",
      "70/295, train_loss: 0.0416, step time: 1.0332\n",
      "71/295, train_loss: 0.3931, step time: 1.0411\n",
      "72/295, train_loss: 0.0388, step time: 1.0341\n",
      "73/295, train_loss: 0.0587, step time: 1.0559\n",
      "74/295, train_loss: 0.3673, step time: 1.0865\n",
      "75/295, train_loss: 0.0800, step time: 1.1198\n",
      "76/295, train_loss: 0.0723, step time: 1.0372\n",
      "77/295, train_loss: 0.0863, step time: 1.0417\n",
      "78/295, train_loss: 0.0697, step time: 1.0391\n",
      "79/295, train_loss: 0.0770, step time: 1.1204\n",
      "80/295, train_loss: 0.0278, step time: 1.0460\n",
      "81/295, train_loss: 0.0573, step time: 1.0366\n",
      "82/295, train_loss: 0.0536, step time: 1.0340\n",
      "83/295, train_loss: 0.0418, step time: 1.0415\n",
      "84/295, train_loss: 0.0373, step time: 1.1201\n",
      "85/295, train_loss: 0.0480, step time: 1.0448\n",
      "86/295, train_loss: 0.4051, step time: 1.0560\n",
      "87/295, train_loss: 0.0406, step time: 1.0456\n",
      "88/295, train_loss: 0.0401, step time: 1.0577\n",
      "89/295, train_loss: 0.0494, step time: 1.0524\n",
      "90/295, train_loss: 0.3867, step time: 1.0429\n",
      "91/295, train_loss: 0.1168, step time: 1.1040\n",
      "92/295, train_loss: 0.0267, step time: 1.0349\n",
      "93/295, train_loss: 0.0643, step time: 1.0588\n",
      "94/295, train_loss: 0.0349, step time: 1.0682\n",
      "95/295, train_loss: 0.0665, step time: 1.0358\n",
      "96/295, train_loss: 0.3760, step time: 1.0714\n",
      "97/295, train_loss: 0.1019, step time: 1.0769\n",
      "98/295, train_loss: 0.0425, step time: 1.1102\n",
      "99/295, train_loss: 0.0715, step time: 1.0360\n",
      "100/295, train_loss: 0.0703, step time: 1.0610\n",
      "101/295, train_loss: 0.0719, step time: 1.0346\n",
      "102/295, train_loss: 0.0602, step time: 1.0835\n",
      "103/295, train_loss: 0.0407, step time: 1.0440\n",
      "104/295, train_loss: 0.0803, step time: 1.0507\n",
      "105/295, train_loss: 0.0764, step time: 1.0358\n",
      "106/295, train_loss: 0.3754, step time: 1.0467\n",
      "107/295, train_loss: 0.0334, step time: 1.0614\n",
      "108/295, train_loss: 0.0267, step time: 1.0641\n",
      "109/295, train_loss: 0.1526, step time: 1.0385\n",
      "110/295, train_loss: 0.0271, step time: 1.0725\n",
      "111/295, train_loss: 0.0687, step time: 1.0375\n",
      "112/295, train_loss: 0.0324, step time: 1.0355\n",
      "113/295, train_loss: 0.0353, step time: 1.0452\n",
      "114/295, train_loss: 0.2880, step time: 1.0474\n",
      "115/295, train_loss: 0.0273, step time: 1.1073\n",
      "116/295, train_loss: 0.1023, step time: 1.1078\n",
      "117/295, train_loss: 0.0741, step time: 1.0462\n",
      "118/295, train_loss: 0.0356, step time: 1.0396\n",
      "119/295, train_loss: 0.0583, step time: 1.0597\n",
      "120/295, train_loss: 0.3863, step time: 1.0528\n",
      "121/295, train_loss: 0.0713, step time: 1.0371\n",
      "122/295, train_loss: 0.0839, step time: 1.0382\n",
      "123/295, train_loss: 0.1108, step time: 1.0571\n",
      "124/295, train_loss: 0.0200, step time: 1.0365\n",
      "125/295, train_loss: 0.1287, step time: 1.0801\n",
      "126/295, train_loss: 0.0960, step time: 1.0372\n",
      "127/295, train_loss: 0.0404, step time: 1.0401\n",
      "128/295, train_loss: 0.0470, step time: 1.0323\n",
      "129/295, train_loss: 0.1163, step time: 1.0370\n",
      "130/295, train_loss: 0.0838, step time: 1.0605\n",
      "131/295, train_loss: 0.0515, step time: 1.0994\n",
      "132/295, train_loss: 0.0453, step time: 1.0386\n",
      "133/295, train_loss: 0.0632, step time: 1.0515\n",
      "134/295, train_loss: 0.1131, step time: 1.0533\n",
      "135/295, train_loss: 0.0412, step time: 1.0544\n",
      "136/295, train_loss: 0.0607, step time: 1.0324\n",
      "137/295, train_loss: 0.2629, step time: 1.0375\n",
      "138/295, train_loss: 0.0564, step time: 1.0395\n",
      "139/295, train_loss: 0.0500, step time: 1.0344\n",
      "140/295, train_loss: 0.0720, step time: 1.0930\n",
      "141/295, train_loss: 0.0916, step time: 1.0539\n",
      "142/295, train_loss: 0.0404, step time: 1.0446\n",
      "143/295, train_loss: 0.1251, step time: 1.0369\n",
      "144/295, train_loss: 0.0449, step time: 1.0626\n",
      "145/295, train_loss: 0.0545, step time: 1.0358\n",
      "146/295, train_loss: 0.0649, step time: 1.0428\n",
      "147/295, train_loss: 0.1310, step time: 1.0925\n",
      "148/295, train_loss: 0.0825, step time: 1.0352\n",
      "149/295, train_loss: 0.1319, step time: 1.0418\n",
      "150/295, train_loss: 0.4647, step time: 1.0357\n",
      "151/295, train_loss: 0.0905, step time: 1.0983\n",
      "152/295, train_loss: 0.0801, step time: 1.0389\n",
      "153/295, train_loss: 0.1324, step time: 1.0410\n",
      "154/295, train_loss: 0.0539, step time: 1.0419\n",
      "155/295, train_loss: 0.0452, step time: 1.0539\n",
      "156/295, train_loss: 0.1037, step time: 1.0363\n",
      "157/295, train_loss: 0.0603, step time: 1.0471\n",
      "158/295, train_loss: 0.0813, step time: 1.0611\n",
      "159/295, train_loss: 0.0405, step time: 1.0466\n",
      "160/295, train_loss: 0.0987, step time: 1.0501\n",
      "161/295, train_loss: 0.0781, step time: 1.0372\n",
      "162/295, train_loss: 0.1224, step time: 1.0403\n",
      "163/295, train_loss: 0.1363, step time: 1.0568\n",
      "164/295, train_loss: 0.0476, step time: 1.0405\n",
      "165/295, train_loss: 0.0721, step time: 1.0583\n",
      "166/295, train_loss: 0.1112, step time: 1.0313\n",
      "167/295, train_loss: 0.0316, step time: 1.0369\n",
      "168/295, train_loss: 0.1102, step time: 1.0675\n",
      "169/295, train_loss: 0.0759, step time: 1.0348\n",
      "170/295, train_loss: 0.0263, step time: 1.0582\n",
      "171/295, train_loss: 0.0850, step time: 1.0460\n",
      "172/295, train_loss: 0.1868, step time: 1.0415\n",
      "173/295, train_loss: 0.0818, step time: 1.0435\n",
      "174/295, train_loss: 0.0298, step time: 1.0437\n",
      "175/295, train_loss: 0.0555, step time: 1.0416\n",
      "176/295, train_loss: 0.0462, step time: 1.1302\n",
      "177/295, train_loss: 0.0346, step time: 1.0379\n",
      "178/295, train_loss: 0.0506, step time: 1.0330\n",
      "179/295, train_loss: 0.0472, step time: 1.0376\n",
      "180/295, train_loss: 0.0442, step time: 1.0371\n",
      "181/295, train_loss: 0.0297, step time: 1.0416\n",
      "182/295, train_loss: 0.0609, step time: 1.0668\n",
      "183/295, train_loss: 0.0388, step time: 1.0572\n",
      "184/295, train_loss: 0.0497, step time: 1.0803\n",
      "185/295, train_loss: 0.0402, step time: 1.0466\n",
      "186/295, train_loss: 0.0597, step time: 1.0334\n",
      "187/295, train_loss: 0.0485, step time: 1.0596\n",
      "188/295, train_loss: 0.0539, step time: 1.0557\n",
      "189/295, train_loss: 0.0566, step time: 1.0503\n",
      "190/295, train_loss: 0.3988, step time: 1.0690\n",
      "191/295, train_loss: 0.0353, step time: 1.0369\n",
      "192/295, train_loss: 0.0476, step time: 1.0676\n",
      "193/295, train_loss: 0.0722, step time: 1.0566\n",
      "194/295, train_loss: 0.0628, step time: 1.0372\n",
      "195/295, train_loss: 0.0868, step time: 1.0545\n",
      "196/295, train_loss: 0.0657, step time: 1.0603\n",
      "197/295, train_loss: 0.0618, step time: 1.0647\n",
      "198/295, train_loss: 0.0787, step time: 1.0641\n",
      "199/295, train_loss: 0.1189, step time: 1.0995\n",
      "200/295, train_loss: 0.0620, step time: 1.0533\n",
      "201/295, train_loss: 0.0636, step time: 1.0338\n",
      "202/295, train_loss: 0.0817, step time: 1.0609\n",
      "203/295, train_loss: 0.0404, step time: 1.1042\n",
      "204/295, train_loss: 0.0709, step time: 1.0356\n",
      "205/295, train_loss: 0.0354, step time: 1.0330\n",
      "206/295, train_loss: 0.0351, step time: 1.0447\n",
      "207/295, train_loss: 0.0987, step time: 1.0737\n",
      "208/295, train_loss: 0.1007, step time: 1.0556\n",
      "209/295, train_loss: 0.0356, step time: 1.0336\n",
      "210/295, train_loss: 0.0599, step time: 1.0339\n",
      "211/295, train_loss: 0.0886, step time: 1.0373\n",
      "212/295, train_loss: 0.0359, step time: 1.0664\n",
      "213/295, train_loss: 0.0272, step time: 1.0513\n",
      "214/295, train_loss: 0.0727, step time: 1.0404\n",
      "215/295, train_loss: 0.3588, step time: 1.0409\n",
      "216/295, train_loss: 0.0358, step time: 1.0351\n",
      "217/295, train_loss: 0.0435, step time: 1.0325\n",
      "218/295, train_loss: 0.0367, step time: 1.0651\n",
      "219/295, train_loss: 0.0549, step time: 1.0389\n",
      "220/295, train_loss: 0.3864, step time: 1.0691\n",
      "221/295, train_loss: 0.1309, step time: 1.0379\n",
      "222/295, train_loss: 0.0621, step time: 1.0523\n",
      "223/295, train_loss: 0.0819, step time: 1.0969\n",
      "224/295, train_loss: 0.0325, step time: 1.0368\n",
      "225/295, train_loss: 0.0705, step time: 1.0572\n",
      "226/295, train_loss: 0.0951, step time: 1.0521\n",
      "227/295, train_loss: 0.0901, step time: 1.0364\n",
      "228/295, train_loss: 0.0723, step time: 1.0360\n",
      "229/295, train_loss: 0.0533, step time: 1.0452\n",
      "230/295, train_loss: 0.1032, step time: 1.0939\n",
      "231/295, train_loss: 0.0289, step time: 1.0741\n",
      "232/295, train_loss: 0.0634, step time: 1.0676\n",
      "233/295, train_loss: 0.0347, step time: 1.0443\n",
      "234/295, train_loss: 0.0770, step time: 1.0376\n",
      "235/295, train_loss: 0.0970, step time: 1.0360\n",
      "236/295, train_loss: 0.0843, step time: 1.0508\n",
      "237/295, train_loss: 0.0979, step time: 1.0501\n",
      "238/295, train_loss: 0.3657, step time: 1.0588\n",
      "239/295, train_loss: 0.0704, step time: 1.0919\n",
      "240/295, train_loss: 0.1347, step time: 1.0354\n",
      "241/295, train_loss: 0.0437, step time: 1.0442\n",
      "242/295, train_loss: 0.0552, step time: 1.0443\n",
      "243/295, train_loss: 0.3837, step time: 1.0353\n",
      "244/295, train_loss: 0.0743, step time: 1.0336\n",
      "245/295, train_loss: 0.0329, step time: 1.0378\n",
      "246/295, train_loss: 0.1094, step time: 1.0422\n",
      "247/295, train_loss: 0.0484, step time: 1.0505\n",
      "248/295, train_loss: 0.1049, step time: 1.0476\n",
      "249/295, train_loss: 0.0359, step time: 1.0528\n",
      "250/295, train_loss: 0.0723, step time: 1.0417\n",
      "251/295, train_loss: 0.0928, step time: 1.0922\n",
      "252/295, train_loss: 0.0303, step time: 1.0455\n",
      "253/295, train_loss: 0.0797, step time: 1.0366\n",
      "254/295, train_loss: 0.1344, step time: 1.0365\n",
      "255/295, train_loss: 0.0673, step time: 1.0415\n",
      "256/295, train_loss: 0.0406, step time: 1.0549\n",
      "257/295, train_loss: 0.0820, step time: 1.0564\n",
      "258/295, train_loss: 0.0446, step time: 1.0524\n",
      "259/295, train_loss: 0.0746, step time: 1.0467\n",
      "260/295, train_loss: 0.1088, step time: 1.0630\n",
      "261/295, train_loss: 0.0594, step time: 1.0624\n",
      "262/295, train_loss: 0.0473, step time: 1.0397\n",
      "263/295, train_loss: 0.3666, step time: 1.0332\n",
      "264/295, train_loss: 0.0304, step time: 1.0377\n",
      "265/295, train_loss: 0.0479, step time: 1.0438\n",
      "266/295, train_loss: 0.0565, step time: 1.1020\n",
      "267/295, train_loss: 0.0320, step time: 1.0820\n",
      "268/295, train_loss: 0.0365, step time: 1.0634\n",
      "269/295, train_loss: 0.0857, step time: 1.0441\n",
      "270/295, train_loss: 0.0872, step time: 1.0431\n",
      "271/295, train_loss: 0.0436, step time: 1.0345\n",
      "272/295, train_loss: 0.0531, step time: 1.0388\n",
      "273/295, train_loss: 0.0789, step time: 1.0358\n",
      "274/295, train_loss: 0.0839, step time: 1.0374\n",
      "275/295, train_loss: 0.0458, step time: 1.0371\n",
      "276/295, train_loss: 0.0706, step time: 1.0481\n",
      "277/295, train_loss: 0.0717, step time: 1.0760\n",
      "278/295, train_loss: 0.1046, step time: 1.0359\n",
      "279/295, train_loss: 0.0846, step time: 1.0640\n",
      "280/295, train_loss: 0.0591, step time: 1.0820\n",
      "281/295, train_loss: 0.0326, step time: 1.0734\n",
      "282/295, train_loss: 0.0531, step time: 1.0364\n",
      "283/295, train_loss: 0.0842, step time: 1.0420\n",
      "284/295, train_loss: 0.3876, step time: 1.0452\n",
      "285/295, train_loss: 0.3682, step time: 1.0997\n",
      "286/295, train_loss: 0.0513, step time: 1.0450\n",
      "287/295, train_loss: 0.0556, step time: 1.0667\n",
      "288/295, train_loss: 0.0697, step time: 1.0292\n",
      "289/295, train_loss: 0.0507, step time: 1.0294\n",
      "290/295, train_loss: 0.0834, step time: 1.0298\n",
      "291/295, train_loss: 0.3874, step time: 1.0296\n",
      "292/295, train_loss: 0.3747, step time: 1.0292\n",
      "293/295, train_loss: 0.0288, step time: 1.0333\n",
      "294/295, train_loss: 0.0822, step time: 1.0300\n",
      "295/295, train_loss: 0.0942, step time: 1.0299\n",
      "epoch 62 average loss: 0.0910\n",
      "current epoch: 62 current mean dice: 0.7867 tc: 0.7467 wt: 0.8521 et: 0.7648\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 62 is: 382.6570\n",
      "----------\n",
      "epoch 63/100\n",
      "1/295, train_loss: 0.0690, step time: 1.1402\n",
      "2/295, train_loss: 0.0441, step time: 1.1315\n",
      "3/295, train_loss: 0.0363, step time: 1.2320\n",
      "4/295, train_loss: 0.0926, step time: 1.1112\n",
      "5/295, train_loss: 0.0785, step time: 1.0379\n",
      "6/295, train_loss: 0.0702, step time: 1.0398\n",
      "7/295, train_loss: 0.3943, step time: 1.0499\n",
      "8/295, train_loss: 0.0644, step time: 1.1193\n",
      "9/295, train_loss: 0.0566, step time: 1.0493\n",
      "10/295, train_loss: 0.0662, step time: 1.0424\n",
      "11/295, train_loss: 0.0827, step time: 1.0399\n",
      "12/295, train_loss: 0.0619, step time: 1.0385\n",
      "13/295, train_loss: 0.0581, step time: 1.0604\n",
      "14/295, train_loss: 0.0624, step time: 1.0650\n",
      "15/295, train_loss: 0.0745, step time: 1.0373\n",
      "16/295, train_loss: 0.0524, step time: 1.0404\n",
      "17/295, train_loss: 0.0344, step time: 1.0308\n",
      "18/295, train_loss: 0.0430, step time: 1.0354\n",
      "19/295, train_loss: 0.0398, step time: 1.0453\n",
      "20/295, train_loss: 0.0891, step time: 1.0808\n",
      "21/295, train_loss: 0.0359, step time: 1.0452\n",
      "22/295, train_loss: 0.0847, step time: 1.0352\n",
      "23/295, train_loss: 0.0318, step time: 1.0340\n",
      "24/295, train_loss: 0.0945, step time: 1.0387\n",
      "25/295, train_loss: 0.0843, step time: 1.0418\n",
      "26/295, train_loss: 0.0542, step time: 1.0600\n",
      "27/295, train_loss: 0.0712, step time: 1.0333\n",
      "28/295, train_loss: 0.0583, step time: 1.0656\n",
      "29/295, train_loss: 0.0843, step time: 1.0818\n",
      "30/295, train_loss: 0.0898, step time: 1.0472\n",
      "31/295, train_loss: 0.0623, step time: 1.0541\n",
      "32/295, train_loss: 0.0946, step time: 1.0419\n",
      "33/295, train_loss: 0.0368, step time: 1.0504\n",
      "34/295, train_loss: 0.0835, step time: 1.0344\n",
      "35/295, train_loss: 0.0588, step time: 1.0556\n",
      "36/295, train_loss: 0.0427, step time: 1.0320\n",
      "37/295, train_loss: 0.0648, step time: 1.0360\n",
      "38/295, train_loss: 0.0897, step time: 1.0506\n",
      "39/295, train_loss: 0.1003, step time: 1.0417\n",
      "40/295, train_loss: 0.0330, step time: 1.0589\n",
      "41/295, train_loss: 0.0434, step time: 1.0426\n",
      "42/295, train_loss: 0.0580, step time: 1.0390\n",
      "43/295, train_loss: 0.0821, step time: 1.0399\n",
      "44/295, train_loss: 0.0819, step time: 1.0358\n",
      "45/295, train_loss: 0.0696, step time: 1.0679\n",
      "46/295, train_loss: 0.0370, step time: 1.1174\n",
      "47/295, train_loss: 0.0619, step time: 1.0348\n",
      "48/295, train_loss: 0.0680, step time: 1.0463\n",
      "49/295, train_loss: 0.0409, step time: 1.0452\n",
      "50/295, train_loss: 0.0904, step time: 1.0586\n",
      "51/295, train_loss: 0.0420, step time: 1.0486\n",
      "52/295, train_loss: 0.0664, step time: 1.0467\n",
      "53/295, train_loss: 0.0513, step time: 1.0350\n",
      "54/295, train_loss: 0.0760, step time: 1.0394\n",
      "55/295, train_loss: 0.0611, step time: 1.0407\n",
      "56/295, train_loss: 0.0525, step time: 1.0895\n",
      "57/295, train_loss: 0.1409, step time: 1.0469\n",
      "58/295, train_loss: 0.0682, step time: 1.0454\n",
      "59/295, train_loss: 0.1191, step time: 1.0410\n",
      "60/295, train_loss: 0.0637, step time: 1.0621\n",
      "61/295, train_loss: 0.0201, step time: 1.0398\n",
      "62/295, train_loss: 0.0396, step time: 1.0516\n",
      "63/295, train_loss: 0.0522, step time: 1.0372\n",
      "64/295, train_loss: 0.1086, step time: 1.0432\n",
      "65/295, train_loss: 0.0553, step time: 1.0775\n",
      "66/295, train_loss: 0.0395, step time: 1.0813\n",
      "67/295, train_loss: 0.0680, step time: 1.0410\n",
      "68/295, train_loss: 0.0821, step time: 1.0631\n",
      "69/295, train_loss: 0.0304, step time: 1.0317\n",
      "70/295, train_loss: 0.0587, step time: 1.0373\n",
      "71/295, train_loss: 0.0721, step time: 1.0472\n",
      "72/295, train_loss: 0.1142, step time: 1.0606\n",
      "73/295, train_loss: 0.0346, step time: 1.0454\n",
      "74/295, train_loss: 0.0741, step time: 1.0381\n",
      "75/295, train_loss: 0.0862, step time: 1.0360\n",
      "76/295, train_loss: 0.0425, step time: 1.0432\n",
      "77/295, train_loss: 0.0654, step time: 1.0416\n",
      "78/295, train_loss: 0.1254, step time: 1.0735\n",
      "79/295, train_loss: 0.0502, step time: 1.0398\n",
      "80/295, train_loss: 0.0513, step time: 1.0416\n",
      "81/295, train_loss: 0.0358, step time: 1.0623\n",
      "82/295, train_loss: 0.0285, step time: 1.0736\n",
      "83/295, train_loss: 0.0501, step time: 1.0376\n",
      "84/295, train_loss: 0.0477, step time: 1.0308\n",
      "85/295, train_loss: 0.3932, step time: 1.0388\n",
      "86/295, train_loss: 0.0511, step time: 1.0887\n",
      "87/295, train_loss: 0.0377, step time: 1.0342\n",
      "88/295, train_loss: 0.0545, step time: 1.0321\n",
      "89/295, train_loss: 0.0592, step time: 1.0395\n",
      "90/295, train_loss: 0.1146, step time: 1.0326\n",
      "91/295, train_loss: 0.3866, step time: 1.0314\n",
      "92/295, train_loss: 0.1082, step time: 1.0575\n",
      "93/295, train_loss: 0.0313, step time: 1.0563\n",
      "94/295, train_loss: 0.0533, step time: 1.0506\n",
      "95/295, train_loss: 0.0876, step time: 1.0401\n",
      "96/295, train_loss: 0.0341, step time: 1.0380\n",
      "97/295, train_loss: 0.0455, step time: 1.0437\n",
      "98/295, train_loss: 0.3862, step time: 1.0665\n",
      "99/295, train_loss: 0.0459, step time: 1.0578\n",
      "100/295, train_loss: 0.0497, step time: 1.0360\n",
      "101/295, train_loss: 0.0911, step time: 1.0364\n",
      "102/295, train_loss: 0.1008, step time: 1.0388\n",
      "103/295, train_loss: 0.0275, step time: 1.0536\n",
      "104/295, train_loss: 0.0658, step time: 1.0376\n",
      "105/295, train_loss: 0.0286, step time: 1.0569\n",
      "106/295, train_loss: 0.0445, step time: 1.0670\n",
      "107/295, train_loss: 0.0625, step time: 1.0387\n",
      "108/295, train_loss: 0.0331, step time: 1.0349\n",
      "109/295, train_loss: 0.0548, step time: 1.0449\n",
      "110/295, train_loss: 0.0846, step time: 1.1051\n",
      "111/295, train_loss: 0.1040, step time: 1.0468\n",
      "112/295, train_loss: 0.0326, step time: 1.0387\n",
      "113/295, train_loss: 0.1108, step time: 1.0690\n",
      "114/295, train_loss: 0.0396, step time: 1.0347\n",
      "115/295, train_loss: 0.0452, step time: 1.0498\n",
      "116/295, train_loss: 0.1343, step time: 1.0444\n",
      "117/295, train_loss: 0.0837, step time: 1.0579\n",
      "118/295, train_loss: 0.0835, step time: 1.0466\n",
      "119/295, train_loss: 0.0459, step time: 1.0493\n",
      "120/295, train_loss: 0.0287, step time: 1.0335\n",
      "121/295, train_loss: 0.4061, step time: 1.0376\n",
      "122/295, train_loss: 0.0686, step time: 1.0508\n",
      "123/295, train_loss: 0.0496, step time: 1.0419\n",
      "124/295, train_loss: 0.0790, step time: 1.0471\n",
      "125/295, train_loss: 0.0983, step time: 1.0305\n",
      "126/295, train_loss: 0.3711, step time: 1.0317\n",
      "127/295, train_loss: 0.3680, step time: 1.0625\n",
      "128/295, train_loss: 0.0328, step time: 1.0360\n",
      "129/295, train_loss: 0.0448, step time: 1.0390\n",
      "130/295, train_loss: 0.0818, step time: 1.0383\n",
      "131/295, train_loss: 0.0344, step time: 1.0444\n",
      "132/295, train_loss: 0.0643, step time: 1.0736\n",
      "133/295, train_loss: 0.0951, step time: 1.0526\n",
      "134/295, train_loss: 0.1186, step time: 1.0547\n",
      "135/295, train_loss: 0.0263, step time: 1.0338\n",
      "136/295, train_loss: 0.2239, step time: 1.0429\n",
      "137/295, train_loss: 0.1166, step time: 1.0322\n",
      "138/295, train_loss: 0.0462, step time: 1.0792\n",
      "139/295, train_loss: 0.0556, step time: 1.0384\n",
      "140/295, train_loss: 0.0522, step time: 1.0431\n",
      "141/295, train_loss: 0.0306, step time: 1.0611\n",
      "142/295, train_loss: 0.1272, step time: 1.0919\n",
      "143/295, train_loss: 0.0522, step time: 1.0705\n",
      "144/295, train_loss: 0.1062, step time: 1.0330\n",
      "145/295, train_loss: 0.0833, step time: 1.0333\n",
      "146/295, train_loss: 0.0754, step time: 1.0314\n",
      "147/295, train_loss: 0.4528, step time: 1.0984\n",
      "148/295, train_loss: 0.0303, step time: 1.0412\n",
      "149/295, train_loss: 0.0314, step time: 1.1091\n",
      "150/295, train_loss: 0.0476, step time: 1.0348\n",
      "151/295, train_loss: 0.0767, step time: 1.0380\n",
      "152/295, train_loss: 0.0711, step time: 1.0606\n",
      "153/295, train_loss: 0.0932, step time: 1.0575\n",
      "154/295, train_loss: 0.3945, step time: 1.1421\n",
      "155/295, train_loss: 0.1276, step time: 1.0367\n",
      "156/295, train_loss: 0.0872, step time: 1.0404\n",
      "157/295, train_loss: 0.0806, step time: 1.0461\n",
      "158/295, train_loss: 0.0864, step time: 1.1102\n",
      "159/295, train_loss: 0.0559, step time: 1.0554\n",
      "160/295, train_loss: 0.0431, step time: 1.0861\n",
      "161/295, train_loss: 0.0365, step time: 1.0551\n",
      "162/295, train_loss: 0.0748, step time: 1.0605\n",
      "163/295, train_loss: 0.0456, step time: 1.0500\n",
      "164/295, train_loss: 0.1025, step time: 1.0333\n",
      "165/295, train_loss: 0.0695, step time: 1.0537\n",
      "166/295, train_loss: 0.0465, step time: 1.0712\n",
      "167/295, train_loss: 0.0468, step time: 1.0418\n",
      "168/295, train_loss: 0.0647, step time: 1.0379\n",
      "169/295, train_loss: 0.0563, step time: 1.0463\n",
      "170/295, train_loss: 0.0315, step time: 1.0536\n",
      "171/295, train_loss: 0.0347, step time: 1.0911\n",
      "172/295, train_loss: 0.0444, step time: 1.0478\n",
      "173/295, train_loss: 0.1124, step time: 1.0563\n",
      "174/295, train_loss: 0.0873, step time: 1.0630\n",
      "175/295, train_loss: 0.0451, step time: 1.0473\n",
      "176/295, train_loss: 0.0827, step time: 1.0335\n",
      "177/295, train_loss: 0.1046, step time: 1.0441\n",
      "178/295, train_loss: 0.0752, step time: 1.0320\n",
      "179/295, train_loss: 0.0364, step time: 1.0858\n",
      "180/295, train_loss: 0.3824, step time: 1.0406\n",
      "181/295, train_loss: 0.3586, step time: 1.1306\n",
      "182/295, train_loss: 0.0322, step time: 1.0393\n",
      "183/295, train_loss: 0.3845, step time: 1.0355\n",
      "184/295, train_loss: 0.1063, step time: 1.0346\n",
      "185/295, train_loss: 0.0631, step time: 1.0720\n",
      "186/295, train_loss: 0.0542, step time: 1.0589\n",
      "187/295, train_loss: 0.0508, step time: 1.0351\n",
      "188/295, train_loss: 0.3676, step time: 1.0403\n",
      "189/295, train_loss: 0.0886, step time: 1.0375\n",
      "190/295, train_loss: 0.1110, step time: 1.0576\n",
      "191/295, train_loss: 0.0675, step time: 1.0569\n",
      "192/295, train_loss: 0.0466, step time: 1.1214\n",
      "193/295, train_loss: 0.0898, step time: 1.0353\n",
      "194/295, train_loss: 0.0400, step time: 1.0542\n",
      "195/295, train_loss: 0.0355, step time: 1.0545\n",
      "196/295, train_loss: 0.0461, step time: 1.0534\n",
      "197/295, train_loss: 0.1363, step time: 1.0538\n",
      "198/295, train_loss: 0.0537, step time: 1.0371\n",
      "199/295, train_loss: 0.0534, step time: 1.0365\n",
      "200/295, train_loss: 0.0448, step time: 1.0466\n",
      "201/295, train_loss: 0.0307, step time: 1.0765\n",
      "202/295, train_loss: 0.0676, step time: 1.0331\n",
      "203/295, train_loss: 0.0400, step time: 1.0455\n",
      "204/295, train_loss: 0.3713, step time: 1.0659\n",
      "205/295, train_loss: 0.3858, step time: 1.0330\n",
      "206/295, train_loss: 0.0459, step time: 1.0490\n",
      "207/295, train_loss: 0.0664, step time: 1.0419\n",
      "208/295, train_loss: 0.0798, step time: 1.0503\n",
      "209/295, train_loss: 0.0758, step time: 1.0459\n",
      "210/295, train_loss: 0.3742, step time: 1.0918\n",
      "211/295, train_loss: 0.0266, step time: 1.0633\n",
      "212/295, train_loss: 0.1001, step time: 1.0402\n",
      "213/295, train_loss: 0.0553, step time: 1.0449\n",
      "214/295, train_loss: 0.1050, step time: 1.0500\n",
      "215/295, train_loss: 0.0956, step time: 1.0395\n",
      "216/295, train_loss: 0.0885, step time: 1.0497\n",
      "217/295, train_loss: 0.0827, step time: 1.0392\n",
      "218/295, train_loss: 0.0652, step time: 1.0336\n",
      "219/295, train_loss: 0.0807, step time: 1.0405\n",
      "220/295, train_loss: 0.0566, step time: 1.0752\n",
      "221/295, train_loss: 0.0588, step time: 1.0602\n",
      "222/295, train_loss: 0.0825, step time: 1.0328\n",
      "223/295, train_loss: 0.0556, step time: 1.0489\n",
      "224/295, train_loss: 0.0798, step time: 1.0363\n",
      "225/295, train_loss: 0.0548, step time: 1.0410\n",
      "226/295, train_loss: 0.0619, step time: 1.0446\n",
      "227/295, train_loss: 0.0843, step time: 1.0732\n",
      "228/295, train_loss: 0.2257, step time: 1.0435\n",
      "229/295, train_loss: 0.0274, step time: 1.0435\n",
      "230/295, train_loss: 0.3864, step time: 1.0637\n",
      "231/295, train_loss: 0.0614, step time: 1.0342\n",
      "232/295, train_loss: 0.0592, step time: 1.0998\n",
      "233/295, train_loss: 0.0587, step time: 1.0644\n",
      "234/295, train_loss: 0.0280, step time: 1.0507\n",
      "235/295, train_loss: 0.0313, step time: 1.0629\n",
      "236/295, train_loss: 0.0466, step time: 1.0499\n",
      "237/295, train_loss: 0.0989, step time: 1.0360\n",
      "238/295, train_loss: 0.0573, step time: 1.0431\n",
      "239/295, train_loss: 0.0574, step time: 1.0380\n",
      "240/295, train_loss: 0.0442, step time: 1.0381\n",
      "241/295, train_loss: 0.0395, step time: 1.0709\n",
      "242/295, train_loss: 0.0571, step time: 1.1043\n",
      "243/295, train_loss: 0.0423, step time: 1.0434\n",
      "244/295, train_loss: 0.1496, step time: 1.0612\n",
      "245/295, train_loss: 0.0392, step time: 1.0604\n",
      "246/295, train_loss: 0.0593, step time: 1.0354\n",
      "247/295, train_loss: 0.0671, step time: 1.0476\n",
      "248/295, train_loss: 0.3647, step time: 1.0400\n",
      "249/295, train_loss: 0.0704, step time: 1.0402\n",
      "250/295, train_loss: 0.1568, step time: 1.0444\n",
      "251/295, train_loss: 0.0287, step time: 1.0508\n",
      "252/295, train_loss: 0.0695, step time: 1.0625\n",
      "253/295, train_loss: 0.0587, step time: 1.0341\n",
      "254/295, train_loss: 0.0411, step time: 1.0373\n",
      "255/295, train_loss: 0.0387, step time: 1.0383\n",
      "256/295, train_loss: 0.0738, step time: 1.0917\n",
      "257/295, train_loss: 0.3673, step time: 1.0546\n",
      "258/295, train_loss: 0.0338, step time: 1.0425\n",
      "259/295, train_loss: 0.1650, step time: 1.0799\n",
      "260/295, train_loss: 0.0377, step time: 1.0337\n",
      "261/295, train_loss: 0.0699, step time: 1.0476\n",
      "262/295, train_loss: 0.0702, step time: 1.0416\n",
      "263/295, train_loss: 0.0522, step time: 1.0365\n",
      "264/295, train_loss: 0.1443, step time: 1.0744\n",
      "265/295, train_loss: 0.0350, step time: 1.0602\n",
      "266/295, train_loss: 0.1023, step time: 1.0758\n",
      "267/295, train_loss: 0.0382, step time: 1.0345\n",
      "268/295, train_loss: 0.1087, step time: 1.0670\n",
      "269/295, train_loss: 0.0388, step time: 1.0366\n",
      "270/295, train_loss: 0.3663, step time: 1.0460\n",
      "271/295, train_loss: 0.0715, step time: 1.0641\n",
      "272/295, train_loss: 0.0847, step time: 1.0456\n",
      "273/295, train_loss: 0.1146, step time: 1.0404\n",
      "274/295, train_loss: 0.0658, step time: 1.0377\n",
      "275/295, train_loss: 0.0734, step time: 1.0370\n",
      "276/295, train_loss: 0.0492, step time: 1.0411\n",
      "277/295, train_loss: 0.0333, step time: 1.0510\n",
      "278/295, train_loss: 0.0861, step time: 1.0448\n",
      "279/295, train_loss: 0.0451, step time: 1.0337\n",
      "280/295, train_loss: 0.0788, step time: 1.0418\n",
      "281/295, train_loss: 0.0563, step time: 1.0940\n",
      "282/295, train_loss: 0.0687, step time: 1.0422\n",
      "283/295, train_loss: 0.0311, step time: 1.0473\n",
      "284/295, train_loss: 0.0340, step time: 1.1257\n",
      "285/295, train_loss: 0.0553, step time: 1.0403\n",
      "286/295, train_loss: 0.0971, step time: 1.0647\n",
      "287/295, train_loss: 0.0315, step time: 1.0370\n",
      "288/295, train_loss: 0.0517, step time: 1.0310\n",
      "289/295, train_loss: 0.0499, step time: 1.0299\n",
      "290/295, train_loss: 0.3925, step time: 1.0301\n",
      "291/295, train_loss: 0.3857, step time: 1.0284\n",
      "292/295, train_loss: 0.0356, step time: 1.0291\n",
      "293/295, train_loss: 0.0824, step time: 1.0298\n",
      "294/295, train_loss: 0.0702, step time: 1.0303\n",
      "295/295, train_loss: 0.0267, step time: 1.0302\n",
      "epoch 63 average loss: 0.0898\n",
      "current epoch: 63 current mean dice: 0.7550 tc: 0.7064 wt: 0.8327 et: 0.7307\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 63 is: 387.2070\n",
      "----------\n",
      "epoch 64/100\n",
      "1/295, train_loss: 0.0857, step time: 1.1042\n",
      "2/295, train_loss: 0.0964, step time: 1.0639\n",
      "3/295, train_loss: 0.0633, step time: 1.0723\n",
      "4/295, train_loss: 0.0790, step time: 1.0734\n",
      "5/295, train_loss: 0.1439, step time: 1.0541\n",
      "6/295, train_loss: 0.2218, step time: 1.0670\n",
      "7/295, train_loss: 0.0506, step time: 1.0482\n",
      "8/295, train_loss: 0.1018, step time: 1.0372\n",
      "9/295, train_loss: 0.0829, step time: 1.0416\n",
      "10/295, train_loss: 0.0365, step time: 1.0529\n",
      "11/295, train_loss: 0.0827, step time: 1.0365\n",
      "12/295, train_loss: 0.0766, step time: 1.0570\n",
      "13/295, train_loss: 0.0475, step time: 1.1120\n",
      "14/295, train_loss: 0.1026, step time: 1.0345\n",
      "15/295, train_loss: 0.0730, step time: 1.0542\n",
      "16/295, train_loss: 0.0560, step time: 1.0539\n",
      "17/295, train_loss: 0.0725, step time: 1.0614\n",
      "18/295, train_loss: 0.0648, step time: 1.0828\n",
      "19/295, train_loss: 0.0873, step time: 1.0314\n",
      "20/295, train_loss: 0.0606, step time: 1.0421\n",
      "21/295, train_loss: 0.3859, step time: 1.0334\n",
      "22/295, train_loss: 0.0305, step time: 1.0392\n",
      "23/295, train_loss: 0.1108, step time: 1.1143\n",
      "24/295, train_loss: 0.0355, step time: 1.0340\n",
      "25/295, train_loss: 0.1206, step time: 1.0406\n",
      "26/295, train_loss: 0.0489, step time: 1.0433\n",
      "27/295, train_loss: 0.0355, step time: 1.0717\n",
      "28/295, train_loss: 0.0857, step time: 1.0385\n",
      "29/295, train_loss: 0.0693, step time: 1.0390\n",
      "30/295, train_loss: 0.0568, step time: 1.0588\n",
      "31/295, train_loss: 0.0948, step time: 1.1173\n",
      "32/295, train_loss: 0.0460, step time: 1.0500\n",
      "33/295, train_loss: 0.0440, step time: 1.0311\n",
      "34/295, train_loss: 0.0583, step time: 1.0323\n",
      "35/295, train_loss: 0.0513, step time: 1.0403\n",
      "36/295, train_loss: 0.0289, step time: 1.0613\n",
      "37/295, train_loss: 0.0422, step time: 1.0664\n",
      "38/295, train_loss: 0.3862, step time: 1.0402\n",
      "39/295, train_loss: 0.0731, step time: 1.0578\n",
      "40/295, train_loss: 0.0475, step time: 1.0485\n",
      "41/295, train_loss: 0.1175, step time: 1.0471\n",
      "42/295, train_loss: 0.1579, step time: 1.0359\n",
      "43/295, train_loss: 0.0325, step time: 1.0381\n",
      "44/295, train_loss: 0.0853, step time: 1.0430\n",
      "45/295, train_loss: 0.0316, step time: 1.0370\n",
      "46/295, train_loss: 0.0365, step time: 1.0505\n",
      "47/295, train_loss: 0.0529, step time: 1.0867\n",
      "48/295, train_loss: 0.1065, step time: 1.0437\n",
      "49/295, train_loss: 0.0751, step time: 1.0360\n",
      "50/295, train_loss: 0.0656, step time: 1.0399\n",
      "51/295, train_loss: 0.0786, step time: 1.1055\n",
      "52/295, train_loss: 0.0554, step time: 1.0597\n",
      "53/295, train_loss: 0.0676, step time: 1.0497\n",
      "54/295, train_loss: 0.0490, step time: 1.0668\n",
      "55/295, train_loss: 0.3665, step time: 1.0414\n",
      "56/295, train_loss: 0.0931, step time: 1.0337\n",
      "57/295, train_loss: 0.0439, step time: 1.0562\n",
      "58/295, train_loss: 0.0204, step time: 1.0458\n",
      "59/295, train_loss: 0.0756, step time: 1.0317\n",
      "60/295, train_loss: 0.0587, step time: 1.0564\n",
      "61/295, train_loss: 0.1108, step time: 1.0384\n",
      "62/295, train_loss: 0.0273, step time: 1.0328\n",
      "63/295, train_loss: 0.0399, step time: 1.0692\n",
      "64/295, train_loss: 0.0659, step time: 1.0403\n",
      "65/295, train_loss: 0.3668, step time: 1.0484\n",
      "66/295, train_loss: 0.0457, step time: 1.0583\n",
      "67/295, train_loss: 0.0646, step time: 1.0551\n",
      "68/295, train_loss: 0.3849, step time: 1.0321\n",
      "69/295, train_loss: 0.0720, step time: 1.0423\n",
      "70/295, train_loss: 0.0521, step time: 1.0578\n",
      "71/295, train_loss: 0.0442, step time: 1.0343\n",
      "72/295, train_loss: 0.0398, step time: 1.0362\n",
      "73/295, train_loss: 0.0444, step time: 1.0312\n",
      "74/295, train_loss: 0.0948, step time: 1.0335\n",
      "75/295, train_loss: 0.0580, step time: 1.0343\n",
      "76/295, train_loss: 0.0522, step time: 1.0546\n",
      "77/295, train_loss: 0.0310, step time: 1.0555\n",
      "78/295, train_loss: 0.0486, step time: 1.0664\n",
      "79/295, train_loss: 0.0825, step time: 1.0516\n",
      "80/295, train_loss: 0.0526, step time: 1.0351\n",
      "81/295, train_loss: 0.1034, step time: 1.1219\n",
      "82/295, train_loss: 0.0994, step time: 1.1026\n",
      "83/295, train_loss: 0.0541, step time: 1.0327\n",
      "84/295, train_loss: 0.0397, step time: 1.0433\n",
      "85/295, train_loss: 0.0355, step time: 1.0491\n",
      "86/295, train_loss: 0.0467, step time: 1.0798\n",
      "87/295, train_loss: 0.3876, step time: 1.1005\n",
      "88/295, train_loss: 0.1277, step time: 1.0523\n",
      "89/295, train_loss: 0.0306, step time: 1.0426\n",
      "90/295, train_loss: 0.0406, step time: 1.0350\n",
      "91/295, train_loss: 0.0404, step time: 1.0549\n",
      "92/295, train_loss: 0.0534, step time: 1.1109\n",
      "93/295, train_loss: 0.0810, step time: 1.0585\n",
      "94/295, train_loss: 0.1202, step time: 1.0458\n",
      "95/295, train_loss: 0.0320, step time: 1.0371\n",
      "96/295, train_loss: 0.0689, step time: 1.0393\n",
      "97/295, train_loss: 0.0770, step time: 1.1247\n",
      "98/295, train_loss: 0.0874, step time: 1.0380\n",
      "99/295, train_loss: 0.0881, step time: 1.1287\n",
      "100/295, train_loss: 0.0385, step time: 1.0326\n",
      "101/295, train_loss: 0.0303, step time: 1.0583\n",
      "102/295, train_loss: 0.0429, step time: 1.0424\n",
      "103/295, train_loss: 0.0303, step time: 1.0477\n",
      "104/295, train_loss: 0.0272, step time: 1.0591\n",
      "105/295, train_loss: 0.0968, step time: 1.0624\n",
      "106/295, train_loss: 0.0432, step time: 1.0441\n",
      "107/295, train_loss: 0.0692, step time: 1.0322\n",
      "108/295, train_loss: 0.0739, step time: 1.0361\n",
      "109/295, train_loss: 0.3861, step time: 1.0360\n",
      "110/295, train_loss: 0.0609, step time: 1.0396\n",
      "111/295, train_loss: 0.3864, step time: 1.1484\n",
      "112/295, train_loss: 0.0914, step time: 1.0311\n",
      "113/295, train_loss: 0.0480, step time: 1.0303\n",
      "114/295, train_loss: 0.0687, step time: 1.1376\n",
      "115/295, train_loss: 0.0480, step time: 1.0479\n",
      "116/295, train_loss: 0.0347, step time: 1.1101\n",
      "117/295, train_loss: 0.0588, step time: 1.0408\n",
      "118/295, train_loss: 0.0591, step time: 1.0433\n",
      "119/295, train_loss: 0.3581, step time: 1.0733\n",
      "120/295, train_loss: 0.0397, step time: 1.0444\n",
      "121/295, train_loss: 0.1015, step time: 1.0367\n",
      "122/295, train_loss: 0.0492, step time: 1.0400\n",
      "123/295, train_loss: 0.3735, step time: 1.0408\n",
      "124/295, train_loss: 0.0704, step time: 1.0359\n",
      "125/295, train_loss: 0.0521, step time: 1.0437\n",
      "126/295, train_loss: 0.0672, step time: 1.0464\n",
      "127/295, train_loss: 0.0766, step time: 1.0470\n",
      "128/295, train_loss: 0.0839, step time: 1.0500\n",
      "129/295, train_loss: 0.0680, step time: 1.0637\n",
      "130/295, train_loss: 0.0346, step time: 1.0371\n",
      "131/295, train_loss: 0.0290, step time: 1.0391\n",
      "132/295, train_loss: 0.0886, step time: 1.0428\n",
      "133/295, train_loss: 0.0520, step time: 1.0396\n",
      "134/295, train_loss: 0.0574, step time: 1.0938\n",
      "135/295, train_loss: 0.0559, step time: 1.0758\n",
      "136/295, train_loss: 0.4510, step time: 1.0440\n",
      "137/295, train_loss: 0.0685, step time: 1.0352\n",
      "138/295, train_loss: 0.0805, step time: 1.0371\n",
      "139/295, train_loss: 0.0564, step time: 1.0499\n",
      "140/295, train_loss: 0.0634, step time: 1.0647\n",
      "141/295, train_loss: 0.0443, step time: 1.0398\n",
      "142/295, train_loss: 0.1136, step time: 1.0758\n",
      "143/295, train_loss: 0.0850, step time: 1.0705\n",
      "144/295, train_loss: 0.0744, step time: 1.0544\n",
      "145/295, train_loss: 0.4035, step time: 1.0429\n",
      "146/295, train_loss: 0.0284, step time: 1.0733\n",
      "147/295, train_loss: 0.0686, step time: 1.0338\n",
      "148/295, train_loss: 0.0310, step time: 1.0427\n",
      "149/295, train_loss: 0.3810, step time: 1.0414\n",
      "150/295, train_loss: 0.0611, step time: 1.0460\n",
      "151/295, train_loss: 0.0383, step time: 1.0483\n",
      "152/295, train_loss: 0.0866, step time: 1.0613\n",
      "153/295, train_loss: 0.0564, step time: 1.0358\n",
      "154/295, train_loss: 0.0649, step time: 1.0450\n",
      "155/295, train_loss: 0.0846, step time: 1.0682\n",
      "156/295, train_loss: 0.0269, step time: 1.0382\n",
      "157/295, train_loss: 0.0502, step time: 1.0593\n",
      "158/295, train_loss: 0.0866, step time: 1.0342\n",
      "159/295, train_loss: 0.1026, step time: 1.0477\n",
      "160/295, train_loss: 0.0348, step time: 1.0788\n",
      "161/295, train_loss: 0.0391, step time: 1.1117\n",
      "162/295, train_loss: 0.0563, step time: 1.0340\n",
      "163/295, train_loss: 0.0454, step time: 1.0374\n",
      "164/295, train_loss: 0.0438, step time: 1.0604\n",
      "165/295, train_loss: 0.0281, step time: 1.0556\n",
      "166/295, train_loss: 0.0550, step time: 1.0506\n",
      "167/295, train_loss: 0.0464, step time: 1.0472\n",
      "168/295, train_loss: 0.0457, step time: 1.0448\n",
      "169/295, train_loss: 0.0322, step time: 1.0532\n",
      "170/295, train_loss: 0.0430, step time: 1.0352\n",
      "171/295, train_loss: 0.0376, step time: 1.0336\n",
      "172/295, train_loss: 0.0581, step time: 1.0414\n",
      "173/295, train_loss: 0.0317, step time: 1.0592\n",
      "174/295, train_loss: 0.0464, step time: 1.0721\n",
      "175/295, train_loss: 0.0633, step time: 1.0419\n",
      "176/295, train_loss: 0.0805, step time: 1.0375\n",
      "177/295, train_loss: 0.0674, step time: 1.0432\n",
      "178/295, train_loss: 0.0315, step time: 1.0413\n",
      "179/295, train_loss: 0.0271, step time: 1.0542\n",
      "180/295, train_loss: 0.0818, step time: 1.0531\n",
      "181/295, train_loss: 0.0580, step time: 1.1113\n",
      "182/295, train_loss: 0.3608, step time: 1.0357\n",
      "183/295, train_loss: 0.3710, step time: 1.0581\n",
      "184/295, train_loss: 0.0750, step time: 1.0590\n",
      "185/295, train_loss: 0.0331, step time: 1.0442\n",
      "186/295, train_loss: 0.0534, step time: 1.0676\n",
      "187/295, train_loss: 0.0730, step time: 1.0731\n",
      "188/295, train_loss: 0.0970, step time: 1.0375\n",
      "189/295, train_loss: 0.0729, step time: 1.0376\n",
      "190/295, train_loss: 0.4013, step time: 1.0459\n",
      "191/295, train_loss: 0.0447, step time: 1.0388\n",
      "192/295, train_loss: 0.0375, step time: 1.0394\n",
      "193/295, train_loss: 0.0361, step time: 1.0425\n",
      "194/295, train_loss: 0.0242, step time: 1.0494\n",
      "195/295, train_loss: 0.0461, step time: 1.0503\n",
      "196/295, train_loss: 0.0405, step time: 1.0689\n",
      "197/295, train_loss: 0.0955, step time: 1.0364\n",
      "198/295, train_loss: 0.0631, step time: 1.0340\n",
      "199/295, train_loss: 0.0253, step time: 1.0329\n",
      "200/295, train_loss: 0.0925, step time: 1.0486\n",
      "201/295, train_loss: 0.0271, step time: 1.0397\n",
      "202/295, train_loss: 0.0303, step time: 1.0750\n",
      "203/295, train_loss: 0.0791, step time: 1.0564\n",
      "204/295, train_loss: 0.0685, step time: 1.0462\n",
      "205/295, train_loss: 0.3894, step time: 1.0358\n",
      "206/295, train_loss: 0.0307, step time: 1.0356\n",
      "207/295, train_loss: 0.0690, step time: 1.0373\n",
      "208/295, train_loss: 0.2166, step time: 1.0339\n",
      "209/295, train_loss: 0.0667, step time: 1.0623\n",
      "210/295, train_loss: 0.0618, step time: 1.0888\n",
      "211/295, train_loss: 0.0794, step time: 1.0495\n",
      "212/295, train_loss: 0.3718, step time: 1.0343\n",
      "213/295, train_loss: 0.0279, step time: 1.0356\n",
      "214/295, train_loss: 0.0319, step time: 1.0403\n",
      "215/295, train_loss: 0.0957, step time: 1.0623\n",
      "216/295, train_loss: 0.3861, step time: 1.0373\n",
      "217/295, train_loss: 0.0837, step time: 1.0375\n",
      "218/295, train_loss: 0.0807, step time: 1.0588\n",
      "219/295, train_loss: 0.0672, step time: 1.0586\n",
      "220/295, train_loss: 0.0803, step time: 1.0454\n",
      "221/295, train_loss: 0.0469, step time: 1.0953\n",
      "222/295, train_loss: 0.0349, step time: 1.0369\n",
      "223/295, train_loss: 0.0716, step time: 1.0453\n",
      "224/295, train_loss: 0.0436, step time: 1.0395\n",
      "225/295, train_loss: 0.1087, step time: 1.0364\n",
      "226/295, train_loss: 0.0712, step time: 1.0543\n",
      "227/295, train_loss: 0.0818, step time: 1.0518\n",
      "228/295, train_loss: 0.0670, step time: 1.0387\n",
      "229/295, train_loss: 0.0642, step time: 1.0427\n",
      "230/295, train_loss: 0.0807, step time: 1.0629\n",
      "231/295, train_loss: 0.1390, step time: 1.0435\n",
      "232/295, train_loss: 0.0407, step time: 1.0439\n",
      "233/295, train_loss: 0.1134, step time: 1.0367\n",
      "234/295, train_loss: 0.0569, step time: 1.0393\n",
      "235/295, train_loss: 0.0533, step time: 1.0380\n",
      "236/295, train_loss: 0.1081, step time: 1.0939\n",
      "237/295, train_loss: 0.0527, step time: 1.0614\n",
      "238/295, train_loss: 0.0624, step time: 1.0368\n",
      "239/295, train_loss: 0.0903, step time: 1.0361\n",
      "240/295, train_loss: 0.1058, step time: 1.0750\n",
      "241/295, train_loss: 0.1171, step time: 1.0505\n",
      "242/295, train_loss: 0.0571, step time: 1.0376\n",
      "243/295, train_loss: 0.0431, step time: 1.0431\n",
      "244/295, train_loss: 0.0611, step time: 1.0718\n",
      "245/295, train_loss: 0.1318, step time: 1.0679\n",
      "246/295, train_loss: 0.0835, step time: 1.0403\n",
      "247/295, train_loss: 0.1092, step time: 1.0386\n",
      "248/295, train_loss: 0.0576, step time: 1.0434\n",
      "249/295, train_loss: 0.0683, step time: 1.0392\n",
      "250/295, train_loss: 0.0815, step time: 1.0489\n",
      "251/295, train_loss: 0.0307, step time: 1.0738\n",
      "252/295, train_loss: 0.3931, step time: 1.0380\n",
      "253/295, train_loss: 0.0889, step time: 1.0384\n",
      "254/295, train_loss: 0.0350, step time: 1.0380\n",
      "255/295, train_loss: 0.1018, step time: 1.0381\n",
      "256/295, train_loss: 0.1735, step time: 1.0399\n",
      "257/295, train_loss: 0.0656, step time: 1.0376\n",
      "258/295, train_loss: 0.3680, step time: 1.0449\n",
      "259/295, train_loss: 0.0320, step time: 1.0626\n",
      "260/295, train_loss: 0.0585, step time: 1.1112\n",
      "261/295, train_loss: 0.1021, step time: 1.0312\n",
      "262/295, train_loss: 0.0988, step time: 1.0413\n",
      "263/295, train_loss: 0.0374, step time: 1.0380\n",
      "264/295, train_loss: 0.0392, step time: 1.0468\n",
      "265/295, train_loss: 0.0769, step time: 1.0359\n",
      "266/295, train_loss: 0.1216, step time: 1.0381\n",
      "267/295, train_loss: 0.0481, step time: 1.0446\n",
      "268/295, train_loss: 0.1006, step time: 1.0675\n",
      "269/295, train_loss: 0.0561, step time: 1.0561\n",
      "270/295, train_loss: 0.0401, step time: 1.0400\n",
      "271/295, train_loss: 0.0326, step time: 1.0325\n",
      "272/295, train_loss: 0.0943, step time: 1.0427\n",
      "273/295, train_loss: 0.0369, step time: 1.0498\n",
      "274/295, train_loss: 0.0359, step time: 1.0357\n",
      "275/295, train_loss: 0.0591, step time: 1.0323\n",
      "276/295, train_loss: 0.0359, step time: 1.0336\n",
      "277/295, train_loss: 0.0549, step time: 1.0529\n",
      "278/295, train_loss: 0.0690, step time: 1.0377\n",
      "279/295, train_loss: 0.1347, step time: 1.0703\n",
      "280/295, train_loss: 0.0954, step time: 1.0650\n",
      "281/295, train_loss: 0.1216, step time: 1.0441\n",
      "282/295, train_loss: 0.0473, step time: 1.0485\n",
      "283/295, train_loss: 0.0540, step time: 1.0656\n",
      "284/295, train_loss: 0.0604, step time: 1.0472\n",
      "285/295, train_loss: 0.0723, step time: 1.0320\n",
      "286/295, train_loss: 0.0889, step time: 1.0594\n",
      "287/295, train_loss: 0.0504, step time: 1.0376\n",
      "288/295, train_loss: 0.0544, step time: 1.0413\n",
      "289/295, train_loss: 0.3681, step time: 1.0422\n",
      "290/295, train_loss: 0.0713, step time: 1.0409\n",
      "291/295, train_loss: 0.1633, step time: 1.0309\n",
      "292/295, train_loss: 0.0557, step time: 1.0311\n",
      "293/295, train_loss: 0.1696, step time: 1.0294\n",
      "294/295, train_loss: 0.0602, step time: 1.0295\n",
      "295/295, train_loss: 0.0436, step time: 1.0300\n",
      "epoch 64 average loss: 0.0898\n",
      "current epoch: 64 current mean dice: 0.7810 tc: 0.7393 wt: 0.8485 et: 0.7601\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 64 is: 386.9819\n",
      "----------\n",
      "epoch 65/100\n",
      "1/295, train_loss: 0.0354, step time: 1.1301\n",
      "2/295, train_loss: 0.0451, step time: 1.0711\n",
      "3/295, train_loss: 0.0520, step time: 1.0509\n",
      "4/295, train_loss: 0.0508, step time: 1.0664\n",
      "5/295, train_loss: 0.0711, step time: 1.0531\n",
      "6/295, train_loss: 0.0327, step time: 1.0513\n",
      "7/295, train_loss: 0.0627, step time: 1.1133\n",
      "8/295, train_loss: 0.0321, step time: 1.0344\n",
      "9/295, train_loss: 0.3841, step time: 1.0350\n",
      "10/295, train_loss: 0.0993, step time: 1.0693\n",
      "11/295, train_loss: 0.0834, step time: 1.0628\n",
      "12/295, train_loss: 0.1297, step time: 1.0415\n",
      "13/295, train_loss: 0.0443, step time: 1.0419\n",
      "14/295, train_loss: 0.0310, step time: 1.0371\n",
      "15/295, train_loss: 0.0968, step time: 1.0361\n",
      "16/295, train_loss: 0.0398, step time: 1.0680\n",
      "17/295, train_loss: 0.0417, step time: 1.1362\n",
      "18/295, train_loss: 0.0464, step time: 1.0550\n",
      "19/295, train_loss: 0.0821, step time: 1.0776\n",
      "20/295, train_loss: 0.0553, step time: 1.0378\n",
      "21/295, train_loss: 0.0499, step time: 1.0425\n",
      "22/295, train_loss: 0.0579, step time: 1.0321\n",
      "23/295, train_loss: 0.0731, step time: 1.0405\n",
      "24/295, train_loss: 0.0297, step time: 1.0512\n",
      "25/295, train_loss: 0.0301, step time: 1.0832\n",
      "26/295, train_loss: 0.0772, step time: 1.0343\n",
      "27/295, train_loss: 0.0375, step time: 1.0613\n",
      "28/295, train_loss: 0.0265, step time: 1.0495\n",
      "29/295, train_loss: 0.0871, step time: 1.0418\n",
      "30/295, train_loss: 0.0533, step time: 1.0719\n",
      "31/295, train_loss: 0.0326, step time: 1.0659\n",
      "32/295, train_loss: 0.0568, step time: 1.0637\n",
      "33/295, train_loss: 0.0450, step time: 1.0756\n",
      "34/295, train_loss: 0.0298, step time: 1.0420\n",
      "35/295, train_loss: 0.0411, step time: 1.0690\n",
      "36/295, train_loss: 0.0623, step time: 1.0707\n",
      "37/295, train_loss: 0.0552, step time: 1.0353\n",
      "38/295, train_loss: 0.0374, step time: 1.0351\n",
      "39/295, train_loss: 0.0990, step time: 1.0431\n",
      "40/295, train_loss: 0.1063, step time: 1.0628\n",
      "41/295, train_loss: 0.0393, step time: 1.1049\n",
      "42/295, train_loss: 0.0716, step time: 1.0483\n",
      "43/295, train_loss: 0.0565, step time: 1.0618\n",
      "44/295, train_loss: 0.0533, step time: 1.0458\n",
      "45/295, train_loss: 0.0846, step time: 1.0856\n",
      "46/295, train_loss: 0.0415, step time: 1.0423\n",
      "47/295, train_loss: 0.0730, step time: 1.0603\n",
      "48/295, train_loss: 0.0381, step time: 1.0884\n",
      "49/295, train_loss: 0.0295, step time: 1.0650\n",
      "50/295, train_loss: 0.0846, step time: 1.0359\n",
      "51/295, train_loss: 0.0625, step time: 1.0617\n",
      "52/295, train_loss: 0.3624, step time: 1.0492\n",
      "53/295, train_loss: 0.0282, step time: 1.0515\n",
      "54/295, train_loss: 0.1106, step time: 1.0442\n",
      "55/295, train_loss: 0.0844, step time: 1.0603\n",
      "56/295, train_loss: 0.0959, step time: 1.0681\n",
      "57/295, train_loss: 0.0570, step time: 1.0371\n",
      "58/295, train_loss: 0.0654, step time: 1.0392\n",
      "59/295, train_loss: 0.0362, step time: 1.0396\n",
      "60/295, train_loss: 0.1129, step time: 1.0417\n",
      "61/295, train_loss: 0.0860, step time: 1.0736\n",
      "62/295, train_loss: 0.0566, step time: 1.0435\n",
      "63/295, train_loss: 0.0701, step time: 1.0350\n",
      "64/295, train_loss: 0.0655, step time: 1.0319\n",
      "65/295, train_loss: 0.0841, step time: 1.0394\n",
      "66/295, train_loss: 0.0466, step time: 1.0462\n",
      "67/295, train_loss: 0.0332, step time: 1.0376\n",
      "68/295, train_loss: 0.1119, step time: 1.0377\n",
      "69/295, train_loss: 0.0543, step time: 1.0996\n",
      "70/295, train_loss: 0.0627, step time: 1.0496\n",
      "71/295, train_loss: 0.0419, step time: 1.0781\n",
      "72/295, train_loss: 0.0280, step time: 1.0604\n",
      "73/295, train_loss: 0.0814, step time: 1.0328\n",
      "74/295, train_loss: 0.0730, step time: 1.0310\n",
      "75/295, train_loss: 0.1677, step time: 1.0307\n",
      "76/295, train_loss: 0.0665, step time: 1.0409\n",
      "77/295, train_loss: 0.1274, step time: 1.0677\n",
      "78/295, train_loss: 0.0306, step time: 1.0353\n",
      "79/295, train_loss: 0.1017, step time: 1.0543\n",
      "80/295, train_loss: 0.0963, step time: 1.0347\n",
      "81/295, train_loss: 0.3665, step time: 1.0330\n",
      "82/295, train_loss: 0.0818, step time: 1.0743\n",
      "83/295, train_loss: 0.0944, step time: 1.0395\n",
      "84/295, train_loss: 0.0823, step time: 1.0403\n",
      "85/295, train_loss: 0.0324, step time: 1.0729\n",
      "86/295, train_loss: 0.0516, step time: 1.0481\n",
      "87/295, train_loss: 0.0847, step time: 1.0432\n",
      "88/295, train_loss: 0.0358, step time: 1.0344\n",
      "89/295, train_loss: 0.0960, step time: 1.0463\n",
      "90/295, train_loss: 0.0651, step time: 1.0425\n",
      "91/295, train_loss: 0.1653, step time: 1.0848\n",
      "92/295, train_loss: 0.0202, step time: 1.0378\n",
      "93/295, train_loss: 0.0681, step time: 1.0540\n",
      "94/295, train_loss: 0.0458, step time: 1.0411\n",
      "95/295, train_loss: 0.0518, step time: 1.0362\n",
      "96/295, train_loss: 0.0634, step time: 1.0411\n",
      "97/295, train_loss: 0.1163, step time: 1.0345\n",
      "98/295, train_loss: 0.0333, step time: 1.0346\n",
      "99/295, train_loss: 0.0941, step time: 1.0617\n",
      "100/295, train_loss: 0.0913, step time: 1.0446\n",
      "101/295, train_loss: 0.0502, step time: 1.0381\n",
      "102/295, train_loss: 0.3841, step time: 1.0372\n",
      "103/295, train_loss: 0.0413, step time: 1.0459\n",
      "104/295, train_loss: 0.0491, step time: 1.0465\n",
      "105/295, train_loss: 0.0809, step time: 1.0371\n",
      "106/295, train_loss: 0.0531, step time: 1.0358\n",
      "107/295, train_loss: 0.0479, step time: 1.0579\n",
      "108/295, train_loss: 0.1095, step time: 1.0518\n",
      "109/295, train_loss: 0.3693, step time: 1.0393\n",
      "110/295, train_loss: 0.0616, step time: 1.0441\n",
      "111/295, train_loss: 0.0535, step time: 1.0915\n",
      "112/295, train_loss: 0.0539, step time: 1.0716\n",
      "113/295, train_loss: 0.0916, step time: 1.0362\n",
      "114/295, train_loss: 0.0674, step time: 1.0449\n",
      "115/295, train_loss: 0.0536, step time: 1.0403\n",
      "116/295, train_loss: 0.0554, step time: 1.0346\n",
      "117/295, train_loss: 0.3869, step time: 1.0366\n",
      "118/295, train_loss: 0.0455, step time: 1.0452\n",
      "119/295, train_loss: 0.0833, step time: 1.0829\n",
      "120/295, train_loss: 0.0733, step time: 1.0954\n",
      "121/295, train_loss: 0.0448, step time: 1.0331\n",
      "122/295, train_loss: 0.1040, step time: 1.0457\n",
      "123/295, train_loss: 0.0266, step time: 1.0399\n",
      "124/295, train_loss: 0.0356, step time: 1.0734\n",
      "125/295, train_loss: 0.0653, step time: 1.0395\n",
      "126/295, train_loss: 0.1667, step time: 1.0510\n",
      "127/295, train_loss: 0.1225, step time: 1.0652\n",
      "128/295, train_loss: 0.0381, step time: 1.0366\n",
      "129/295, train_loss: 0.0334, step time: 1.0361\n",
      "130/295, train_loss: 0.0459, step time: 1.0626\n",
      "131/295, train_loss: 0.0387, step time: 1.0616\n",
      "132/295, train_loss: 0.0711, step time: 1.0368\n",
      "133/295, train_loss: 0.3868, step time: 1.0367\n",
      "134/295, train_loss: 0.0966, step time: 1.0389\n",
      "135/295, train_loss: 0.3664, step time: 1.0416\n",
      "136/295, train_loss: 0.0397, step time: 1.0613\n",
      "137/295, train_loss: 0.0431, step time: 1.0534\n",
      "138/295, train_loss: 0.0426, step time: 1.0521\n",
      "139/295, train_loss: 0.0444, step time: 1.0571\n",
      "140/295, train_loss: 0.0365, step time: 1.0385\n",
      "141/295, train_loss: 0.0477, step time: 1.0399\n",
      "142/295, train_loss: 0.0335, step time: 1.1074\n",
      "143/295, train_loss: 0.1467, step time: 1.0614\n",
      "144/295, train_loss: 0.0559, step time: 1.0618\n",
      "145/295, train_loss: 0.0426, step time: 1.0365\n",
      "146/295, train_loss: 0.4495, step time: 1.0320\n",
      "147/295, train_loss: 0.1105, step time: 1.1091\n",
      "148/295, train_loss: 0.0306, step time: 1.0350\n",
      "149/295, train_loss: 0.0720, step time: 1.0389\n",
      "150/295, train_loss: 0.0748, step time: 1.0418\n",
      "151/295, train_loss: 0.0996, step time: 1.0450\n",
      "152/295, train_loss: 0.0723, step time: 1.0384\n",
      "153/295, train_loss: 0.0586, step time: 1.0979\n",
      "154/295, train_loss: 0.3925, step time: 1.0402\n",
      "155/295, train_loss: 0.0911, step time: 1.0365\n",
      "156/295, train_loss: 0.0354, step time: 1.0350\n",
      "157/295, train_loss: 0.0639, step time: 1.0642\n",
      "158/295, train_loss: 0.0937, step time: 1.0387\n",
      "159/295, train_loss: 0.0806, step time: 1.0684\n",
      "160/295, train_loss: 0.0968, step time: 1.0793\n",
      "161/295, train_loss: 0.0311, step time: 1.0418\n",
      "162/295, train_loss: 0.0670, step time: 1.0520\n",
      "163/295, train_loss: 0.1047, step time: 1.0397\n",
      "164/295, train_loss: 0.0449, step time: 1.0363\n",
      "165/295, train_loss: 0.0814, step time: 1.0375\n",
      "166/295, train_loss: 0.1412, step time: 1.0493\n",
      "167/295, train_loss: 0.0582, step time: 1.0745\n",
      "168/295, train_loss: 0.0999, step time: 1.0341\n",
      "169/295, train_loss: 0.0842, step time: 1.0363\n",
      "170/295, train_loss: 0.1136, step time: 1.0447\n",
      "171/295, train_loss: 0.0452, step time: 1.0429\n",
      "172/295, train_loss: 0.0483, step time: 1.0491\n",
      "173/295, train_loss: 0.0746, step time: 1.0627\n",
      "174/295, train_loss: 0.0731, step time: 1.0395\n",
      "175/295, train_loss: 0.0540, step time: 1.0441\n",
      "176/295, train_loss: 0.1138, step time: 1.0792\n",
      "177/295, train_loss: 0.0852, step time: 1.0672\n",
      "178/295, train_loss: 0.0426, step time: 1.0550\n",
      "179/295, train_loss: 0.0449, step time: 1.0438\n",
      "180/295, train_loss: 0.0841, step time: 1.0359\n",
      "181/295, train_loss: 0.0405, step time: 1.0391\n",
      "182/295, train_loss: 0.3985, step time: 1.0459\n",
      "183/295, train_loss: 0.0563, step time: 1.0577\n",
      "184/295, train_loss: 0.1163, step time: 1.0398\n",
      "185/295, train_loss: 0.0487, step time: 1.0367\n",
      "186/295, train_loss: 0.0452, step time: 1.0571\n",
      "187/295, train_loss: 0.0670, step time: 1.0556\n",
      "188/295, train_loss: 0.0326, step time: 1.0386\n",
      "189/295, train_loss: 0.0359, step time: 1.0696\n",
      "190/295, train_loss: 0.0827, step time: 1.0436\n",
      "191/295, train_loss: 0.3640, step time: 1.0579\n",
      "192/295, train_loss: 0.0896, step time: 1.0362\n",
      "193/295, train_loss: 0.0427, step time: 1.0391\n",
      "194/295, train_loss: 0.0605, step time: 1.0424\n",
      "195/295, train_loss: 0.0662, step time: 1.0391\n",
      "196/295, train_loss: 0.0708, step time: 1.0462\n",
      "197/295, train_loss: 0.0803, step time: 1.0391\n",
      "198/295, train_loss: 0.0594, step time: 1.0452\n",
      "199/295, train_loss: 0.0524, step time: 1.0415\n",
      "200/295, train_loss: 0.0415, step time: 1.0634\n",
      "201/295, train_loss: 0.3857, step time: 1.0343\n",
      "202/295, train_loss: 0.0284, step time: 1.0368\n",
      "203/295, train_loss: 0.0514, step time: 1.0568\n",
      "204/295, train_loss: 0.0443, step time: 1.0434\n",
      "205/295, train_loss: 0.0357, step time: 1.0415\n",
      "206/295, train_loss: 0.0724, step time: 1.0602\n",
      "207/295, train_loss: 0.3588, step time: 1.0385\n",
      "208/295, train_loss: 0.1887, step time: 1.0843\n",
      "209/295, train_loss: 0.0797, step time: 1.0605\n",
      "210/295, train_loss: 0.0749, step time: 1.0333\n",
      "211/295, train_loss: 0.0597, step time: 1.0658\n",
      "212/295, train_loss: 0.0606, step time: 1.0397\n",
      "213/295, train_loss: 0.0760, step time: 1.0572\n",
      "214/295, train_loss: 0.0651, step time: 1.0411\n",
      "215/295, train_loss: 0.1163, step time: 1.0533\n",
      "216/295, train_loss: 0.0692, step time: 1.0608\n",
      "217/295, train_loss: 0.0519, step time: 1.0451\n",
      "218/295, train_loss: 0.0706, step time: 1.0700\n",
      "219/295, train_loss: 0.1302, step time: 1.0808\n",
      "220/295, train_loss: 0.0506, step time: 1.0361\n",
      "221/295, train_loss: 0.0309, step time: 1.0348\n",
      "222/295, train_loss: 0.0289, step time: 1.0400\n",
      "223/295, train_loss: 0.3950, step time: 1.0446\n",
      "224/295, train_loss: 0.0631, step time: 1.0530\n",
      "225/295, train_loss: 0.1155, step time: 1.0436\n",
      "226/295, train_loss: 0.0795, step time: 1.0460\n",
      "227/295, train_loss: 0.3921, step time: 1.0685\n",
      "228/295, train_loss: 0.0360, step time: 1.0864\n",
      "229/295, train_loss: 0.0882, step time: 1.0419\n",
      "230/295, train_loss: 0.0383, step time: 1.0454\n",
      "231/295, train_loss: 0.1056, step time: 1.0498\n",
      "232/295, train_loss: 0.0392, step time: 1.0533\n",
      "233/295, train_loss: 0.0402, step time: 1.0497\n",
      "234/295, train_loss: 0.0801, step time: 1.0353\n",
      "235/295, train_loss: 0.0887, step time: 1.0386\n",
      "236/295, train_loss: 0.0865, step time: 1.0370\n",
      "237/295, train_loss: 0.0922, step time: 1.0589\n",
      "238/295, train_loss: 0.0783, step time: 1.0455\n",
      "239/295, train_loss: 0.0570, step time: 1.0389\n",
      "240/295, train_loss: 0.3938, step time: 1.0520\n",
      "241/295, train_loss: 0.0885, step time: 1.0366\n",
      "242/295, train_loss: 0.3909, step time: 1.0379\n",
      "243/295, train_loss: 0.0560, step time: 1.0337\n",
      "244/295, train_loss: 0.0286, step time: 1.0425\n",
      "245/295, train_loss: 0.0364, step time: 1.0350\n",
      "246/295, train_loss: 0.0476, step time: 1.0408\n",
      "247/295, train_loss: 0.0675, step time: 1.0681\n",
      "248/295, train_loss: 0.0674, step time: 1.0544\n",
      "249/295, train_loss: 0.0669, step time: 1.0755\n",
      "250/295, train_loss: 0.0533, step time: 1.0445\n",
      "251/295, train_loss: 0.0678, step time: 1.0455\n",
      "252/295, train_loss: 0.2248, step time: 1.0362\n",
      "253/295, train_loss: 0.0790, step time: 1.0324\n",
      "254/295, train_loss: 0.0563, step time: 1.0540\n",
      "255/295, train_loss: 0.0364, step time: 1.0456\n",
      "256/295, train_loss: 0.4063, step time: 1.0337\n",
      "257/295, train_loss: 0.0690, step time: 1.0390\n",
      "258/295, train_loss: 0.0997, step time: 1.0509\n",
      "259/295, train_loss: 0.0615, step time: 1.0458\n",
      "260/295, train_loss: 0.0745, step time: 1.0456\n",
      "261/295, train_loss: 0.0957, step time: 1.1135\n",
      "262/295, train_loss: 0.0466, step time: 1.0367\n",
      "263/295, train_loss: 0.0617, step time: 1.0347\n",
      "264/295, train_loss: 0.0798, step time: 1.1510\n",
      "265/295, train_loss: 0.0455, step time: 1.0508\n",
      "266/295, train_loss: 0.0353, step time: 1.0342\n",
      "267/295, train_loss: 0.0352, step time: 1.0687\n",
      "268/295, train_loss: 0.0479, step time: 1.0466\n",
      "269/295, train_loss: 0.0361, step time: 1.0393\n",
      "270/295, train_loss: 0.0368, step time: 1.0568\n",
      "271/295, train_loss: 0.0391, step time: 1.0717\n",
      "272/295, train_loss: 0.0733, step time: 1.0423\n",
      "273/295, train_loss: 0.1417, step time: 1.0655\n",
      "274/295, train_loss: 0.3775, step time: 1.0521\n",
      "275/295, train_loss: 0.0778, step time: 1.0415\n",
      "276/295, train_loss: 0.0273, step time: 1.0960\n",
      "277/295, train_loss: 0.0271, step time: 1.0358\n",
      "278/295, train_loss: 0.0654, step time: 1.0486\n",
      "279/295, train_loss: 0.0872, step time: 1.0430\n",
      "280/295, train_loss: 0.0393, step time: 1.0442\n",
      "281/295, train_loss: 0.1213, step time: 1.0832\n",
      "282/295, train_loss: 0.0588, step time: 1.0464\n",
      "283/295, train_loss: 0.0697, step time: 1.0983\n",
      "284/295, train_loss: 0.1059, step time: 1.0338\n",
      "285/295, train_loss: 0.0964, step time: 1.0388\n",
      "286/295, train_loss: 0.0859, step time: 1.0603\n",
      "287/295, train_loss: 0.3780, step time: 1.0463\n",
      "288/295, train_loss: 0.0269, step time: 1.0309\n",
      "289/295, train_loss: 0.1076, step time: 1.0300\n",
      "290/295, train_loss: 0.0435, step time: 1.0300\n",
      "291/295, train_loss: 0.0651, step time: 1.0361\n",
      "292/295, train_loss: 0.0675, step time: 1.0301\n",
      "293/295, train_loss: 0.0398, step time: 1.0299\n",
      "294/295, train_loss: 0.0536, step time: 1.0299\n",
      "295/295, train_loss: 0.3727, step time: 1.0301\n",
      "epoch 65 average loss: 0.0902\n",
      "current epoch: 65 current mean dice: 0.7875 tc: 0.7500 wt: 0.8498 et: 0.7653\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 65 is: 383.4813\n",
      "----------\n",
      "epoch 66/100\n",
      "1/295, train_loss: 0.1605, step time: 1.1782\n",
      "2/295, train_loss: 0.0404, step time: 1.0812\n",
      "3/295, train_loss: 0.3844, step time: 1.0764\n",
      "4/295, train_loss: 0.0400, step time: 1.1409\n",
      "5/295, train_loss: 0.0676, step time: 1.0526\n",
      "6/295, train_loss: 0.0702, step time: 1.0376\n",
      "7/295, train_loss: 0.0728, step time: 1.0305\n",
      "8/295, train_loss: 0.1473, step time: 1.0446\n",
      "9/295, train_loss: 0.0948, step time: 1.0301\n",
      "10/295, train_loss: 0.0575, step time: 1.0325\n",
      "11/295, train_loss: 0.3748, step time: 1.0391\n",
      "12/295, train_loss: 0.0272, step time: 1.0540\n",
      "13/295, train_loss: 0.0421, step time: 1.0645\n",
      "14/295, train_loss: 0.0306, step time: 1.0584\n",
      "15/295, train_loss: 0.0376, step time: 1.0813\n",
      "16/295, train_loss: 0.0670, step time: 1.0354\n",
      "17/295, train_loss: 0.0354, step time: 1.0439\n",
      "18/295, train_loss: 0.0287, step time: 1.0472\n",
      "19/295, train_loss: 0.0408, step time: 1.0538\n",
      "20/295, train_loss: 0.0804, step time: 1.0338\n",
      "21/295, train_loss: 0.0803, step time: 1.0458\n",
      "22/295, train_loss: 0.0809, step time: 1.0437\n",
      "23/295, train_loss: 0.0361, step time: 1.0572\n",
      "24/295, train_loss: 0.0570, step time: 1.0641\n",
      "25/295, train_loss: 0.0407, step time: 1.1051\n",
      "26/295, train_loss: 0.0890, step time: 1.0335\n",
      "27/295, train_loss: 0.0449, step time: 1.0670\n",
      "28/295, train_loss: 0.0320, step time: 1.0550\n",
      "29/295, train_loss: 0.3852, step time: 1.0608\n",
      "30/295, train_loss: 0.0355, step time: 1.0391\n",
      "31/295, train_loss: 0.0460, step time: 1.0553\n",
      "32/295, train_loss: 0.0757, step time: 1.0311\n",
      "33/295, train_loss: 0.0301, step time: 1.0362\n",
      "34/295, train_loss: 0.0910, step time: 1.0947\n",
      "35/295, train_loss: 0.0803, step time: 1.0441\n",
      "36/295, train_loss: 0.0593, step time: 1.0721\n",
      "37/295, train_loss: 0.0592, step time: 1.0509\n",
      "38/295, train_loss: 0.0590, step time: 1.0342\n",
      "39/295, train_loss: 0.0593, step time: 1.0591\n",
      "40/295, train_loss: 0.0951, step time: 1.0827\n",
      "41/295, train_loss: 0.1132, step time: 1.0519\n",
      "42/295, train_loss: 0.1219, step time: 1.0436\n",
      "43/295, train_loss: 0.0707, step time: 1.0835\n",
      "44/295, train_loss: 0.0867, step time: 1.0377\n",
      "45/295, train_loss: 0.0628, step time: 1.0374\n",
      "46/295, train_loss: 0.0928, step time: 1.0659\n",
      "47/295, train_loss: 0.0918, step time: 1.0351\n",
      "48/295, train_loss: 0.0973, step time: 1.0407\n",
      "49/295, train_loss: 0.0553, step time: 1.0612\n",
      "50/295, train_loss: 0.0442, step time: 1.0590\n",
      "51/295, train_loss: 0.0632, step time: 1.0409\n",
      "52/295, train_loss: 0.0481, step time: 1.0346\n",
      "53/295, train_loss: 0.0517, step time: 1.0334\n",
      "54/295, train_loss: 0.0456, step time: 1.0354\n",
      "55/295, train_loss: 0.0824, step time: 1.0390\n",
      "56/295, train_loss: 0.0448, step time: 1.0389\n",
      "57/295, train_loss: 0.0393, step time: 1.0404\n",
      "58/295, train_loss: 0.0664, step time: 1.0512\n",
      "59/295, train_loss: 0.0606, step time: 1.0378\n",
      "60/295, train_loss: 0.0793, step time: 1.0675\n",
      "61/295, train_loss: 0.0774, step time: 1.0916\n",
      "62/295, train_loss: 0.0462, step time: 1.0469\n",
      "63/295, train_loss: 0.0726, step time: 1.0364\n",
      "64/295, train_loss: 0.0453, step time: 1.0780\n",
      "65/295, train_loss: 0.0624, step time: 1.0332\n",
      "66/295, train_loss: 0.0721, step time: 1.0389\n",
      "67/295, train_loss: 0.1005, step time: 1.0736\n",
      "68/295, train_loss: 0.0664, step time: 1.1760\n",
      "69/295, train_loss: 0.0822, step time: 1.0378\n",
      "70/295, train_loss: 0.0887, step time: 1.0364\n",
      "71/295, train_loss: 0.0650, step time: 1.0608\n",
      "72/295, train_loss: 0.0681, step time: 1.0587\n",
      "73/295, train_loss: 0.0464, step time: 1.0428\n",
      "74/295, train_loss: 0.1013, step time: 1.0502\n",
      "75/295, train_loss: 0.0618, step time: 1.0318\n",
      "76/295, train_loss: 0.0622, step time: 1.0328\n",
      "77/295, train_loss: 0.1045, step time: 1.0349\n",
      "78/295, train_loss: 0.0502, step time: 1.0544\n",
      "79/295, train_loss: 0.0671, step time: 1.0767\n",
      "80/295, train_loss: 0.0849, step time: 1.0317\n",
      "81/295, train_loss: 0.0643, step time: 1.0372\n",
      "82/295, train_loss: 0.0704, step time: 1.0372\n",
      "83/295, train_loss: 0.3734, step time: 1.0595\n",
      "84/295, train_loss: 0.0485, step time: 1.0571\n",
      "85/295, train_loss: 0.0548, step time: 1.0333\n",
      "86/295, train_loss: 0.0594, step time: 1.0410\n",
      "87/295, train_loss: 0.0810, step time: 1.0635\n",
      "88/295, train_loss: 0.0583, step time: 1.0324\n",
      "89/295, train_loss: 0.0410, step time: 1.0415\n",
      "90/295, train_loss: 0.0614, step time: 1.0771\n",
      "91/295, train_loss: 0.0289, step time: 1.0967\n",
      "92/295, train_loss: 0.0560, step time: 1.0525\n",
      "93/295, train_loss: 0.0786, step time: 1.0392\n",
      "94/295, train_loss: 0.0466, step time: 1.0694\n",
      "95/295, train_loss: 0.0755, step time: 1.0647\n",
      "96/295, train_loss: 0.0288, step time: 1.0602\n",
      "97/295, train_loss: 0.0511, step time: 1.0666\n",
      "98/295, train_loss: 0.0329, step time: 1.0407\n",
      "99/295, train_loss: 0.3890, step time: 1.0389\n",
      "100/295, train_loss: 0.1099, step time: 1.0403\n",
      "101/295, train_loss: 0.0360, step time: 1.0538\n",
      "102/295, train_loss: 0.0293, step time: 1.0576\n",
      "103/295, train_loss: 0.3586, step time: 1.0355\n",
      "104/295, train_loss: 0.0809, step time: 1.0706\n",
      "105/295, train_loss: 0.0832, step time: 1.0353\n",
      "106/295, train_loss: 0.0722, step time: 1.0534\n",
      "107/295, train_loss: 0.0412, step time: 1.0341\n",
      "108/295, train_loss: 0.0677, step time: 1.0356\n",
      "109/295, train_loss: 0.0498, step time: 1.0382\n",
      "110/295, train_loss: 0.0791, step time: 1.1380\n",
      "111/295, train_loss: 0.0981, step time: 1.1027\n",
      "112/295, train_loss: 0.3954, step time: 1.0378\n",
      "113/295, train_loss: 0.0427, step time: 1.0338\n",
      "114/295, train_loss: 0.0395, step time: 1.0449\n",
      "115/295, train_loss: 0.0317, step time: 1.0447\n",
      "116/295, train_loss: 0.0471, step time: 1.0658\n",
      "117/295, train_loss: 0.0376, step time: 1.0422\n",
      "118/295, train_loss: 0.1154, step time: 1.0460\n",
      "119/295, train_loss: 0.0436, step time: 1.0396\n",
      "120/295, train_loss: 0.0996, step time: 1.0772\n",
      "121/295, train_loss: 0.0853, step time: 1.0322\n",
      "122/295, train_loss: 0.0333, step time: 1.0922\n",
      "123/295, train_loss: 0.1163, step time: 1.1200\n",
      "124/295, train_loss: 0.0308, step time: 1.0420\n",
      "125/295, train_loss: 0.0528, step time: 1.0336\n",
      "126/295, train_loss: 0.0388, step time: 1.0422\n",
      "127/295, train_loss: 0.0341, step time: 1.0439\n",
      "128/295, train_loss: 0.0338, step time: 1.0435\n",
      "129/295, train_loss: 0.1351, step time: 1.0673\n",
      "130/295, train_loss: 0.0866, step time: 1.0552\n",
      "131/295, train_loss: 0.0673, step time: 1.0323\n",
      "132/295, train_loss: 0.1142, step time: 1.0184\n",
      "133/295, train_loss: 0.0382, step time: 1.0548\n",
      "134/295, train_loss: 0.0624, step time: 1.0532\n",
      "135/295, train_loss: 0.0386, step time: 1.0398\n",
      "136/295, train_loss: 0.3648, step time: 1.0585\n",
      "137/295, train_loss: 0.0927, step time: 1.0658\n",
      "138/295, train_loss: 0.0808, step time: 1.0672\n",
      "139/295, train_loss: 0.0629, step time: 1.0966\n",
      "140/295, train_loss: 0.0519, step time: 1.0324\n",
      "141/295, train_loss: 0.0397, step time: 1.0447\n",
      "142/295, train_loss: 0.0353, step time: 1.0330\n",
      "143/295, train_loss: 0.0465, step time: 1.0653\n",
      "144/295, train_loss: 0.0943, step time: 1.0661\n",
      "145/295, train_loss: 0.0532, step time: 1.0375\n",
      "146/295, train_loss: 0.0517, step time: 1.0601\n",
      "147/295, train_loss: 0.0859, step time: 1.0537\n",
      "148/295, train_loss: 0.3607, step time: 1.0455\n",
      "149/295, train_loss: 0.0572, step time: 1.1542\n",
      "150/295, train_loss: 0.0292, step time: 1.0776\n",
      "151/295, train_loss: 0.0394, step time: 1.0358\n",
      "152/295, train_loss: 0.0620, step time: 1.0462\n",
      "153/295, train_loss: 0.3994, step time: 1.0647\n",
      "154/295, train_loss: 0.1072, step time: 1.0565\n",
      "155/295, train_loss: 0.0433, step time: 1.0372\n",
      "156/295, train_loss: 0.0840, step time: 1.0368\n",
      "157/295, train_loss: 0.0448, step time: 1.0483\n",
      "158/295, train_loss: 0.0591, step time: 1.0955\n",
      "159/295, train_loss: 0.0521, step time: 1.0373\n",
      "160/295, train_loss: 0.0543, step time: 1.0355\n",
      "161/295, train_loss: 0.0304, step time: 1.0321\n",
      "162/295, train_loss: 0.0567, step time: 1.0425\n",
      "163/295, train_loss: 0.0292, step time: 1.0318\n",
      "164/295, train_loss: 0.0784, step time: 1.0864\n",
      "165/295, train_loss: 0.0521, step time: 1.0376\n",
      "166/295, train_loss: 0.0738, step time: 1.0403\n",
      "167/295, train_loss: 0.0320, step time: 1.0422\n",
      "168/295, train_loss: 0.0865, step time: 1.0360\n",
      "169/295, train_loss: 0.1038, step time: 1.0423\n",
      "170/295, train_loss: 0.0573, step time: 1.0424\n",
      "171/295, train_loss: 0.0364, step time: 1.0428\n",
      "172/295, train_loss: 0.0301, step time: 1.0775\n",
      "173/295, train_loss: 0.0338, step time: 1.0582\n",
      "174/295, train_loss: 0.0834, step time: 1.0444\n",
      "175/295, train_loss: 0.3664, step time: 1.0582\n",
      "176/295, train_loss: 0.0428, step time: 1.1691\n",
      "177/295, train_loss: 0.4029, step time: 1.0645\n",
      "178/295, train_loss: 0.0445, step time: 1.0474\n",
      "179/295, train_loss: 0.0684, step time: 1.0362\n",
      "180/295, train_loss: 0.1399, step time: 1.0591\n",
      "181/295, train_loss: 0.0328, step time: 1.0521\n",
      "182/295, train_loss: 0.0267, step time: 1.0332\n",
      "183/295, train_loss: 0.0632, step time: 1.0366\n",
      "184/295, train_loss: 0.0864, step time: 1.0376\n",
      "185/295, train_loss: 0.0422, step time: 1.0438\n",
      "186/295, train_loss: 0.0573, step time: 1.0368\n",
      "187/295, train_loss: 0.0663, step time: 1.0383\n",
      "188/295, train_loss: 0.0553, step time: 1.0531\n",
      "189/295, train_loss: 0.0261, step time: 1.0562\n",
      "190/295, train_loss: 0.0537, step time: 1.0539\n",
      "191/295, train_loss: 0.0337, step time: 1.0425\n",
      "192/295, train_loss: 0.0809, step time: 1.0410\n",
      "193/295, train_loss: 0.0667, step time: 1.0491\n",
      "194/295, train_loss: 0.2009, step time: 1.0575\n",
      "195/295, train_loss: 0.0816, step time: 1.0319\n",
      "196/295, train_loss: 0.0688, step time: 1.0584\n",
      "197/295, train_loss: 0.0574, step time: 1.0804\n",
      "198/295, train_loss: 0.0506, step time: 1.0331\n",
      "199/295, train_loss: 0.0574, step time: 1.0604\n",
      "200/295, train_loss: 0.0590, step time: 1.0398\n",
      "201/295, train_loss: 0.0709, step time: 1.0664\n",
      "202/295, train_loss: 0.2243, step time: 1.0525\n",
      "203/295, train_loss: 0.0815, step time: 1.0393\n",
      "204/295, train_loss: 0.0323, step time: 1.0402\n",
      "205/295, train_loss: 0.0346, step time: 1.0458\n",
      "206/295, train_loss: 0.0423, step time: 1.0546\n",
      "207/295, train_loss: 0.0660, step time: 1.0340\n",
      "208/295, train_loss: 0.0828, step time: 1.0470\n",
      "209/295, train_loss: 0.0692, step time: 1.0332\n",
      "210/295, train_loss: 0.0736, step time: 1.0453\n",
      "211/295, train_loss: 0.0644, step time: 1.0443\n",
      "212/295, train_loss: 0.0663, step time: 1.0474\n",
      "213/295, train_loss: 0.0963, step time: 1.0961\n",
      "214/295, train_loss: 0.0596, step time: 1.0492\n",
      "215/295, train_loss: 0.3686, step time: 1.1173\n",
      "216/295, train_loss: 0.1072, step time: 1.0482\n",
      "217/295, train_loss: 0.0570, step time: 1.0966\n",
      "218/295, train_loss: 0.0300, step time: 1.0411\n",
      "219/295, train_loss: 0.0514, step time: 1.0570\n",
      "220/295, train_loss: 0.0438, step time: 1.0370\n",
      "221/295, train_loss: 0.0351, step time: 1.0368\n",
      "222/295, train_loss: 0.0435, step time: 1.0370\n",
      "223/295, train_loss: 0.1703, step time: 1.0554\n",
      "224/295, train_loss: 0.0390, step time: 1.0361\n",
      "225/295, train_loss: 0.3849, step time: 1.0379\n",
      "226/295, train_loss: 0.4565, step time: 1.0358\n",
      "227/295, train_loss: 0.0412, step time: 1.0390\n",
      "228/295, train_loss: 0.0790, step time: 1.0700\n",
      "229/295, train_loss: 0.0308, step time: 1.0410\n",
      "230/295, train_loss: 0.0825, step time: 1.0594\n",
      "231/295, train_loss: 0.0400, step time: 1.0557\n",
      "232/295, train_loss: 0.0355, step time: 1.0689\n",
      "233/295, train_loss: 0.0513, step time: 1.1047\n",
      "234/295, train_loss: 0.0204, step time: 1.0872\n",
      "235/295, train_loss: 0.0720, step time: 1.0345\n",
      "236/295, train_loss: 0.3836, step time: 1.0310\n",
      "237/295, train_loss: 0.0688, step time: 1.0377\n",
      "238/295, train_loss: 0.0931, step time: 1.0384\n",
      "239/295, train_loss: 0.0513, step time: 1.0782\n",
      "240/295, train_loss: 0.0348, step time: 1.0392\n",
      "241/295, train_loss: 0.0527, step time: 1.0447\n",
      "242/295, train_loss: 0.0542, step time: 1.0695\n",
      "243/295, train_loss: 0.0679, step time: 1.0781\n",
      "244/295, train_loss: 0.0633, step time: 1.0380\n",
      "245/295, train_loss: 0.0504, step time: 1.0431\n",
      "246/295, train_loss: 0.0612, step time: 1.0707\n",
      "247/295, train_loss: 0.0440, step time: 1.0435\n",
      "248/295, train_loss: 0.0410, step time: 1.0381\n",
      "249/295, train_loss: 0.3956, step time: 1.0392\n",
      "250/295, train_loss: 0.0549, step time: 1.0514\n",
      "251/295, train_loss: 0.1161, step time: 1.0662\n",
      "252/295, train_loss: 0.0656, step time: 1.0358\n",
      "253/295, train_loss: 0.3846, step time: 1.0362\n",
      "254/295, train_loss: 0.0733, step time: 1.0355\n",
      "255/295, train_loss: 0.1017, step time: 1.0367\n",
      "256/295, train_loss: 0.0432, step time: 1.0637\n",
      "257/295, train_loss: 0.0581, step time: 1.0355\n",
      "258/295, train_loss: 0.0705, step time: 1.0558\n",
      "259/295, train_loss: 0.3824, step time: 1.0912\n",
      "260/295, train_loss: 0.1163, step time: 1.0483\n",
      "261/295, train_loss: 0.0835, step time: 1.0418\n",
      "262/295, train_loss: 0.1110, step time: 1.0550\n",
      "263/295, train_loss: 0.1040, step time: 1.0510\n",
      "264/295, train_loss: 0.0874, step time: 1.0325\n",
      "265/295, train_loss: 0.0776, step time: 1.0397\n",
      "266/295, train_loss: 0.0498, step time: 1.0487\n",
      "267/295, train_loss: 0.3828, step time: 1.0410\n",
      "268/295, train_loss: 0.1135, step time: 1.0628\n",
      "269/295, train_loss: 0.0354, step time: 1.0392\n",
      "270/295, train_loss: 0.0687, step time: 1.0390\n",
      "271/295, train_loss: 0.0612, step time: 1.0332\n",
      "272/295, train_loss: 0.0482, step time: 1.0450\n",
      "273/295, train_loss: 0.0885, step time: 1.0778\n",
      "274/295, train_loss: 0.0462, step time: 1.0560\n",
      "275/295, train_loss: 0.0805, step time: 1.0761\n",
      "276/295, train_loss: 0.1149, step time: 1.0379\n",
      "277/295, train_loss: 0.0612, step time: 1.0389\n",
      "278/295, train_loss: 0.0455, step time: 1.0373\n",
      "279/295, train_loss: 0.1207, step time: 1.0582\n",
      "280/295, train_loss: 0.0253, step time: 1.0383\n",
      "281/295, train_loss: 0.0997, step time: 1.0405\n",
      "282/295, train_loss: 0.1278, step time: 1.0449\n",
      "283/295, train_loss: 0.0941, step time: 1.0420\n",
      "284/295, train_loss: 0.1455, step time: 1.0383\n",
      "285/295, train_loss: 0.1080, step time: 1.0469\n",
      "286/295, train_loss: 0.0339, step time: 1.0357\n",
      "287/295, train_loss: 0.3669, step time: 1.0444\n",
      "288/295, train_loss: 0.0253, step time: 1.0435\n",
      "289/295, train_loss: 0.0566, step time: 1.0578\n",
      "290/295, train_loss: 0.0267, step time: 1.0300\n",
      "291/295, train_loss: 0.0698, step time: 1.0309\n",
      "292/295, train_loss: 0.0355, step time: 1.0304\n",
      "293/295, train_loss: 0.3737, step time: 1.0304\n",
      "294/295, train_loss: 0.0294, step time: 1.0308\n",
      "295/295, train_loss: 0.0304, step time: 1.0293\n",
      "epoch 66 average loss: 0.0886\n",
      "current epoch: 66 current mean dice: 0.7654 tc: 0.7236 wt: 0.8314 et: 0.7483\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 66 is: 380.7844\n",
      "----------\n",
      "epoch 67/100\n",
      "1/295, train_loss: 0.0670, step time: 1.0883\n",
      "2/295, train_loss: 0.0921, step time: 1.1695\n",
      "3/295, train_loss: 0.0932, step time: 1.0700\n",
      "4/295, train_loss: 0.0529, step time: 1.1093\n",
      "5/295, train_loss: 0.0523, step time: 1.0547\n",
      "6/295, train_loss: 0.0291, step time: 1.0404\n",
      "7/295, train_loss: 0.0354, step time: 1.0584\n",
      "8/295, train_loss: 0.3808, step time: 1.0296\n",
      "9/295, train_loss: 0.3815, step time: 1.0411\n",
      "10/295, train_loss: 0.0514, step time: 1.0382\n",
      "11/295, train_loss: 0.0382, step time: 1.0366\n",
      "12/295, train_loss: 0.0947, step time: 1.0321\n",
      "13/295, train_loss: 0.0840, step time: 1.1259\n",
      "14/295, train_loss: 0.0577, step time: 1.0334\n",
      "15/295, train_loss: 0.0674, step time: 1.0722\n",
      "16/295, train_loss: 0.0338, step time: 1.0396\n",
      "17/295, train_loss: 0.0596, step time: 1.0562\n",
      "18/295, train_loss: 0.1053, step time: 1.1229\n",
      "19/295, train_loss: 0.0330, step time: 1.0329\n",
      "20/295, train_loss: 0.0350, step time: 1.0567\n",
      "21/295, train_loss: 0.0937, step time: 1.0440\n",
      "22/295, train_loss: 0.0861, step time: 1.0878\n",
      "23/295, train_loss: 0.0552, step time: 1.0338\n",
      "24/295, train_loss: 0.0265, step time: 1.0340\n",
      "25/295, train_loss: 0.0657, step time: 1.0418\n",
      "26/295, train_loss: 0.0580, step time: 1.0652\n",
      "27/295, train_loss: 0.4014, step time: 1.0468\n",
      "28/295, train_loss: 0.0475, step time: 1.0389\n",
      "29/295, train_loss: 0.0773, step time: 1.0386\n",
      "30/295, train_loss: 0.0204, step time: 1.0560\n",
      "31/295, train_loss: 0.0765, step time: 1.0362\n",
      "32/295, train_loss: 0.0299, step time: 1.0369\n",
      "33/295, train_loss: 0.0705, step time: 1.0452\n",
      "34/295, train_loss: 0.0644, step time: 1.0778\n",
      "35/295, train_loss: 0.0575, step time: 1.0909\n",
      "36/295, train_loss: 0.0987, step time: 1.0403\n",
      "37/295, train_loss: 0.0340, step time: 1.0984\n",
      "38/295, train_loss: 0.0417, step time: 1.0407\n",
      "39/295, train_loss: 0.0663, step time: 1.0595\n",
      "40/295, train_loss: 0.0428, step time: 1.0657\n",
      "41/295, train_loss: 0.0799, step time: 1.0424\n",
      "42/295, train_loss: 0.3824, step time: 1.0492\n",
      "43/295, train_loss: 0.0647, step time: 1.0435\n",
      "44/295, train_loss: 0.1242, step time: 1.0383\n",
      "45/295, train_loss: 0.0627, step time: 1.0396\n",
      "46/295, train_loss: 0.0644, step time: 1.0366\n",
      "47/295, train_loss: 0.3828, step time: 1.0524\n",
      "48/295, train_loss: 0.0604, step time: 1.0375\n",
      "49/295, train_loss: 0.0841, step time: 1.0550\n",
      "50/295, train_loss: 0.0808, step time: 1.0513\n",
      "51/295, train_loss: 0.0269, step time: 1.0359\n",
      "52/295, train_loss: 0.0712, step time: 1.1125\n",
      "53/295, train_loss: 0.0799, step time: 1.0310\n",
      "54/295, train_loss: 0.0520, step time: 1.0352\n",
      "55/295, train_loss: 0.0603, step time: 1.0305\n",
      "56/295, train_loss: 0.0572, step time: 1.0371\n",
      "57/295, train_loss: 0.0349, step time: 1.0418\n",
      "58/295, train_loss: 0.0441, step time: 1.0414\n",
      "59/295, train_loss: 0.0820, step time: 1.0631\n",
      "60/295, train_loss: 0.0752, step time: 1.0395\n",
      "61/295, train_loss: 0.0310, step time: 1.0534\n",
      "62/295, train_loss: 0.0392, step time: 1.0433\n",
      "63/295, train_loss: 0.0516, step time: 1.0825\n",
      "64/295, train_loss: 0.0843, step time: 1.0483\n",
      "65/295, train_loss: 0.0558, step time: 1.0382\n",
      "66/295, train_loss: 0.0490, step time: 1.0438\n",
      "67/295, train_loss: 0.0622, step time: 1.0573\n",
      "68/295, train_loss: 0.0808, step time: 1.0880\n",
      "69/295, train_loss: 0.0270, step time: 1.0395\n",
      "70/295, train_loss: 0.0468, step time: 1.0454\n",
      "71/295, train_loss: 0.0505, step time: 1.0334\n",
      "72/295, train_loss: 0.0586, step time: 1.0437\n",
      "73/295, train_loss: 0.0644, step time: 1.0496\n",
      "74/295, train_loss: 0.1037, step time: 1.0447\n",
      "75/295, train_loss: 0.1053, step time: 1.0881\n",
      "76/295, train_loss: 0.0597, step time: 1.0871\n",
      "77/295, train_loss: 0.0567, step time: 1.0364\n",
      "78/295, train_loss: 0.3824, step time: 1.0320\n",
      "79/295, train_loss: 0.0280, step time: 1.0369\n",
      "80/295, train_loss: 0.1130, step time: 1.0405\n",
      "81/295, train_loss: 0.0386, step time: 1.0476\n",
      "82/295, train_loss: 0.0640, step time: 1.0452\n",
      "83/295, train_loss: 0.0386, step time: 1.0450\n",
      "84/295, train_loss: 0.0289, step time: 1.0298\n",
      "85/295, train_loss: 0.1156, step time: 1.0318\n",
      "86/295, train_loss: 0.3855, step time: 1.0321\n",
      "87/295, train_loss: 0.0299, step time: 1.0504\n",
      "88/295, train_loss: 0.3656, step time: 1.0766\n",
      "89/295, train_loss: 0.1669, step time: 1.0698\n",
      "90/295, train_loss: 0.0724, step time: 1.1003\n",
      "91/295, train_loss: 0.0357, step time: 1.0458\n",
      "92/295, train_loss: 0.0298, step time: 1.0384\n",
      "93/295, train_loss: 0.0342, step time: 1.0317\n",
      "94/295, train_loss: 0.0395, step time: 1.0324\n",
      "95/295, train_loss: 0.3632, step time: 1.0525\n",
      "96/295, train_loss: 0.0553, step time: 1.0396\n",
      "97/295, train_loss: 0.1039, step time: 1.0499\n",
      "98/295, train_loss: 0.0311, step time: 1.0538\n",
      "99/295, train_loss: 0.0795, step time: 1.0742\n",
      "100/295, train_loss: 0.2158, step time: 1.0437\n",
      "101/295, train_loss: 0.0871, step time: 1.0682\n",
      "102/295, train_loss: 0.0824, step time: 1.0394\n",
      "103/295, train_loss: 0.3873, step time: 1.0386\n",
      "104/295, train_loss: 0.0607, step time: 1.0527\n",
      "105/295, train_loss: 0.0620, step time: 1.0418\n",
      "106/295, train_loss: 0.0737, step time: 1.0365\n",
      "107/295, train_loss: 0.3576, step time: 1.0474\n",
      "108/295, train_loss: 0.0896, step time: 1.0332\n",
      "109/295, train_loss: 0.0264, step time: 1.0614\n",
      "110/295, train_loss: 0.0514, step time: 1.0394\n",
      "111/295, train_loss: 0.0290, step time: 1.0570\n",
      "112/295, train_loss: 0.0790, step time: 1.0760\n",
      "113/295, train_loss: 0.0418, step time: 1.0392\n",
      "114/295, train_loss: 0.0717, step time: 1.0547\n",
      "115/295, train_loss: 0.0548, step time: 1.0468\n",
      "116/295, train_loss: 0.0339, step time: 1.0463\n",
      "117/295, train_loss: 0.0470, step time: 1.0503\n",
      "118/295, train_loss: 0.1054, step time: 1.0416\n",
      "119/295, train_loss: 0.0544, step time: 1.0588\n",
      "120/295, train_loss: 0.0907, step time: 1.0368\n",
      "121/295, train_loss: 0.0794, step time: 1.0337\n",
      "122/295, train_loss: 0.0470, step time: 1.0832\n",
      "123/295, train_loss: 0.0877, step time: 1.0327\n",
      "124/295, train_loss: 0.0390, step time: 1.0517\n",
      "125/295, train_loss: 0.0423, step time: 1.0492\n",
      "126/295, train_loss: 0.0695, step time: 1.0897\n",
      "127/295, train_loss: 0.0664, step time: 1.0341\n",
      "128/295, train_loss: 0.0450, step time: 1.0464\n",
      "129/295, train_loss: 0.1507, step time: 1.0883\n",
      "130/295, train_loss: 0.0685, step time: 1.0334\n",
      "131/295, train_loss: 0.0523, step time: 1.0441\n",
      "132/295, train_loss: 0.1031, step time: 1.0762\n",
      "133/295, train_loss: 0.0684, step time: 1.0393\n",
      "134/295, train_loss: 0.0782, step time: 1.0721\n",
      "135/295, train_loss: 0.0525, step time: 1.0330\n",
      "136/295, train_loss: 0.0859, step time: 1.0545\n",
      "137/295, train_loss: 0.0335, step time: 1.0410\n",
      "138/295, train_loss: 0.0386, step time: 1.0406\n",
      "139/295, train_loss: 0.0628, step time: 1.0382\n",
      "140/295, train_loss: 0.0258, step time: 1.0627\n",
      "141/295, train_loss: 0.0446, step time: 1.0498\n",
      "142/295, train_loss: 0.1979, step time: 1.0533\n",
      "143/295, train_loss: 0.0431, step time: 1.0400\n",
      "144/295, train_loss: 0.0578, step time: 1.1139\n",
      "145/295, train_loss: 0.0412, step time: 1.0524\n",
      "146/295, train_loss: 0.0701, step time: 1.0985\n",
      "147/295, train_loss: 0.0897, step time: 1.0693\n",
      "148/295, train_loss: 0.0986, step time: 1.0314\n",
      "149/295, train_loss: 0.3701, step time: 1.0766\n",
      "150/295, train_loss: 0.0347, step time: 1.0400\n",
      "151/295, train_loss: 0.0455, step time: 1.0423\n",
      "152/295, train_loss: 0.0817, step time: 1.0612\n",
      "153/295, train_loss: 0.0603, step time: 1.0450\n",
      "154/295, train_loss: 0.1209, step time: 1.0325\n",
      "155/295, train_loss: 0.0591, step time: 1.0535\n",
      "156/295, train_loss: 0.0663, step time: 1.0463\n",
      "157/295, train_loss: 0.3855, step time: 1.0515\n",
      "158/295, train_loss: 0.0770, step time: 1.0830\n",
      "159/295, train_loss: 0.0822, step time: 1.0432\n",
      "160/295, train_loss: 0.0403, step time: 1.0690\n",
      "161/295, train_loss: 0.0493, step time: 1.0639\n",
      "162/295, train_loss: 0.3746, step time: 1.0381\n",
      "163/295, train_loss: 0.0695, step time: 1.0371\n",
      "164/295, train_loss: 0.1061, step time: 1.0560\n",
      "165/295, train_loss: 0.1463, step time: 1.0397\n",
      "166/295, train_loss: 0.1320, step time: 1.0750\n",
      "167/295, train_loss: 0.0392, step time: 1.0389\n",
      "168/295, train_loss: 0.0682, step time: 1.0351\n",
      "169/295, train_loss: 0.0664, step time: 1.0326\n",
      "170/295, train_loss: 0.0508, step time: 1.0381\n",
      "171/295, train_loss: 0.0784, step time: 1.0348\n",
      "172/295, train_loss: 0.0284, step time: 1.0546\n",
      "173/295, train_loss: 0.0785, step time: 1.0394\n",
      "174/295, train_loss: 0.0327, step time: 1.0596\n",
      "175/295, train_loss: 0.0963, step time: 1.0592\n",
      "176/295, train_loss: 0.0925, step time: 1.1148\n",
      "177/295, train_loss: 0.0692, step time: 1.0436\n",
      "178/295, train_loss: 0.0772, step time: 1.0579\n",
      "179/295, train_loss: 0.0532, step time: 1.0388\n",
      "180/295, train_loss: 0.0446, step time: 1.0682\n",
      "181/295, train_loss: 0.0284, step time: 1.0324\n",
      "182/295, train_loss: 0.3737, step time: 1.0484\n",
      "183/295, train_loss: 0.1031, step time: 1.0539\n",
      "184/295, train_loss: 0.0586, step time: 1.0768\n",
      "185/295, train_loss: 0.0360, step time: 1.0527\n",
      "186/295, train_loss: 0.0909, step time: 1.0412\n",
      "187/295, train_loss: 0.0850, step time: 1.0379\n",
      "188/295, train_loss: 0.0476, step time: 1.0549\n",
      "189/295, train_loss: 0.0287, step time: 1.0397\n",
      "190/295, train_loss: 0.0438, step time: 1.0503\n",
      "191/295, train_loss: 0.0815, step time: 1.0382\n",
      "192/295, train_loss: 0.1167, step time: 1.0894\n",
      "193/295, train_loss: 0.0501, step time: 1.0397\n",
      "194/295, train_loss: 0.0626, step time: 1.0385\n",
      "195/295, train_loss: 0.0292, step time: 1.0317\n",
      "196/295, train_loss: 0.0965, step time: 1.0822\n",
      "197/295, train_loss: 0.0433, step time: 1.0500\n",
      "198/295, train_loss: 0.0911, step time: 1.0434\n",
      "199/295, train_loss: 0.0846, step time: 1.0617\n",
      "200/295, train_loss: 0.0485, step time: 1.0435\n",
      "201/295, train_loss: 0.0552, step time: 1.0556\n",
      "202/295, train_loss: 0.0725, step time: 1.0945\n",
      "203/295, train_loss: 0.1206, step time: 1.0377\n",
      "204/295, train_loss: 0.0497, step time: 1.0524\n",
      "205/295, train_loss: 0.0420, step time: 1.0408\n",
      "206/295, train_loss: 0.0862, step time: 1.0545\n",
      "207/295, train_loss: 0.3833, step time: 1.1201\n",
      "208/295, train_loss: 0.0299, step time: 1.0385\n",
      "209/295, train_loss: 0.0339, step time: 1.0417\n",
      "210/295, train_loss: 0.1237, step time: 1.0362\n",
      "211/295, train_loss: 0.0365, step time: 1.0371\n",
      "212/295, train_loss: 0.0386, step time: 1.0559\n",
      "213/295, train_loss: 0.0910, step time: 1.0528\n",
      "214/295, train_loss: 0.0638, step time: 1.0361\n",
      "215/295, train_loss: 0.0669, step time: 1.0643\n",
      "216/295, train_loss: 0.0686, step time: 1.0487\n",
      "217/295, train_loss: 0.0796, step time: 1.0360\n",
      "218/295, train_loss: 0.0641, step time: 1.0590\n",
      "219/295, train_loss: 0.0664, step time: 1.0319\n",
      "220/295, train_loss: 0.0703, step time: 1.0379\n",
      "221/295, train_loss: 0.0639, step time: 1.0340\n",
      "222/295, train_loss: 0.3634, step time: 1.0621\n",
      "223/295, train_loss: 0.0397, step time: 1.0438\n",
      "224/295, train_loss: 0.0838, step time: 1.0648\n",
      "225/295, train_loss: 0.0533, step time: 1.0638\n",
      "226/295, train_loss: 0.1148, step time: 1.0415\n",
      "227/295, train_loss: 0.0540, step time: 1.0315\n",
      "228/295, train_loss: 0.0502, step time: 1.0433\n",
      "229/295, train_loss: 0.0506, step time: 1.0521\n",
      "230/295, train_loss: 0.0384, step time: 1.0313\n",
      "231/295, train_loss: 0.0410, step time: 1.0629\n",
      "232/295, train_loss: 0.0684, step time: 1.0566\n",
      "233/295, train_loss: 0.0523, step time: 1.0349\n",
      "234/295, train_loss: 0.0739, step time: 1.0601\n",
      "235/295, train_loss: 0.0469, step time: 1.0553\n",
      "236/295, train_loss: 0.0745, step time: 1.1088\n",
      "237/295, train_loss: 0.0741, step time: 1.0356\n",
      "238/295, train_loss: 0.0508, step time: 1.0409\n",
      "239/295, train_loss: 0.4605, step time: 1.0558\n",
      "240/295, train_loss: 0.0918, step time: 1.0396\n",
      "241/295, train_loss: 0.1030, step time: 1.0324\n",
      "242/295, train_loss: 0.0945, step time: 1.0421\n",
      "243/295, train_loss: 0.0461, step time: 1.0458\n",
      "244/295, train_loss: 0.0499, step time: 1.1232\n",
      "245/295, train_loss: 0.0609, step time: 1.0336\n",
      "246/295, train_loss: 0.0303, step time: 1.0545\n",
      "247/295, train_loss: 0.0340, step time: 1.0362\n",
      "248/295, train_loss: 0.0512, step time: 1.0401\n",
      "249/295, train_loss: 0.0407, step time: 1.0559\n",
      "250/295, train_loss: 0.3621, step time: 1.0361\n",
      "251/295, train_loss: 0.0857, step time: 1.0383\n",
      "252/295, train_loss: 0.0459, step time: 1.0386\n",
      "253/295, train_loss: 0.0974, step time: 1.0430\n",
      "254/295, train_loss: 0.0324, step time: 1.0419\n",
      "255/295, train_loss: 0.0504, step time: 1.0547\n",
      "256/295, train_loss: 0.0488, step time: 1.0484\n",
      "257/295, train_loss: 0.0285, step time: 1.0520\n",
      "258/295, train_loss: 0.0410, step time: 1.0412\n",
      "259/295, train_loss: 0.0405, step time: 1.0559\n",
      "260/295, train_loss: 0.0446, step time: 1.0736\n",
      "261/295, train_loss: 0.0443, step time: 1.0545\n",
      "262/295, train_loss: 0.1162, step time: 1.0612\n",
      "263/295, train_loss: 0.3940, step time: 1.0427\n",
      "264/295, train_loss: 0.0840, step time: 1.0322\n",
      "265/295, train_loss: 0.0254, step time: 1.0376\n",
      "266/295, train_loss: 0.0631, step time: 1.0354\n",
      "267/295, train_loss: 0.0540, step time: 1.0413\n",
      "268/295, train_loss: 0.1113, step time: 1.0312\n",
      "269/295, train_loss: 0.0579, step time: 1.0564\n",
      "270/295, train_loss: 0.0522, step time: 1.0409\n",
      "271/295, train_loss: 0.0683, step time: 1.0379\n",
      "272/295, train_loss: 0.0717, step time: 1.0484\n",
      "273/295, train_loss: 0.0486, step time: 1.0421\n",
      "274/295, train_loss: 0.0571, step time: 1.0526\n",
      "275/295, train_loss: 0.1041, step time: 1.0528\n",
      "276/295, train_loss: 0.0657, step time: 1.0521\n",
      "277/295, train_loss: 0.3979, step time: 1.0514\n",
      "278/295, train_loss: 0.0466, step time: 1.0424\n",
      "279/295, train_loss: 0.0401, step time: 1.0698\n",
      "280/295, train_loss: 0.0431, step time: 1.0582\n",
      "281/295, train_loss: 0.0450, step time: 1.0621\n",
      "282/295, train_loss: 0.0252, step time: 1.1066\n",
      "283/295, train_loss: 0.0285, step time: 1.0316\n",
      "284/295, train_loss: 0.3660, step time: 1.0356\n",
      "285/295, train_loss: 0.0336, step time: 1.0368\n",
      "286/295, train_loss: 0.0882, step time: 1.0807\n",
      "287/295, train_loss: 0.0544, step time: 1.0665\n",
      "288/295, train_loss: 0.0427, step time: 1.1053\n",
      "289/295, train_loss: 0.0343, step time: 1.0309\n",
      "290/295, train_loss: 0.0338, step time: 1.0315\n",
      "291/295, train_loss: 0.0330, step time: 1.0301\n",
      "292/295, train_loss: 0.0301, step time: 1.0308\n",
      "293/295, train_loss: 0.0381, step time: 1.0304\n",
      "294/295, train_loss: 0.0924, step time: 1.0308\n",
      "295/295, train_loss: 0.0720, step time: 1.0311\n",
      "epoch 67 average loss: 0.0872\n",
      "current epoch: 67 current mean dice: 0.7784 tc: 0.7364 wt: 0.8433 et: 0.7599\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 67 is: 386.9742\n",
      "----------\n",
      "epoch 68/100\n",
      "1/295, train_loss: 0.0958, step time: 1.1108\n",
      "2/295, train_loss: 0.0687, step time: 1.0926\n",
      "3/295, train_loss: 0.0653, step time: 1.0652\n",
      "4/295, train_loss: 0.1289, step time: 1.0885\n",
      "5/295, train_loss: 0.0465, step time: 1.0525\n",
      "6/295, train_loss: 0.4021, step time: 1.0470\n",
      "7/295, train_loss: 0.0633, step time: 1.0770\n",
      "8/295, train_loss: 0.0286, step time: 1.0398\n",
      "9/295, train_loss: 0.0294, step time: 1.0405\n",
      "10/295, train_loss: 0.0374, step time: 1.0705\n",
      "11/295, train_loss: 0.0511, step time: 1.0382\n",
      "12/295, train_loss: 0.0777, step time: 1.1024\n",
      "13/295, train_loss: 0.0827, step time: 1.0500\n",
      "14/295, train_loss: 0.0832, step time: 1.0980\n",
      "15/295, train_loss: 0.0509, step time: 1.0420\n",
      "16/295, train_loss: 0.0325, step time: 1.0355\n",
      "17/295, train_loss: 0.0394, step time: 1.0877\n",
      "18/295, train_loss: 0.0380, step time: 1.0332\n",
      "19/295, train_loss: 0.0240, step time: 1.0386\n",
      "20/295, train_loss: 0.0317, step time: 1.0420\n",
      "21/295, train_loss: 0.1418, step time: 1.0446\n",
      "22/295, train_loss: 0.1656, step time: 1.0384\n",
      "23/295, train_loss: 0.0400, step time: 1.0337\n",
      "24/295, train_loss: 0.0384, step time: 1.0513\n",
      "25/295, train_loss: 0.1014, step time: 1.0483\n",
      "26/295, train_loss: 0.3930, step time: 1.1021\n",
      "27/295, train_loss: 0.0492, step time: 1.0356\n",
      "28/295, train_loss: 0.0335, step time: 1.0336\n",
      "29/295, train_loss: 0.0337, step time: 1.0470\n",
      "30/295, train_loss: 0.0679, step time: 1.0470\n",
      "31/295, train_loss: 0.0299, step time: 1.1622\n",
      "32/295, train_loss: 0.0884, step time: 1.0344\n",
      "33/295, train_loss: 0.0612, step time: 1.0354\n",
      "34/295, train_loss: 0.0334, step time: 1.0456\n",
      "35/295, train_loss: 0.0530, step time: 1.0984\n",
      "36/295, train_loss: 0.1284, step time: 1.0365\n",
      "37/295, train_loss: 0.0329, step time: 1.0410\n",
      "38/295, train_loss: 0.0906, step time: 1.1034\n",
      "39/295, train_loss: 0.0277, step time: 1.0564\n",
      "40/295, train_loss: 0.0441, step time: 1.0403\n",
      "41/295, train_loss: 0.0680, step time: 1.0587\n",
      "42/295, train_loss: 0.0784, step time: 1.0541\n",
      "43/295, train_loss: 0.0430, step time: 1.0336\n",
      "44/295, train_loss: 0.0763, step time: 1.0376\n",
      "45/295, train_loss: 0.0416, step time: 1.0486\n",
      "46/295, train_loss: 0.1052, step time: 1.0347\n",
      "47/295, train_loss: 0.3828, step time: 1.0440\n",
      "48/295, train_loss: 0.0435, step time: 1.0440\n",
      "49/295, train_loss: 0.0292, step time: 1.0359\n",
      "50/295, train_loss: 0.0373, step time: 1.0380\n",
      "51/295, train_loss: 0.0319, step time: 1.0591\n",
      "52/295, train_loss: 0.0369, step time: 1.0424\n",
      "53/295, train_loss: 0.0367, step time: 1.0452\n",
      "54/295, train_loss: 0.0804, step time: 1.0402\n",
      "55/295, train_loss: 0.1150, step time: 1.0605\n",
      "56/295, train_loss: 0.0927, step time: 1.0443\n",
      "57/295, train_loss: 0.0338, step time: 1.0562\n",
      "58/295, train_loss: 0.0286, step time: 1.0694\n",
      "59/295, train_loss: 0.0832, step time: 1.0624\n",
      "60/295, train_loss: 0.0454, step time: 1.0381\n",
      "61/295, train_loss: 0.3723, step time: 1.0548\n",
      "62/295, train_loss: 0.0262, step time: 1.0350\n",
      "63/295, train_loss: 0.0798, step time: 1.0357\n",
      "64/295, train_loss: 0.0844, step time: 1.0405\n",
      "65/295, train_loss: 0.0893, step time: 1.0458\n",
      "66/295, train_loss: 0.0812, step time: 1.0706\n",
      "67/295, train_loss: 0.0436, step time: 1.1056\n",
      "68/295, train_loss: 0.0995, step time: 1.0519\n",
      "69/295, train_loss: 0.0830, step time: 1.0418\n",
      "70/295, train_loss: 0.0564, step time: 1.0412\n",
      "71/295, train_loss: 0.0315, step time: 1.1013\n",
      "72/295, train_loss: 0.0425, step time: 1.0547\n",
      "73/295, train_loss: 0.3823, step time: 1.0636\n",
      "74/295, train_loss: 0.0541, step time: 1.0330\n",
      "75/295, train_loss: 0.0760, step time: 1.0329\n",
      "76/295, train_loss: 0.0593, step time: 1.0636\n",
      "77/295, train_loss: 0.0543, step time: 1.0343\n",
      "78/295, train_loss: 0.0644, step time: 1.0419\n",
      "79/295, train_loss: 0.0821, step time: 1.0659\n",
      "80/295, train_loss: 0.0801, step time: 1.0370\n",
      "81/295, train_loss: 0.0285, step time: 1.0681\n",
      "82/295, train_loss: 0.0542, step time: 1.0404\n",
      "83/295, train_loss: 0.0551, step time: 1.0405\n",
      "84/295, train_loss: 0.0816, step time: 1.0916\n",
      "85/295, train_loss: 0.0660, step time: 1.0449\n",
      "86/295, train_loss: 0.0358, step time: 1.0509\n",
      "87/295, train_loss: 0.0873, step time: 1.0410\n",
      "88/295, train_loss: 0.0328, step time: 1.0385\n",
      "89/295, train_loss: 0.1075, step time: 1.0443\n",
      "90/295, train_loss: 0.0611, step time: 1.0818\n",
      "91/295, train_loss: 0.0503, step time: 1.0352\n",
      "92/295, train_loss: 0.0409, step time: 1.0444\n",
      "93/295, train_loss: 0.0492, step time: 1.0922\n",
      "94/295, train_loss: 0.0506, step time: 1.0399\n",
      "95/295, train_loss: 0.0779, step time: 1.0369\n",
      "96/295, train_loss: 0.0686, step time: 1.0738\n",
      "97/295, train_loss: 0.0513, step time: 1.0673\n",
      "98/295, train_loss: 0.0335, step time: 1.0431\n",
      "99/295, train_loss: 0.0639, step time: 1.0314\n",
      "100/295, train_loss: 0.0375, step time: 1.0612\n",
      "101/295, train_loss: 0.0461, step time: 1.0349\n",
      "102/295, train_loss: 0.0466, step time: 1.0375\n",
      "103/295, train_loss: 0.0700, step time: 1.0516\n",
      "104/295, train_loss: 0.0607, step time: 1.1045\n",
      "105/295, train_loss: 0.0486, step time: 1.0318\n",
      "106/295, train_loss: 0.3624, step time: 1.0566\n",
      "107/295, train_loss: 0.0401, step time: 1.0370\n",
      "108/295, train_loss: 0.0715, step time: 1.0367\n",
      "109/295, train_loss: 0.1090, step time: 1.0477\n",
      "110/295, train_loss: 0.1240, step time: 1.0650\n",
      "111/295, train_loss: 0.0876, step time: 1.0471\n",
      "112/295, train_loss: 0.0827, step time: 1.0573\n",
      "113/295, train_loss: 0.0649, step time: 1.0525\n",
      "114/295, train_loss: 0.3820, step time: 1.0636\n",
      "115/295, train_loss: 0.0626, step time: 1.0478\n",
      "116/295, train_loss: 0.0533, step time: 1.0380\n",
      "117/295, train_loss: 0.0559, step time: 1.0621\n",
      "118/295, train_loss: 0.0279, step time: 1.0366\n",
      "119/295, train_loss: 0.0779, step time: 1.0468\n",
      "120/295, train_loss: 0.0740, step time: 1.0354\n",
      "121/295, train_loss: 0.0545, step time: 1.0654\n",
      "122/295, train_loss: 0.0623, step time: 1.0446\n",
      "123/295, train_loss: 0.0268, step time: 1.0349\n",
      "124/295, train_loss: 0.0674, step time: 1.0604\n",
      "125/295, train_loss: 0.0414, step time: 1.0341\n",
      "126/295, train_loss: 0.0545, step time: 1.0352\n",
      "127/295, train_loss: 0.0453, step time: 1.0356\n",
      "128/295, train_loss: 0.0595, step time: 1.0458\n",
      "129/295, train_loss: 0.0364, step time: 1.0518\n",
      "130/295, train_loss: 0.3850, step time: 1.0481\n",
      "131/295, train_loss: 0.0408, step time: 1.0425\n",
      "132/295, train_loss: 0.0263, step time: 1.0475\n",
      "133/295, train_loss: 0.0480, step time: 1.0374\n",
      "134/295, train_loss: 0.0339, step time: 1.1475\n",
      "135/295, train_loss: 0.0552, step time: 1.0363\n",
      "136/295, train_loss: 0.0245, step time: 1.0360\n",
      "137/295, train_loss: 0.3868, step time: 1.0727\n",
      "138/295, train_loss: 0.3649, step time: 1.0409\n",
      "139/295, train_loss: 0.0726, step time: 1.0582\n",
      "140/295, train_loss: 0.0977, step time: 1.0603\n",
      "141/295, train_loss: 0.0275, step time: 1.0436\n",
      "142/295, train_loss: 0.0581, step time: 1.0796\n",
      "143/295, train_loss: 0.0298, step time: 1.0587\n",
      "144/295, train_loss: 0.0418, step time: 1.0409\n",
      "145/295, train_loss: 0.1024, step time: 1.0519\n",
      "146/295, train_loss: 0.0716, step time: 1.0772\n",
      "147/295, train_loss: 0.0453, step time: 1.0311\n",
      "148/295, train_loss: 0.0879, step time: 1.0436\n",
      "149/295, train_loss: 0.0499, step time: 1.0392\n",
      "150/295, train_loss: 0.3811, step time: 1.0754\n",
      "151/295, train_loss: 0.0354, step time: 1.0367\n",
      "152/295, train_loss: 0.0807, step time: 1.0679\n",
      "153/295, train_loss: 0.0387, step time: 1.0324\n",
      "154/295, train_loss: 0.0677, step time: 1.0483\n",
      "155/295, train_loss: 0.1037, step time: 1.0621\n",
      "156/295, train_loss: 0.0362, step time: 1.0519\n",
      "157/295, train_loss: 0.0935, step time: 1.0361\n",
      "158/295, train_loss: 0.1166, step time: 1.0354\n",
      "159/295, train_loss: 0.3833, step time: 1.0429\n",
      "160/295, train_loss: 0.0284, step time: 1.0742\n",
      "161/295, train_loss: 0.0340, step time: 1.0395\n",
      "162/295, train_loss: 0.0610, step time: 1.0387\n",
      "163/295, train_loss: 0.0986, step time: 1.0521\n",
      "164/295, train_loss: 0.0609, step time: 1.0853\n",
      "165/295, train_loss: 0.1040, step time: 1.0368\n",
      "166/295, train_loss: 0.0589, step time: 1.0527\n",
      "167/295, train_loss: 0.0681, step time: 1.0417\n",
      "168/295, train_loss: 0.3827, step time: 1.0357\n",
      "169/295, train_loss: 0.0837, step time: 1.0656\n",
      "170/295, train_loss: 0.0926, step time: 1.0853\n",
      "171/295, train_loss: 0.0957, step time: 1.0627\n",
      "172/295, train_loss: 0.0499, step time: 1.0544\n",
      "173/295, train_loss: 0.0766, step time: 1.0438\n",
      "174/295, train_loss: 0.3841, step time: 1.0596\n",
      "175/295, train_loss: 0.0470, step time: 1.0772\n",
      "176/295, train_loss: 0.0257, step time: 1.0788\n",
      "177/295, train_loss: 0.0534, step time: 1.0366\n",
      "178/295, train_loss: 0.0783, step time: 1.0353\n",
      "179/295, train_loss: 0.0397, step time: 1.0387\n",
      "180/295, train_loss: 0.0403, step time: 1.0399\n",
      "181/295, train_loss: 0.3727, step time: 1.0382\n",
      "182/295, train_loss: 0.1141, step time: 1.0433\n",
      "183/295, train_loss: 0.0436, step time: 1.0324\n",
      "184/295, train_loss: 0.0344, step time: 1.0365\n",
      "185/295, train_loss: 0.0426, step time: 1.0384\n",
      "186/295, train_loss: 0.0678, step time: 1.0313\n",
      "187/295, train_loss: 0.0589, step time: 1.0591\n",
      "188/295, train_loss: 0.1160, step time: 1.0438\n",
      "189/295, train_loss: 0.0801, step time: 1.0502\n",
      "190/295, train_loss: 0.0846, step time: 1.0805\n",
      "191/295, train_loss: 0.0676, step time: 1.0349\n",
      "192/295, train_loss: 0.0507, step time: 1.0350\n",
      "193/295, train_loss: 0.0705, step time: 1.0417\n",
      "194/295, train_loss: 0.1030, step time: 1.0329\n",
      "195/295, train_loss: 0.0791, step time: 1.0408\n",
      "196/295, train_loss: 0.0807, step time: 1.0568\n",
      "197/295, train_loss: 0.3653, step time: 1.0431\n",
      "198/295, train_loss: 0.0345, step time: 1.0528\n",
      "199/295, train_loss: 0.1187, step time: 1.0440\n",
      "200/295, train_loss: 0.0871, step time: 1.0443\n",
      "201/295, train_loss: 0.0686, step time: 1.0367\n",
      "202/295, train_loss: 0.0698, step time: 1.0357\n",
      "203/295, train_loss: 0.0686, step time: 1.0475\n",
      "204/295, train_loss: 0.0855, step time: 1.0557\n",
      "205/295, train_loss: 0.0514, step time: 1.0544\n",
      "206/295, train_loss: 0.3683, step time: 1.0321\n",
      "207/295, train_loss: 0.3637, step time: 1.0359\n",
      "208/295, train_loss: 0.0632, step time: 1.0354\n",
      "209/295, train_loss: 0.0455, step time: 1.0677\n",
      "210/295, train_loss: 0.0673, step time: 1.0629\n",
      "211/295, train_loss: 0.0513, step time: 1.0401\n",
      "212/295, train_loss: 0.1006, step time: 1.0333\n",
      "213/295, train_loss: 0.0412, step time: 1.0430\n",
      "214/295, train_loss: 0.0288, step time: 1.0633\n",
      "215/295, train_loss: 0.0392, step time: 1.0372\n",
      "216/295, train_loss: 0.0685, step time: 1.0378\n",
      "217/295, train_loss: 0.0476, step time: 1.0387\n",
      "218/295, train_loss: 0.0778, step time: 1.0433\n",
      "219/295, train_loss: 0.0640, step time: 1.0447\n",
      "220/295, train_loss: 0.0541, step time: 1.0521\n",
      "221/295, train_loss: 0.0345, step time: 1.0388\n",
      "222/295, train_loss: 0.0288, step time: 1.0333\n",
      "223/295, train_loss: 0.0983, step time: 1.0393\n",
      "224/295, train_loss: 0.0572, step time: 1.0661\n",
      "225/295, train_loss: 0.0303, step time: 1.0465\n",
      "226/295, train_loss: 0.0273, step time: 1.0889\n",
      "227/295, train_loss: 0.0564, step time: 1.0522\n",
      "228/295, train_loss: 0.1484, step time: 1.0689\n",
      "229/295, train_loss: 0.0326, step time: 1.0436\n",
      "230/295, train_loss: 0.1214, step time: 1.0416\n",
      "231/295, train_loss: 0.0849, step time: 1.0562\n",
      "232/295, train_loss: 0.0740, step time: 1.0714\n",
      "233/295, train_loss: 0.0530, step time: 1.0342\n",
      "234/295, train_loss: 0.0581, step time: 1.0350\n",
      "235/295, train_loss: 0.0328, step time: 1.0336\n",
      "236/295, train_loss: 0.0537, step time: 1.0371\n",
      "237/295, train_loss: 0.0940, step time: 1.0390\n",
      "238/295, train_loss: 0.0304, step time: 1.0337\n",
      "239/295, train_loss: 0.0629, step time: 1.0414\n",
      "240/295, train_loss: 0.0446, step time: 1.0663\n",
      "241/295, train_loss: 0.0494, step time: 1.0564\n",
      "242/295, train_loss: 0.0603, step time: 1.0383\n",
      "243/295, train_loss: 0.0419, step time: 1.0354\n",
      "244/295, train_loss: 0.0704, step time: 1.0554\n",
      "245/295, train_loss: 0.1217, step time: 1.0340\n",
      "246/295, train_loss: 0.0543, step time: 1.0606\n",
      "247/295, train_loss: 0.0644, step time: 1.0382\n",
      "248/295, train_loss: 0.4569, step time: 1.0400\n",
      "249/295, train_loss: 0.0893, step time: 1.1032\n",
      "250/295, train_loss: 0.3601, step time: 1.0681\n",
      "251/295, train_loss: 0.0504, step time: 1.0416\n",
      "252/295, train_loss: 0.0449, step time: 1.0734\n",
      "253/295, train_loss: 0.0529, step time: 1.0469\n",
      "254/295, train_loss: 0.0435, step time: 1.0348\n",
      "255/295, train_loss: 0.0590, step time: 1.0503\n",
      "256/295, train_loss: 0.0469, step time: 1.0558\n",
      "257/295, train_loss: 0.0866, step time: 1.0397\n",
      "258/295, train_loss: 0.0288, step time: 1.0568\n",
      "259/295, train_loss: 0.0565, step time: 1.1051\n",
      "260/295, train_loss: 0.0826, step time: 1.0601\n",
      "261/295, train_loss: 0.0649, step time: 1.0419\n",
      "262/295, train_loss: 0.0525, step time: 1.0481\n",
      "263/295, train_loss: 0.0691, step time: 1.0437\n",
      "264/295, train_loss: 0.0874, step time: 1.0547\n",
      "265/295, train_loss: 0.0909, step time: 1.0360\n",
      "266/295, train_loss: 0.0663, step time: 1.0335\n",
      "267/295, train_loss: 0.0473, step time: 1.0320\n",
      "268/295, train_loss: 0.0332, step time: 1.0406\n",
      "269/295, train_loss: 0.0199, step time: 1.0580\n",
      "270/295, train_loss: 0.3736, step time: 1.0431\n",
      "271/295, train_loss: 0.0508, step time: 1.0427\n",
      "272/295, train_loss: 0.0646, step time: 1.0406\n",
      "273/295, train_loss: 0.0489, step time: 1.0602\n",
      "274/295, train_loss: 0.0730, step time: 1.0576\n",
      "275/295, train_loss: 0.0511, step time: 1.0455\n",
      "276/295, train_loss: 0.1145, step time: 1.0442\n",
      "277/295, train_loss: 0.2258, step time: 1.0639\n",
      "278/295, train_loss: 0.0802, step time: 1.0594\n",
      "279/295, train_loss: 0.0574, step time: 1.0388\n",
      "280/295, train_loss: 0.0647, step time: 1.0367\n",
      "281/295, train_loss: 0.0751, step time: 1.0466\n",
      "282/295, train_loss: 0.0486, step time: 1.0584\n",
      "283/295, train_loss: 0.0573, step time: 1.0812\n",
      "284/295, train_loss: 0.0415, step time: 1.0381\n",
      "285/295, train_loss: 0.3971, step time: 1.0355\n",
      "286/295, train_loss: 0.0647, step time: 1.0429\n",
      "287/295, train_loss: 0.0991, step time: 1.0456\n",
      "288/295, train_loss: 0.0810, step time: 1.0889\n",
      "289/295, train_loss: 0.0355, step time: 1.0300\n",
      "290/295, train_loss: 0.0870, step time: 1.0308\n",
      "291/295, train_loss: 0.0551, step time: 1.0288\n",
      "292/295, train_loss: 0.1063, step time: 1.0298\n",
      "293/295, train_loss: 0.0610, step time: 1.0307\n",
      "294/295, train_loss: 0.0336, step time: 1.0306\n",
      "295/295, train_loss: 0.0442, step time: 1.0300\n",
      "epoch 68 average loss: 0.0865\n",
      "current epoch: 68 current mean dice: 0.7818 tc: 0.7376 wt: 0.8425 et: 0.7685\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 68 is: 390.1721\n",
      "----------\n",
      "epoch 69/100\n",
      "1/295, train_loss: 0.0613, step time: 1.1106\n",
      "2/295, train_loss: 0.0698, step time: 1.0994\n",
      "3/295, train_loss: 0.0519, step time: 1.0684\n",
      "4/295, train_loss: 0.0518, step time: 1.0547\n",
      "5/295, train_loss: 0.0798, step time: 1.1220\n",
      "6/295, train_loss: 0.0330, step time: 1.0447\n",
      "7/295, train_loss: 0.0640, step time: 1.0788\n",
      "8/295, train_loss: 0.0469, step time: 1.0347\n",
      "9/295, train_loss: 0.0898, step time: 1.0369\n",
      "10/295, train_loss: 0.0424, step time: 1.0363\n",
      "11/295, train_loss: 0.0939, step time: 1.0591\n",
      "12/295, train_loss: 0.0668, step time: 1.0684\n",
      "13/295, train_loss: 0.0874, step time: 1.0324\n",
      "14/295, train_loss: 0.1798, step time: 1.0487\n",
      "15/295, train_loss: 0.0676, step time: 1.0554\n",
      "16/295, train_loss: 0.0955, step time: 1.0361\n",
      "17/295, train_loss: 0.0809, step time: 1.0384\n",
      "18/295, train_loss: 0.3576, step time: 1.0695\n",
      "19/295, train_loss: 0.0723, step time: 1.0382\n",
      "20/295, train_loss: 0.0590, step time: 1.0458\n",
      "21/295, train_loss: 0.0570, step time: 1.0964\n",
      "22/295, train_loss: 0.0983, step time: 1.0645\n",
      "23/295, train_loss: 0.0365, step time: 1.0407\n",
      "24/295, train_loss: 0.0450, step time: 1.0539\n",
      "25/295, train_loss: 0.0820, step time: 1.0353\n",
      "26/295, train_loss: 0.0909, step time: 1.0333\n",
      "27/295, train_loss: 0.1039, step time: 1.0561\n",
      "28/295, train_loss: 0.0433, step time: 1.0816\n",
      "29/295, train_loss: 0.0487, step time: 1.0837\n",
      "30/295, train_loss: 0.0557, step time: 1.0633\n",
      "31/295, train_loss: 0.0489, step time: 1.0880\n",
      "32/295, train_loss: 0.0270, step time: 1.0356\n",
      "33/295, train_loss: 0.0309, step time: 1.0389\n",
      "34/295, train_loss: 0.0805, step time: 1.0448\n",
      "35/295, train_loss: 0.0573, step time: 1.0513\n",
      "36/295, train_loss: 0.2658, step time: 1.1048\n",
      "37/295, train_loss: 0.0762, step time: 1.0621\n",
      "38/295, train_loss: 0.0315, step time: 1.0652\n",
      "39/295, train_loss: 0.3874, step time: 1.0534\n",
      "40/295, train_loss: 0.0759, step time: 1.0538\n",
      "41/295, train_loss: 0.0545, step time: 1.0414\n",
      "42/295, train_loss: 0.3631, step time: 1.0636\n",
      "43/295, train_loss: 0.0444, step time: 1.0348\n",
      "44/295, train_loss: 0.0682, step time: 1.1205\n",
      "45/295, train_loss: 0.0476, step time: 1.0530\n",
      "46/295, train_loss: 0.0283, step time: 1.0381\n",
      "47/295, train_loss: 0.3874, step time: 1.0523\n",
      "48/295, train_loss: 0.0264, step time: 1.0320\n",
      "49/295, train_loss: 0.0825, step time: 1.0450\n",
      "50/295, train_loss: 0.0641, step time: 1.0533\n",
      "51/295, train_loss: 0.1005, step time: 1.0608\n",
      "52/295, train_loss: 0.0348, step time: 1.0830\n",
      "53/295, train_loss: 0.1171, step time: 1.0363\n",
      "54/295, train_loss: 0.0594, step time: 1.0429\n",
      "55/295, train_loss: 0.0857, step time: 1.0638\n",
      "56/295, train_loss: 0.0684, step time: 1.0639\n",
      "57/295, train_loss: 0.4064, step time: 1.1024\n",
      "58/295, train_loss: 0.0453, step time: 1.0492\n",
      "59/295, train_loss: 0.0315, step time: 1.0935\n",
      "60/295, train_loss: 0.1492, step time: 1.0633\n",
      "61/295, train_loss: 0.0554, step time: 1.0633\n",
      "62/295, train_loss: 0.0708, step time: 1.1044\n",
      "63/295, train_loss: 0.0592, step time: 1.0408\n",
      "64/295, train_loss: 0.3842, step time: 1.0336\n",
      "65/295, train_loss: 0.0874, step time: 1.0473\n",
      "66/295, train_loss: 0.0399, step time: 1.0304\n",
      "67/295, train_loss: 0.1488, step time: 1.0328\n",
      "68/295, train_loss: 0.0266, step time: 1.0665\n",
      "69/295, train_loss: 0.0569, step time: 1.0477\n",
      "70/295, train_loss: 0.0914, step time: 1.0506\n",
      "71/295, train_loss: 0.0290, step time: 1.0335\n",
      "72/295, train_loss: 0.0475, step time: 1.0346\n",
      "73/295, train_loss: 0.0340, step time: 1.0520\n",
      "74/295, train_loss: 0.0704, step time: 1.0698\n",
      "75/295, train_loss: 0.0320, step time: 1.0337\n",
      "76/295, train_loss: 0.0697, step time: 1.0416\n",
      "77/295, train_loss: 0.0793, step time: 1.0485\n",
      "78/295, train_loss: 0.0827, step time: 1.1132\n",
      "79/295, train_loss: 0.0696, step time: 1.0358\n",
      "80/295, train_loss: 0.0292, step time: 1.0402\n",
      "81/295, train_loss: 0.0526, step time: 1.0774\n",
      "82/295, train_loss: 0.0477, step time: 1.0788\n",
      "83/295, train_loss: 0.0866, step time: 1.0360\n",
      "84/295, train_loss: 0.0480, step time: 1.0878\n",
      "85/295, train_loss: 0.0302, step time: 1.0406\n",
      "86/295, train_loss: 0.1157, step time: 1.0363\n",
      "87/295, train_loss: 0.0615, step time: 1.0416\n",
      "88/295, train_loss: 0.3932, step time: 1.0537\n",
      "89/295, train_loss: 0.1178, step time: 1.0376\n",
      "90/295, train_loss: 0.1134, step time: 1.0578\n",
      "91/295, train_loss: 0.0507, step time: 1.0358\n",
      "92/295, train_loss: 0.4562, step time: 1.0323\n",
      "93/295, train_loss: 0.0304, step time: 1.0614\n",
      "94/295, train_loss: 0.0384, step time: 1.0564\n",
      "95/295, train_loss: 0.0387, step time: 1.0342\n",
      "96/295, train_loss: 0.0643, step time: 1.0520\n",
      "97/295, train_loss: 0.0487, step time: 1.0598\n",
      "98/295, train_loss: 0.0385, step time: 1.0745\n",
      "99/295, train_loss: 0.0658, step time: 1.0418\n",
      "100/295, train_loss: 0.0547, step time: 1.0429\n",
      "101/295, train_loss: 0.0443, step time: 1.0411\n",
      "102/295, train_loss: 0.0304, step time: 1.0444\n",
      "103/295, train_loss: 0.0397, step time: 1.0333\n",
      "104/295, train_loss: 0.0862, step time: 1.0654\n",
      "105/295, train_loss: 0.0510, step time: 1.0602\n",
      "106/295, train_loss: 0.1125, step time: 1.0502\n",
      "107/295, train_loss: 0.0434, step time: 1.0577\n",
      "108/295, train_loss: 0.0714, step time: 1.0808\n",
      "109/295, train_loss: 0.0420, step time: 1.0395\n",
      "110/295, train_loss: 0.0915, step time: 1.0608\n",
      "111/295, train_loss: 0.0808, step time: 1.0412\n",
      "112/295, train_loss: 0.0451, step time: 1.0666\n",
      "113/295, train_loss: 0.0339, step time: 1.0432\n",
      "114/295, train_loss: 0.0641, step time: 1.0695\n",
      "115/295, train_loss: 0.0453, step time: 1.0461\n",
      "116/295, train_loss: 0.0778, step time: 1.0407\n",
      "117/295, train_loss: 0.0507, step time: 1.0421\n",
      "118/295, train_loss: 0.0449, step time: 1.0668\n",
      "119/295, train_loss: 0.0546, step time: 1.0347\n",
      "120/295, train_loss: 0.0580, step time: 1.0420\n",
      "121/295, train_loss: 0.0502, step time: 1.0473\n",
      "122/295, train_loss: 0.0281, step time: 1.0745\n",
      "123/295, train_loss: 0.0843, step time: 1.0358\n",
      "124/295, train_loss: 0.0374, step time: 1.0640\n",
      "125/295, train_loss: 0.3813, step time: 1.0604\n",
      "126/295, train_loss: 0.0511, step time: 1.0363\n",
      "127/295, train_loss: 0.0325, step time: 1.0402\n",
      "128/295, train_loss: 0.0566, step time: 1.0936\n",
      "129/295, train_loss: 0.0512, step time: 1.0781\n",
      "130/295, train_loss: 0.0555, step time: 1.0320\n",
      "131/295, train_loss: 0.3628, step time: 1.0460\n",
      "132/295, train_loss: 0.0423, step time: 1.0664\n",
      "133/295, train_loss: 0.0682, step time: 1.0395\n",
      "134/295, train_loss: 0.0371, step time: 1.0446\n",
      "135/295, train_loss: 0.0318, step time: 1.0583\n",
      "136/295, train_loss: 0.1076, step time: 1.0473\n",
      "137/295, train_loss: 0.0695, step time: 1.0468\n",
      "138/295, train_loss: 0.0427, step time: 1.0412\n",
      "139/295, train_loss: 0.0731, step time: 1.0761\n",
      "140/295, train_loss: 0.0998, step time: 1.0386\n",
      "141/295, train_loss: 0.0196, step time: 1.0458\n",
      "142/295, train_loss: 0.0459, step time: 1.0445\n",
      "143/295, train_loss: 0.0730, step time: 1.0747\n",
      "144/295, train_loss: 0.0334, step time: 1.0570\n",
      "145/295, train_loss: 0.4028, step time: 1.1194\n",
      "146/295, train_loss: 0.0686, step time: 1.0375\n",
      "147/295, train_loss: 0.0369, step time: 1.0403\n",
      "148/295, train_loss: 0.0498, step time: 1.0588\n",
      "149/295, train_loss: 0.0342, step time: 1.0347\n",
      "150/295, train_loss: 0.0648, step time: 1.0398\n",
      "151/295, train_loss: 0.0582, step time: 1.0431\n",
      "152/295, train_loss: 0.1159, step time: 1.0416\n",
      "153/295, train_loss: 0.0675, step time: 1.0569\n",
      "154/295, train_loss: 0.0445, step time: 1.0364\n",
      "155/295, train_loss: 0.0808, step time: 1.0439\n",
      "156/295, train_loss: 0.0645, step time: 1.1069\n",
      "157/295, train_loss: 0.3849, step time: 1.0331\n",
      "158/295, train_loss: 0.0829, step time: 1.0395\n",
      "159/295, train_loss: 0.0526, step time: 1.0380\n",
      "160/295, train_loss: 0.0299, step time: 1.0465\n",
      "161/295, train_loss: 0.0797, step time: 1.0404\n",
      "162/295, train_loss: 0.0693, step time: 1.0397\n",
      "163/295, train_loss: 0.0494, step time: 1.0414\n",
      "164/295, train_loss: 0.3739, step time: 1.0517\n",
      "165/295, train_loss: 0.0308, step time: 1.0629\n",
      "166/295, train_loss: 0.0578, step time: 1.1236\n",
      "167/295, train_loss: 0.0396, step time: 1.0362\n",
      "168/295, train_loss: 0.3832, step time: 1.0462\n",
      "169/295, train_loss: 0.0536, step time: 1.0454\n",
      "170/295, train_loss: 0.0365, step time: 1.0630\n",
      "171/295, train_loss: 0.0973, step time: 1.0404\n",
      "172/295, train_loss: 0.0505, step time: 1.0660\n",
      "173/295, train_loss: 0.0627, step time: 1.0589\n",
      "174/295, train_loss: 0.3719, step time: 1.0359\n",
      "175/295, train_loss: 0.0340, step time: 1.0663\n",
      "176/295, train_loss: 0.0587, step time: 1.0449\n",
      "177/295, train_loss: 0.0849, step time: 1.0373\n",
      "178/295, train_loss: 0.0606, step time: 1.0427\n",
      "179/295, train_loss: 0.1081, step time: 1.0466\n",
      "180/295, train_loss: 0.1028, step time: 1.0326\n",
      "181/295, train_loss: 0.0424, step time: 1.0381\n",
      "182/295, train_loss: 0.3605, step time: 1.0512\n",
      "183/295, train_loss: 0.0280, step time: 1.0629\n",
      "184/295, train_loss: 0.0371, step time: 1.0920\n",
      "185/295, train_loss: 0.0385, step time: 1.0384\n",
      "186/295, train_loss: 0.0342, step time: 1.0834\n",
      "187/295, train_loss: 0.0424, step time: 1.0393\n",
      "188/295, train_loss: 0.0268, step time: 1.0538\n",
      "189/295, train_loss: 0.0327, step time: 1.0377\n",
      "190/295, train_loss: 0.0414, step time: 1.0352\n",
      "191/295, train_loss: 0.0714, step time: 1.0391\n",
      "192/295, train_loss: 0.1057, step time: 1.1183\n",
      "193/295, train_loss: 0.0615, step time: 1.0386\n",
      "194/295, train_loss: 0.0530, step time: 1.0324\n",
      "195/295, train_loss: 0.0741, step time: 1.0620\n",
      "196/295, train_loss: 0.0690, step time: 1.0438\n",
      "197/295, train_loss: 0.0612, step time: 1.0640\n",
      "198/295, train_loss: 0.0808, step time: 1.0503\n",
      "199/295, train_loss: 0.0579, step time: 1.0413\n",
      "200/295, train_loss: 0.0620, step time: 1.0365\n",
      "201/295, train_loss: 0.0372, step time: 1.0584\n",
      "202/295, train_loss: 0.3750, step time: 1.0468\n",
      "203/295, train_loss: 0.0870, step time: 1.0418\n",
      "204/295, train_loss: 0.0681, step time: 1.0354\n",
      "205/295, train_loss: 0.0306, step time: 1.0356\n",
      "206/295, train_loss: 0.3661, step time: 1.0404\n",
      "207/295, train_loss: 0.0621, step time: 1.0611\n",
      "208/295, train_loss: 0.0719, step time: 1.0573\n",
      "209/295, train_loss: 0.1019, step time: 1.0623\n",
      "210/295, train_loss: 0.0498, step time: 1.0529\n",
      "211/295, train_loss: 0.0667, step time: 1.0368\n",
      "212/295, train_loss: 0.0636, step time: 1.0422\n",
      "213/295, train_loss: 0.0437, step time: 1.0522\n",
      "214/295, train_loss: 0.0418, step time: 1.0443\n",
      "215/295, train_loss: 0.0619, step time: 1.0351\n",
      "216/295, train_loss: 0.0797, step time: 1.0388\n",
      "217/295, train_loss: 0.1034, step time: 1.0362\n",
      "218/295, train_loss: 0.0514, step time: 1.0315\n",
      "219/295, train_loss: 0.0916, step time: 1.0346\n",
      "220/295, train_loss: 0.0790, step time: 1.0395\n",
      "221/295, train_loss: 0.0413, step time: 1.0409\n",
      "222/295, train_loss: 0.0817, step time: 1.0914\n",
      "223/295, train_loss: 0.3832, step time: 1.0573\n",
      "224/295, train_loss: 0.0464, step time: 1.0470\n",
      "225/295, train_loss: 0.0243, step time: 1.0893\n",
      "226/295, train_loss: 0.0666, step time: 1.0381\n",
      "227/295, train_loss: 0.0472, step time: 1.0420\n",
      "228/295, train_loss: 0.0643, step time: 1.0398\n",
      "229/295, train_loss: 0.3941, step time: 1.0520\n",
      "230/295, train_loss: 0.0957, step time: 1.0986\n",
      "231/295, train_loss: 0.0290, step time: 1.0662\n",
      "232/295, train_loss: 0.0921, step time: 1.0403\n",
      "233/295, train_loss: 0.3854, step time: 1.0341\n",
      "234/295, train_loss: 0.0322, step time: 1.0438\n",
      "235/295, train_loss: 0.0374, step time: 1.0545\n",
      "236/295, train_loss: 0.0548, step time: 1.0435\n",
      "237/295, train_loss: 0.0609, step time: 1.0387\n",
      "238/295, train_loss: 0.0508, step time: 1.0429\n",
      "239/295, train_loss: 0.0568, step time: 1.0481\n",
      "240/295, train_loss: 0.3663, step time: 1.0865\n",
      "241/295, train_loss: 0.1130, step time: 1.0317\n",
      "242/295, train_loss: 0.0418, step time: 1.0394\n",
      "243/295, train_loss: 0.0825, step time: 1.0332\n",
      "244/295, train_loss: 0.0272, step time: 1.0378\n",
      "245/295, train_loss: 0.0723, step time: 1.0378\n",
      "246/295, train_loss: 0.0513, step time: 1.0430\n",
      "247/295, train_loss: 0.0354, step time: 1.0498\n",
      "248/295, train_loss: 0.0820, step time: 1.0549\n",
      "249/295, train_loss: 0.0606, step time: 1.0425\n",
      "250/295, train_loss: 0.0272, step time: 1.0409\n",
      "251/295, train_loss: 0.0993, step time: 1.0376\n",
      "252/295, train_loss: 0.0258, step time: 1.0366\n",
      "253/295, train_loss: 0.0605, step time: 1.0391\n",
      "254/295, train_loss: 0.0348, step time: 1.0598\n",
      "255/295, train_loss: 0.1086, step time: 1.0385\n",
      "256/295, train_loss: 0.0569, step time: 1.0448\n",
      "257/295, train_loss: 0.0346, step time: 1.0372\n",
      "258/295, train_loss: 0.0937, step time: 1.0353\n",
      "259/295, train_loss: 0.0486, step time: 1.0558\n",
      "260/295, train_loss: 0.1275, step time: 1.0696\n",
      "261/295, train_loss: 0.0461, step time: 1.0605\n",
      "262/295, train_loss: 0.0846, step time: 1.0420\n",
      "263/295, train_loss: 0.1204, step time: 1.0359\n",
      "264/295, train_loss: 0.0339, step time: 1.0377\n",
      "265/295, train_loss: 0.0343, step time: 1.0433\n",
      "266/295, train_loss: 0.0449, step time: 1.0444\n",
      "267/295, train_loss: 0.0345, step time: 1.0873\n",
      "268/295, train_loss: 0.0493, step time: 1.0302\n",
      "269/295, train_loss: 0.0899, step time: 1.0397\n",
      "270/295, train_loss: 0.0842, step time: 1.0356\n",
      "271/295, train_loss: 0.0788, step time: 1.0506\n",
      "272/295, train_loss: 0.0288, step time: 1.0353\n",
      "273/295, train_loss: 0.0286, step time: 1.0378\n",
      "274/295, train_loss: 0.0819, step time: 1.0333\n",
      "275/295, train_loss: 0.0897, step time: 1.0385\n",
      "276/295, train_loss: 0.0937, step time: 1.0609\n",
      "277/295, train_loss: 0.1221, step time: 1.0459\n",
      "278/295, train_loss: 0.0458, step time: 1.0387\n",
      "279/295, train_loss: 0.0961, step time: 1.0343\n",
      "280/295, train_loss: 0.0467, step time: 1.0359\n",
      "281/295, train_loss: 0.0657, step time: 1.0498\n",
      "282/295, train_loss: 0.0778, step time: 1.1304\n",
      "283/295, train_loss: 0.0307, step time: 1.0514\n",
      "284/295, train_loss: 0.0799, step time: 1.0373\n",
      "285/295, train_loss: 0.0677, step time: 1.0617\n",
      "286/295, train_loss: 0.0555, step time: 1.0357\n",
      "287/295, train_loss: 0.0543, step time: 1.0448\n",
      "288/295, train_loss: 0.0765, step time: 1.0449\n",
      "289/295, train_loss: 0.0330, step time: 1.0295\n",
      "290/295, train_loss: 0.0368, step time: 1.0305\n",
      "291/295, train_loss: 0.0549, step time: 1.0293\n",
      "292/295, train_loss: 0.0532, step time: 1.0298\n",
      "293/295, train_loss: 0.0844, step time: 1.0313\n",
      "294/295, train_loss: 0.1257, step time: 1.0295\n",
      "295/295, train_loss: 0.0830, step time: 1.0294\n",
      "epoch 69 average loss: 0.0866\n",
      "current epoch: 69 current mean dice: 0.7857 tc: 0.7494 wt: 0.8403 et: 0.7710\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 69 is: 380.5785\n",
      "----------\n",
      "epoch 70/100\n",
      "1/295, train_loss: 0.0290, step time: 1.0496\n",
      "2/295, train_loss: 0.0335, step time: 1.0481\n",
      "3/295, train_loss: 0.1018, step time: 1.0414\n",
      "4/295, train_loss: 0.4002, step time: 1.0557\n",
      "5/295, train_loss: 0.0424, step time: 1.0570\n",
      "6/295, train_loss: 0.0616, step time: 1.0484\n",
      "7/295, train_loss: 0.0395, step time: 1.0401\n",
      "8/295, train_loss: 0.0540, step time: 1.0451\n",
      "9/295, train_loss: 0.0994, step time: 1.0498\n",
      "10/295, train_loss: 0.3947, step time: 1.0416\n",
      "11/295, train_loss: 0.0720, step time: 1.0353\n",
      "12/295, train_loss: 0.0299, step time: 1.0676\n",
      "13/295, train_loss: 0.1255, step time: 1.0722\n",
      "14/295, train_loss: 0.0368, step time: 1.0332\n",
      "15/295, train_loss: 0.0446, step time: 1.0742\n",
      "16/295, train_loss: 0.0332, step time: 1.0562\n",
      "17/295, train_loss: 0.0348, step time: 1.0322\n",
      "18/295, train_loss: 0.0967, step time: 1.0601\n",
      "19/295, train_loss: 0.0823, step time: 1.0601\n",
      "20/295, train_loss: 0.0319, step time: 1.0447\n",
      "21/295, train_loss: 0.0567, step time: 1.0310\n",
      "22/295, train_loss: 0.0944, step time: 1.0440\n",
      "23/295, train_loss: 0.0713, step time: 1.0658\n",
      "24/295, train_loss: 0.0675, step time: 1.0626\n",
      "25/295, train_loss: 0.1057, step time: 1.0352\n",
      "26/295, train_loss: 0.0485, step time: 1.0801\n",
      "27/295, train_loss: 0.0508, step time: 1.0504\n",
      "28/295, train_loss: 0.0686, step time: 1.0704\n",
      "29/295, train_loss: 0.0643, step time: 1.0436\n",
      "30/295, train_loss: 0.0515, step time: 1.0327\n",
      "31/295, train_loss: 0.0643, step time: 1.0540\n",
      "32/295, train_loss: 0.3804, step time: 1.0506\n",
      "33/295, train_loss: 0.0788, step time: 1.0367\n",
      "34/295, train_loss: 0.1140, step time: 1.0493\n",
      "35/295, train_loss: 0.0313, step time: 1.1058\n",
      "36/295, train_loss: 0.0628, step time: 1.0372\n",
      "37/295, train_loss: 0.0687, step time: 1.0537\n",
      "38/295, train_loss: 0.0464, step time: 1.0858\n",
      "39/295, train_loss: 0.0334, step time: 1.0329\n",
      "40/295, train_loss: 0.0620, step time: 1.0862\n",
      "41/295, train_loss: 0.0683, step time: 1.0342\n",
      "42/295, train_loss: 0.0420, step time: 1.0726\n",
      "43/295, train_loss: 0.0661, step time: 1.0536\n",
      "44/295, train_loss: 0.0359, step time: 1.0297\n",
      "45/295, train_loss: 0.0719, step time: 1.0542\n",
      "46/295, train_loss: 0.0644, step time: 1.0299\n",
      "47/295, train_loss: 0.0316, step time: 1.0415\n",
      "48/295, train_loss: 0.0289, step time: 1.0297\n",
      "49/295, train_loss: 0.3822, step time: 1.0309\n",
      "50/295, train_loss: 0.0529, step time: 1.0324\n",
      "51/295, train_loss: 0.0525, step time: 1.0320\n",
      "52/295, train_loss: 0.0461, step time: 1.0385\n",
      "53/295, train_loss: 0.0504, step time: 1.0806\n",
      "54/295, train_loss: 0.0360, step time: 1.0492\n",
      "55/295, train_loss: 0.0413, step time: 1.0397\n",
      "56/295, train_loss: 0.0750, step time: 1.0456\n",
      "57/295, train_loss: 0.0502, step time: 1.0586\n",
      "58/295, train_loss: 0.0913, step time: 1.0370\n",
      "59/295, train_loss: 0.0808, step time: 1.0606\n",
      "60/295, train_loss: 0.0470, step time: 1.0396\n",
      "61/295, train_loss: 0.1124, step time: 1.0340\n",
      "62/295, train_loss: 0.1008, step time: 1.0393\n",
      "63/295, train_loss: 0.0571, step time: 1.0388\n",
      "64/295, train_loss: 0.0246, step time: 1.0559\n",
      "65/295, train_loss: 0.1000, step time: 1.0609\n",
      "66/295, train_loss: 0.1038, step time: 1.0322\n",
      "67/295, train_loss: 0.3861, step time: 1.0426\n",
      "68/295, train_loss: 0.0378, step time: 1.0351\n",
      "69/295, train_loss: 0.0835, step time: 1.0369\n",
      "70/295, train_loss: 0.0430, step time: 1.0381\n",
      "71/295, train_loss: 0.0510, step time: 1.0325\n",
      "72/295, train_loss: 0.0346, step time: 1.1199\n",
      "73/295, train_loss: 0.0491, step time: 1.0379\n",
      "74/295, train_loss: 0.0447, step time: 1.0385\n",
      "75/295, train_loss: 0.0628, step time: 1.0390\n",
      "76/295, train_loss: 0.0484, step time: 1.0300\n",
      "77/295, train_loss: 0.3791, step time: 1.0372\n",
      "78/295, train_loss: 0.0812, step time: 1.0302\n",
      "79/295, train_loss: 0.0727, step time: 1.0353\n",
      "80/295, train_loss: 0.0880, step time: 1.0863\n",
      "81/295, train_loss: 0.0428, step time: 1.0584\n",
      "82/295, train_loss: 0.0297, step time: 1.0641\n",
      "83/295, train_loss: 0.3635, step time: 1.0417\n",
      "84/295, train_loss: 0.3839, step time: 1.0611\n",
      "85/295, train_loss: 0.0516, step time: 1.0579\n",
      "86/295, train_loss: 0.0675, step time: 1.0822\n",
      "87/295, train_loss: 0.0874, step time: 1.0484\n",
      "88/295, train_loss: 0.3612, step time: 1.0419\n",
      "89/295, train_loss: 0.0812, step time: 1.0999\n",
      "90/295, train_loss: 0.0760, step time: 1.0840\n",
      "91/295, train_loss: 0.0555, step time: 1.0411\n",
      "92/295, train_loss: 0.3633, step time: 1.0395\n",
      "93/295, train_loss: 0.0439, step time: 1.0332\n",
      "94/295, train_loss: 0.0559, step time: 1.0735\n",
      "95/295, train_loss: 0.0291, step time: 1.0405\n",
      "96/295, train_loss: 0.0352, step time: 1.0990\n",
      "97/295, train_loss: 0.0318, step time: 1.0498\n",
      "98/295, train_loss: 0.0750, step time: 1.0319\n",
      "99/295, train_loss: 0.0548, step time: 1.0349\n",
      "100/295, train_loss: 0.0647, step time: 1.0396\n",
      "101/295, train_loss: 0.0832, step time: 1.0347\n",
      "102/295, train_loss: 0.0814, step time: 1.0353\n",
      "103/295, train_loss: 0.0675, step time: 1.0979\n",
      "104/295, train_loss: 0.0256, step time: 1.0371\n",
      "105/295, train_loss: 0.0545, step time: 1.0609\n",
      "106/295, train_loss: 0.0614, step time: 1.0555\n",
      "107/295, train_loss: 0.0677, step time: 1.0510\n",
      "108/295, train_loss: 0.0517, step time: 1.0397\n",
      "109/295, train_loss: 0.0422, step time: 1.0516\n",
      "110/295, train_loss: 0.0420, step time: 1.0429\n",
      "111/295, train_loss: 0.0337, step time: 1.0357\n",
      "112/295, train_loss: 0.4520, step time: 1.0676\n",
      "113/295, train_loss: 0.0377, step time: 1.0873\n",
      "114/295, train_loss: 0.0411, step time: 1.0419\n",
      "115/295, train_loss: 0.3845, step time: 1.0420\n",
      "116/295, train_loss: 0.0848, step time: 1.0486\n",
      "117/295, train_loss: 0.1058, step time: 1.0559\n",
      "118/295, train_loss: 0.3699, step time: 1.0563\n",
      "119/295, train_loss: 0.0509, step time: 1.0433\n",
      "120/295, train_loss: 0.0717, step time: 1.0652\n",
      "121/295, train_loss: 0.0329, step time: 1.0590\n",
      "122/295, train_loss: 0.0888, step time: 1.0324\n",
      "123/295, train_loss: 0.0451, step time: 1.0854\n",
      "124/295, train_loss: 0.1006, step time: 1.0386\n",
      "125/295, train_loss: 0.3726, step time: 1.0405\n",
      "126/295, train_loss: 0.0987, step time: 1.0341\n",
      "127/295, train_loss: 0.0299, step time: 1.0561\n",
      "128/295, train_loss: 0.0538, step time: 1.0739\n",
      "129/295, train_loss: 0.0730, step time: 1.0429\n",
      "130/295, train_loss: 0.0516, step time: 1.0336\n",
      "131/295, train_loss: 0.0632, step time: 1.0465\n",
      "132/295, train_loss: 0.0910, step time: 1.0825\n",
      "133/295, train_loss: 0.0479, step time: 1.0373\n",
      "134/295, train_loss: 0.0367, step time: 1.0333\n",
      "135/295, train_loss: 0.3649, step time: 1.0350\n",
      "136/295, train_loss: 0.0766, step time: 1.0690\n",
      "137/295, train_loss: 0.0697, step time: 1.0353\n",
      "138/295, train_loss: 0.0266, step time: 1.0468\n",
      "139/295, train_loss: 0.0481, step time: 1.0363\n",
      "140/295, train_loss: 0.0671, step time: 1.0377\n",
      "141/295, train_loss: 0.0619, step time: 1.0368\n",
      "142/295, train_loss: 0.0293, step time: 1.0414\n",
      "143/295, train_loss: 0.0359, step time: 1.0932\n",
      "144/295, train_loss: 0.0872, step time: 1.1210\n",
      "145/295, train_loss: 0.0829, step time: 1.0381\n",
      "146/295, train_loss: 0.0623, step time: 1.0371\n",
      "147/295, train_loss: 0.0806, step time: 1.1241\n",
      "148/295, train_loss: 0.0839, step time: 1.0650\n",
      "149/295, train_loss: 0.1135, step time: 1.0449\n",
      "150/295, train_loss: 0.0411, step time: 1.0451\n",
      "151/295, train_loss: 0.0580, step time: 1.0544\n",
      "152/295, train_loss: 0.0676, step time: 1.0364\n",
      "153/295, train_loss: 0.0376, step time: 1.0359\n",
      "154/295, train_loss: 0.3639, step time: 1.0414\n",
      "155/295, train_loss: 0.0632, step time: 1.0854\n",
      "156/295, train_loss: 0.0199, step time: 1.0799\n",
      "157/295, train_loss: 0.0585, step time: 1.0831\n",
      "158/295, train_loss: 0.0673, step time: 1.0445\n",
      "159/295, train_loss: 0.0410, step time: 1.0583\n",
      "160/295, train_loss: 0.0638, step time: 1.0317\n",
      "161/295, train_loss: 0.0556, step time: 1.0361\n",
      "162/295, train_loss: 0.0758, step time: 1.0478\n",
      "163/295, train_loss: 0.0771, step time: 1.0395\n",
      "164/295, train_loss: 0.0681, step time: 1.0507\n",
      "165/295, train_loss: 0.0497, step time: 1.0326\n",
      "166/295, train_loss: 0.0408, step time: 1.0355\n",
      "167/295, train_loss: 0.0342, step time: 1.0457\n",
      "168/295, train_loss: 0.1063, step time: 1.0529\n",
      "169/295, train_loss: 0.1475, step time: 1.0407\n",
      "170/295, train_loss: 0.0516, step time: 1.0432\n",
      "171/295, train_loss: 0.1122, step time: 1.0388\n",
      "172/295, train_loss: 0.0404, step time: 1.0534\n",
      "173/295, train_loss: 0.3833, step time: 1.0448\n",
      "174/295, train_loss: 0.0804, step time: 1.0390\n",
      "175/295, train_loss: 0.0802, step time: 1.0388\n",
      "176/295, train_loss: 0.0907, step time: 1.0975\n",
      "177/295, train_loss: 0.0458, step time: 1.0400\n",
      "178/295, train_loss: 0.0410, step time: 1.0553\n",
      "179/295, train_loss: 0.0837, step time: 1.0350\n",
      "180/295, train_loss: 0.0565, step time: 1.0415\n",
      "181/295, train_loss: 0.0423, step time: 1.0477\n",
      "182/295, train_loss: 0.0440, step time: 1.0371\n",
      "183/295, train_loss: 0.0804, step time: 1.0374\n",
      "184/295, train_loss: 0.0663, step time: 1.0969\n",
      "185/295, train_loss: 0.0693, step time: 1.0876\n",
      "186/295, train_loss: 0.0719, step time: 1.0344\n",
      "187/295, train_loss: 0.0283, step time: 1.0362\n",
      "188/295, train_loss: 0.0931, step time: 1.0410\n",
      "189/295, train_loss: 0.0350, step time: 1.0397\n",
      "190/295, train_loss: 0.0557, step time: 1.0404\n",
      "191/295, train_loss: 0.0603, step time: 1.0469\n",
      "192/295, train_loss: 0.0632, step time: 1.0392\n",
      "193/295, train_loss: 0.0296, step time: 1.0650\n",
      "194/295, train_loss: 0.0434, step time: 1.0352\n",
      "195/295, train_loss: 0.0489, step time: 1.0352\n",
      "196/295, train_loss: 0.0800, step time: 1.0437\n",
      "197/295, train_loss: 0.1030, step time: 1.0371\n",
      "198/295, train_loss: 0.0268, step time: 1.0372\n",
      "199/295, train_loss: 0.0535, step time: 1.0378\n",
      "200/295, train_loss: 0.0581, step time: 1.0420\n",
      "201/295, train_loss: 0.1013, step time: 1.0808\n",
      "202/295, train_loss: 0.3919, step time: 1.0414\n",
      "203/295, train_loss: 0.0325, step time: 1.0663\n",
      "204/295, train_loss: 0.0422, step time: 1.0481\n",
      "205/295, train_loss: 0.0702, step time: 1.0406\n",
      "206/295, train_loss: 0.0275, step time: 1.0417\n",
      "207/295, train_loss: 0.0275, step time: 1.0531\n",
      "208/295, train_loss: 0.0991, step time: 1.1293\n",
      "209/295, train_loss: 0.1428, step time: 1.0518\n",
      "210/295, train_loss: 0.0568, step time: 1.0809\n",
      "211/295, train_loss: 0.0367, step time: 1.0358\n",
      "212/295, train_loss: 0.1033, step time: 1.0391\n",
      "213/295, train_loss: 0.3837, step time: 1.0499\n",
      "214/295, train_loss: 0.0490, step time: 1.0336\n",
      "215/295, train_loss: 0.0865, step time: 1.0325\n",
      "216/295, train_loss: 0.1159, step time: 1.0591\n",
      "217/295, train_loss: 0.0615, step time: 1.0401\n",
      "218/295, train_loss: 0.0562, step time: 1.0381\n",
      "219/295, train_loss: 0.1193, step time: 1.0749\n",
      "220/295, train_loss: 0.0730, step time: 1.0621\n",
      "221/295, train_loss: 0.0357, step time: 1.0320\n",
      "222/295, train_loss: 0.0534, step time: 1.0380\n",
      "223/295, train_loss: 0.0475, step time: 1.0472\n",
      "224/295, train_loss: 0.0266, step time: 1.0530\n",
      "225/295, train_loss: 0.0283, step time: 1.0359\n",
      "226/295, train_loss: 0.0305, step time: 1.0376\n",
      "227/295, train_loss: 0.0425, step time: 1.0370\n",
      "228/295, train_loss: 0.0638, step time: 1.0361\n",
      "229/295, train_loss: 0.1141, step time: 1.0589\n",
      "230/295, train_loss: 0.0927, step time: 1.0588\n",
      "231/295, train_loss: 0.0255, step time: 1.0489\n",
      "232/295, train_loss: 0.0248, step time: 1.0441\n",
      "233/295, train_loss: 0.0332, step time: 1.0377\n",
      "234/295, train_loss: 0.0553, step time: 1.0360\n",
      "235/295, train_loss: 0.0371, step time: 1.0429\n",
      "236/295, train_loss: 0.0799, step time: 1.0794\n",
      "237/295, train_loss: 0.0855, step time: 1.0347\n",
      "238/295, train_loss: 0.0429, step time: 1.0405\n",
      "239/295, train_loss: 0.0272, step time: 1.1002\n",
      "240/295, train_loss: 0.0787, step time: 1.0455\n",
      "241/295, train_loss: 0.0299, step time: 1.0340\n",
      "242/295, train_loss: 0.0360, step time: 1.0382\n",
      "243/295, train_loss: 0.0724, step time: 1.0426\n",
      "244/295, train_loss: 0.1657, step time: 1.0382\n",
      "245/295, train_loss: 0.0265, step time: 1.0534\n",
      "246/295, train_loss: 0.0856, step time: 1.0458\n",
      "247/295, train_loss: 0.0762, step time: 1.0569\n",
      "248/295, train_loss: 0.3581, step time: 1.1073\n",
      "249/295, train_loss: 0.0866, step time: 1.0543\n",
      "250/295, train_loss: 0.0332, step time: 1.0378\n",
      "251/295, train_loss: 0.0499, step time: 1.0581\n",
      "252/295, train_loss: 0.0456, step time: 1.0368\n",
      "253/295, train_loss: 0.3882, step time: 1.0321\n",
      "254/295, train_loss: 0.0648, step time: 1.0324\n",
      "255/295, train_loss: 0.0628, step time: 1.0347\n",
      "256/295, train_loss: 0.0792, step time: 1.0477\n",
      "257/295, train_loss: 0.0803, step time: 1.1534\n",
      "258/295, train_loss: 0.0537, step time: 1.0639\n",
      "259/295, train_loss: 0.0410, step time: 1.0436\n",
      "260/295, train_loss: 0.0839, step time: 1.0561\n",
      "261/295, train_loss: 0.0457, step time: 1.0390\n",
      "262/295, train_loss: 0.0354, step time: 1.0333\n",
      "263/295, train_loss: 0.0326, step time: 1.0494\n",
      "264/295, train_loss: 0.0583, step time: 1.0653\n",
      "265/295, train_loss: 0.0462, step time: 1.0510\n",
      "266/295, train_loss: 0.1244, step time: 1.0503\n",
      "267/295, train_loss: 0.0324, step time: 1.0349\n",
      "268/295, train_loss: 0.0531, step time: 1.0360\n",
      "269/295, train_loss: 0.0708, step time: 1.0442\n",
      "270/295, train_loss: 0.0524, step time: 1.0469\n",
      "271/295, train_loss: 0.0940, step time: 1.0340\n",
      "272/295, train_loss: 0.0946, step time: 1.0399\n",
      "273/295, train_loss: 0.0507, step time: 1.0597\n",
      "274/295, train_loss: 0.3284, step time: 1.0609\n",
      "275/295, train_loss: 0.0566, step time: 1.0324\n",
      "276/295, train_loss: 0.0733, step time: 1.0370\n",
      "277/295, train_loss: 0.0534, step time: 1.0375\n",
      "278/295, train_loss: 0.0445, step time: 1.0400\n",
      "279/295, train_loss: 0.0291, step time: 1.0362\n",
      "280/295, train_loss: 0.0429, step time: 1.0412\n",
      "281/295, train_loss: 0.0286, step time: 1.0402\n",
      "282/295, train_loss: 0.0619, step time: 1.0313\n",
      "283/295, train_loss: 0.0682, step time: 1.1016\n",
      "284/295, train_loss: 0.0453, step time: 1.0447\n",
      "285/295, train_loss: 0.0431, step time: 1.0384\n",
      "286/295, train_loss: 0.0621, step time: 1.0349\n",
      "287/295, train_loss: 0.1111, step time: 1.0416\n",
      "288/295, train_loss: 0.0623, step time: 1.0653\n",
      "289/295, train_loss: 0.0522, step time: 1.0292\n",
      "290/295, train_loss: 0.0808, step time: 1.0294\n",
      "291/295, train_loss: 0.0264, step time: 1.0294\n",
      "292/295, train_loss: 0.0807, step time: 1.0294\n",
      "293/295, train_loss: 0.3689, step time: 1.0293\n",
      "294/295, train_loss: 0.0792, step time: 1.0287\n",
      "295/295, train_loss: 0.1306, step time: 1.0300\n",
      "epoch 70 average loss: 0.0860\n",
      "current epoch: 70 current mean dice: 0.7836 tc: 0.7439 wt: 0.8439 et: 0.7674\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 70 is: 386.2721\n",
      "----------\n",
      "epoch 71/100\n",
      "1/295, train_loss: 0.0473, step time: 1.1504\n",
      "2/295, train_loss: 0.0339, step time: 1.1215\n",
      "3/295, train_loss: 0.0563, step time: 1.0719\n",
      "4/295, train_loss: 0.0646, step time: 1.0506\n",
      "5/295, train_loss: 0.0798, step time: 1.0471\n",
      "6/295, train_loss: 0.0425, step time: 1.0621\n",
      "7/295, train_loss: 0.4001, step time: 1.0382\n",
      "8/295, train_loss: 0.0923, step time: 1.0369\n",
      "9/295, train_loss: 0.0312, step time: 1.0672\n",
      "10/295, train_loss: 0.0605, step time: 1.0474\n",
      "11/295, train_loss: 0.0612, step time: 1.0330\n",
      "12/295, train_loss: 0.0260, step time: 1.0915\n",
      "13/295, train_loss: 0.0823, step time: 1.0464\n",
      "14/295, train_loss: 0.0314, step time: 1.0352\n",
      "15/295, train_loss: 0.0823, step time: 1.0459\n",
      "16/295, train_loss: 0.1058, step time: 1.0417\n",
      "17/295, train_loss: 0.0785, step time: 1.0359\n",
      "18/295, train_loss: 0.0291, step time: 1.0578\n",
      "19/295, train_loss: 0.0777, step time: 1.0518\n",
      "20/295, train_loss: 0.0447, step time: 1.0674\n",
      "21/295, train_loss: 0.0582, step time: 1.0328\n",
      "22/295, train_loss: 0.0485, step time: 1.0433\n",
      "23/295, train_loss: 0.0726, step time: 1.0696\n",
      "24/295, train_loss: 0.0945, step time: 1.0658\n",
      "25/295, train_loss: 0.0536, step time: 1.0330\n",
      "26/295, train_loss: 0.3626, step time: 1.0359\n",
      "27/295, train_loss: 0.0330, step time: 1.0600\n",
      "28/295, train_loss: 0.0333, step time: 1.0624\n",
      "29/295, train_loss: 0.3894, step time: 1.0508\n",
      "30/295, train_loss: 0.0643, step time: 1.0378\n",
      "31/295, train_loss: 0.0514, step time: 1.0398\n",
      "32/295, train_loss: 0.0480, step time: 1.0403\n",
      "33/295, train_loss: 0.3722, step time: 1.0340\n",
      "34/295, train_loss: 0.0384, step time: 1.0441\n",
      "35/295, train_loss: 0.0279, step time: 1.0347\n",
      "36/295, train_loss: 0.1020, step time: 1.0333\n",
      "37/295, train_loss: 0.0296, step time: 1.0749\n",
      "38/295, train_loss: 0.0682, step time: 1.0541\n",
      "39/295, train_loss: 0.0335, step time: 1.0344\n",
      "40/295, train_loss: 0.0490, step time: 1.0538\n",
      "41/295, train_loss: 0.0271, step time: 1.0327\n",
      "42/295, train_loss: 0.1042, step time: 1.0373\n",
      "43/295, train_loss: 0.0913, step time: 1.0341\n",
      "44/295, train_loss: 0.0501, step time: 1.0447\n",
      "45/295, train_loss: 0.0989, step time: 1.0527\n",
      "46/295, train_loss: 0.1173, step time: 1.0330\n",
      "47/295, train_loss: 0.0669, step time: 1.0467\n",
      "48/295, train_loss: 0.0500, step time: 1.0740\n",
      "49/295, train_loss: 0.0617, step time: 1.0454\n",
      "50/295, train_loss: 0.0559, step time: 1.0609\n",
      "51/295, train_loss: 0.0615, step time: 1.0352\n",
      "52/295, train_loss: 0.0290, step time: 1.0537\n",
      "53/295, train_loss: 0.0533, step time: 1.0422\n",
      "54/295, train_loss: 0.0369, step time: 1.0311\n",
      "55/295, train_loss: 0.1150, step time: 1.0433\n",
      "56/295, train_loss: 0.0833, step time: 1.0355\n",
      "57/295, train_loss: 0.0472, step time: 1.0556\n",
      "58/295, train_loss: 0.0737, step time: 1.0508\n",
      "59/295, train_loss: 0.2285, step time: 1.0590\n",
      "60/295, train_loss: 0.0484, step time: 1.0332\n",
      "61/295, train_loss: 0.0291, step time: 1.0397\n",
      "62/295, train_loss: 0.0763, step time: 1.0686\n",
      "63/295, train_loss: 0.0513, step time: 1.1046\n",
      "64/295, train_loss: 0.3835, step time: 1.0358\n",
      "65/295, train_loss: 0.0978, step time: 1.0367\n",
      "66/295, train_loss: 0.0616, step time: 1.0382\n",
      "67/295, train_loss: 0.0699, step time: 1.0645\n",
      "68/295, train_loss: 0.0414, step time: 1.0498\n",
      "69/295, train_loss: 0.0283, step time: 1.0404\n",
      "70/295, train_loss: 0.0530, step time: 1.0442\n",
      "71/295, train_loss: 0.0967, step time: 1.0310\n",
      "72/295, train_loss: 0.0384, step time: 1.0373\n",
      "73/295, train_loss: 0.0453, step time: 1.0293\n",
      "74/295, train_loss: 0.0397, step time: 1.0507\n",
      "75/295, train_loss: 0.0643, step time: 1.0439\n",
      "76/295, train_loss: 0.0399, step time: 1.0551\n",
      "77/295, train_loss: 0.0997, step time: 1.0433\n",
      "78/295, train_loss: 0.0333, step time: 1.0671\n",
      "79/295, train_loss: 0.0612, step time: 1.0516\n",
      "80/295, train_loss: 0.0476, step time: 1.0475\n",
      "81/295, train_loss: 0.4439, step time: 1.0413\n",
      "82/295, train_loss: 0.0799, step time: 1.0508\n",
      "83/295, train_loss: 0.0478, step time: 1.0461\n",
      "84/295, train_loss: 0.0818, step time: 1.0333\n",
      "85/295, train_loss: 0.0328, step time: 1.0613\n",
      "86/295, train_loss: 0.0391, step time: 1.1495\n",
      "87/295, train_loss: 0.0829, step time: 1.0501\n",
      "88/295, train_loss: 0.0355, step time: 1.0364\n",
      "89/295, train_loss: 0.0318, step time: 1.0696\n",
      "90/295, train_loss: 0.1057, step time: 1.0422\n",
      "91/295, train_loss: 0.3635, step time: 1.0364\n",
      "92/295, train_loss: 0.0371, step time: 1.0600\n",
      "93/295, train_loss: 0.0540, step time: 1.0742\n",
      "94/295, train_loss: 0.0670, step time: 1.0493\n",
      "95/295, train_loss: 0.0440, step time: 1.0745\n",
      "96/295, train_loss: 0.0965, step time: 1.0361\n",
      "97/295, train_loss: 0.0236, step time: 1.0392\n",
      "98/295, train_loss: 0.0510, step time: 1.0491\n",
      "99/295, train_loss: 0.0254, step time: 1.0337\n",
      "100/295, train_loss: 0.0423, step time: 1.0334\n",
      "101/295, train_loss: 0.0244, step time: 1.0561\n",
      "102/295, train_loss: 0.0268, step time: 1.0437\n",
      "103/295, train_loss: 0.0808, step time: 1.1017\n",
      "104/295, train_loss: 0.3842, step time: 1.0611\n",
      "105/295, train_loss: 0.0905, step time: 1.0345\n",
      "106/295, train_loss: 0.0512, step time: 1.0327\n",
      "107/295, train_loss: 0.0784, step time: 1.0381\n",
      "108/295, train_loss: 0.0318, step time: 1.0341\n",
      "109/295, train_loss: 0.0641, step time: 1.0308\n",
      "110/295, train_loss: 0.0415, step time: 1.0835\n",
      "111/295, train_loss: 0.3640, step time: 1.0403\n",
      "112/295, train_loss: 0.0679, step time: 1.0400\n",
      "113/295, train_loss: 0.0364, step time: 1.0353\n",
      "114/295, train_loss: 0.0794, step time: 1.0397\n",
      "115/295, train_loss: 0.3827, step time: 1.0472\n",
      "116/295, train_loss: 0.0556, step time: 1.0442\n",
      "117/295, train_loss: 0.1043, step time: 1.0543\n",
      "118/295, train_loss: 0.0776, step time: 1.0354\n",
      "119/295, train_loss: 0.0655, step time: 1.0412\n",
      "120/295, train_loss: 0.0881, step time: 1.0532\n",
      "121/295, train_loss: 0.0794, step time: 1.0362\n",
      "122/295, train_loss: 0.0502, step time: 1.0513\n",
      "123/295, train_loss: 0.0787, step time: 1.0408\n",
      "124/295, train_loss: 0.3894, step time: 1.0444\n",
      "125/295, train_loss: 0.0602, step time: 1.0671\n",
      "126/295, train_loss: 0.0574, step time: 1.0367\n",
      "127/295, train_loss: 0.0316, step time: 1.0362\n",
      "128/295, train_loss: 0.0517, step time: 1.0337\n",
      "129/295, train_loss: 0.0654, step time: 1.0537\n",
      "130/295, train_loss: 0.0617, step time: 1.0705\n",
      "131/295, train_loss: 0.0487, step time: 1.0383\n",
      "132/295, train_loss: 0.1036, step time: 1.0378\n",
      "133/295, train_loss: 0.0723, step time: 1.0515\n",
      "134/295, train_loss: 0.0517, step time: 1.0323\n",
      "135/295, train_loss: 0.0337, step time: 1.0357\n",
      "136/295, train_loss: 0.3786, step time: 1.0382\n",
      "137/295, train_loss: 0.3733, step time: 1.0498\n",
      "138/295, train_loss: 0.3707, step time: 1.0620\n",
      "139/295, train_loss: 0.0697, step time: 1.0699\n",
      "140/295, train_loss: 0.0261, step time: 1.0344\n",
      "141/295, train_loss: 0.0617, step time: 1.0528\n",
      "142/295, train_loss: 0.0645, step time: 1.1350\n",
      "143/295, train_loss: 0.0342, step time: 1.0380\n",
      "144/295, train_loss: 0.0790, step time: 1.0455\n",
      "145/295, train_loss: 0.0450, step time: 1.0563\n",
      "146/295, train_loss: 0.1030, step time: 1.0455\n",
      "147/295, train_loss: 0.0538, step time: 1.0338\n",
      "148/295, train_loss: 0.0458, step time: 1.0325\n",
      "149/295, train_loss: 0.0608, step time: 1.0471\n",
      "150/295, train_loss: 0.0543, step time: 1.0451\n",
      "151/295, train_loss: 0.0438, step time: 1.0551\n",
      "152/295, train_loss: 0.0830, step time: 1.0984\n",
      "153/295, train_loss: 0.0545, step time: 1.0541\n",
      "154/295, train_loss: 0.0351, step time: 1.0389\n",
      "155/295, train_loss: 0.0922, step time: 1.0376\n",
      "156/295, train_loss: 0.0554, step time: 1.0380\n",
      "157/295, train_loss: 0.0717, step time: 1.0363\n",
      "158/295, train_loss: 0.0436, step time: 1.0523\n",
      "159/295, train_loss: 0.0256, step time: 1.0591\n",
      "160/295, train_loss: 0.3915, step time: 1.0372\n",
      "161/295, train_loss: 0.3595, step time: 1.0397\n",
      "162/295, train_loss: 0.0623, step time: 1.0328\n",
      "163/295, train_loss: 0.0658, step time: 1.0333\n",
      "164/295, train_loss: 0.0678, step time: 1.0344\n",
      "165/295, train_loss: 0.1477, step time: 1.0745\n",
      "166/295, train_loss: 0.0905, step time: 1.0369\n",
      "167/295, train_loss: 0.0837, step time: 1.0699\n",
      "168/295, train_loss: 0.0432, step time: 1.0379\n",
      "169/295, train_loss: 0.3577, step time: 1.0499\n",
      "170/295, train_loss: 0.0833, step time: 1.0311\n",
      "171/295, train_loss: 0.0484, step time: 1.0365\n",
      "172/295, train_loss: 0.0737, step time: 1.0468\n",
      "173/295, train_loss: 0.0275, step time: 1.0526\n",
      "174/295, train_loss: 0.0410, step time: 1.0507\n",
      "175/295, train_loss: 0.3814, step time: 1.0364\n",
      "176/295, train_loss: 0.0494, step time: 1.0543\n",
      "177/295, train_loss: 0.0568, step time: 1.0512\n",
      "178/295, train_loss: 0.0802, step time: 1.0358\n",
      "179/295, train_loss: 0.1494, step time: 1.0629\n",
      "180/295, train_loss: 0.0759, step time: 1.0734\n",
      "181/295, train_loss: 0.1040, step time: 1.0571\n",
      "182/295, train_loss: 0.0418, step time: 1.0374\n",
      "183/295, train_loss: 0.0804, step time: 1.0397\n",
      "184/295, train_loss: 0.0352, step time: 1.1170\n",
      "185/295, train_loss: 0.1135, step time: 1.0371\n",
      "186/295, train_loss: 0.0561, step time: 1.0398\n",
      "187/295, train_loss: 0.1227, step time: 1.0501\n",
      "188/295, train_loss: 0.0510, step time: 1.0362\n",
      "189/295, train_loss: 0.0373, step time: 1.0660\n",
      "190/295, train_loss: 0.0428, step time: 1.0319\n",
      "191/295, train_loss: 0.0292, step time: 1.0580\n",
      "192/295, train_loss: 0.0336, step time: 1.0470\n",
      "193/295, train_loss: 0.0435, step time: 1.0405\n",
      "194/295, train_loss: 0.0297, step time: 1.0497\n",
      "195/295, train_loss: 0.3893, step time: 1.1133\n",
      "196/295, train_loss: 0.0605, step time: 1.0310\n",
      "197/295, train_loss: 0.0818, step time: 1.0413\n",
      "198/295, train_loss: 0.0570, step time: 1.0361\n",
      "199/295, train_loss: 0.0527, step time: 1.0787\n",
      "200/295, train_loss: 0.0287, step time: 1.0482\n",
      "201/295, train_loss: 0.0658, step time: 1.0496\n",
      "202/295, train_loss: 0.1417, step time: 1.0432\n",
      "203/295, train_loss: 0.0436, step time: 1.0469\n",
      "204/295, train_loss: 0.0333, step time: 1.0416\n",
      "205/295, train_loss: 0.0300, step time: 1.1261\n",
      "206/295, train_loss: 0.0974, step time: 1.0396\n",
      "207/295, train_loss: 0.0658, step time: 1.0870\n",
      "208/295, train_loss: 0.0526, step time: 1.0447\n",
      "209/295, train_loss: 0.0471, step time: 1.0372\n",
      "210/295, train_loss: 0.0608, step time: 1.0591\n",
      "211/295, train_loss: 0.0533, step time: 1.0411\n",
      "212/295, train_loss: 0.0882, step time: 1.0369\n",
      "213/295, train_loss: 0.0425, step time: 1.0434\n",
      "214/295, train_loss: 0.0359, step time: 1.0369\n",
      "215/295, train_loss: 0.1118, step time: 1.0440\n",
      "216/295, train_loss: 0.0547, step time: 1.0648\n",
      "217/295, train_loss: 0.0782, step time: 1.0378\n",
      "218/295, train_loss: 0.0564, step time: 1.0361\n",
      "219/295, train_loss: 0.0946, step time: 1.0720\n",
      "220/295, train_loss: 0.0976, step time: 1.0374\n",
      "221/295, train_loss: 0.0286, step time: 1.0371\n",
      "222/295, train_loss: 0.0920, step time: 1.0366\n",
      "223/295, train_loss: 0.0687, step time: 1.0554\n",
      "224/295, train_loss: 0.0387, step time: 1.0378\n",
      "225/295, train_loss: 0.0663, step time: 1.0464\n",
      "226/295, train_loss: 0.0465, step time: 1.0392\n",
      "227/295, train_loss: 0.0361, step time: 1.0499\n",
      "228/295, train_loss: 0.0898, step time: 1.0609\n",
      "229/295, train_loss: 0.1103, step time: 1.0671\n",
      "230/295, train_loss: 0.0486, step time: 1.0633\n",
      "231/295, train_loss: 0.0314, step time: 1.0613\n",
      "232/295, train_loss: 0.0778, step time: 1.0508\n",
      "233/295, train_loss: 0.0365, step time: 1.0484\n",
      "234/295, train_loss: 0.0508, step time: 1.0554\n",
      "235/295, train_loss: 0.0612, step time: 1.0327\n",
      "236/295, train_loss: 0.0288, step time: 1.0349\n",
      "237/295, train_loss: 0.0846, step time: 1.0342\n",
      "238/295, train_loss: 0.0362, step time: 1.0428\n",
      "239/295, train_loss: 0.0390, step time: 1.0348\n",
      "240/295, train_loss: 0.1252, step time: 1.0593\n",
      "241/295, train_loss: 0.0582, step time: 1.0422\n",
      "242/295, train_loss: 0.0522, step time: 1.0475\n",
      "243/295, train_loss: 0.0319, step time: 1.0597\n",
      "244/295, train_loss: 0.0543, step time: 1.0305\n",
      "245/295, train_loss: 0.0466, step time: 1.0362\n",
      "246/295, train_loss: 0.0929, step time: 1.0365\n",
      "247/295, train_loss: 0.0664, step time: 1.0354\n",
      "248/295, train_loss: 0.0711, step time: 1.0357\n",
      "249/295, train_loss: 0.0285, step time: 1.0636\n",
      "250/295, train_loss: 0.0796, step time: 1.0417\n",
      "251/295, train_loss: 0.0413, step time: 1.0552\n",
      "252/295, train_loss: 0.0519, step time: 1.0397\n",
      "253/295, train_loss: 0.0701, step time: 1.0419\n",
      "254/295, train_loss: 0.0741, step time: 1.0361\n",
      "255/295, train_loss: 0.0870, step time: 1.0666\n",
      "256/295, train_loss: 0.0775, step time: 1.0601\n",
      "257/295, train_loss: 0.0660, step time: 1.0390\n",
      "258/295, train_loss: 0.0809, step time: 1.0417\n",
      "259/295, train_loss: 0.0609, step time: 1.0709\n",
      "260/295, train_loss: 0.0392, step time: 1.0416\n",
      "261/295, train_loss: 0.3842, step time: 1.0553\n",
      "262/295, train_loss: 0.1067, step time: 1.0350\n",
      "263/295, train_loss: 0.0373, step time: 1.0362\n",
      "264/295, train_loss: 0.0829, step time: 1.0995\n",
      "265/295, train_loss: 0.0568, step time: 1.0345\n",
      "266/295, train_loss: 0.0286, step time: 1.0383\n",
      "267/295, train_loss: 0.0604, step time: 1.0337\n",
      "268/295, train_loss: 0.0698, step time: 1.0348\n",
      "269/295, train_loss: 0.0337, step time: 1.0337\n",
      "270/295, train_loss: 0.0493, step time: 1.0394\n",
      "271/295, train_loss: 0.0520, step time: 1.0659\n",
      "272/295, train_loss: 0.0620, step time: 1.0441\n",
      "273/295, train_loss: 0.0451, step time: 1.0345\n",
      "274/295, train_loss: 0.0666, step time: 1.0339\n",
      "275/295, train_loss: 0.0628, step time: 1.0611\n",
      "276/295, train_loss: 0.0811, step time: 1.1815\n",
      "277/295, train_loss: 0.0535, step time: 1.0316\n",
      "278/295, train_loss: 0.1201, step time: 1.0329\n",
      "279/295, train_loss: 0.1423, step time: 1.0380\n",
      "280/295, train_loss: 0.0698, step time: 1.0381\n",
      "281/295, train_loss: 0.0191, step time: 1.0510\n",
      "282/295, train_loss: 0.0801, step time: 1.0436\n",
      "283/295, train_loss: 0.0823, step time: 1.0400\n",
      "284/295, train_loss: 0.0417, step time: 1.0480\n",
      "285/295, train_loss: 0.0336, step time: 1.0452\n",
      "286/295, train_loss: 0.1157, step time: 1.0389\n",
      "287/295, train_loss: 0.0429, step time: 1.0400\n",
      "288/295, train_loss: 0.0431, step time: 1.0333\n",
      "289/295, train_loss: 0.0274, step time: 1.0292\n",
      "290/295, train_loss: 0.0673, step time: 1.0283\n",
      "291/295, train_loss: 0.3653, step time: 1.0288\n",
      "292/295, train_loss: 0.0437, step time: 1.0279\n",
      "293/295, train_loss: 0.3842, step time: 1.0282\n",
      "294/295, train_loss: 0.0501, step time: 1.0290\n",
      "295/295, train_loss: 0.0407, step time: 1.0283\n",
      "epoch 71 average loss: 0.0853\n",
      "current epoch: 71 current mean dice: 0.7852 tc: 0.7410 wt: 0.8468 et: 0.7725\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 71 is: 385.8645\n",
      "----------\n",
      "epoch 72/100\n",
      "1/295, train_loss: 0.0978, step time: 1.1296\n",
      "2/295, train_loss: 0.2204, step time: 1.1198\n",
      "3/295, train_loss: 0.0853, step time: 1.0484\n",
      "4/295, train_loss: 0.0440, step time: 1.0771\n",
      "5/295, train_loss: 0.0634, step time: 1.0613\n",
      "6/295, train_loss: 0.0524, step time: 1.0395\n",
      "7/295, train_loss: 0.0509, step time: 1.0575\n",
      "8/295, train_loss: 0.0491, step time: 1.0673\n",
      "9/295, train_loss: 0.0672, step time: 1.1063\n",
      "10/295, train_loss: 0.3798, step time: 1.0341\n",
      "11/295, train_loss: 0.0251, step time: 1.0660\n",
      "12/295, train_loss: 0.0417, step time: 1.0370\n",
      "13/295, train_loss: 0.0365, step time: 1.0338\n",
      "14/295, train_loss: 0.0637, step time: 1.0705\n",
      "15/295, train_loss: 0.0979, step time: 1.0482\n",
      "16/295, train_loss: 0.0779, step time: 1.0319\n",
      "17/295, train_loss: 0.1055, step time: 1.0543\n",
      "18/295, train_loss: 0.0712, step time: 1.0375\n",
      "19/295, train_loss: 0.0404, step time: 1.0711\n",
      "20/295, train_loss: 0.0501, step time: 1.0397\n",
      "21/295, train_loss: 0.0409, step time: 1.0392\n",
      "22/295, train_loss: 0.3816, step time: 1.0760\n",
      "23/295, train_loss: 0.0943, step time: 1.0440\n",
      "24/295, train_loss: 0.0489, step time: 1.0431\n",
      "25/295, train_loss: 0.1139, step time: 1.0365\n",
      "26/295, train_loss: 0.3891, step time: 1.0365\n",
      "27/295, train_loss: 0.0508, step time: 1.0507\n",
      "28/295, train_loss: 0.0831, step time: 1.0574\n",
      "29/295, train_loss: 0.0468, step time: 1.0538\n",
      "30/295, train_loss: 0.1006, step time: 1.0662\n",
      "31/295, train_loss: 0.0275, step time: 1.0416\n",
      "32/295, train_loss: 0.0303, step time: 1.0358\n",
      "33/295, train_loss: 0.0528, step time: 1.0673\n",
      "34/295, train_loss: 0.0497, step time: 1.0637\n",
      "35/295, train_loss: 0.0244, step time: 1.0669\n",
      "36/295, train_loss: 0.0886, step time: 1.0772\n",
      "37/295, train_loss: 0.0665, step time: 1.0360\n",
      "38/295, train_loss: 0.0715, step time: 1.0457\n",
      "39/295, train_loss: 0.3807, step time: 1.0545\n",
      "40/295, train_loss: 0.0329, step time: 1.1117\n",
      "41/295, train_loss: 0.0400, step time: 1.0343\n",
      "42/295, train_loss: 0.0538, step time: 1.0731\n",
      "43/295, train_loss: 0.0248, step time: 1.0590\n",
      "44/295, train_loss: 0.3817, step time: 1.0351\n",
      "45/295, train_loss: 0.0525, step time: 1.0694\n",
      "46/295, train_loss: 0.0676, step time: 1.0326\n",
      "47/295, train_loss: 0.0294, step time: 1.0357\n",
      "48/295, train_loss: 0.0905, step time: 1.0642\n",
      "49/295, train_loss: 0.0531, step time: 1.0460\n",
      "50/295, train_loss: 0.0805, step time: 1.0742\n",
      "51/295, train_loss: 0.0443, step time: 1.0463\n",
      "52/295, train_loss: 0.0417, step time: 1.0343\n",
      "53/295, train_loss: 0.0666, step time: 1.0624\n",
      "54/295, train_loss: 0.0405, step time: 1.0408\n",
      "55/295, train_loss: 0.0434, step time: 1.0349\n",
      "56/295, train_loss: 0.0386, step time: 1.0450\n",
      "57/295, train_loss: 0.0630, step time: 1.0569\n",
      "58/295, train_loss: 0.0645, step time: 1.0995\n",
      "59/295, train_loss: 0.0662, step time: 1.0344\n",
      "60/295, train_loss: 0.0531, step time: 1.0567\n",
      "61/295, train_loss: 0.0336, step time: 1.0473\n",
      "62/295, train_loss: 0.0311, step time: 1.0403\n",
      "63/295, train_loss: 0.0305, step time: 1.0641\n",
      "64/295, train_loss: 0.0495, step time: 1.0374\n",
      "65/295, train_loss: 0.0613, step time: 1.0348\n",
      "66/295, train_loss: 0.1190, step time: 1.0383\n",
      "67/295, train_loss: 0.0530, step time: 1.0586\n",
      "68/295, train_loss: 0.0296, step time: 1.0370\n",
      "69/295, train_loss: 0.0715, step time: 1.0369\n",
      "70/295, train_loss: 0.0368, step time: 1.0392\n",
      "71/295, train_loss: 0.0472, step time: 1.0425\n",
      "72/295, train_loss: 0.3685, step time: 1.0413\n",
      "73/295, train_loss: 0.0815, step time: 1.0454\n",
      "74/295, train_loss: 0.0532, step time: 1.0369\n",
      "75/295, train_loss: 0.0555, step time: 1.0363\n",
      "76/295, train_loss: 0.0546, step time: 1.0667\n",
      "77/295, train_loss: 0.0438, step time: 1.0295\n",
      "78/295, train_loss: 0.0317, step time: 1.0331\n",
      "79/295, train_loss: 0.0823, step time: 1.1304\n",
      "80/295, train_loss: 0.0433, step time: 1.0919\n",
      "81/295, train_loss: 0.0350, step time: 1.0520\n",
      "82/295, train_loss: 0.0983, step time: 1.0404\n",
      "83/295, train_loss: 0.0629, step time: 1.0569\n",
      "84/295, train_loss: 0.0283, step time: 1.0584\n",
      "85/295, train_loss: 0.0674, step time: 1.0356\n",
      "86/295, train_loss: 0.0465, step time: 1.0585\n",
      "87/295, train_loss: 0.0393, step time: 1.0688\n",
      "88/295, train_loss: 0.1116, step time: 1.0804\n",
      "89/295, train_loss: 0.0819, step time: 1.0405\n",
      "90/295, train_loss: 0.0274, step time: 1.0333\n",
      "91/295, train_loss: 0.0437, step time: 1.0352\n",
      "92/295, train_loss: 0.0781, step time: 1.0393\n",
      "93/295, train_loss: 0.0826, step time: 1.0581\n",
      "94/295, train_loss: 0.0552, step time: 1.0549\n",
      "95/295, train_loss: 0.0952, step time: 1.0347\n",
      "96/295, train_loss: 0.0425, step time: 1.0401\n",
      "97/295, train_loss: 0.0731, step time: 1.0831\n",
      "98/295, train_loss: 0.3838, step time: 1.0408\n",
      "99/295, train_loss: 0.0708, step time: 1.0293\n",
      "100/295, train_loss: 0.0410, step time: 1.0301\n",
      "101/295, train_loss: 0.0455, step time: 1.0366\n",
      "102/295, train_loss: 0.0319, step time: 1.0456\n",
      "103/295, train_loss: 0.0489, step time: 1.0603\n",
      "104/295, train_loss: 0.0704, step time: 1.0794\n",
      "105/295, train_loss: 0.0754, step time: 1.0496\n",
      "106/295, train_loss: 0.0384, step time: 1.0437\n",
      "107/295, train_loss: 0.1033, step time: 1.0812\n",
      "108/295, train_loss: 0.0271, step time: 1.0668\n",
      "109/295, train_loss: 0.0335, step time: 1.0533\n",
      "110/295, train_loss: 0.0819, step time: 1.0457\n",
      "111/295, train_loss: 0.0270, step time: 1.0670\n",
      "112/295, train_loss: 0.0261, step time: 1.0375\n",
      "113/295, train_loss: 0.3808, step time: 1.0352\n",
      "114/295, train_loss: 0.0629, step time: 1.0482\n",
      "115/295, train_loss: 0.0775, step time: 1.0709\n",
      "116/295, train_loss: 0.0812, step time: 1.0367\n",
      "117/295, train_loss: 0.0624, step time: 1.0345\n",
      "118/295, train_loss: 0.0746, step time: 1.0611\n",
      "119/295, train_loss: 0.0868, step time: 1.0624\n",
      "120/295, train_loss: 0.0797, step time: 1.0333\n",
      "121/295, train_loss: 0.3829, step time: 1.0392\n",
      "122/295, train_loss: 0.0768, step time: 1.0646\n",
      "123/295, train_loss: 0.0581, step time: 1.0308\n",
      "124/295, train_loss: 0.0325, step time: 1.0386\n",
      "125/295, train_loss: 0.0541, step time: 1.0558\n",
      "126/295, train_loss: 0.0323, step time: 1.0518\n",
      "127/295, train_loss: 0.0663, step time: 1.0446\n",
      "128/295, train_loss: 0.0770, step time: 1.0636\n",
      "129/295, train_loss: 0.0885, step time: 1.0337\n",
      "130/295, train_loss: 0.0367, step time: 1.0530\n",
      "131/295, train_loss: 0.0687, step time: 1.0417\n",
      "132/295, train_loss: 0.0337, step time: 1.0412\n",
      "133/295, train_loss: 0.0614, step time: 1.0444\n",
      "134/295, train_loss: 0.0981, step time: 1.0430\n",
      "135/295, train_loss: 0.0234, step time: 1.0353\n",
      "136/295, train_loss: 0.1460, step time: 1.0642\n",
      "137/295, train_loss: 0.0342, step time: 1.0339\n",
      "138/295, train_loss: 0.0704, step time: 1.0330\n",
      "139/295, train_loss: 0.0295, step time: 1.0393\n",
      "140/295, train_loss: 0.0601, step time: 1.0593\n",
      "141/295, train_loss: 0.0791, step time: 1.0674\n",
      "142/295, train_loss: 0.3657, step time: 1.0455\n",
      "143/295, train_loss: 0.0808, step time: 1.0412\n",
      "144/295, train_loss: 0.0312, step time: 1.0415\n",
      "145/295, train_loss: 0.0990, step time: 1.0387\n",
      "146/295, train_loss: 0.0570, step time: 1.0434\n",
      "147/295, train_loss: 0.3605, step time: 1.0511\n",
      "148/295, train_loss: 0.0616, step time: 1.0471\n",
      "149/295, train_loss: 0.3647, step time: 1.0709\n",
      "150/295, train_loss: 0.0664, step time: 1.0352\n",
      "151/295, train_loss: 0.1137, step time: 1.0374\n",
      "152/295, train_loss: 0.0420, step time: 1.0398\n",
      "153/295, train_loss: 0.0290, step time: 1.0357\n",
      "154/295, train_loss: 0.0332, step time: 1.0376\n",
      "155/295, train_loss: 0.0943, step time: 1.0340\n",
      "156/295, train_loss: 0.0794, step time: 1.0324\n",
      "157/295, train_loss: 0.0631, step time: 1.0369\n",
      "158/295, train_loss: 0.0293, step time: 1.0418\n",
      "159/295, train_loss: 0.0950, step time: 1.0417\n",
      "160/295, train_loss: 0.1029, step time: 1.0635\n",
      "161/295, train_loss: 0.0646, step time: 1.0369\n",
      "162/295, train_loss: 0.0588, step time: 1.0639\n",
      "163/295, train_loss: 0.3988, step time: 1.0327\n",
      "164/295, train_loss: 0.0658, step time: 1.0387\n",
      "165/295, train_loss: 0.0365, step time: 1.0309\n",
      "166/295, train_loss: 0.0353, step time: 1.0365\n",
      "167/295, train_loss: 0.0468, step time: 1.0318\n",
      "168/295, train_loss: 0.0502, step time: 1.0396\n",
      "169/295, train_loss: 0.0775, step time: 1.2499\n",
      "170/295, train_loss: 0.0460, step time: 1.0508\n",
      "171/295, train_loss: 0.0647, step time: 1.0359\n",
      "172/295, train_loss: 0.0663, step time: 1.1054\n",
      "173/295, train_loss: 0.0778, step time: 1.0403\n",
      "174/295, train_loss: 0.0443, step time: 1.0402\n",
      "175/295, train_loss: 0.1012, step time: 1.0651\n",
      "176/295, train_loss: 0.0369, step time: 1.0581\n",
      "177/295, train_loss: 0.0333, step time: 1.0358\n",
      "178/295, train_loss: 0.1004, step time: 1.0394\n",
      "179/295, train_loss: 0.1149, step time: 1.0340\n",
      "180/295, train_loss: 0.0556, step time: 1.0531\n",
      "181/295, train_loss: 0.0901, step time: 1.0430\n",
      "182/295, train_loss: 0.0681, step time: 1.0337\n",
      "183/295, train_loss: 0.0818, step time: 1.0605\n",
      "184/295, train_loss: 0.0448, step time: 1.1206\n",
      "185/295, train_loss: 0.3615, step time: 1.0338\n",
      "186/295, train_loss: 0.0369, step time: 1.0430\n",
      "187/295, train_loss: 0.0960, step time: 1.0495\n",
      "188/295, train_loss: 0.0258, step time: 1.0633\n",
      "189/295, train_loss: 0.1460, step time: 1.0341\n",
      "190/295, train_loss: 0.0429, step time: 1.0370\n",
      "191/295, train_loss: 0.0274, step time: 1.0353\n",
      "192/295, train_loss: 0.1260, step time: 1.0325\n",
      "193/295, train_loss: 0.0743, step time: 1.0319\n",
      "194/295, train_loss: 0.0625, step time: 1.0363\n",
      "195/295, train_loss: 0.0862, step time: 1.0359\n",
      "196/295, train_loss: 0.0192, step time: 1.0375\n",
      "197/295, train_loss: 0.0486, step time: 1.0656\n",
      "198/295, train_loss: 0.0394, step time: 1.0762\n",
      "199/295, train_loss: 0.1003, step time: 1.0334\n",
      "200/295, train_loss: 0.0810, step time: 1.0455\n",
      "201/295, train_loss: 0.3570, step time: 1.0488\n",
      "202/295, train_loss: 0.0510, step time: 1.0771\n",
      "203/295, train_loss: 0.0264, step time: 1.0367\n",
      "204/295, train_loss: 0.0378, step time: 1.0355\n",
      "205/295, train_loss: 0.0443, step time: 1.0348\n",
      "206/295, train_loss: 0.0337, step time: 1.0375\n",
      "207/295, train_loss: 0.1358, step time: 1.0363\n",
      "208/295, train_loss: 0.0377, step time: 1.0648\n",
      "209/295, train_loss: 0.0462, step time: 1.0396\n",
      "210/295, train_loss: 0.0326, step time: 1.0580\n",
      "211/295, train_loss: 0.0329, step time: 1.0403\n",
      "212/295, train_loss: 0.0570, step time: 1.0408\n",
      "213/295, train_loss: 0.0652, step time: 1.0426\n",
      "214/295, train_loss: 0.0561, step time: 1.0886\n",
      "215/295, train_loss: 0.1219, step time: 1.0534\n",
      "216/295, train_loss: 0.0367, step time: 1.0330\n",
      "217/295, train_loss: 0.0811, step time: 1.0349\n",
      "218/295, train_loss: 0.0611, step time: 1.0442\n",
      "219/295, train_loss: 0.0457, step time: 1.0634\n",
      "220/295, train_loss: 0.0528, step time: 1.0415\n",
      "221/295, train_loss: 0.3639, step time: 1.0714\n",
      "222/295, train_loss: 0.1059, step time: 1.0340\n",
      "223/295, train_loss: 0.0291, step time: 1.0380\n",
      "224/295, train_loss: 0.0578, step time: 1.0393\n",
      "225/295, train_loss: 0.0648, step time: 1.0333\n",
      "226/295, train_loss: 0.0608, step time: 1.0417\n",
      "227/295, train_loss: 0.0612, step time: 1.1185\n",
      "228/295, train_loss: 0.0452, step time: 1.0367\n",
      "229/295, train_loss: 0.0651, step time: 1.0399\n",
      "230/295, train_loss: 0.0419, step time: 1.0431\n",
      "231/295, train_loss: 0.1078, step time: 1.1178\n",
      "232/295, train_loss: 0.0522, step time: 1.0313\n",
      "233/295, train_loss: 0.0520, step time: 1.0388\n",
      "234/295, train_loss: 0.0521, step time: 1.0414\n",
      "235/295, train_loss: 0.0270, step time: 1.0414\n",
      "236/295, train_loss: 0.0755, step time: 1.0368\n",
      "237/295, train_loss: 0.0663, step time: 1.0398\n",
      "238/295, train_loss: 0.0814, step time: 1.0434\n",
      "239/295, train_loss: 0.0252, step time: 1.0386\n",
      "240/295, train_loss: 0.0841, step time: 1.0386\n",
      "241/295, train_loss: 0.0292, step time: 1.0377\n",
      "242/295, train_loss: 0.0815, step time: 1.0389\n",
      "243/295, train_loss: 0.4433, step time: 1.0613\n",
      "244/295, train_loss: 0.0396, step time: 1.0416\n",
      "245/295, train_loss: 0.0537, step time: 1.0678\n",
      "246/295, train_loss: 0.0649, step time: 1.0656\n",
      "247/295, train_loss: 0.0495, step time: 1.1093\n",
      "248/295, train_loss: 0.0475, step time: 1.0366\n",
      "249/295, train_loss: 0.0376, step time: 1.0390\n",
      "250/295, train_loss: 0.0598, step time: 1.0624\n",
      "251/295, train_loss: 0.0525, step time: 1.0360\n",
      "252/295, train_loss: 0.0334, step time: 1.0523\n",
      "253/295, train_loss: 0.0639, step time: 1.0369\n",
      "254/295, train_loss: 0.0902, step time: 1.0354\n",
      "255/295, train_loss: 0.0404, step time: 1.0349\n",
      "256/295, train_loss: 0.0427, step time: 1.0379\n",
      "257/295, train_loss: 0.0382, step time: 1.0548\n",
      "258/295, train_loss: 0.0282, step time: 1.0398\n",
      "259/295, train_loss: 0.0575, step time: 1.0861\n",
      "260/295, train_loss: 0.0919, step time: 1.0665\n",
      "261/295, train_loss: 0.0690, step time: 1.0338\n",
      "262/295, train_loss: 0.0787, step time: 1.0402\n",
      "263/295, train_loss: 0.1380, step time: 1.0345\n",
      "264/295, train_loss: 0.0467, step time: 1.0434\n",
      "265/295, train_loss: 0.0830, step time: 1.0438\n",
      "266/295, train_loss: 0.0372, step time: 1.0418\n",
      "267/295, train_loss: 0.0561, step time: 1.0428\n",
      "268/295, train_loss: 0.0518, step time: 1.0354\n",
      "269/295, train_loss: 0.0522, step time: 1.0317\n",
      "270/295, train_loss: 0.0787, step time: 1.0435\n",
      "271/295, train_loss: 0.0656, step time: 1.0394\n",
      "272/295, train_loss: 0.0518, step time: 1.0463\n",
      "273/295, train_loss: 0.3722, step time: 1.0603\n",
      "274/295, train_loss: 0.0602, step time: 1.0338\n",
      "275/295, train_loss: 0.0733, step time: 1.0389\n",
      "276/295, train_loss: 0.0619, step time: 1.0349\n",
      "277/295, train_loss: 0.0806, step time: 1.0435\n",
      "278/295, train_loss: 0.0490, step time: 1.0422\n",
      "279/295, train_loss: 0.0407, step time: 1.0457\n",
      "280/295, train_loss: 0.0484, step time: 1.0360\n",
      "281/295, train_loss: 0.0561, step time: 1.0415\n",
      "282/295, train_loss: 0.3915, step time: 1.0330\n",
      "283/295, train_loss: 0.0334, step time: 1.0483\n",
      "284/295, train_loss: 0.0773, step time: 1.0573\n",
      "285/295, train_loss: 0.0285, step time: 1.0356\n",
      "286/295, train_loss: 0.0278, step time: 1.0392\n",
      "287/295, train_loss: 0.0505, step time: 1.0472\n",
      "288/295, train_loss: 0.3701, step time: 1.0457\n",
      "289/295, train_loss: 0.0332, step time: 1.0296\n",
      "290/295, train_loss: 0.0443, step time: 1.0293\n",
      "291/295, train_loss: 0.3818, step time: 1.0297\n",
      "292/295, train_loss: 0.0769, step time: 1.0288\n",
      "293/295, train_loss: 0.0924, step time: 1.0310\n",
      "294/295, train_loss: 0.0621, step time: 1.0287\n",
      "295/295, train_loss: 0.3789, step time: 1.0312\n",
      "epoch 72 average loss: 0.0842\n",
      "current epoch: 72 current mean dice: 0.7765 tc: 0.7344 wt: 0.8404 et: 0.7599\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 72 is: 380.2583\n",
      "----------\n",
      "epoch 73/100\n",
      "1/295, train_loss: 0.1141, step time: 1.1560\n",
      "2/295, train_loss: 0.0318, step time: 1.0656\n",
      "3/295, train_loss: 0.3906, step time: 1.0758\n",
      "4/295, train_loss: 0.0607, step time: 1.0931\n",
      "5/295, train_loss: 0.0740, step time: 1.0294\n",
      "6/295, train_loss: 0.0791, step time: 1.0338\n",
      "7/295, train_loss: 0.3691, step time: 1.0440\n",
      "8/295, train_loss: 0.0403, step time: 1.0728\n",
      "9/295, train_loss: 0.0225, step time: 1.0540\n",
      "10/295, train_loss: 0.0264, step time: 1.0591\n",
      "11/295, train_loss: 0.3821, step time: 1.0794\n",
      "12/295, train_loss: 0.0381, step time: 1.0378\n",
      "13/295, train_loss: 0.0510, step time: 1.0550\n",
      "14/295, train_loss: 0.3570, step time: 1.0586\n",
      "15/295, train_loss: 0.0577, step time: 1.0335\n",
      "16/295, train_loss: 0.0366, step time: 1.0752\n",
      "17/295, train_loss: 0.0490, step time: 1.0525\n",
      "18/295, train_loss: 0.0510, step time: 1.0365\n",
      "19/295, train_loss: 0.0941, step time: 1.0470\n",
      "20/295, train_loss: 0.0589, step time: 1.0541\n",
      "21/295, train_loss: 0.0579, step time: 1.0292\n",
      "22/295, train_loss: 0.0569, step time: 1.0327\n",
      "23/295, train_loss: 0.0623, step time: 1.0334\n",
      "24/295, train_loss: 0.0322, step time: 1.0376\n",
      "25/295, train_loss: 0.0486, step time: 1.0308\n",
      "26/295, train_loss: 0.0747, step time: 1.0353\n",
      "27/295, train_loss: 0.0752, step time: 1.0432\n",
      "28/295, train_loss: 0.0294, step time: 1.0507\n",
      "29/295, train_loss: 0.0422, step time: 1.0486\n",
      "30/295, train_loss: 0.0676, step time: 1.0670\n",
      "31/295, train_loss: 0.3725, step time: 1.0350\n",
      "32/295, train_loss: 0.3810, step time: 1.0389\n",
      "33/295, train_loss: 0.1020, step time: 1.0882\n",
      "34/295, train_loss: 0.0967, step time: 1.0546\n",
      "35/295, train_loss: 0.0988, step time: 1.0796\n",
      "36/295, train_loss: 0.0809, step time: 1.0314\n",
      "37/295, train_loss: 0.0408, step time: 1.0370\n",
      "38/295, train_loss: 0.0905, step time: 1.0409\n",
      "39/295, train_loss: 0.0769, step time: 1.0671\n",
      "40/295, train_loss: 0.0275, step time: 1.0465\n",
      "41/295, train_loss: 0.0597, step time: 1.0477\n",
      "42/295, train_loss: 0.0726, step time: 1.0554\n",
      "43/295, train_loss: 0.0402, step time: 1.0424\n",
      "44/295, train_loss: 0.0472, step time: 1.0464\n",
      "45/295, train_loss: 0.1010, step time: 1.0374\n",
      "46/295, train_loss: 0.0629, step time: 1.0561\n",
      "47/295, train_loss: 0.0509, step time: 1.0352\n",
      "48/295, train_loss: 0.0332, step time: 1.0634\n",
      "49/295, train_loss: 0.0791, step time: 1.0504\n",
      "50/295, train_loss: 0.0411, step time: 1.0360\n",
      "51/295, train_loss: 0.0504, step time: 1.0699\n",
      "52/295, train_loss: 0.0405, step time: 1.0601\n",
      "53/295, train_loss: 0.1021, step time: 1.0481\n",
      "54/295, train_loss: 0.0959, step time: 1.0424\n",
      "55/295, train_loss: 0.0703, step time: 1.0356\n",
      "56/295, train_loss: 0.0386, step time: 1.0324\n",
      "57/295, train_loss: 0.0497, step time: 1.0320\n",
      "58/295, train_loss: 0.0259, step time: 1.0354\n",
      "59/295, train_loss: 0.0517, step time: 1.0526\n",
      "60/295, train_loss: 0.0616, step time: 1.0765\n",
      "61/295, train_loss: 0.0277, step time: 1.0909\n",
      "62/295, train_loss: 0.0406, step time: 1.0827\n",
      "63/295, train_loss: 0.0451, step time: 1.0696\n",
      "64/295, train_loss: 0.0901, step time: 1.0419\n",
      "65/295, train_loss: 0.0812, step time: 1.0316\n",
      "66/295, train_loss: 0.0490, step time: 1.0946\n",
      "67/295, train_loss: 0.0628, step time: 1.0415\n",
      "68/295, train_loss: 0.0579, step time: 1.0390\n",
      "69/295, train_loss: 0.0726, step time: 1.0355\n",
      "70/295, train_loss: 0.0538, step time: 1.0660\n",
      "71/295, train_loss: 0.0564, step time: 1.0702\n",
      "72/295, train_loss: 0.0663, step time: 1.0650\n",
      "73/295, train_loss: 0.2213, step time: 1.1171\n",
      "74/295, train_loss: 0.4424, step time: 1.0613\n",
      "75/295, train_loss: 0.0292, step time: 1.0344\n",
      "76/295, train_loss: 0.0260, step time: 1.0393\n",
      "77/295, train_loss: 0.0851, step time: 1.0616\n",
      "78/295, train_loss: 0.0874, step time: 1.1065\n",
      "79/295, train_loss: 0.0931, step time: 1.0353\n",
      "80/295, train_loss: 0.0618, step time: 1.0518\n",
      "81/295, train_loss: 0.0891, step time: 1.0323\n",
      "82/295, train_loss: 0.0413, step time: 1.0404\n",
      "83/295, train_loss: 0.0611, step time: 1.0621\n",
      "84/295, train_loss: 0.0590, step time: 1.0463\n",
      "85/295, train_loss: 0.0338, step time: 1.0362\n",
      "86/295, train_loss: 0.0475, step time: 1.0595\n",
      "87/295, train_loss: 0.3623, step time: 1.0612\n",
      "88/295, train_loss: 0.0446, step time: 1.0730\n",
      "89/295, train_loss: 0.0289, step time: 1.0465\n",
      "90/295, train_loss: 0.0461, step time: 1.0622\n",
      "91/295, train_loss: 0.0457, step time: 1.0635\n",
      "92/295, train_loss: 0.0494, step time: 1.0521\n",
      "93/295, train_loss: 0.3884, step time: 1.0287\n",
      "94/295, train_loss: 0.0643, step time: 1.0341\n",
      "95/295, train_loss: 0.0816, step time: 1.0463\n",
      "96/295, train_loss: 0.1169, step time: 1.0372\n",
      "97/295, train_loss: 0.0349, step time: 1.0606\n",
      "98/295, train_loss: 0.1097, step time: 1.0319\n",
      "99/295, train_loss: 0.0926, step time: 1.0353\n",
      "100/295, train_loss: 0.0425, step time: 1.0519\n",
      "101/295, train_loss: 0.0421, step time: 1.0399\n",
      "102/295, train_loss: 0.0487, step time: 1.0369\n",
      "103/295, train_loss: 0.0325, step time: 1.0361\n",
      "104/295, train_loss: 0.0554, step time: 1.0417\n",
      "105/295, train_loss: 0.0754, step time: 1.0537\n",
      "106/295, train_loss: 0.0502, step time: 1.0549\n",
      "107/295, train_loss: 0.0524, step time: 1.0373\n",
      "108/295, train_loss: 0.0816, step time: 1.0378\n",
      "109/295, train_loss: 0.0372, step time: 1.0528\n",
      "110/295, train_loss: 0.3799, step time: 1.0336\n",
      "111/295, train_loss: 0.0543, step time: 1.0573\n",
      "112/295, train_loss: 0.0447, step time: 1.1086\n",
      "113/295, train_loss: 0.0324, step time: 1.0397\n",
      "114/295, train_loss: 0.0875, step time: 1.0315\n",
      "115/295, train_loss: 0.0584, step time: 1.0391\n",
      "116/295, train_loss: 0.0718, step time: 1.0507\n",
      "117/295, train_loss: 0.0406, step time: 1.0758\n",
      "118/295, train_loss: 0.0772, step time: 1.0832\n",
      "119/295, train_loss: 0.0254, step time: 1.0322\n",
      "120/295, train_loss: 0.0341, step time: 1.0370\n",
      "121/295, train_loss: 0.0265, step time: 1.0360\n",
      "122/295, train_loss: 0.3629, step time: 1.0363\n",
      "123/295, train_loss: 0.0557, step time: 1.0351\n",
      "124/295, train_loss: 0.0394, step time: 1.0338\n",
      "125/295, train_loss: 0.0347, step time: 1.0551\n",
      "126/295, train_loss: 0.0520, step time: 1.0375\n",
      "127/295, train_loss: 0.0772, step time: 1.0586\n",
      "128/295, train_loss: 0.0409, step time: 1.0413\n",
      "129/295, train_loss: 0.0588, step time: 1.0338\n",
      "130/295, train_loss: 0.1395, step time: 1.0954\n",
      "131/295, train_loss: 0.0630, step time: 1.0347\n",
      "132/295, train_loss: 0.0671, step time: 1.0320\n",
      "133/295, train_loss: 0.0283, step time: 1.0536\n",
      "134/295, train_loss: 0.3811, step time: 1.0375\n",
      "135/295, train_loss: 0.0352, step time: 1.0490\n",
      "136/295, train_loss: 0.0285, step time: 1.0555\n",
      "137/295, train_loss: 0.0837, step time: 1.0373\n",
      "138/295, train_loss: 0.0328, step time: 1.0398\n",
      "139/295, train_loss: 0.0905, step time: 1.0388\n",
      "140/295, train_loss: 0.0498, step time: 1.0328\n",
      "141/295, train_loss: 0.1013, step time: 1.0736\n",
      "142/295, train_loss: 0.0272, step time: 1.0348\n",
      "143/295, train_loss: 0.0770, step time: 1.0439\n",
      "144/295, train_loss: 0.0488, step time: 1.0380\n",
      "145/295, train_loss: 0.0949, step time: 1.0417\n",
      "146/295, train_loss: 0.0433, step time: 1.0861\n",
      "147/295, train_loss: 0.0793, step time: 1.0578\n",
      "148/295, train_loss: 0.0411, step time: 1.0704\n",
      "149/295, train_loss: 0.0288, step time: 1.0517\n",
      "150/295, train_loss: 0.0463, step time: 1.0391\n",
      "151/295, train_loss: 0.0756, step time: 1.0311\n",
      "152/295, train_loss: 0.0496, step time: 1.0331\n",
      "153/295, train_loss: 0.0437, step time: 1.0473\n",
      "154/295, train_loss: 0.3821, step time: 1.1180\n",
      "155/295, train_loss: 0.0320, step time: 1.0346\n",
      "156/295, train_loss: 0.0670, step time: 1.0380\n",
      "157/295, train_loss: 0.0791, step time: 1.0585\n",
      "158/295, train_loss: 0.0509, step time: 1.0299\n",
      "159/295, train_loss: 0.0491, step time: 1.0393\n",
      "160/295, train_loss: 0.0610, step time: 1.0386\n",
      "161/295, train_loss: 0.0294, step time: 1.0318\n",
      "162/295, train_loss: 0.0390, step time: 1.0361\n",
      "163/295, train_loss: 0.0247, step time: 1.0457\n",
      "164/295, train_loss: 0.0324, step time: 1.0335\n",
      "165/295, train_loss: 0.0522, step time: 1.0334\n",
      "166/295, train_loss: 0.0764, step time: 1.0436\n",
      "167/295, train_loss: 0.0671, step time: 1.0512\n",
      "168/295, train_loss: 0.0244, step time: 1.0397\n",
      "169/295, train_loss: 0.0787, step time: 1.0348\n",
      "170/295, train_loss: 0.0320, step time: 1.0348\n",
      "171/295, train_loss: 0.3602, step time: 1.0370\n",
      "172/295, train_loss: 0.0818, step time: 1.0376\n",
      "173/295, train_loss: 0.0554, step time: 1.0382\n",
      "174/295, train_loss: 0.0358, step time: 1.0334\n",
      "175/295, train_loss: 0.3790, step time: 1.0576\n",
      "176/295, train_loss: 0.0757, step time: 1.0372\n",
      "177/295, train_loss: 0.0554, step time: 1.0394\n",
      "178/295, train_loss: 0.0288, step time: 1.0341\n",
      "179/295, train_loss: 0.0450, step time: 1.0357\n",
      "180/295, train_loss: 0.0615, step time: 1.0353\n",
      "181/295, train_loss: 0.0491, step time: 1.0376\n",
      "182/295, train_loss: 0.0846, step time: 1.0375\n",
      "183/295, train_loss: 0.0594, step time: 1.0389\n",
      "184/295, train_loss: 0.0609, step time: 1.0358\n",
      "185/295, train_loss: 0.0904, step time: 1.0451\n",
      "186/295, train_loss: 0.1041, step time: 1.0516\n",
      "187/295, train_loss: 0.3822, step time: 1.0399\n",
      "188/295, train_loss: 0.0327, step time: 1.0511\n",
      "189/295, train_loss: 0.1559, step time: 1.0358\n",
      "190/295, train_loss: 0.3836, step time: 1.0354\n",
      "191/295, train_loss: 0.0522, step time: 1.0662\n",
      "192/295, train_loss: 0.0678, step time: 1.0914\n",
      "193/295, train_loss: 0.0524, step time: 1.0388\n",
      "194/295, train_loss: 0.1030, step time: 1.0384\n",
      "195/295, train_loss: 0.0762, step time: 1.0401\n",
      "196/295, train_loss: 0.0623, step time: 1.0459\n",
      "197/295, train_loss: 0.0790, step time: 1.0849\n",
      "198/295, train_loss: 0.0475, step time: 1.0493\n",
      "199/295, train_loss: 0.0189, step time: 1.0549\n",
      "200/295, train_loss: 0.0691, step time: 1.0556\n",
      "201/295, train_loss: 0.0537, step time: 1.0357\n",
      "202/295, train_loss: 0.0428, step time: 1.0498\n",
      "203/295, train_loss: 0.0790, step time: 1.0421\n",
      "204/295, train_loss: 0.1147, step time: 1.0353\n",
      "205/295, train_loss: 0.0516, step time: 1.0466\n",
      "206/295, train_loss: 0.0763, step time: 1.0686\n",
      "207/295, train_loss: 0.0287, step time: 1.0325\n",
      "208/295, train_loss: 0.1253, step time: 1.0327\n",
      "209/295, train_loss: 0.0636, step time: 1.0419\n",
      "210/295, train_loss: 0.0789, step time: 1.0477\n",
      "211/295, train_loss: 0.0471, step time: 1.0510\n",
      "212/295, train_loss: 0.0629, step time: 1.0584\n",
      "213/295, train_loss: 0.0800, step time: 1.0370\n",
      "214/295, train_loss: 0.0275, step time: 1.0395\n",
      "215/295, train_loss: 0.0743, step time: 1.0441\n",
      "216/295, train_loss: 0.3618, step time: 1.0416\n",
      "217/295, train_loss: 0.0737, step time: 1.0577\n",
      "218/295, train_loss: 0.0806, step time: 1.0611\n",
      "219/295, train_loss: 0.0276, step time: 1.0356\n",
      "220/295, train_loss: 0.0508, step time: 1.0325\n",
      "221/295, train_loss: 0.0670, step time: 1.0545\n",
      "222/295, train_loss: 0.0752, step time: 1.0410\n",
      "223/295, train_loss: 0.0865, step time: 1.0378\n",
      "224/295, train_loss: 0.0821, step time: 1.0393\n",
      "225/295, train_loss: 0.0365, step time: 1.0392\n",
      "226/295, train_loss: 0.0670, step time: 1.1044\n",
      "227/295, train_loss: 0.0560, step time: 1.0756\n",
      "228/295, train_loss: 0.0325, step time: 1.0506\n",
      "229/295, train_loss: 0.0783, step time: 1.0525\n",
      "230/295, train_loss: 0.0597, step time: 1.0778\n",
      "231/295, train_loss: 0.3665, step time: 1.0433\n",
      "232/295, train_loss: 0.0318, step time: 1.0472\n",
      "233/295, train_loss: 0.0605, step time: 1.0741\n",
      "234/295, train_loss: 0.0678, step time: 1.0395\n",
      "235/295, train_loss: 0.0378, step time: 1.0404\n",
      "236/295, train_loss: 0.0493, step time: 1.0457\n",
      "237/295, train_loss: 0.0464, step time: 1.1478\n",
      "238/295, train_loss: 0.0345, step time: 1.0552\n",
      "239/295, train_loss: 0.0380, step time: 1.0381\n",
      "240/295, train_loss: 0.0496, step time: 1.0418\n",
      "241/295, train_loss: 0.0552, step time: 1.0383\n",
      "242/295, train_loss: 0.0353, step time: 1.0678\n",
      "243/295, train_loss: 0.0978, step time: 1.0409\n",
      "244/295, train_loss: 0.3701, step time: 1.0374\n",
      "245/295, train_loss: 0.0728, step time: 1.0433\n",
      "246/295, train_loss: 0.0387, step time: 1.0681\n",
      "247/295, train_loss: 0.0650, step time: 1.0449\n",
      "248/295, train_loss: 0.0301, step time: 1.0478\n",
      "249/295, train_loss: 0.0634, step time: 1.0387\n",
      "250/295, train_loss: 0.0599, step time: 1.0925\n",
      "251/295, train_loss: 0.0244, step time: 1.0396\n",
      "252/295, train_loss: 0.0405, step time: 1.0600\n",
      "253/295, train_loss: 0.0660, step time: 1.0466\n",
      "254/295, train_loss: 0.0313, step time: 1.0542\n",
      "255/295, train_loss: 0.0444, step time: 1.1337\n",
      "256/295, train_loss: 0.0659, step time: 1.0415\n",
      "257/295, train_loss: 0.0817, step time: 1.0331\n",
      "258/295, train_loss: 0.0366, step time: 1.0415\n",
      "259/295, train_loss: 0.0369, step time: 1.1278\n",
      "260/295, train_loss: 0.0848, step time: 1.0033\n",
      "261/295, train_loss: 0.1190, step time: 1.0773\n",
      "262/295, train_loss: 0.0494, step time: 1.0464\n",
      "263/295, train_loss: 0.0470, step time: 1.0391\n",
      "264/295, train_loss: 0.0646, step time: 1.0421\n",
      "265/295, train_loss: 0.0609, step time: 1.0364\n",
      "266/295, train_loss: 0.0667, step time: 1.0391\n",
      "267/295, train_loss: 0.0926, step time: 1.0358\n",
      "268/295, train_loss: 0.0846, step time: 1.0535\n",
      "269/295, train_loss: 0.1470, step time: 1.0367\n",
      "270/295, train_loss: 0.0427, step time: 1.0327\n",
      "271/295, train_loss: 0.0336, step time: 1.0377\n",
      "272/295, train_loss: 0.1089, step time: 1.0352\n",
      "273/295, train_loss: 0.0812, step time: 1.0568\n",
      "274/295, train_loss: 0.1098, step time: 1.0505\n",
      "275/295, train_loss: 0.0459, step time: 1.0465\n",
      "276/295, train_loss: 0.0321, step time: 1.0395\n",
      "277/295, train_loss: 0.0369, step time: 1.0392\n",
      "278/295, train_loss: 0.3986, step time: 1.0494\n",
      "279/295, train_loss: 0.1172, step time: 1.0403\n",
      "280/295, train_loss: 0.0700, step time: 1.0816\n",
      "281/295, train_loss: 0.0525, step time: 1.0788\n",
      "282/295, train_loss: 0.0314, step time: 1.0606\n",
      "283/295, train_loss: 0.0699, step time: 1.0448\n",
      "284/295, train_loss: 0.0507, step time: 1.0561\n",
      "285/295, train_loss: 0.0289, step time: 1.0339\n",
      "286/295, train_loss: 0.1265, step time: 1.0814\n",
      "287/295, train_loss: 0.0597, step time: 1.0419\n",
      "288/295, train_loss: 0.0377, step time: 1.0313\n",
      "289/295, train_loss: 0.0332, step time: 1.0306\n",
      "290/295, train_loss: 0.0268, step time: 1.0291\n",
      "291/295, train_loss: 0.0413, step time: 1.0288\n",
      "292/295, train_loss: 0.0427, step time: 1.0301\n",
      "293/295, train_loss: 0.0797, step time: 1.0299\n",
      "294/295, train_loss: 0.3757, step time: 1.0294\n",
      "295/295, train_loss: 0.0906, step time: 1.0301\n",
      "epoch 73 average loss: 0.0838\n",
      "current epoch: 73 current mean dice: 0.7603 tc: 0.7119 wt: 0.8348 et: 0.7399\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 73 is: 379.7658\n",
      "----------\n",
      "epoch 74/100\n",
      "1/295, train_loss: 0.1444, step time: 1.1173\n",
      "2/295, train_loss: 0.0318, step time: 1.1280\n",
      "3/295, train_loss: 0.0304, step time: 1.1451\n",
      "4/295, train_loss: 0.0324, step time: 1.0678\n",
      "5/295, train_loss: 0.0866, step time: 1.0349\n",
      "6/295, train_loss: 0.0750, step time: 1.0620\n",
      "7/295, train_loss: 0.0323, step time: 1.0567\n",
      "8/295, train_loss: 0.0279, step time: 1.0385\n",
      "9/295, train_loss: 0.0766, step time: 1.0591\n",
      "10/295, train_loss: 0.0626, step time: 1.0343\n",
      "11/295, train_loss: 0.0623, step time: 1.1050\n",
      "12/295, train_loss: 0.0963, step time: 1.0382\n",
      "13/295, train_loss: 0.1016, step time: 1.0578\n",
      "14/295, train_loss: 0.0853, step time: 1.0310\n",
      "15/295, train_loss: 0.0772, step time: 1.0291\n",
      "16/295, train_loss: 0.0784, step time: 1.0339\n",
      "17/295, train_loss: 0.3798, step time: 1.0308\n",
      "18/295, train_loss: 0.1127, step time: 1.0868\n",
      "19/295, train_loss: 0.0994, step time: 1.0370\n",
      "20/295, train_loss: 0.0401, step time: 1.1141\n",
      "21/295, train_loss: 0.0358, step time: 1.0383\n",
      "22/295, train_loss: 0.0474, step time: 1.0338\n",
      "23/295, train_loss: 0.0403, step time: 1.0498\n",
      "24/295, train_loss: 0.0390, step time: 1.0501\n",
      "25/295, train_loss: 0.0458, step time: 1.0310\n",
      "26/295, train_loss: 0.0321, step time: 1.0577\n",
      "27/295, train_loss: 0.0492, step time: 1.0317\n",
      "28/295, train_loss: 0.0568, step time: 1.0337\n",
      "29/295, train_loss: 0.0949, step time: 1.0352\n",
      "30/295, train_loss: 0.0591, step time: 1.0891\n",
      "31/295, train_loss: 0.0494, step time: 1.0517\n",
      "32/295, train_loss: 0.0786, step time: 1.0440\n",
      "33/295, train_loss: 0.0611, step time: 1.0350\n",
      "34/295, train_loss: 0.0268, step time: 1.0365\n",
      "35/295, train_loss: 0.0736, step time: 1.0783\n",
      "36/295, train_loss: 0.3892, step time: 1.0315\n",
      "37/295, train_loss: 0.0390, step time: 1.0507\n",
      "38/295, train_loss: 0.0316, step time: 1.0471\n",
      "39/295, train_loss: 0.0558, step time: 1.0443\n",
      "40/295, train_loss: 0.0769, step time: 1.0352\n",
      "41/295, train_loss: 0.0607, step time: 1.0614\n",
      "42/295, train_loss: 0.0406, step time: 1.0487\n",
      "43/295, train_loss: 0.0412, step time: 1.1117\n",
      "44/295, train_loss: 0.0894, step time: 1.0373\n",
      "45/295, train_loss: 0.0775, step time: 1.0429\n",
      "46/295, train_loss: 0.0821, step time: 1.0561\n",
      "47/295, train_loss: 0.0440, step time: 1.0325\n",
      "48/295, train_loss: 0.0423, step time: 1.0892\n",
      "49/295, train_loss: 0.0468, step time: 1.0819\n",
      "50/295, train_loss: 0.0394, step time: 1.0329\n",
      "51/295, train_loss: 0.0426, step time: 1.0363\n",
      "52/295, train_loss: 0.0613, step time: 1.0378\n",
      "53/295, train_loss: 0.0428, step time: 1.0651\n",
      "54/295, train_loss: 0.0541, step time: 1.0446\n",
      "55/295, train_loss: 0.0896, step time: 1.0617\n",
      "56/295, train_loss: 0.0494, step time: 1.0553\n",
      "57/295, train_loss: 0.0358, step time: 1.0342\n",
      "58/295, train_loss: 0.0665, step time: 1.0342\n",
      "59/295, train_loss: 0.0376, step time: 1.0409\n",
      "60/295, train_loss: 0.0644, step time: 1.0703\n",
      "61/295, train_loss: 0.0839, step time: 1.0552\n",
      "62/295, train_loss: 0.0559, step time: 1.0412\n",
      "63/295, train_loss: 0.0333, step time: 1.0529\n",
      "64/295, train_loss: 0.0489, step time: 1.0328\n",
      "65/295, train_loss: 0.0433, step time: 1.0480\n",
      "66/295, train_loss: 0.0329, step time: 1.0417\n",
      "67/295, train_loss: 0.1134, step time: 1.0504\n",
      "68/295, train_loss: 0.0640, step time: 1.0616\n",
      "69/295, train_loss: 0.4383, step time: 1.0400\n",
      "70/295, train_loss: 0.0594, step time: 1.0962\n",
      "71/295, train_loss: 0.0610, step time: 1.0972\n",
      "72/295, train_loss: 0.0264, step time: 1.0561\n",
      "73/295, train_loss: 0.0440, step time: 1.0431\n",
      "74/295, train_loss: 0.0276, step time: 1.0383\n",
      "75/295, train_loss: 0.0892, step time: 1.0396\n",
      "76/295, train_loss: 0.0665, step time: 1.0789\n",
      "77/295, train_loss: 0.1029, step time: 1.0388\n",
      "78/295, train_loss: 0.0902, step time: 1.0505\n",
      "79/295, train_loss: 0.0692, step time: 1.0333\n",
      "80/295, train_loss: 0.0190, step time: 1.0381\n",
      "81/295, train_loss: 0.0323, step time: 1.0319\n",
      "82/295, train_loss: 0.0628, step time: 1.0377\n",
      "83/295, train_loss: 0.0535, step time: 1.0660\n",
      "84/295, train_loss: 0.0606, step time: 1.0578\n",
      "85/295, train_loss: 0.0818, step time: 1.0598\n",
      "86/295, train_loss: 0.1361, step time: 1.0300\n",
      "87/295, train_loss: 0.0668, step time: 1.0357\n",
      "88/295, train_loss: 0.0335, step time: 1.0697\n",
      "89/295, train_loss: 0.0299, step time: 1.0629\n",
      "90/295, train_loss: 0.0369, step time: 1.0464\n",
      "91/295, train_loss: 0.0720, step time: 1.0329\n",
      "92/295, train_loss: 0.1080, step time: 1.0545\n",
      "93/295, train_loss: 0.0822, step time: 1.0462\n",
      "94/295, train_loss: 0.0458, step time: 1.0440\n",
      "95/295, train_loss: 0.0677, step time: 1.1087\n",
      "96/295, train_loss: 0.0555, step time: 1.0625\n",
      "97/295, train_loss: 0.3800, step time: 1.0531\n",
      "98/295, train_loss: 0.0285, step time: 1.0522\n",
      "99/295, train_loss: 0.0644, step time: 1.0340\n",
      "100/295, train_loss: 0.0823, step time: 1.0361\n",
      "101/295, train_loss: 0.0777, step time: 1.0332\n",
      "102/295, train_loss: 0.0554, step time: 1.0360\n",
      "103/295, train_loss: 0.0531, step time: 1.0629\n",
      "104/295, train_loss: 0.0328, step time: 1.0676\n",
      "105/295, train_loss: 0.3705, step time: 1.0403\n",
      "106/295, train_loss: 0.0494, step time: 1.0584\n",
      "107/295, train_loss: 0.0718, step time: 1.0401\n",
      "108/295, train_loss: 0.0484, step time: 1.0590\n",
      "109/295, train_loss: 0.0501, step time: 1.0323\n",
      "110/295, train_loss: 0.0597, step time: 1.0315\n",
      "111/295, train_loss: 0.0477, step time: 1.0454\n",
      "112/295, train_loss: 0.3816, step time: 1.1279\n",
      "113/295, train_loss: 0.0702, step time: 1.0858\n",
      "114/295, train_loss: 0.0371, step time: 1.0548\n",
      "115/295, train_loss: 0.0815, step time: 1.0373\n",
      "116/295, train_loss: 0.0773, step time: 1.0392\n",
      "117/295, train_loss: 0.3644, step time: 1.1023\n",
      "118/295, train_loss: 0.1137, step time: 1.0356\n",
      "119/295, train_loss: 0.0353, step time: 1.0351\n",
      "120/295, train_loss: 0.0359, step time: 1.0609\n",
      "121/295, train_loss: 0.0697, step time: 1.0723\n",
      "122/295, train_loss: 0.0600, step time: 1.0811\n",
      "123/295, train_loss: 0.3893, step time: 1.1223\n",
      "124/295, train_loss: 0.0525, step time: 1.0568\n",
      "125/295, train_loss: 0.0424, step time: 1.0319\n",
      "126/295, train_loss: 0.0448, step time: 1.0390\n",
      "127/295, train_loss: 0.0640, step time: 1.0547\n",
      "128/295, train_loss: 0.0281, step time: 1.0453\n",
      "129/295, train_loss: 0.0501, step time: 1.0323\n",
      "130/295, train_loss: 0.0511, step time: 1.0378\n",
      "131/295, train_loss: 0.0266, step time: 1.0360\n",
      "132/295, train_loss: 0.0277, step time: 1.0390\n",
      "133/295, train_loss: 0.0516, step time: 1.0416\n",
      "134/295, train_loss: 0.0513, step time: 1.0327\n",
      "135/295, train_loss: 0.0449, step time: 1.0390\n",
      "136/295, train_loss: 0.0770, step time: 1.1015\n",
      "137/295, train_loss: 0.0796, step time: 1.0389\n",
      "138/295, train_loss: 0.0547, step time: 1.0372\n",
      "139/295, train_loss: 0.0249, step time: 1.0354\n",
      "140/295, train_loss: 0.0696, step time: 1.0373\n",
      "141/295, train_loss: 0.0774, step time: 1.0382\n",
      "142/295, train_loss: 0.0470, step time: 1.0378\n",
      "143/295, train_loss: 0.0259, step time: 1.0390\n",
      "144/295, train_loss: 0.0421, step time: 1.0738\n",
      "145/295, train_loss: 0.3616, step time: 1.0957\n",
      "146/295, train_loss: 0.0286, step time: 1.0375\n",
      "147/295, train_loss: 0.0781, step time: 1.1299\n",
      "148/295, train_loss: 0.1517, step time: 1.0352\n",
      "149/295, train_loss: 0.0249, step time: 1.0437\n",
      "150/295, train_loss: 0.0853, step time: 1.1585\n",
      "151/295, train_loss: 0.0308, step time: 1.0381\n",
      "152/295, train_loss: 0.0512, step time: 1.0617\n",
      "153/295, train_loss: 0.0410, step time: 1.1013\n",
      "154/295, train_loss: 0.0251, step time: 1.0740\n",
      "155/295, train_loss: 0.0881, step time: 1.0360\n",
      "156/295, train_loss: 0.0775, step time: 1.0681\n",
      "157/295, train_loss: 0.0539, step time: 1.0593\n",
      "158/295, train_loss: 0.0613, step time: 1.0328\n",
      "159/295, train_loss: 0.0594, step time: 1.0393\n",
      "160/295, train_loss: 0.0604, step time: 1.0488\n",
      "161/295, train_loss: 0.0809, step time: 1.0461\n",
      "162/295, train_loss: 0.0727, step time: 1.0743\n",
      "163/295, train_loss: 0.0590, step time: 1.0329\n",
      "164/295, train_loss: 0.0965, step time: 1.0484\n",
      "165/295, train_loss: 0.3733, step time: 1.0542\n",
      "166/295, train_loss: 0.0572, step time: 1.0627\n",
      "167/295, train_loss: 0.0696, step time: 1.0585\n",
      "168/295, train_loss: 0.0799, step time: 1.0322\n",
      "169/295, train_loss: 0.0287, step time: 1.0346\n",
      "170/295, train_loss: 0.0584, step time: 1.0339\n",
      "171/295, train_loss: 0.0346, step time: 1.0643\n",
      "172/295, train_loss: 0.1076, step time: 1.1149\n",
      "173/295, train_loss: 0.0423, step time: 1.0576\n",
      "174/295, train_loss: 0.0410, step time: 1.0341\n",
      "175/295, train_loss: 0.0713, step time: 1.0338\n",
      "176/295, train_loss: 0.0738, step time: 1.0582\n",
      "177/295, train_loss: 0.0512, step time: 1.0725\n",
      "178/295, train_loss: 0.0529, step time: 1.0343\n",
      "179/295, train_loss: 0.0662, step time: 1.0392\n",
      "180/295, train_loss: 0.0497, step time: 1.1041\n",
      "181/295, train_loss: 0.0777, step time: 1.0413\n",
      "182/295, train_loss: 0.0747, step time: 1.1250\n",
      "183/295, train_loss: 0.0856, step time: 1.0667\n",
      "184/295, train_loss: 0.0239, step time: 1.0601\n",
      "185/295, train_loss: 0.0443, step time: 1.0448\n",
      "186/295, train_loss: 0.0661, step time: 1.0453\n",
      "187/295, train_loss: 0.0788, step time: 1.0471\n",
      "188/295, train_loss: 0.0529, step time: 1.0417\n",
      "189/295, train_loss: 0.0514, step time: 1.0938\n",
      "190/295, train_loss: 0.0958, step time: 1.0377\n",
      "191/295, train_loss: 0.3601, step time: 1.0849\n",
      "192/295, train_loss: 0.3577, step time: 1.0328\n",
      "193/295, train_loss: 0.0340, step time: 1.0340\n",
      "194/295, train_loss: 0.0291, step time: 1.0952\n",
      "195/295, train_loss: 0.0673, step time: 1.1074\n",
      "196/295, train_loss: 0.0517, step time: 1.0623\n",
      "197/295, train_loss: 0.1016, step time: 1.0688\n",
      "198/295, train_loss: 0.0602, step time: 1.0385\n",
      "199/295, train_loss: 0.0626, step time: 1.0696\n",
      "200/295, train_loss: 0.0909, step time: 1.0426\n",
      "201/295, train_loss: 0.0618, step time: 1.0858\n",
      "202/295, train_loss: 0.0592, step time: 1.0944\n",
      "203/295, train_loss: 0.1160, step time: 1.0827\n",
      "204/295, train_loss: 0.0264, step time: 1.1010\n",
      "205/295, train_loss: 0.0575, step time: 1.0607\n",
      "206/295, train_loss: 0.0670, step time: 1.0782\n",
      "207/295, train_loss: 0.0405, step time: 1.0511\n",
      "208/295, train_loss: 0.0916, step time: 1.0349\n",
      "209/295, train_loss: 0.0446, step time: 1.0346\n",
      "210/295, train_loss: 0.0346, step time: 1.0407\n",
      "211/295, train_loss: 0.0330, step time: 1.0418\n",
      "212/295, train_loss: 0.0630, step time: 1.0392\n",
      "213/295, train_loss: 0.0624, step time: 1.0466\n",
      "214/295, train_loss: 0.3806, step time: 1.0386\n",
      "215/295, train_loss: 0.0270, step time: 1.0872\n",
      "216/295, train_loss: 0.0269, step time: 1.1176\n",
      "217/295, train_loss: 0.0919, step time: 1.0310\n",
      "218/295, train_loss: 0.0456, step time: 1.0374\n",
      "219/295, train_loss: 0.0295, step time: 1.0441\n",
      "220/295, train_loss: 0.0286, step time: 1.0645\n",
      "221/295, train_loss: 0.0488, step time: 1.0858\n",
      "222/295, train_loss: 0.2157, step time: 1.0613\n",
      "223/295, train_loss: 0.0594, step time: 1.0592\n",
      "224/295, train_loss: 0.0943, step time: 1.0565\n",
      "225/295, train_loss: 0.0469, step time: 1.0640\n",
      "226/295, train_loss: 0.0581, step time: 1.0430\n",
      "227/295, train_loss: 0.0731, step time: 1.0557\n",
      "228/295, train_loss: 0.3803, step time: 1.0672\n",
      "229/295, train_loss: 0.0741, step time: 1.0475\n",
      "230/295, train_loss: 0.0361, step time: 1.0415\n",
      "231/295, train_loss: 0.0348, step time: 1.0621\n",
      "232/295, train_loss: 0.3814, step time: 1.0387\n",
      "233/295, train_loss: 0.0793, step time: 1.0366\n",
      "234/295, train_loss: 0.0408, step time: 1.0390\n",
      "235/295, train_loss: 0.0630, step time: 1.0368\n",
      "236/295, train_loss: 0.0347, step time: 1.0420\n",
      "237/295, train_loss: 0.0673, step time: 1.0740\n",
      "238/295, train_loss: 0.1195, step time: 1.0597\n",
      "239/295, train_loss: 0.3807, step time: 1.0760\n",
      "240/295, train_loss: 0.0234, step time: 1.0615\n",
      "241/295, train_loss: 0.0426, step time: 1.0362\n",
      "242/295, train_loss: 0.0446, step time: 1.0395\n",
      "243/295, train_loss: 0.3834, step time: 1.0408\n",
      "244/295, train_loss: 0.3635, step time: 1.0369\n",
      "245/295, train_loss: 0.3683, step time: 1.0314\n",
      "246/295, train_loss: 0.0468, step time: 1.0349\n",
      "247/295, train_loss: 0.3786, step time: 1.0584\n",
      "248/295, train_loss: 0.0464, step time: 1.0610\n",
      "249/295, train_loss: 0.0314, step time: 1.0361\n",
      "250/295, train_loss: 0.0947, step time: 1.0442\n",
      "251/295, train_loss: 0.0822, step time: 1.0889\n",
      "252/295, train_loss: 0.1226, step time: 1.0701\n",
      "253/295, train_loss: 0.0466, step time: 1.0377\n",
      "254/295, train_loss: 0.0971, step time: 1.0431\n",
      "255/295, train_loss: 0.0523, step time: 1.0421\n",
      "256/295, train_loss: 0.0617, step time: 1.0499\n",
      "257/295, train_loss: 0.0381, step time: 1.0344\n",
      "258/295, train_loss: 0.0772, step time: 1.0553\n",
      "259/295, train_loss: 0.3620, step time: 1.0407\n",
      "260/295, train_loss: 0.0296, step time: 1.0390\n",
      "261/295, train_loss: 0.0793, step time: 1.0511\n",
      "262/295, train_loss: 0.0497, step time: 1.0974\n",
      "263/295, train_loss: 0.0407, step time: 1.0736\n",
      "264/295, train_loss: 0.0571, step time: 1.0636\n",
      "265/295, train_loss: 0.0629, step time: 1.0458\n",
      "266/295, train_loss: 0.0299, step time: 1.0383\n",
      "267/295, train_loss: 0.0521, step time: 1.0652\n",
      "268/295, train_loss: 0.0534, step time: 1.0830\n",
      "269/295, train_loss: 0.0421, step time: 1.1357\n",
      "270/295, train_loss: 0.0810, step time: 1.0350\n",
      "271/295, train_loss: 0.0285, step time: 1.0321\n",
      "272/295, train_loss: 0.0364, step time: 1.1291\n",
      "273/295, train_loss: 0.0368, step time: 1.0338\n",
      "274/295, train_loss: 0.0564, step time: 1.0374\n",
      "275/295, train_loss: 0.0918, step time: 1.0422\n",
      "276/295, train_loss: 0.0817, step time: 1.0545\n",
      "277/295, train_loss: 0.3986, step time: 1.0553\n",
      "278/295, train_loss: 0.0544, step time: 1.0639\n",
      "279/295, train_loss: 0.0620, step time: 1.0373\n",
      "280/295, train_loss: 0.0398, step time: 1.0348\n",
      "281/295, train_loss: 0.0476, step time: 1.0392\n",
      "282/295, train_loss: 0.0427, step time: 1.1265\n",
      "283/295, train_loss: 0.0641, step time: 1.0486\n",
      "284/295, train_loss: 0.0287, step time: 1.0332\n",
      "285/295, train_loss: 0.0493, step time: 1.0497\n",
      "286/295, train_loss: 0.0961, step time: 1.1243\n",
      "287/295, train_loss: 0.0441, step time: 1.0450\n",
      "288/295, train_loss: 0.0279, step time: 1.0473\n",
      "289/295, train_loss: 0.1090, step time: 1.0282\n",
      "290/295, train_loss: 0.0780, step time: 1.0288\n",
      "291/295, train_loss: 0.0312, step time: 1.0289\n",
      "292/295, train_loss: 0.1206, step time: 1.0286\n",
      "293/295, train_loss: 0.0386, step time: 1.0284\n",
      "294/295, train_loss: 0.0888, step time: 1.0298\n",
      "295/295, train_loss: 0.0377, step time: 1.0308\n",
      "epoch 74 average loss: 0.0833\n",
      "current epoch: 74 current mean dice: 0.7851 tc: 0.7406 wt: 0.8460 et: 0.7729\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 74 is: 385.7988\n",
      "----------\n",
      "epoch 75/100\n",
      "1/295, train_loss: 0.0460, step time: 1.1012\n",
      "2/295, train_loss: 0.0582, step time: 1.1240\n",
      "3/295, train_loss: 0.0320, step time: 1.0500\n",
      "4/295, train_loss: 0.0238, step time: 1.1472\n",
      "5/295, train_loss: 0.0903, step time: 1.0391\n",
      "6/295, train_loss: 0.0859, step time: 1.0394\n",
      "7/295, train_loss: 0.0656, step time: 1.0642\n",
      "8/295, train_loss: 0.0622, step time: 1.1069\n",
      "9/295, train_loss: 0.0404, step time: 1.1025\n",
      "10/295, train_loss: 0.0898, step time: 1.0506\n",
      "11/295, train_loss: 0.3799, step time: 1.0349\n",
      "12/295, train_loss: 0.0711, step time: 1.0311\n",
      "13/295, train_loss: 0.0663, step time: 1.0395\n",
      "14/295, train_loss: 0.0319, step time: 1.0383\n",
      "15/295, train_loss: 0.0748, step time: 1.1040\n",
      "16/295, train_loss: 0.0374, step time: 1.0681\n",
      "17/295, train_loss: 0.0619, step time: 1.0540\n",
      "18/295, train_loss: 0.0326, step time: 1.0744\n",
      "19/295, train_loss: 0.0520, step time: 1.0351\n",
      "20/295, train_loss: 0.1001, step time: 1.0572\n",
      "21/295, train_loss: 0.0605, step time: 1.0911\n",
      "22/295, train_loss: 0.0643, step time: 1.0333\n",
      "23/295, train_loss: 0.0421, step time: 1.0556\n",
      "24/295, train_loss: 0.0313, step time: 1.0783\n",
      "25/295, train_loss: 0.0417, step time: 1.0469\n",
      "26/295, train_loss: 0.0269, step time: 1.0397\n",
      "27/295, train_loss: 0.0576, step time: 1.0528\n",
      "28/295, train_loss: 0.1053, step time: 1.0515\n",
      "29/295, train_loss: 0.3797, step time: 1.0733\n",
      "30/295, train_loss: 0.0321, step time: 1.0483\n",
      "31/295, train_loss: 0.0626, step time: 1.0557\n",
      "32/295, train_loss: 0.0450, step time: 1.0312\n",
      "33/295, train_loss: 0.0438, step time: 1.0573\n",
      "34/295, train_loss: 0.0383, step time: 1.0378\n",
      "35/295, train_loss: 0.0277, step time: 1.0529\n",
      "36/295, train_loss: 0.0569, step time: 1.0399\n",
      "37/295, train_loss: 0.0545, step time: 1.0378\n",
      "38/295, train_loss: 0.0807, step time: 1.0774\n",
      "39/295, train_loss: 0.0636, step time: 1.0660\n",
      "40/295, train_loss: 0.0338, step time: 1.0370\n",
      "41/295, train_loss: 0.3827, step time: 1.0580\n",
      "42/295, train_loss: 0.1019, step time: 1.0542\n",
      "43/295, train_loss: 0.0526, step time: 1.0377\n",
      "44/295, train_loss: 0.0609, step time: 1.0442\n",
      "45/295, train_loss: 0.0764, step time: 1.0921\n",
      "46/295, train_loss: 0.0396, step time: 1.0309\n",
      "47/295, train_loss: 0.0346, step time: 1.0404\n",
      "48/295, train_loss: 0.0553, step time: 1.0588\n",
      "49/295, train_loss: 0.3664, step time: 1.0560\n",
      "50/295, train_loss: 0.0750, step time: 1.0520\n",
      "51/295, train_loss: 0.0782, step time: 1.0337\n",
      "52/295, train_loss: 0.0878, step time: 1.0569\n",
      "53/295, train_loss: 0.0390, step time: 1.0451\n",
      "54/295, train_loss: 0.0644, step time: 1.0331\n",
      "55/295, train_loss: 0.0318, step time: 1.0342\n",
      "56/295, train_loss: 0.0774, step time: 1.0498\n",
      "57/295, train_loss: 0.0803, step time: 1.0530\n",
      "58/295, train_loss: 0.0460, step time: 1.0987\n",
      "59/295, train_loss: 0.1087, step time: 1.0537\n",
      "60/295, train_loss: 0.0305, step time: 1.0721\n",
      "61/295, train_loss: 0.0517, step time: 1.0377\n",
      "62/295, train_loss: 0.0482, step time: 1.0992\n",
      "63/295, train_loss: 0.0406, step time: 1.0488\n",
      "64/295, train_loss: 0.0713, step time: 1.0802\n",
      "65/295, train_loss: 0.0507, step time: 1.0373\n",
      "66/295, train_loss: 0.0573, step time: 1.0459\n",
      "67/295, train_loss: 0.0365, step time: 1.0465\n",
      "68/295, train_loss: 0.0524, step time: 1.0692\n",
      "69/295, train_loss: 0.0475, step time: 1.0648\n",
      "70/295, train_loss: 0.0511, step time: 1.0662\n",
      "71/295, train_loss: 0.0740, step time: 1.0385\n",
      "72/295, train_loss: 0.0652, step time: 1.0446\n",
      "73/295, train_loss: 0.0505, step time: 1.0500\n",
      "74/295, train_loss: 0.0284, step time: 1.0380\n",
      "75/295, train_loss: 0.0253, step time: 1.0353\n",
      "76/295, train_loss: 0.0536, step time: 1.0307\n",
      "77/295, train_loss: 0.0490, step time: 1.1182\n",
      "78/295, train_loss: 0.0281, step time: 1.0331\n",
      "79/295, train_loss: 0.3566, step time: 1.1128\n",
      "80/295, train_loss: 0.1056, step time: 1.1111\n",
      "81/295, train_loss: 0.1098, step time: 1.0466\n",
      "82/295, train_loss: 0.0346, step time: 1.0505\n",
      "83/295, train_loss: 0.0567, step time: 1.0433\n",
      "84/295, train_loss: 0.0612, step time: 1.0547\n",
      "85/295, train_loss: 0.3727, step time: 1.0342\n",
      "86/295, train_loss: 0.0593, step time: 1.0582\n",
      "87/295, train_loss: 0.0435, step time: 1.0330\n",
      "88/295, train_loss: 0.0738, step time: 1.0715\n",
      "89/295, train_loss: 0.0875, step time: 1.1705\n",
      "90/295, train_loss: 0.1151, step time: 1.0551\n",
      "91/295, train_loss: 0.0317, step time: 1.0389\n",
      "92/295, train_loss: 0.0266, step time: 1.0420\n",
      "93/295, train_loss: 0.0456, step time: 1.0783\n",
      "94/295, train_loss: 0.0427, step time: 1.0523\n",
      "95/295, train_loss: 0.0275, step time: 1.0408\n",
      "96/295, train_loss: 0.0561, step time: 1.0407\n",
      "97/295, train_loss: 0.0800, step time: 1.0492\n",
      "98/295, train_loss: 0.0805, step time: 1.0730\n",
      "99/295, train_loss: 0.0642, step time: 1.0509\n",
      "100/295, train_loss: 0.0793, step time: 1.0480\n",
      "101/295, train_loss: 0.0476, step time: 1.0382\n",
      "102/295, train_loss: 0.1141, step time: 1.0884\n",
      "103/295, train_loss: 0.1242, step time: 1.0461\n",
      "104/295, train_loss: 0.3857, step time: 1.0541\n",
      "105/295, train_loss: 0.1364, step time: 1.0420\n",
      "106/295, train_loss: 0.0615, step time: 1.0953\n",
      "107/295, train_loss: 0.0279, step time: 1.1351\n",
      "108/295, train_loss: 0.0315, step time: 1.0338\n",
      "109/295, train_loss: 0.0680, step time: 1.0526\n",
      "110/295, train_loss: 0.0621, step time: 1.0573\n",
      "111/295, train_loss: 0.3792, step time: 1.0837\n",
      "112/295, train_loss: 0.0258, step time: 1.0773\n",
      "113/295, train_loss: 0.0491, step time: 1.0403\n",
      "114/295, train_loss: 0.0373, step time: 1.0441\n",
      "115/295, train_loss: 0.0466, step time: 1.0540\n",
      "116/295, train_loss: 0.0525, step time: 1.0369\n",
      "117/295, train_loss: 0.0263, step time: 1.0623\n",
      "118/295, train_loss: 0.0594, step time: 1.0728\n",
      "119/295, train_loss: 0.0308, step time: 1.0408\n",
      "120/295, train_loss: 0.3803, step time: 1.0331\n",
      "121/295, train_loss: 0.0712, step time: 1.0385\n",
      "122/295, train_loss: 0.0262, step time: 1.0537\n",
      "123/295, train_loss: 0.0754, step time: 1.0331\n",
      "124/295, train_loss: 0.0636, step time: 1.0361\n",
      "125/295, train_loss: 0.0301, step time: 1.0426\n",
      "126/295, train_loss: 0.3624, step time: 1.0384\n",
      "127/295, train_loss: 0.0557, step time: 1.0420\n",
      "128/295, train_loss: 0.0432, step time: 1.0695\n",
      "129/295, train_loss: 0.3996, step time: 1.0340\n",
      "130/295, train_loss: 0.0956, step time: 1.0472\n",
      "131/295, train_loss: 0.0708, step time: 1.0565\n",
      "132/295, train_loss: 0.3906, step time: 1.0694\n",
      "133/295, train_loss: 0.0440, step time: 1.0519\n",
      "134/295, train_loss: 0.0471, step time: 1.0398\n",
      "135/295, train_loss: 0.0879, step time: 1.0576\n",
      "136/295, train_loss: 0.0424, step time: 1.0580\n",
      "137/295, train_loss: 0.0285, step time: 1.0376\n",
      "138/295, train_loss: 0.0940, step time: 1.0434\n",
      "139/295, train_loss: 0.0628, step time: 1.0377\n",
      "140/295, train_loss: 0.0185, step time: 1.0362\n",
      "141/295, train_loss: 0.1147, step time: 1.0364\n",
      "142/295, train_loss: 0.0402, step time: 1.0614\n",
      "143/295, train_loss: 0.0443, step time: 1.0443\n",
      "144/295, train_loss: 0.0460, step time: 1.0375\n",
      "145/295, train_loss: 0.0841, step time: 1.0571\n",
      "146/295, train_loss: 0.0602, step time: 1.0505\n",
      "147/295, train_loss: 0.3747, step time: 1.0776\n",
      "148/295, train_loss: 0.0502, step time: 1.0331\n",
      "149/295, train_loss: 0.0317, step time: 1.0601\n",
      "150/295, train_loss: 0.0289, step time: 1.0330\n",
      "151/295, train_loss: 0.0528, step time: 1.0344\n",
      "152/295, train_loss: 0.0547, step time: 1.0421\n",
      "153/295, train_loss: 0.0384, step time: 1.1244\n",
      "154/295, train_loss: 0.0664, step time: 1.0395\n",
      "155/295, train_loss: 0.1348, step time: 1.0411\n",
      "156/295, train_loss: 0.0525, step time: 1.0621\n",
      "157/295, train_loss: 0.0479, step time: 1.0342\n",
      "158/295, train_loss: 0.0563, step time: 1.0382\n",
      "159/295, train_loss: 0.0883, step time: 1.0357\n",
      "160/295, train_loss: 0.0394, step time: 1.0450\n",
      "161/295, train_loss: 0.0584, step time: 1.0569\n",
      "162/295, train_loss: 0.0585, step time: 1.0429\n",
      "163/295, train_loss: 0.0688, step time: 1.0524\n",
      "164/295, train_loss: 0.0580, step time: 1.0375\n",
      "165/295, train_loss: 0.0799, step time: 1.0593\n",
      "166/295, train_loss: 0.0436, step time: 1.0400\n",
      "167/295, train_loss: 0.0708, step time: 1.0373\n",
      "168/295, train_loss: 0.0458, step time: 1.0563\n",
      "169/295, train_loss: 0.0665, step time: 1.0440\n",
      "170/295, train_loss: 0.0875, step time: 1.0316\n",
      "171/295, train_loss: 0.3807, step time: 1.0450\n",
      "172/295, train_loss: 0.0354, step time: 1.0830\n",
      "173/295, train_loss: 0.0597, step time: 1.0395\n",
      "174/295, train_loss: 0.0760, step time: 1.0572\n",
      "175/295, train_loss: 0.0625, step time: 1.0362\n",
      "176/295, train_loss: 0.0679, step time: 1.0430\n",
      "177/295, train_loss: 0.0322, step time: 1.0524\n",
      "178/295, train_loss: 0.0395, step time: 1.0650\n",
      "179/295, train_loss: 0.1060, step time: 1.0338\n",
      "180/295, train_loss: 0.1019, step time: 1.0741\n",
      "181/295, train_loss: 0.0780, step time: 1.0476\n",
      "182/295, train_loss: 0.0594, step time: 1.0330\n",
      "183/295, train_loss: 0.0356, step time: 1.0357\n",
      "184/295, train_loss: 0.0802, step time: 1.0392\n",
      "185/295, train_loss: 0.3810, step time: 1.0376\n",
      "186/295, train_loss: 0.0330, step time: 1.0588\n",
      "187/295, train_loss: 0.0705, step time: 1.0778\n",
      "188/295, train_loss: 0.0339, step time: 1.0349\n",
      "189/295, train_loss: 0.0288, step time: 1.1088\n",
      "190/295, train_loss: 0.0435, step time: 1.0641\n",
      "191/295, train_loss: 0.0617, step time: 1.0380\n",
      "192/295, train_loss: 0.0364, step time: 1.0429\n",
      "193/295, train_loss: 0.0844, step time: 1.0327\n",
      "194/295, train_loss: 0.1469, step time: 1.0387\n",
      "195/295, train_loss: 0.0523, step time: 1.0367\n",
      "196/295, train_loss: 0.0340, step time: 1.0373\n",
      "197/295, train_loss: 0.0424, step time: 1.0920\n",
      "198/295, train_loss: 0.0248, step time: 1.0568\n",
      "199/295, train_loss: 0.0373, step time: 1.0372\n",
      "200/295, train_loss: 0.0421, step time: 1.0427\n",
      "201/295, train_loss: 0.0417, step time: 1.0700\n",
      "202/295, train_loss: 0.0495, step time: 1.0469\n",
      "203/295, train_loss: 0.0808, step time: 1.0358\n",
      "204/295, train_loss: 0.0975, step time: 1.0363\n",
      "205/295, train_loss: 0.0366, step time: 1.1010\n",
      "206/295, train_loss: 0.0888, step time: 1.0468\n",
      "207/295, train_loss: 0.0912, step time: 1.0640\n",
      "208/295, train_loss: 0.0461, step time: 1.0611\n",
      "209/295, train_loss: 0.0409, step time: 1.0377\n",
      "210/295, train_loss: 0.0623, step time: 1.0417\n",
      "211/295, train_loss: 0.0667, step time: 1.0321\n",
      "212/295, train_loss: 0.0297, step time: 1.0337\n",
      "213/295, train_loss: 0.0418, step time: 1.0540\n",
      "214/295, train_loss: 0.0772, step time: 1.0889\n",
      "215/295, train_loss: 0.0784, step time: 1.0434\n",
      "216/295, train_loss: 0.0747, step time: 1.0473\n",
      "217/295, train_loss: 0.0795, step time: 1.0775\n",
      "218/295, train_loss: 0.3621, step time: 1.0366\n",
      "219/295, train_loss: 0.0485, step time: 1.0389\n",
      "220/295, train_loss: 0.1016, step time: 1.0647\n",
      "221/295, train_loss: 0.0515, step time: 1.0619\n",
      "222/295, train_loss: 0.0494, step time: 1.0393\n",
      "223/295, train_loss: 0.0698, step time: 1.0445\n",
      "224/295, train_loss: 0.0608, step time: 1.0510\n",
      "225/295, train_loss: 0.0458, step time: 1.0484\n",
      "226/295, train_loss: 0.0321, step time: 1.0532\n",
      "227/295, train_loss: 0.0318, step time: 1.0845\n",
      "228/295, train_loss: 0.0733, step time: 1.0465\n",
      "229/295, train_loss: 0.0959, step time: 1.0533\n",
      "230/295, train_loss: 0.0767, step time: 1.0493\n",
      "231/295, train_loss: 0.0370, step time: 1.0656\n",
      "232/295, train_loss: 0.0336, step time: 1.0491\n",
      "233/295, train_loss: 0.0406, step time: 1.0436\n",
      "234/295, train_loss: 0.3605, step time: 1.0618\n",
      "235/295, train_loss: 0.3656, step time: 1.0387\n",
      "236/295, train_loss: 0.0902, step time: 1.1271\n",
      "237/295, train_loss: 0.0662, step time: 1.0362\n",
      "238/295, train_loss: 0.0940, step time: 1.0638\n",
      "239/295, train_loss: 0.0499, step time: 1.0405\n",
      "240/295, train_loss: 0.0386, step time: 1.0346\n",
      "241/295, train_loss: 0.0517, step time: 1.0546\n",
      "242/295, train_loss: 0.0416, step time: 1.0700\n",
      "243/295, train_loss: 0.0830, step time: 1.0319\n",
      "244/295, train_loss: 0.0780, step time: 1.0392\n",
      "245/295, train_loss: 0.0487, step time: 1.0353\n",
      "246/295, train_loss: 0.3711, step time: 1.0382\n",
      "247/295, train_loss: 0.0741, step time: 1.0396\n",
      "248/295, train_loss: 0.0406, step time: 1.0730\n",
      "249/295, train_loss: 0.1189, step time: 1.0698\n",
      "250/295, train_loss: 0.0276, step time: 1.0674\n",
      "251/295, train_loss: 0.0310, step time: 1.0811\n",
      "252/295, train_loss: 0.0285, step time: 1.0416\n",
      "253/295, train_loss: 0.0543, step time: 1.0539\n",
      "254/295, train_loss: 0.0265, step time: 1.0892\n",
      "255/295, train_loss: 0.0371, step time: 1.0372\n",
      "256/295, train_loss: 0.0988, step time: 1.0407\n",
      "257/295, train_loss: 0.0531, step time: 1.0407\n",
      "258/295, train_loss: 0.0275, step time: 1.0385\n",
      "259/295, train_loss: 0.0226, step time: 1.0342\n",
      "260/295, train_loss: 0.0496, step time: 1.0621\n",
      "261/295, train_loss: 0.0776, step time: 1.0342\n",
      "262/295, train_loss: 0.0422, step time: 1.0411\n",
      "263/295, train_loss: 0.0577, step time: 1.0503\n",
      "264/295, train_loss: 0.0302, step time: 1.0649\n",
      "265/295, train_loss: 0.0335, step time: 1.0376\n",
      "266/295, train_loss: 0.0557, step time: 1.0431\n",
      "267/295, train_loss: 0.0423, step time: 1.0370\n",
      "268/295, train_loss: 0.4347, step time: 1.0396\n",
      "269/295, train_loss: 0.0772, step time: 1.0927\n",
      "270/295, train_loss: 0.0250, step time: 1.0358\n",
      "271/295, train_loss: 0.0527, step time: 1.0373\n",
      "272/295, train_loss: 0.0675, step time: 1.0352\n",
      "273/295, train_loss: 0.0747, step time: 1.0601\n",
      "274/295, train_loss: 0.0502, step time: 1.0358\n",
      "275/295, train_loss: 0.0611, step time: 1.0423\n",
      "276/295, train_loss: 0.3627, step time: 1.0394\n",
      "277/295, train_loss: 0.0646, step time: 1.0357\n",
      "278/295, train_loss: 0.0594, step time: 1.0410\n",
      "279/295, train_loss: 0.0929, step time: 1.0371\n",
      "280/295, train_loss: 0.0781, step time: 1.0337\n",
      "281/295, train_loss: 0.0753, step time: 1.0401\n",
      "282/295, train_loss: 0.0485, step time: 1.0483\n",
      "283/295, train_loss: 0.0774, step time: 1.1036\n",
      "284/295, train_loss: 0.0807, step time: 1.0358\n",
      "285/295, train_loss: 0.0606, step time: 1.0363\n",
      "286/295, train_loss: 0.0255, step time: 1.0407\n",
      "287/295, train_loss: 0.0845, step time: 1.0339\n",
      "288/295, train_loss: 0.0934, step time: 1.0325\n",
      "289/295, train_loss: 0.0662, step time: 1.0342\n",
      "290/295, train_loss: 0.0282, step time: 1.0296\n",
      "291/295, train_loss: 0.0464, step time: 1.0303\n",
      "292/295, train_loss: 0.3875, step time: 1.0290\n",
      "293/295, train_loss: 0.0811, step time: 1.0293\n",
      "294/295, train_loss: 0.2136, step time: 1.0284\n",
      "295/295, train_loss: 0.0416, step time: 1.0288\n",
      "epoch 75 average loss: 0.0826\n",
      "current epoch: 75 current mean dice: 0.7740 tc: 0.7276 wt: 0.8411 et: 0.7571\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 75 is: 386.7889\n",
      "----------\n",
      "epoch 76/100\n",
      "1/295, train_loss: 0.3618, step time: 1.0714\n",
      "2/295, train_loss: 0.0682, step time: 1.1302\n",
      "3/295, train_loss: 0.1133, step time: 1.0826\n",
      "4/295, train_loss: 0.0476, step time: 1.1007\n",
      "5/295, train_loss: 0.0972, step time: 1.0396\n",
      "6/295, train_loss: 0.0482, step time: 1.0616\n",
      "7/295, train_loss: 0.0322, step time: 1.0809\n",
      "8/295, train_loss: 0.0515, step time: 1.0290\n",
      "9/295, train_loss: 0.0554, step time: 1.0638\n",
      "10/295, train_loss: 0.0668, step time: 1.0461\n",
      "11/295, train_loss: 0.0776, step time: 1.0576\n",
      "12/295, train_loss: 0.0267, step time: 1.0563\n",
      "13/295, train_loss: 0.0814, step time: 1.0391\n",
      "14/295, train_loss: 0.0428, step time: 1.0513\n",
      "15/295, train_loss: 0.0357, step time: 1.0436\n",
      "16/295, train_loss: 0.0262, step time: 1.0370\n",
      "17/295, train_loss: 0.0843, step time: 1.0553\n",
      "18/295, train_loss: 0.0914, step time: 1.0683\n",
      "19/295, train_loss: 0.0754, step time: 1.0499\n",
      "20/295, train_loss: 0.0771, step time: 1.0476\n",
      "21/295, train_loss: 0.1093, step time: 1.0364\n",
      "22/295, train_loss: 0.0940, step time: 1.0421\n",
      "23/295, train_loss: 0.0952, step time: 1.0345\n",
      "24/295, train_loss: 0.0617, step time: 1.0432\n",
      "25/295, train_loss: 0.0321, step time: 1.0635\n",
      "26/295, train_loss: 0.0543, step time: 1.0392\n",
      "27/295, train_loss: 0.3686, step time: 1.0662\n",
      "28/295, train_loss: 0.0567, step time: 1.0441\n",
      "29/295, train_loss: 0.0580, step time: 1.1563\n",
      "30/295, train_loss: 0.0518, step time: 1.0395\n",
      "31/295, train_loss: 0.0587, step time: 1.0437\n",
      "32/295, train_loss: 0.0398, step time: 1.0523\n",
      "33/295, train_loss: 0.0821, step time: 1.0455\n",
      "34/295, train_loss: 0.0596, step time: 1.0584\n",
      "35/295, train_loss: 0.0373, step time: 1.1400\n",
      "36/295, train_loss: 0.0248, step time: 1.0442\n",
      "37/295, train_loss: 0.0414, step time: 1.0326\n",
      "38/295, train_loss: 0.0356, step time: 1.0398\n",
      "39/295, train_loss: 0.0889, step time: 1.0843\n",
      "40/295, train_loss: 0.0304, step time: 1.0306\n",
      "41/295, train_loss: 0.0387, step time: 1.0384\n",
      "42/295, train_loss: 0.0459, step time: 1.0556\n",
      "43/295, train_loss: 0.0370, step time: 1.0435\n",
      "44/295, train_loss: 0.0350, step time: 1.0833\n",
      "45/295, train_loss: 0.0519, step time: 1.0917\n",
      "46/295, train_loss: 0.0437, step time: 1.0786\n",
      "47/295, train_loss: 0.0657, step time: 1.0476\n",
      "48/295, train_loss: 0.0561, step time: 1.0307\n",
      "49/295, train_loss: 0.0541, step time: 1.0434\n",
      "50/295, train_loss: 0.0610, step time: 1.0406\n",
      "51/295, train_loss: 0.0540, step time: 1.0574\n",
      "52/295, train_loss: 0.0505, step time: 1.0323\n",
      "53/295, train_loss: 0.0663, step time: 1.0350\n",
      "54/295, train_loss: 0.0421, step time: 1.0547\n",
      "55/295, train_loss: 0.0915, step time: 1.0395\n",
      "56/295, train_loss: 0.0408, step time: 1.0373\n",
      "57/295, train_loss: 0.0319, step time: 1.0378\n",
      "58/295, train_loss: 0.3735, step time: 1.0332\n",
      "59/295, train_loss: 0.1378, step time: 1.0494\n",
      "60/295, train_loss: 0.0614, step time: 1.0576\n",
      "61/295, train_loss: 0.0705, step time: 1.0954\n",
      "62/295, train_loss: 0.0651, step time: 1.0580\n",
      "63/295, train_loss: 0.0522, step time: 1.0608\n",
      "64/295, train_loss: 0.3612, step time: 1.0303\n",
      "65/295, train_loss: 0.0501, step time: 1.0294\n",
      "66/295, train_loss: 0.0971, step time: 1.0336\n",
      "67/295, train_loss: 0.0631, step time: 1.0491\n",
      "68/295, train_loss: 0.0579, step time: 1.0343\n",
      "69/295, train_loss: 0.0289, step time: 1.0417\n",
      "70/295, train_loss: 0.0328, step time: 1.0604\n",
      "71/295, train_loss: 0.3977, step time: 1.0307\n",
      "72/295, train_loss: 0.0878, step time: 1.0364\n",
      "73/295, train_loss: 0.0280, step time: 1.0323\n",
      "74/295, train_loss: 0.0636, step time: 1.0488\n",
      "75/295, train_loss: 0.0499, step time: 1.0323\n",
      "76/295, train_loss: 0.0326, step time: 1.0894\n",
      "77/295, train_loss: 0.1389, step time: 1.0950\n",
      "78/295, train_loss: 0.3905, step time: 1.0727\n",
      "79/295, train_loss: 0.0423, step time: 1.0555\n",
      "80/295, train_loss: 0.0325, step time: 1.0314\n",
      "81/295, train_loss: 0.0776, step time: 1.0312\n",
      "82/295, train_loss: 0.0499, step time: 1.0307\n",
      "83/295, train_loss: 0.3672, step time: 1.0347\n",
      "84/295, train_loss: 0.0520, step time: 1.0375\n",
      "85/295, train_loss: 0.0432, step time: 1.0435\n",
      "86/295, train_loss: 0.0692, step time: 1.0457\n",
      "87/295, train_loss: 0.0409, step time: 1.0955\n",
      "88/295, train_loss: 0.0558, step time: 1.0451\n",
      "89/295, train_loss: 0.0743, step time: 1.0428\n",
      "90/295, train_loss: 0.1335, step time: 1.0359\n",
      "91/295, train_loss: 0.1043, step time: 1.0384\n",
      "92/295, train_loss: 0.0434, step time: 1.0887\n",
      "93/295, train_loss: 0.0631, step time: 1.0624\n",
      "94/295, train_loss: 0.0512, step time: 1.0318\n",
      "95/295, train_loss: 0.0444, step time: 1.0484\n",
      "96/295, train_loss: 0.1160, step time: 1.0441\n",
      "97/295, train_loss: 0.0567, step time: 1.0619\n",
      "98/295, train_loss: 0.0705, step time: 1.0315\n",
      "99/295, train_loss: 0.0388, step time: 1.0399\n",
      "100/295, train_loss: 0.0595, step time: 1.0629\n",
      "101/295, train_loss: 0.0373, step time: 1.0361\n",
      "102/295, train_loss: 0.0990, step time: 1.0325\n",
      "103/295, train_loss: 0.0279, step time: 1.0311\n",
      "104/295, train_loss: 0.0277, step time: 1.1180\n",
      "105/295, train_loss: 0.0669, step time: 1.0548\n",
      "106/295, train_loss: 0.0675, step time: 1.0554\n",
      "107/295, train_loss: 0.0383, step time: 1.0619\n",
      "108/295, train_loss: 0.0264, step time: 1.0467\n",
      "109/295, train_loss: 0.3798, step time: 1.0404\n",
      "110/295, train_loss: 0.0510, step time: 1.0328\n",
      "111/295, train_loss: 0.1137, step time: 1.0324\n",
      "112/295, train_loss: 0.0420, step time: 1.0338\n",
      "113/295, train_loss: 0.0758, step time: 1.0310\n",
      "114/295, train_loss: 0.0457, step time: 1.0671\n",
      "115/295, train_loss: 0.0336, step time: 1.1263\n",
      "116/295, train_loss: 0.0785, step time: 1.0491\n",
      "117/295, train_loss: 0.0450, step time: 1.0492\n",
      "118/295, train_loss: 0.0277, step time: 1.1559\n",
      "119/295, train_loss: 0.0474, step time: 1.0496\n",
      "120/295, train_loss: 0.0593, step time: 1.0335\n",
      "121/295, train_loss: 0.0589, step time: 1.0500\n",
      "122/295, train_loss: 0.0746, step time: 1.0734\n",
      "123/295, train_loss: 0.0773, step time: 1.0342\n",
      "124/295, train_loss: 0.0418, step time: 1.0338\n",
      "125/295, train_loss: 0.0485, step time: 1.0504\n",
      "126/295, train_loss: 0.0341, step time: 1.0532\n",
      "127/295, train_loss: 0.0359, step time: 1.0497\n",
      "128/295, train_loss: 0.0530, step time: 1.0447\n",
      "129/295, train_loss: 0.0879, step time: 1.0337\n",
      "130/295, train_loss: 0.0842, step time: 1.0631\n",
      "131/295, train_loss: 0.0689, step time: 1.0427\n",
      "132/295, train_loss: 0.0660, step time: 1.0353\n",
      "133/295, train_loss: 0.0884, step time: 1.0692\n",
      "134/295, train_loss: 0.1006, step time: 1.0392\n",
      "135/295, train_loss: 0.1018, step time: 1.0635\n",
      "136/295, train_loss: 0.3873, step time: 1.0612\n",
      "137/295, train_loss: 0.0306, step time: 1.0515\n",
      "138/295, train_loss: 0.0534, step time: 1.0383\n",
      "139/295, train_loss: 0.0268, step time: 1.0830\n",
      "140/295, train_loss: 0.0580, step time: 1.0424\n",
      "141/295, train_loss: 0.0246, step time: 1.0349\n",
      "142/295, train_loss: 0.0711, step time: 1.0363\n",
      "143/295, train_loss: 0.0501, step time: 1.0794\n",
      "144/295, train_loss: 0.3816, step time: 1.1569\n",
      "145/295, train_loss: 0.4342, step time: 1.0303\n",
      "146/295, train_loss: 0.0324, step time: 1.0734\n",
      "147/295, train_loss: 0.0794, step time: 1.0539\n",
      "148/295, train_loss: 0.0547, step time: 1.0371\n",
      "149/295, train_loss: 0.0308, step time: 1.0617\n",
      "150/295, train_loss: 0.0316, step time: 1.0480\n",
      "151/295, train_loss: 0.0282, step time: 1.0616\n",
      "152/295, train_loss: 0.0499, step time: 1.0551\n",
      "153/295, train_loss: 0.0771, step time: 1.0330\n",
      "154/295, train_loss: 0.0805, step time: 1.0365\n",
      "155/295, train_loss: 0.0490, step time: 1.0474\n",
      "156/295, train_loss: 0.0416, step time: 1.0314\n",
      "157/295, train_loss: 0.0790, step time: 1.0584\n",
      "158/295, train_loss: 0.3769, step time: 1.0829\n",
      "159/295, train_loss: 0.0395, step time: 1.0335\n",
      "160/295, train_loss: 0.0264, step time: 1.0314\n",
      "161/295, train_loss: 0.0241, step time: 1.0534\n",
      "162/295, train_loss: 0.0324, step time: 1.0407\n",
      "163/295, train_loss: 0.3820, step time: 1.0359\n",
      "164/295, train_loss: 0.0739, step time: 1.0442\n",
      "165/295, train_loss: 0.0631, step time: 1.0610\n",
      "166/295, train_loss: 0.3622, step time: 1.0474\n",
      "167/295, train_loss: 0.0718, step time: 1.0329\n",
      "168/295, train_loss: 0.0251, step time: 1.0431\n",
      "169/295, train_loss: 0.0844, step time: 1.0414\n",
      "170/295, train_loss: 0.0529, step time: 1.0458\n",
      "171/295, train_loss: 0.0241, step time: 1.0506\n",
      "172/295, train_loss: 0.0548, step time: 1.0690\n",
      "173/295, train_loss: 0.3797, step time: 1.1046\n",
      "174/295, train_loss: 0.0553, step time: 1.0430\n",
      "175/295, train_loss: 0.0451, step time: 1.0395\n",
      "176/295, train_loss: 0.0446, step time: 1.0730\n",
      "177/295, train_loss: 0.0449, step time: 1.0590\n",
      "178/295, train_loss: 0.0755, step time: 1.0352\n",
      "179/295, train_loss: 0.0622, step time: 1.0561\n",
      "180/295, train_loss: 0.0513, step time: 1.0370\n",
      "181/295, train_loss: 0.0596, step time: 1.0413\n",
      "182/295, train_loss: 0.0667, step time: 1.0503\n",
      "183/295, train_loss: 0.2120, step time: 1.0800\n",
      "184/295, train_loss: 0.0479, step time: 1.0708\n",
      "185/295, train_loss: 0.0308, step time: 1.0590\n",
      "186/295, train_loss: 0.0755, step time: 1.1068\n",
      "187/295, train_loss: 0.0676, step time: 1.0503\n",
      "188/295, train_loss: 0.0465, step time: 1.0365\n",
      "189/295, train_loss: 0.0772, step time: 1.0369\n",
      "190/295, train_loss: 0.0606, step time: 1.0485\n",
      "191/295, train_loss: 0.0484, step time: 1.0626\n",
      "192/295, train_loss: 0.3747, step time: 1.0439\n",
      "193/295, train_loss: 0.0590, step time: 1.0406\n",
      "194/295, train_loss: 0.0315, step time: 1.0437\n",
      "195/295, train_loss: 0.0186, step time: 1.0956\n",
      "196/295, train_loss: 0.0489, step time: 1.0364\n",
      "197/295, train_loss: 0.0402, step time: 1.0373\n",
      "198/295, train_loss: 0.1200, step time: 1.0371\n",
      "199/295, train_loss: 0.0318, step time: 1.0547\n",
      "200/295, train_loss: 0.0371, step time: 1.0408\n",
      "201/295, train_loss: 0.0473, step time: 1.0658\n",
      "202/295, train_loss: 0.0953, step time: 1.0715\n",
      "203/295, train_loss: 0.0739, step time: 1.0829\n",
      "204/295, train_loss: 0.0789, step time: 1.0468\n",
      "205/295, train_loss: 0.0402, step time: 1.0331\n",
      "206/295, train_loss: 0.0238, step time: 1.0361\n",
      "207/295, train_loss: 0.0658, step time: 1.0416\n",
      "208/295, train_loss: 0.0804, step time: 1.0555\n",
      "209/295, train_loss: 0.0377, step time: 1.0518\n",
      "210/295, train_loss: 0.1095, step time: 1.0399\n",
      "211/295, train_loss: 0.0371, step time: 1.0609\n",
      "212/295, train_loss: 0.0565, step time: 1.0562\n",
      "213/295, train_loss: 0.0991, step time: 1.0683\n",
      "214/295, train_loss: 0.0418, step time: 1.0318\n",
      "215/295, train_loss: 0.0684, step time: 1.1073\n",
      "216/295, train_loss: 0.0560, step time: 1.0335\n",
      "217/295, train_loss: 0.0941, step time: 1.0326\n",
      "218/295, train_loss: 0.3599, step time: 1.0396\n",
      "219/295, train_loss: 0.0366, step time: 1.0446\n",
      "220/295, train_loss: 0.0659, step time: 1.0460\n",
      "221/295, train_loss: 0.0284, step time: 1.0367\n",
      "222/295, train_loss: 0.0761, step time: 1.0850\n",
      "223/295, train_loss: 0.0337, step time: 1.0469\n",
      "224/295, train_loss: 0.0615, step time: 1.0400\n",
      "225/295, train_loss: 0.3787, step time: 1.0392\n",
      "226/295, train_loss: 0.0390, step time: 1.1225\n",
      "227/295, train_loss: 0.0632, step time: 1.0384\n",
      "228/295, train_loss: 0.0406, step time: 1.0378\n",
      "229/295, train_loss: 0.1059, step time: 1.1166\n",
      "230/295, train_loss: 0.0607, step time: 1.1018\n",
      "231/295, train_loss: 0.0638, step time: 1.0393\n",
      "232/295, train_loss: 0.0522, step time: 1.0513\n",
      "233/295, train_loss: 0.0614, step time: 1.0467\n",
      "234/295, train_loss: 0.0861, step time: 1.0394\n",
      "235/295, train_loss: 0.0310, step time: 1.0454\n",
      "236/295, train_loss: 0.0649, step time: 1.0535\n",
      "237/295, train_loss: 0.0283, step time: 1.0368\n",
      "238/295, train_loss: 0.0909, step time: 1.0369\n",
      "239/295, train_loss: 0.0474, step time: 1.0326\n",
      "240/295, train_loss: 0.0376, step time: 1.0409\n",
      "241/295, train_loss: 0.0366, step time: 1.0356\n",
      "242/295, train_loss: 0.0759, step time: 1.0368\n",
      "243/295, train_loss: 0.0393, step time: 1.0377\n",
      "244/295, train_loss: 0.1011, step time: 1.0385\n",
      "245/295, train_loss: 0.0848, step time: 1.0575\n",
      "246/295, train_loss: 0.0334, step time: 1.0567\n",
      "247/295, train_loss: 0.0753, step time: 1.0419\n",
      "248/295, train_loss: 0.0786, step time: 1.0340\n",
      "249/295, train_loss: 0.1174, step time: 1.0377\n",
      "250/295, train_loss: 0.0620, step time: 1.0653\n",
      "251/295, train_loss: 0.0322, step time: 1.0469\n",
      "252/295, train_loss: 0.0890, step time: 1.0363\n",
      "253/295, train_loss: 0.0268, step time: 1.0720\n",
      "254/295, train_loss: 0.0326, step time: 1.1053\n",
      "255/295, train_loss: 0.0634, step time: 1.0605\n",
      "256/295, train_loss: 0.0506, step time: 1.0317\n",
      "257/295, train_loss: 0.0579, step time: 1.0607\n",
      "258/295, train_loss: 0.0443, step time: 1.0597\n",
      "259/295, train_loss: 0.0425, step time: 1.1045\n",
      "260/295, train_loss: 0.0694, step time: 1.0342\n",
      "261/295, train_loss: 0.0965, step time: 1.0516\n",
      "262/295, train_loss: 0.0489, step time: 1.0404\n",
      "263/295, train_loss: 0.0737, step time: 1.0554\n",
      "264/295, train_loss: 0.0522, step time: 1.0368\n",
      "265/295, train_loss: 0.0274, step time: 1.0436\n",
      "266/295, train_loss: 0.0344, step time: 1.0923\n",
      "267/295, train_loss: 0.0421, step time: 1.0759\n",
      "268/295, train_loss: 0.0725, step time: 1.0374\n",
      "269/295, train_loss: 0.0411, step time: 1.0358\n",
      "270/295, train_loss: 0.0805, step time: 1.0350\n",
      "271/295, train_loss: 0.0750, step time: 1.0632\n",
      "272/295, train_loss: 0.0440, step time: 1.0614\n",
      "273/295, train_loss: 0.0454, step time: 1.0363\n",
      "274/295, train_loss: 0.0744, step time: 1.0424\n",
      "275/295, train_loss: 0.0858, step time: 1.0342\n",
      "276/295, train_loss: 0.0274, step time: 1.0476\n",
      "277/295, train_loss: 0.0298, step time: 1.0358\n",
      "278/295, train_loss: 0.0796, step time: 1.0375\n",
      "279/295, train_loss: 0.0825, step time: 1.0498\n",
      "280/295, train_loss: 0.0417, step time: 1.0407\n",
      "281/295, train_loss: 0.0862, step time: 1.0448\n",
      "282/295, train_loss: 0.3568, step time: 1.0417\n",
      "283/295, train_loss: 0.0602, step time: 1.0902\n",
      "284/295, train_loss: 0.0389, step time: 1.0328\n",
      "285/295, train_loss: 0.0530, step time: 1.0397\n",
      "286/295, train_loss: 0.3668, step time: 1.0556\n",
      "287/295, train_loss: 0.3797, step time: 1.0368\n",
      "288/295, train_loss: 0.0505, step time: 1.0313\n",
      "289/295, train_loss: 0.0265, step time: 1.0289\n",
      "290/295, train_loss: 0.0236, step time: 1.0303\n",
      "291/295, train_loss: 0.3814, step time: 1.0299\n",
      "292/295, train_loss: 0.0574, step time: 1.0286\n",
      "293/295, train_loss: 0.0277, step time: 1.0305\n",
      "294/295, train_loss: 0.0454, step time: 1.0298\n",
      "295/295, train_loss: 0.0798, step time: 1.0298\n",
      "epoch 76 average loss: 0.0821\n",
      "current epoch: 76 current mean dice: 0.7700 tc: 0.7227 wt: 0.8388 et: 0.7523\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 76 is: 381.5403\n",
      "----------\n",
      "epoch 77/100\n",
      "1/295, train_loss: 0.0617, step time: 1.1018\n",
      "2/295, train_loss: 0.0415, step time: 1.0704\n",
      "3/295, train_loss: 0.0698, step time: 1.1009\n",
      "4/295, train_loss: 0.1135, step time: 1.1114\n",
      "5/295, train_loss: 0.0278, step time: 1.0534\n",
      "6/295, train_loss: 0.0368, step time: 1.0601\n",
      "7/295, train_loss: 0.0802, step time: 1.0641\n",
      "8/295, train_loss: 0.0481, step time: 1.0914\n",
      "9/295, train_loss: 0.0711, step time: 1.0317\n",
      "10/295, train_loss: 0.3618, step time: 1.0382\n",
      "11/295, train_loss: 0.0306, step time: 1.0448\n",
      "12/295, train_loss: 0.0428, step time: 1.0548\n",
      "13/295, train_loss: 0.2095, step time: 1.0345\n",
      "14/295, train_loss: 0.1157, step time: 1.0409\n",
      "15/295, train_loss: 0.0327, step time: 1.0676\n",
      "16/295, train_loss: 0.0901, step time: 1.0886\n",
      "17/295, train_loss: 0.0774, step time: 1.0351\n",
      "18/295, train_loss: 0.0606, step time: 1.0591\n",
      "19/295, train_loss: 0.0771, step time: 1.0530\n",
      "20/295, train_loss: 0.0268, step time: 1.0801\n",
      "21/295, train_loss: 0.0915, step time: 1.0438\n",
      "22/295, train_loss: 0.0285, step time: 1.0299\n",
      "23/295, train_loss: 0.0674, step time: 1.0301\n",
      "24/295, train_loss: 0.3805, step time: 1.0328\n",
      "25/295, train_loss: 0.3760, step time: 1.0367\n",
      "26/295, train_loss: 0.0944, step time: 1.0423\n",
      "27/295, train_loss: 0.0549, step time: 1.0398\n",
      "28/295, train_loss: 0.0840, step time: 1.0370\n",
      "29/295, train_loss: 0.0273, step time: 1.0439\n",
      "30/295, train_loss: 0.3607, step time: 1.0518\n",
      "31/295, train_loss: 0.0627, step time: 1.0917\n",
      "32/295, train_loss: 0.0621, step time: 1.0361\n",
      "33/295, train_loss: 0.0377, step time: 1.0596\n",
      "34/295, train_loss: 0.0659, step time: 1.0435\n",
      "35/295, train_loss: 0.0807, step time: 1.0496\n",
      "36/295, train_loss: 0.0522, step time: 1.0369\n",
      "37/295, train_loss: 0.0588, step time: 1.0369\n",
      "38/295, train_loss: 0.0839, step time: 1.0364\n",
      "39/295, train_loss: 0.0774, step time: 1.0382\n",
      "40/295, train_loss: 0.0387, step time: 1.0658\n",
      "41/295, train_loss: 0.0322, step time: 1.0349\n",
      "42/295, train_loss: 0.0319, step time: 1.0970\n",
      "43/295, train_loss: 0.0510, step time: 1.0842\n",
      "44/295, train_loss: 0.0246, step time: 1.0454\n",
      "45/295, train_loss: 0.0583, step time: 1.0794\n",
      "46/295, train_loss: 0.0452, step time: 1.0371\n",
      "47/295, train_loss: 0.0612, step time: 1.0999\n",
      "48/295, train_loss: 0.0250, step time: 1.1214\n",
      "49/295, train_loss: 0.0579, step time: 1.0461\n",
      "50/295, train_loss: 0.3745, step time: 1.0420\n",
      "51/295, train_loss: 0.0483, step time: 1.0459\n",
      "52/295, train_loss: 0.0721, step time: 1.0970\n",
      "53/295, train_loss: 0.0519, step time: 1.0311\n",
      "54/295, train_loss: 0.0397, step time: 1.0448\n",
      "55/295, train_loss: 0.0396, step time: 1.0450\n",
      "56/295, train_loss: 0.0405, step time: 1.0515\n",
      "57/295, train_loss: 0.0967, step time: 1.0391\n",
      "58/295, train_loss: 0.0777, step time: 1.0406\n",
      "59/295, train_loss: 0.0405, step time: 1.0455\n",
      "60/295, train_loss: 0.0376, step time: 1.0508\n",
      "61/295, train_loss: 0.3967, step time: 1.0630\n",
      "62/295, train_loss: 0.0365, step time: 1.0442\n",
      "63/295, train_loss: 0.0405, step time: 1.0961\n",
      "64/295, train_loss: 0.0752, step time: 1.0354\n",
      "65/295, train_loss: 0.0649, step time: 1.0915\n",
      "66/295, train_loss: 0.0256, step time: 1.0344\n",
      "67/295, train_loss: 0.0618, step time: 1.0575\n",
      "68/295, train_loss: 0.0529, step time: 1.0530\n",
      "69/295, train_loss: 0.0494, step time: 1.0442\n",
      "70/295, train_loss: 0.0824, step time: 1.0868\n",
      "71/295, train_loss: 0.1215, step time: 1.0510\n",
      "72/295, train_loss: 0.0353, step time: 1.0805\n",
      "73/295, train_loss: 0.0590, step time: 1.0368\n",
      "74/295, train_loss: 0.0805, step time: 1.0399\n",
      "75/295, train_loss: 0.4331, step time: 1.1103\n",
      "76/295, train_loss: 0.0339, step time: 1.0613\n",
      "77/295, train_loss: 0.0367, step time: 1.0440\n",
      "78/295, train_loss: 0.3867, step time: 1.0569\n",
      "79/295, train_loss: 0.0492, step time: 1.0497\n",
      "80/295, train_loss: 0.0464, step time: 1.0584\n",
      "81/295, train_loss: 0.0697, step time: 1.0394\n",
      "82/295, train_loss: 0.0350, step time: 1.1253\n",
      "83/295, train_loss: 0.0285, step time: 1.0577\n",
      "84/295, train_loss: 0.0769, step time: 1.0363\n",
      "85/295, train_loss: 0.0497, step time: 1.0399\n",
      "86/295, train_loss: 0.3770, step time: 1.0409\n",
      "87/295, train_loss: 0.0566, step time: 1.0468\n",
      "88/295, train_loss: 0.0544, step time: 1.0714\n",
      "89/295, train_loss: 0.0478, step time: 1.0635\n",
      "90/295, train_loss: 0.0258, step time: 1.0586\n",
      "91/295, train_loss: 0.0517, step time: 1.0517\n",
      "92/295, train_loss: 0.1040, step time: 1.0380\n",
      "93/295, train_loss: 0.0327, step time: 1.0547\n",
      "94/295, train_loss: 0.1109, step time: 1.0377\n",
      "95/295, train_loss: 0.0676, step time: 1.0348\n",
      "96/295, train_loss: 0.0944, step time: 1.0680\n",
      "97/295, train_loss: 0.0528, step time: 1.0618\n",
      "98/295, train_loss: 0.0808, step time: 1.0352\n",
      "99/295, train_loss: 0.3569, step time: 1.0462\n",
      "100/295, train_loss: 0.0458, step time: 1.0342\n",
      "101/295, train_loss: 0.0471, step time: 1.0430\n",
      "102/295, train_loss: 0.0599, step time: 1.0587\n",
      "103/295, train_loss: 0.0626, step time: 1.0493\n",
      "104/295, train_loss: 0.0516, step time: 1.0425\n",
      "105/295, train_loss: 0.0464, step time: 1.0477\n",
      "106/295, train_loss: 0.3774, step time: 1.0390\n",
      "107/295, train_loss: 0.0440, step time: 1.0371\n",
      "108/295, train_loss: 0.0596, step time: 1.0598\n",
      "109/295, train_loss: 0.1102, step time: 1.0429\n",
      "110/295, train_loss: 0.0529, step time: 1.0465\n",
      "111/295, train_loss: 0.0310, step time: 1.0849\n",
      "112/295, train_loss: 0.0361, step time: 1.1042\n",
      "113/295, train_loss: 0.0520, step time: 1.0356\n",
      "114/295, train_loss: 0.0244, step time: 1.0675\n",
      "115/295, train_loss: 0.0866, step time: 1.0481\n",
      "116/295, train_loss: 0.0711, step time: 1.0382\n",
      "117/295, train_loss: 0.0479, step time: 1.0392\n",
      "118/295, train_loss: 0.0518, step time: 1.0593\n",
      "119/295, train_loss: 0.0366, step time: 1.0746\n",
      "120/295, train_loss: 0.0758, step time: 1.0390\n",
      "121/295, train_loss: 0.0281, step time: 1.0898\n",
      "122/295, train_loss: 0.0640, step time: 1.0423\n",
      "123/295, train_loss: 0.1004, step time: 1.0313\n",
      "124/295, train_loss: 0.0769, step time: 1.0481\n",
      "125/295, train_loss: 0.0701, step time: 1.0327\n",
      "126/295, train_loss: 0.0263, step time: 1.0585\n",
      "127/295, train_loss: 0.0399, step time: 1.0500\n",
      "128/295, train_loss: 0.0618, step time: 1.0665\n",
      "129/295, train_loss: 0.0661, step time: 1.0373\n",
      "130/295, train_loss: 0.0462, step time: 1.0594\n",
      "131/295, train_loss: 0.0508, step time: 1.0747\n",
      "132/295, train_loss: 0.1144, step time: 1.0387\n",
      "133/295, train_loss: 0.0613, step time: 1.0351\n",
      "134/295, train_loss: 0.0554, step time: 1.0429\n",
      "135/295, train_loss: 0.0295, step time: 1.0632\n",
      "136/295, train_loss: 0.3624, step time: 1.0384\n",
      "137/295, train_loss: 0.1019, step time: 1.0769\n",
      "138/295, train_loss: 0.0419, step time: 1.0699\n",
      "139/295, train_loss: 0.0619, step time: 1.0545\n",
      "140/295, train_loss: 0.0410, step time: 1.0352\n",
      "141/295, train_loss: 0.0239, step time: 1.0402\n",
      "142/295, train_loss: 0.0774, step time: 1.0317\n",
      "143/295, train_loss: 0.0724, step time: 1.0414\n",
      "144/295, train_loss: 0.3597, step time: 1.0437\n",
      "145/295, train_loss: 0.0322, step time: 1.1093\n",
      "146/295, train_loss: 0.0328, step time: 1.0709\n",
      "147/295, train_loss: 0.0320, step time: 1.0360\n",
      "148/295, train_loss: 0.0312, step time: 1.0583\n",
      "149/295, train_loss: 0.0792, step time: 1.0999\n",
      "150/295, train_loss: 0.0424, step time: 1.0456\n",
      "151/295, train_loss: 0.0938, step time: 1.0697\n",
      "152/295, train_loss: 0.0659, step time: 1.0407\n",
      "153/295, train_loss: 0.0217, step time: 1.0382\n",
      "154/295, train_loss: 0.0287, step time: 1.0765\n",
      "155/295, train_loss: 0.3789, step time: 1.0566\n",
      "156/295, train_loss: 0.0500, step time: 1.0390\n",
      "157/295, train_loss: 0.0302, step time: 1.0516\n",
      "158/295, train_loss: 0.0493, step time: 1.0521\n",
      "159/295, train_loss: 0.0182, step time: 1.0618\n",
      "160/295, train_loss: 0.0388, step time: 1.0347\n",
      "161/295, train_loss: 0.0760, step time: 1.0602\n",
      "162/295, train_loss: 0.0326, step time: 1.0568\n",
      "163/295, train_loss: 0.0419, step time: 1.0334\n",
      "164/295, train_loss: 0.0590, step time: 1.0413\n",
      "165/295, train_loss: 0.0359, step time: 1.0625\n",
      "166/295, train_loss: 0.0387, step time: 1.0333\n",
      "167/295, train_loss: 0.0612, step time: 1.0500\n",
      "168/295, train_loss: 0.3808, step time: 1.0355\n",
      "169/295, train_loss: 0.0285, step time: 1.0422\n",
      "170/295, train_loss: 0.3644, step time: 1.0403\n",
      "171/295, train_loss: 0.0322, step time: 1.0895\n",
      "172/295, train_loss: 0.0802, step time: 1.0467\n",
      "173/295, train_loss: 0.3800, step time: 1.1095\n",
      "174/295, train_loss: 0.0326, step time: 1.0715\n",
      "175/295, train_loss: 0.0711, step time: 1.0676\n",
      "176/295, train_loss: 0.0948, step time: 1.0347\n",
      "177/295, train_loss: 0.0378, step time: 1.0382\n",
      "178/295, train_loss: 0.0871, step time: 1.0619\n",
      "179/295, train_loss: 0.1348, step time: 1.0799\n",
      "180/295, train_loss: 0.0878, step time: 1.0486\n",
      "181/295, train_loss: 0.0360, step time: 1.0335\n",
      "182/295, train_loss: 0.0622, step time: 1.0374\n",
      "183/295, train_loss: 0.0695, step time: 1.0415\n",
      "184/295, train_loss: 0.0537, step time: 1.1105\n",
      "185/295, train_loss: 0.1315, step time: 1.0622\n",
      "186/295, train_loss: 0.0537, step time: 1.0330\n",
      "187/295, train_loss: 0.1330, step time: 1.0402\n",
      "188/295, train_loss: 0.0445, step time: 1.0348\n",
      "189/295, train_loss: 0.0862, step time: 1.0555\n",
      "190/295, train_loss: 0.0499, step time: 1.0371\n",
      "191/295, train_loss: 0.0531, step time: 1.0398\n",
      "192/295, train_loss: 0.1091, step time: 1.0412\n",
      "193/295, train_loss: 0.0502, step time: 1.0514\n",
      "194/295, train_loss: 0.0461, step time: 1.0399\n",
      "195/295, train_loss: 0.0789, step time: 1.0391\n",
      "196/295, train_loss: 0.0435, step time: 1.0690\n",
      "197/295, train_loss: 0.0245, step time: 1.0671\n",
      "198/295, train_loss: 0.0704, step time: 1.0648\n",
      "199/295, train_loss: 0.0660, step time: 1.0351\n",
      "200/295, train_loss: 0.0262, step time: 1.0623\n",
      "201/295, train_loss: 0.0808, step time: 1.0423\n",
      "202/295, train_loss: 0.0277, step time: 1.0540\n",
      "203/295, train_loss: 0.0804, step time: 1.0418\n",
      "204/295, train_loss: 0.0509, step time: 1.0646\n",
      "205/295, train_loss: 0.0577, step time: 1.0437\n",
      "206/295, train_loss: 0.0598, step time: 1.0551\n",
      "207/295, train_loss: 0.0362, step time: 1.0378\n",
      "208/295, train_loss: 0.0596, step time: 1.0339\n",
      "209/295, train_loss: 0.0872, step time: 1.0605\n",
      "210/295, train_loss: 0.0254, step time: 1.0579\n",
      "211/295, train_loss: 0.0504, step time: 1.0376\n",
      "212/295, train_loss: 0.0418, step time: 1.0479\n",
      "213/295, train_loss: 0.0752, step time: 1.0617\n",
      "214/295, train_loss: 0.0445, step time: 1.0674\n",
      "215/295, train_loss: 0.0838, step time: 1.0359\n",
      "216/295, train_loss: 0.0956, step time: 1.0429\n",
      "217/295, train_loss: 0.0791, step time: 1.0604\n",
      "218/295, train_loss: 0.3717, step time: 1.0509\n",
      "219/295, train_loss: 0.0515, step time: 1.0335\n",
      "220/295, train_loss: 0.0922, step time: 1.0516\n",
      "221/295, train_loss: 0.0399, step time: 1.1286\n",
      "222/295, train_loss: 0.0776, step time: 1.0403\n",
      "223/295, train_loss: 0.0747, step time: 1.0388\n",
      "224/295, train_loss: 0.0661, step time: 1.0385\n",
      "225/295, train_loss: 0.0474, step time: 1.0613\n",
      "226/295, train_loss: 0.0737, step time: 1.1628\n",
      "227/295, train_loss: 0.0435, step time: 1.0422\n",
      "228/295, train_loss: 0.0595, step time: 1.0357\n",
      "229/295, train_loss: 0.0349, step time: 1.0413\n",
      "230/295, train_loss: 0.0307, step time: 1.0420\n",
      "231/295, train_loss: 0.0367, step time: 1.0459\n",
      "232/295, train_loss: 0.0556, step time: 1.0458\n",
      "233/295, train_loss: 0.0802, step time: 1.0645\n",
      "234/295, train_loss: 0.0487, step time: 1.0618\n",
      "235/295, train_loss: 0.0792, step time: 1.0444\n",
      "236/295, train_loss: 0.0804, step time: 1.0337\n",
      "237/295, train_loss: 0.0424, step time: 1.0338\n",
      "238/295, train_loss: 0.0497, step time: 1.0466\n",
      "239/295, train_loss: 0.0637, step time: 1.0619\n",
      "240/295, train_loss: 0.0532, step time: 1.0444\n",
      "241/295, train_loss: 0.0427, step time: 1.0378\n",
      "242/295, train_loss: 0.0507, step time: 1.0326\n",
      "243/295, train_loss: 0.0271, step time: 1.0320\n",
      "244/295, train_loss: 0.0422, step time: 1.0544\n",
      "245/295, train_loss: 0.0267, step time: 1.0589\n",
      "246/295, train_loss: 0.1076, step time: 1.0320\n",
      "247/295, train_loss: 0.0752, step time: 1.0357\n",
      "248/295, train_loss: 0.0588, step time: 1.0376\n",
      "249/295, train_loss: 0.3790, step time: 1.0799\n",
      "250/295, train_loss: 0.0514, step time: 1.0389\n",
      "251/295, train_loss: 0.0354, step time: 1.1082\n",
      "252/295, train_loss: 0.0281, step time: 1.0429\n",
      "253/295, train_loss: 0.0958, step time: 1.0757\n",
      "254/295, train_loss: 0.0655, step time: 1.0414\n",
      "255/295, train_loss: 0.0622, step time: 1.0476\n",
      "256/295, train_loss: 0.1004, step time: 1.0493\n",
      "257/295, train_loss: 0.0242, step time: 1.0417\n",
      "258/295, train_loss: 0.0549, step time: 1.0346\n",
      "259/295, train_loss: 0.0445, step time: 1.0478\n",
      "260/295, train_loss: 0.0318, step time: 1.0442\n",
      "261/295, train_loss: 0.3667, step time: 1.0675\n",
      "262/295, train_loss: 0.0301, step time: 1.0430\n",
      "263/295, train_loss: 0.0560, step time: 1.0421\n",
      "264/295, train_loss: 0.0312, step time: 1.0350\n",
      "265/295, train_loss: 0.0448, step time: 1.0360\n",
      "266/295, train_loss: 0.0675, step time: 1.0418\n",
      "267/295, train_loss: 0.0766, step time: 1.0379\n",
      "268/295, train_loss: 0.0692, step time: 1.0398\n",
      "269/295, train_loss: 0.0333, step time: 1.0372\n",
      "270/295, train_loss: 0.0675, step time: 1.0465\n",
      "271/295, train_loss: 0.0344, step time: 1.0941\n",
      "272/295, train_loss: 0.0587, step time: 1.0337\n",
      "273/295, train_loss: 0.0494, step time: 1.0436\n",
      "274/295, train_loss: 0.3901, step time: 1.0753\n",
      "275/295, train_loss: 0.0857, step time: 1.0553\n",
      "276/295, train_loss: 0.3701, step time: 1.0395\n",
      "277/295, train_loss: 0.0738, step time: 1.0380\n",
      "278/295, train_loss: 0.0765, step time: 1.0671\n",
      "279/295, train_loss: 0.0411, step time: 1.0405\n",
      "280/295, train_loss: 0.0876, step time: 1.0370\n",
      "281/295, train_loss: 0.0744, step time: 1.0457\n",
      "282/295, train_loss: 0.0383, step time: 1.0468\n",
      "283/295, train_loss: 0.0467, step time: 1.0475\n",
      "284/295, train_loss: 0.0655, step time: 1.0445\n",
      "285/295, train_loss: 0.0402, step time: 1.0351\n",
      "286/295, train_loss: 0.0581, step time: 1.0563\n",
      "287/295, train_loss: 0.0269, step time: 1.0393\n",
      "288/295, train_loss: 0.0653, step time: 1.0295\n",
      "289/295, train_loss: 0.0287, step time: 1.0288\n",
      "290/295, train_loss: 0.0985, step time: 1.0295\n",
      "291/295, train_loss: 0.0511, step time: 1.0281\n",
      "292/295, train_loss: 0.0396, step time: 1.0292\n",
      "293/295, train_loss: 0.0468, step time: 1.0287\n",
      "294/295, train_loss: 0.0446, step time: 1.0316\n",
      "295/295, train_loss: 0.0729, step time: 1.0281\n",
      "epoch 77 average loss: 0.0816\n",
      "current epoch: 77 current mean dice: 0.7785 tc: 0.7295 wt: 0.8456 et: 0.7652\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 77 is: 379.8455\n",
      "----------\n",
      "epoch 78/100\n",
      "1/295, train_loss: 0.0658, step time: 1.0826\n",
      "2/295, train_loss: 0.0938, step time: 1.0603\n",
      "3/295, train_loss: 0.0561, step time: 1.0443\n",
      "4/295, train_loss: 0.0642, step time: 1.0531\n",
      "5/295, train_loss: 0.0743, step time: 1.0620\n",
      "6/295, train_loss: 0.0424, step time: 1.0461\n",
      "7/295, train_loss: 0.0589, step time: 1.0527\n",
      "8/295, train_loss: 0.0323, step time: 1.0501\n",
      "9/295, train_loss: 0.3613, step time: 1.0435\n",
      "10/295, train_loss: 0.0617, step time: 1.0426\n",
      "11/295, train_loss: 0.0584, step time: 1.0381\n",
      "12/295, train_loss: 0.0443, step time: 1.0680\n",
      "13/295, train_loss: 0.0857, step time: 1.0435\n",
      "14/295, train_loss: 0.0239, step time: 1.0317\n",
      "15/295, train_loss: 0.1115, step time: 1.0357\n",
      "16/295, train_loss: 0.0709, step time: 1.0656\n",
      "17/295, train_loss: 0.0934, step time: 1.0525\n",
      "18/295, train_loss: 0.0649, step time: 1.0390\n",
      "19/295, train_loss: 0.0516, step time: 1.0604\n",
      "20/295, train_loss: 0.0480, step time: 1.0351\n",
      "21/295, train_loss: 0.0283, step time: 1.0406\n",
      "22/295, train_loss: 0.0309, step time: 1.1686\n",
      "23/295, train_loss: 0.0752, step time: 1.0904\n",
      "24/295, train_loss: 0.0264, step time: 1.0453\n",
      "25/295, train_loss: 0.0346, step time: 1.0563\n",
      "26/295, train_loss: 0.0523, step time: 1.0424\n",
      "27/295, train_loss: 0.0571, step time: 1.0348\n",
      "28/295, train_loss: 0.0469, step time: 1.0355\n",
      "29/295, train_loss: 0.0712, step time: 1.0614\n",
      "30/295, train_loss: 0.3619, step time: 1.0830\n",
      "31/295, train_loss: 0.0618, step time: 1.0464\n",
      "32/295, train_loss: 0.0254, step time: 1.0670\n",
      "33/295, train_loss: 0.0844, step time: 1.0381\n",
      "34/295, train_loss: 0.0790, step time: 1.0587\n",
      "35/295, train_loss: 0.0606, step time: 1.0322\n",
      "36/295, train_loss: 0.0317, step time: 1.0421\n",
      "37/295, train_loss: 0.1131, step time: 1.0419\n",
      "38/295, train_loss: 0.0299, step time: 1.0834\n",
      "39/295, train_loss: 0.0310, step time: 1.0366\n",
      "40/295, train_loss: 0.0362, step time: 1.0719\n",
      "41/295, train_loss: 0.0762, step time: 1.0417\n",
      "42/295, train_loss: 0.0982, step time: 1.0639\n",
      "43/295, train_loss: 0.0615, step time: 1.0406\n",
      "44/295, train_loss: 0.0431, step time: 1.0421\n",
      "45/295, train_loss: 0.0350, step time: 1.0656\n",
      "46/295, train_loss: 0.0915, step time: 1.0435\n",
      "47/295, train_loss: 0.0725, step time: 1.0647\n",
      "48/295, train_loss: 0.3793, step time: 1.0340\n",
      "49/295, train_loss: 0.0623, step time: 1.0303\n",
      "50/295, train_loss: 0.3917, step time: 1.0313\n",
      "51/295, train_loss: 0.0367, step time: 1.0354\n",
      "52/295, train_loss: 0.0461, step time: 1.0464\n",
      "53/295, train_loss: 0.0732, step time: 1.0567\n",
      "54/295, train_loss: 0.0452, step time: 1.0525\n",
      "55/295, train_loss: 0.0834, step time: 1.0380\n",
      "56/295, train_loss: 0.0468, step time: 1.0402\n",
      "57/295, train_loss: 0.0656, step time: 1.0890\n",
      "58/295, train_loss: 0.0412, step time: 1.0461\n",
      "59/295, train_loss: 0.0733, step time: 1.0461\n",
      "60/295, train_loss: 0.0721, step time: 1.0672\n",
      "61/295, train_loss: 0.0267, step time: 1.0540\n",
      "62/295, train_loss: 0.0488, step time: 1.0507\n",
      "63/295, train_loss: 0.0963, step time: 1.0400\n",
      "64/295, train_loss: 0.0409, step time: 1.0371\n",
      "65/295, train_loss: 0.3604, step time: 1.0723\n",
      "66/295, train_loss: 0.0881, step time: 1.0382\n",
      "67/295, train_loss: 0.0592, step time: 1.0491\n",
      "68/295, train_loss: 0.0314, step time: 1.0412\n",
      "69/295, train_loss: 0.0395, step time: 1.0368\n",
      "70/295, train_loss: 0.0405, step time: 1.0427\n",
      "71/295, train_loss: 0.0670, step time: 1.0487\n",
      "72/295, train_loss: 0.0236, step time: 1.0604\n",
      "73/295, train_loss: 0.0712, step time: 1.0385\n",
      "74/295, train_loss: 0.0328, step time: 1.0431\n",
      "75/295, train_loss: 0.0327, step time: 1.1163\n",
      "76/295, train_loss: 0.0811, step time: 1.0414\n",
      "77/295, train_loss: 0.0359, step time: 1.0460\n",
      "78/295, train_loss: 0.0520, step time: 1.0467\n",
      "79/295, train_loss: 0.0506, step time: 1.0594\n",
      "80/295, train_loss: 0.0810, step time: 1.0624\n",
      "81/295, train_loss: 0.0582, step time: 1.0344\n",
      "82/295, train_loss: 0.0280, step time: 1.0588\n",
      "83/295, train_loss: 0.0243, step time: 1.0913\n",
      "84/295, train_loss: 0.0580, step time: 1.0512\n",
      "85/295, train_loss: 0.0643, step time: 1.0405\n",
      "86/295, train_loss: 0.0302, step time: 1.0430\n",
      "87/295, train_loss: 0.1183, step time: 1.0413\n",
      "88/295, train_loss: 0.0487, step time: 1.0491\n",
      "89/295, train_loss: 0.0362, step time: 1.0576\n",
      "90/295, train_loss: 0.0590, step time: 1.0857\n",
      "91/295, train_loss: 0.0943, step time: 1.0664\n",
      "92/295, train_loss: 0.0656, step time: 1.0508\n",
      "93/295, train_loss: 0.1045, step time: 1.0561\n",
      "94/295, train_loss: 0.0224, step time: 1.0392\n",
      "95/295, train_loss: 0.0585, step time: 1.0655\n",
      "96/295, train_loss: 0.0605, step time: 1.0332\n",
      "97/295, train_loss: 0.0440, step time: 1.0701\n",
      "98/295, train_loss: 0.1040, step time: 1.0411\n",
      "99/295, train_loss: 0.0432, step time: 1.0430\n",
      "100/295, train_loss: 0.0487, step time: 1.0323\n",
      "101/295, train_loss: 0.0590, step time: 1.0604\n",
      "102/295, train_loss: 0.3602, step time: 1.0552\n",
      "103/295, train_loss: 0.0604, step time: 1.0598\n",
      "104/295, train_loss: 0.0479, step time: 1.0587\n",
      "105/295, train_loss: 0.0595, step time: 1.0587\n",
      "106/295, train_loss: 0.0982, step time: 1.0404\n",
      "107/295, train_loss: 0.1117, step time: 1.0315\n",
      "108/295, train_loss: 0.0775, step time: 1.0475\n",
      "109/295, train_loss: 0.0561, step time: 1.0404\n",
      "110/295, train_loss: 0.0345, step time: 1.0393\n",
      "111/295, train_loss: 0.0851, step time: 1.0676\n",
      "112/295, train_loss: 0.0419, step time: 1.0392\n",
      "113/295, train_loss: 0.0753, step time: 1.0372\n",
      "114/295, train_loss: 0.0718, step time: 1.0504\n",
      "115/295, train_loss: 0.0674, step time: 1.0855\n",
      "116/295, train_loss: 0.0273, step time: 1.0362\n",
      "117/295, train_loss: 0.0983, step time: 1.0394\n",
      "118/295, train_loss: 0.0463, step time: 1.0680\n",
      "119/295, train_loss: 0.0503, step time: 1.0386\n",
      "120/295, train_loss: 0.0257, step time: 1.0689\n",
      "121/295, train_loss: 0.0442, step time: 1.0445\n",
      "122/295, train_loss: 0.0556, step time: 1.0927\n",
      "123/295, train_loss: 0.0572, step time: 1.0395\n",
      "124/295, train_loss: 0.0491, step time: 1.0385\n",
      "125/295, train_loss: 0.0613, step time: 1.0378\n",
      "126/295, train_loss: 0.0291, step time: 1.0599\n",
      "127/295, train_loss: 0.0493, step time: 1.0349\n",
      "128/295, train_loss: 0.3817, step time: 1.0611\n",
      "129/295, train_loss: 0.1208, step time: 1.1278\n",
      "130/295, train_loss: 0.0923, step time: 1.0416\n",
      "131/295, train_loss: 0.3969, step time: 1.0329\n",
      "132/295, train_loss: 0.0687, step time: 1.0315\n",
      "133/295, train_loss: 0.0358, step time: 1.1086\n",
      "134/295, train_loss: 0.0681, step time: 1.0362\n",
      "135/295, train_loss: 0.3861, step time: 1.0341\n",
      "136/295, train_loss: 0.0247, step time: 1.0369\n",
      "137/295, train_loss: 0.3706, step time: 1.0415\n",
      "138/295, train_loss: 0.0616, step time: 1.0386\n",
      "139/295, train_loss: 0.0485, step time: 1.0371\n",
      "140/295, train_loss: 0.0812, step time: 1.1077\n",
      "141/295, train_loss: 0.0449, step time: 1.0395\n",
      "142/295, train_loss: 0.0768, step time: 1.0394\n",
      "143/295, train_loss: 0.0634, step time: 1.0574\n",
      "144/295, train_loss: 0.0253, step time: 1.0372\n",
      "145/295, train_loss: 0.0864, step time: 1.0580\n",
      "146/295, train_loss: 0.0683, step time: 1.1156\n",
      "147/295, train_loss: 0.0340, step time: 1.0446\n",
      "148/295, train_loss: 0.0267, step time: 1.0511\n",
      "149/295, train_loss: 0.0559, step time: 1.0643\n",
      "150/295, train_loss: 0.0305, step time: 1.0407\n",
      "151/295, train_loss: 0.0570, step time: 1.0410\n",
      "152/295, train_loss: 0.0598, step time: 1.0381\n",
      "153/295, train_loss: 0.0570, step time: 1.0378\n",
      "154/295, train_loss: 0.0802, step time: 1.0353\n",
      "155/295, train_loss: 0.0277, step time: 1.0374\n",
      "156/295, train_loss: 0.0359, step time: 1.0839\n",
      "157/295, train_loss: 0.0249, step time: 1.0386\n",
      "158/295, train_loss: 0.0888, step time: 1.0539\n",
      "159/295, train_loss: 0.0273, step time: 1.0418\n",
      "160/295, train_loss: 0.0640, step time: 1.0499\n",
      "161/295, train_loss: 0.0793, step time: 1.1053\n",
      "162/295, train_loss: 0.0996, step time: 1.0400\n",
      "163/295, train_loss: 0.1335, step time: 1.0353\n",
      "164/295, train_loss: 0.0589, step time: 1.0434\n",
      "165/295, train_loss: 0.0659, step time: 1.0328\n",
      "166/295, train_loss: 0.0268, step time: 1.0367\n",
      "167/295, train_loss: 0.0517, step time: 1.0309\n",
      "168/295, train_loss: 0.0367, step time: 1.0593\n",
      "169/295, train_loss: 0.0507, step time: 1.0538\n",
      "170/295, train_loss: 0.0321, step time: 1.0373\n",
      "171/295, train_loss: 0.0389, step time: 1.0353\n",
      "172/295, train_loss: 0.0313, step time: 1.0554\n",
      "173/295, train_loss: 0.0747, step time: 1.0355\n",
      "174/295, train_loss: 0.0374, step time: 1.1632\n",
      "175/295, train_loss: 0.0868, step time: 1.0560\n",
      "176/295, train_loss: 0.0439, step time: 1.0461\n",
      "177/295, train_loss: 0.0561, step time: 1.0409\n",
      "178/295, train_loss: 0.0380, step time: 1.0352\n",
      "179/295, train_loss: 0.0321, step time: 1.0396\n",
      "180/295, train_loss: 0.0281, step time: 1.0346\n",
      "181/295, train_loss: 0.0644, step time: 1.0434\n",
      "182/295, train_loss: 0.1022, step time: 1.0442\n",
      "183/295, train_loss: 0.0557, step time: 1.0490\n",
      "184/295, train_loss: 0.0284, step time: 1.0447\n",
      "185/295, train_loss: 0.3778, step time: 1.0578\n",
      "186/295, train_loss: 0.0809, step time: 1.0582\n",
      "187/295, train_loss: 0.0301, step time: 1.0542\n",
      "188/295, train_loss: 0.0551, step time: 1.0443\n",
      "189/295, train_loss: 0.0406, step time: 1.0750\n",
      "190/295, train_loss: 0.0409, step time: 1.0345\n",
      "191/295, train_loss: 0.0463, step time: 1.0377\n",
      "192/295, train_loss: 0.0637, step time: 1.0391\n",
      "193/295, train_loss: 0.0264, step time: 1.0775\n",
      "194/295, train_loss: 0.0448, step time: 1.0392\n",
      "195/295, train_loss: 0.0360, step time: 1.0330\n",
      "196/295, train_loss: 0.0687, step time: 1.0384\n",
      "197/295, train_loss: 0.3698, step time: 1.0379\n",
      "198/295, train_loss: 0.0494, step time: 1.0442\n",
      "199/295, train_loss: 0.0492, step time: 1.0661\n",
      "200/295, train_loss: 0.0652, step time: 1.0328\n",
      "201/295, train_loss: 0.0722, step time: 1.0448\n",
      "202/295, train_loss: 0.3570, step time: 1.0359\n",
      "203/295, train_loss: 0.0394, step time: 1.0392\n",
      "204/295, train_loss: 0.0286, step time: 1.0497\n",
      "205/295, train_loss: 0.0400, step time: 1.0370\n",
      "206/295, train_loss: 0.0702, step time: 1.0370\n",
      "207/295, train_loss: 0.0631, step time: 1.0406\n",
      "208/295, train_loss: 0.0702, step time: 1.0548\n",
      "209/295, train_loss: 0.0791, step time: 1.0462\n",
      "210/295, train_loss: 0.0792, step time: 1.0361\n",
      "211/295, train_loss: 0.0336, step time: 1.1013\n",
      "212/295, train_loss: 0.0755, step time: 1.0341\n",
      "213/295, train_loss: 0.0414, step time: 1.0311\n",
      "214/295, train_loss: 0.1332, step time: 1.0671\n",
      "215/295, train_loss: 0.0405, step time: 1.0687\n",
      "216/295, train_loss: 0.1503, step time: 1.0465\n",
      "217/295, train_loss: 0.0719, step time: 1.0489\n",
      "218/295, train_loss: 0.0441, step time: 1.0321\n",
      "219/295, train_loss: 0.0794, step time: 1.0379\n",
      "220/295, train_loss: 0.0488, step time: 1.0792\n",
      "221/295, train_loss: 0.0467, step time: 1.0567\n",
      "222/295, train_loss: 0.0318, step time: 1.0395\n",
      "223/295, train_loss: 0.0241, step time: 1.0382\n",
      "224/295, train_loss: 0.0394, step time: 1.0701\n",
      "225/295, train_loss: 0.3768, step time: 1.0351\n",
      "226/295, train_loss: 0.1096, step time: 1.0398\n",
      "227/295, train_loss: 0.0894, step time: 1.0378\n",
      "228/295, train_loss: 0.0777, step time: 1.0539\n",
      "229/295, train_loss: 0.0771, step time: 1.0313\n",
      "230/295, train_loss: 0.3785, step time: 1.0448\n",
      "231/295, train_loss: 0.0383, step time: 1.0509\n",
      "232/295, train_loss: 0.0428, step time: 1.0942\n",
      "233/295, train_loss: 0.2099, step time: 1.0503\n",
      "234/295, train_loss: 0.0393, step time: 1.0456\n",
      "235/295, train_loss: 0.0374, step time: 1.0480\n",
      "236/295, train_loss: 0.0528, step time: 1.0442\n",
      "237/295, train_loss: 0.0477, step time: 1.0435\n",
      "238/295, train_loss: 0.0757, step time: 1.0433\n",
      "239/295, train_loss: 0.0386, step time: 1.0369\n",
      "240/295, train_loss: 0.0427, step time: 1.0355\n",
      "241/295, train_loss: 0.0483, step time: 1.1705\n",
      "242/295, train_loss: 0.0552, step time: 1.0546\n",
      "243/295, train_loss: 0.3782, step time: 1.0392\n",
      "244/295, train_loss: 0.0538, step time: 1.0731\n",
      "245/295, train_loss: 0.0532, step time: 1.0386\n",
      "246/295, train_loss: 0.0750, step time: 1.0377\n",
      "247/295, train_loss: 0.0315, step time: 1.0352\n",
      "248/295, train_loss: 0.0510, step time: 1.0358\n",
      "249/295, train_loss: 0.0764, step time: 1.0366\n",
      "250/295, train_loss: 0.3667, step time: 1.0387\n",
      "251/295, train_loss: 0.0285, step time: 1.0618\n",
      "252/295, train_loss: 0.0598, step time: 1.0502\n",
      "253/295, train_loss: 0.3655, step time: 1.0419\n",
      "254/295, train_loss: 0.0724, step time: 1.0383\n",
      "255/295, train_loss: 0.0818, step time: 1.0374\n",
      "256/295, train_loss: 0.1003, step time: 1.0377\n",
      "257/295, train_loss: 0.0349, step time: 1.0367\n",
      "258/295, train_loss: 0.0792, step time: 1.0368\n",
      "259/295, train_loss: 0.0809, step time: 1.0382\n",
      "260/295, train_loss: 0.0768, step time: 1.0530\n",
      "261/295, train_loss: 0.3805, step time: 1.0396\n",
      "262/295, train_loss: 0.0470, step time: 1.0498\n",
      "263/295, train_loss: 0.0516, step time: 1.0465\n",
      "264/295, train_loss: 0.0341, step time: 1.0418\n",
      "265/295, train_loss: 0.0503, step time: 1.0434\n",
      "266/295, train_loss: 0.0783, step time: 1.0700\n",
      "267/295, train_loss: 0.0780, step time: 1.1029\n",
      "268/295, train_loss: 0.0436, step time: 1.0605\n",
      "269/295, train_loss: 0.0391, step time: 1.0909\n",
      "270/295, train_loss: 0.0319, step time: 1.0330\n",
      "271/295, train_loss: 0.0517, step time: 1.0317\n",
      "272/295, train_loss: 0.0405, step time: 1.0334\n",
      "273/295, train_loss: 0.0417, step time: 1.0360\n",
      "274/295, train_loss: 0.0258, step time: 1.0390\n",
      "275/295, train_loss: 0.0801, step time: 1.0845\n",
      "276/295, train_loss: 0.0525, step time: 1.0471\n",
      "277/295, train_loss: 0.0466, step time: 1.0432\n",
      "278/295, train_loss: 0.0320, step time: 1.0595\n",
      "279/295, train_loss: 0.0513, step time: 1.0376\n",
      "280/295, train_loss: 0.0187, step time: 1.0387\n",
      "281/295, train_loss: 0.0350, step time: 1.0376\n",
      "282/295, train_loss: 0.0284, step time: 1.0594\n",
      "283/295, train_loss: 0.0645, step time: 1.0309\n",
      "284/295, train_loss: 0.4314, step time: 1.0364\n",
      "285/295, train_loss: 0.0770, step time: 1.0432\n",
      "286/295, train_loss: 0.0476, step time: 1.0551\n",
      "287/295, train_loss: 0.0507, step time: 1.0653\n",
      "288/295, train_loss: 0.0384, step time: 1.0409\n",
      "289/295, train_loss: 0.3745, step time: 1.0295\n",
      "290/295, train_loss: 0.0530, step time: 1.0294\n",
      "291/295, train_loss: 0.0545, step time: 1.0297\n",
      "292/295, train_loss: 0.0702, step time: 1.0301\n",
      "293/295, train_loss: 0.3758, step time: 1.0293\n",
      "294/295, train_loss: 0.0949, step time: 1.0295\n",
      "295/295, train_loss: 0.0846, step time: 1.0290\n",
      "epoch 78 average loss: 0.0814\n",
      "current epoch: 78 current mean dice: 0.7691 tc: 0.7246 wt: 0.8366 et: 0.7528\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 78 is: 386.8102\n",
      "----------\n",
      "epoch 79/100\n",
      "1/295, train_loss: 0.0765, step time: 1.0945\n",
      "2/295, train_loss: 0.0396, step time: 1.0908\n",
      "3/295, train_loss: 0.0651, step time: 1.0731\n",
      "4/295, train_loss: 0.0281, step time: 1.0631\n",
      "5/295, train_loss: 0.0365, step time: 1.0319\n",
      "6/295, train_loss: 0.0353, step time: 1.1306\n",
      "7/295, train_loss: 0.0561, step time: 1.0698\n",
      "8/295, train_loss: 0.0323, step time: 1.1519\n",
      "9/295, train_loss: 0.0928, step time: 1.0586\n",
      "10/295, train_loss: 0.0578, step time: 1.0677\n",
      "11/295, train_loss: 0.0493, step time: 1.0484\n",
      "12/295, train_loss: 0.3771, step time: 1.0380\n",
      "13/295, train_loss: 0.1175, step time: 1.0415\n",
      "14/295, train_loss: 0.1090, step time: 1.0409\n",
      "15/295, train_loss: 0.0374, step time: 1.0334\n",
      "16/295, train_loss: 0.0252, step time: 1.0920\n",
      "17/295, train_loss: 0.0499, step time: 1.0376\n",
      "18/295, train_loss: 0.0304, step time: 1.0549\n",
      "19/295, train_loss: 0.0985, step time: 1.0763\n",
      "20/295, train_loss: 0.3623, step time: 1.0386\n",
      "21/295, train_loss: 0.0751, step time: 1.0401\n",
      "22/295, train_loss: 0.0426, step time: 1.0322\n",
      "23/295, train_loss: 0.0620, step time: 1.0779\n",
      "24/295, train_loss: 0.0516, step time: 1.0601\n",
      "25/295, train_loss: 0.0580, step time: 1.0316\n",
      "26/295, train_loss: 0.0238, step time: 1.0456\n",
      "27/295, train_loss: 0.0409, step time: 1.0517\n",
      "28/295, train_loss: 0.0961, step time: 1.0465\n",
      "29/295, train_loss: 0.0491, step time: 1.0871\n",
      "30/295, train_loss: 0.0271, step time: 1.0428\n",
      "31/295, train_loss: 0.0957, step time: 1.0524\n",
      "32/295, train_loss: 0.1043, step time: 1.0840\n",
      "33/295, train_loss: 0.0579, step time: 1.0615\n",
      "34/295, train_loss: 0.0742, step time: 1.0632\n",
      "35/295, train_loss: 0.0440, step time: 1.0391\n",
      "36/295, train_loss: 0.0495, step time: 1.0520\n",
      "37/295, train_loss: 0.0388, step time: 1.1083\n",
      "38/295, train_loss: 0.0624, step time: 1.0331\n",
      "39/295, train_loss: 0.3795, step time: 1.0595\n",
      "40/295, train_loss: 0.0437, step time: 1.0353\n",
      "41/295, train_loss: 0.0780, step time: 1.0376\n",
      "42/295, train_loss: 0.0430, step time: 1.0690\n",
      "43/295, train_loss: 0.0643, step time: 1.0364\n",
      "44/295, train_loss: 0.0515, step time: 1.0989\n",
      "45/295, train_loss: 0.0479, step time: 1.0544\n",
      "46/295, train_loss: 0.0411, step time: 1.0396\n",
      "47/295, train_loss: 0.0526, step time: 1.0450\n",
      "48/295, train_loss: 0.0459, step time: 1.0311\n",
      "49/295, train_loss: 0.0588, step time: 1.0408\n",
      "50/295, train_loss: 0.0380, step time: 1.0460\n",
      "51/295, train_loss: 0.0360, step time: 1.0478\n",
      "52/295, train_loss: 0.0892, step time: 1.0368\n",
      "53/295, train_loss: 0.0708, step time: 1.0434\n",
      "54/295, train_loss: 0.0632, step time: 1.0362\n",
      "55/295, train_loss: 0.0815, step time: 1.0470\n",
      "56/295, train_loss: 0.0562, step time: 1.0567\n",
      "57/295, train_loss: 0.0483, step time: 1.0329\n",
      "58/295, train_loss: 0.0639, step time: 1.0318\n",
      "59/295, train_loss: 0.0749, step time: 1.0719\n",
      "60/295, train_loss: 0.0771, step time: 1.0568\n",
      "61/295, train_loss: 0.0359, step time: 1.0404\n",
      "62/295, train_loss: 0.1326, step time: 1.0578\n",
      "63/295, train_loss: 0.0583, step time: 1.0882\n",
      "64/295, train_loss: 0.0720, step time: 1.0552\n",
      "65/295, train_loss: 0.0738, step time: 1.0429\n",
      "66/295, train_loss: 0.0844, step time: 1.0357\n",
      "67/295, train_loss: 0.0306, step time: 1.0498\n",
      "68/295, train_loss: 0.0551, step time: 1.0647\n",
      "69/295, train_loss: 0.0613, step time: 1.0522\n",
      "70/295, train_loss: 0.0732, step time: 1.0553\n",
      "71/295, train_loss: 0.0443, step time: 1.0430\n",
      "72/295, train_loss: 0.0401, step time: 1.0391\n",
      "73/295, train_loss: 0.0456, step time: 1.0301\n",
      "74/295, train_loss: 0.0900, step time: 1.0286\n",
      "75/295, train_loss: 0.0762, step time: 1.0313\n",
      "76/295, train_loss: 0.0610, step time: 1.0463\n",
      "77/295, train_loss: 0.0805, step time: 1.0408\n",
      "78/295, train_loss: 0.3796, step time: 1.0376\n",
      "79/295, train_loss: 0.0396, step time: 1.0611\n",
      "80/295, train_loss: 0.0635, step time: 1.0316\n",
      "81/295, train_loss: 0.0834, step time: 1.0971\n",
      "82/295, train_loss: 0.0960, step time: 1.0434\n",
      "83/295, train_loss: 0.0578, step time: 1.0524\n",
      "84/295, train_loss: 0.0520, step time: 1.0476\n",
      "85/295, train_loss: 0.0611, step time: 1.0387\n",
      "86/295, train_loss: 0.0558, step time: 1.0432\n",
      "87/295, train_loss: 0.0290, step time: 1.0451\n",
      "88/295, train_loss: 0.1128, step time: 1.1442\n",
      "89/295, train_loss: 0.3664, step time: 1.0895\n",
      "90/295, train_loss: 0.0768, step time: 1.0588\n",
      "91/295, train_loss: 0.0350, step time: 1.0301\n",
      "92/295, train_loss: 0.0279, step time: 1.0312\n",
      "93/295, train_loss: 0.0352, step time: 1.0507\n",
      "94/295, train_loss: 0.0599, step time: 1.1259\n",
      "95/295, train_loss: 0.0755, step time: 1.0542\n",
      "96/295, train_loss: 0.0380, step time: 1.0606\n",
      "97/295, train_loss: 0.0496, step time: 1.0540\n",
      "98/295, train_loss: 0.0769, step time: 1.0427\n",
      "99/295, train_loss: 0.0695, step time: 1.0320\n",
      "100/295, train_loss: 0.0576, step time: 1.0397\n",
      "101/295, train_loss: 0.0269, step time: 1.0564\n",
      "102/295, train_loss: 0.0805, step time: 1.0332\n",
      "103/295, train_loss: 0.0440, step time: 1.0335\n",
      "104/295, train_loss: 0.0304, step time: 1.0458\n",
      "105/295, train_loss: 0.3637, step time: 1.0429\n",
      "106/295, train_loss: 0.0335, step time: 1.0483\n",
      "107/295, train_loss: 0.0725, step time: 1.0539\n",
      "108/295, train_loss: 0.0325, step time: 1.0381\n",
      "109/295, train_loss: 0.0741, step time: 1.0386\n",
      "110/295, train_loss: 0.0483, step time: 1.0329\n",
      "111/295, train_loss: 0.0430, step time: 1.0408\n",
      "112/295, train_loss: 0.0588, step time: 1.0485\n",
      "113/295, train_loss: 0.3875, step time: 1.0448\n",
      "114/295, train_loss: 0.0406, step time: 1.0395\n",
      "115/295, train_loss: 0.3611, step time: 1.0372\n",
      "116/295, train_loss: 0.0368, step time: 1.0564\n",
      "117/295, train_loss: 0.0231, step time: 1.0408\n",
      "118/295, train_loss: 0.0414, step time: 1.0444\n",
      "119/295, train_loss: 0.1170, step time: 1.0386\n",
      "120/295, train_loss: 0.0947, step time: 1.0371\n",
      "121/295, train_loss: 0.0269, step time: 1.1096\n",
      "122/295, train_loss: 0.0641, step time: 1.0788\n",
      "123/295, train_loss: 0.0575, step time: 1.0472\n",
      "124/295, train_loss: 0.0708, step time: 1.0327\n",
      "125/295, train_loss: 0.0318, step time: 1.0558\n",
      "126/295, train_loss: 0.0309, step time: 1.0564\n",
      "127/295, train_loss: 0.0404, step time: 1.0387\n",
      "128/295, train_loss: 0.3726, step time: 1.0672\n",
      "129/295, train_loss: 0.0524, step time: 1.0436\n",
      "130/295, train_loss: 0.1259, step time: 1.0569\n",
      "131/295, train_loss: 0.0425, step time: 1.0435\n",
      "132/295, train_loss: 0.0428, step time: 1.0349\n",
      "133/295, train_loss: 0.0470, step time: 1.0360\n",
      "134/295, train_loss: 0.0655, step time: 1.0616\n",
      "135/295, train_loss: 0.0531, step time: 1.1184\n",
      "136/295, train_loss: 0.0781, step time: 1.0320\n",
      "137/295, train_loss: 0.1328, step time: 1.0409\n",
      "138/295, train_loss: 0.0856, step time: 1.0367\n",
      "139/295, train_loss: 0.3811, step time: 1.0446\n",
      "140/295, train_loss: 0.0439, step time: 1.0414\n",
      "141/295, train_loss: 0.0878, step time: 1.0352\n",
      "142/295, train_loss: 0.0735, step time: 1.0385\n",
      "143/295, train_loss: 0.0747, step time: 1.0364\n",
      "144/295, train_loss: 0.0537, step time: 1.0585\n",
      "145/295, train_loss: 0.0443, step time: 1.0480\n",
      "146/295, train_loss: 0.0487, step time: 1.0442\n",
      "147/295, train_loss: 0.0332, step time: 1.0729\n",
      "148/295, train_loss: 0.0677, step time: 1.0767\n",
      "149/295, train_loss: 0.0427, step time: 1.0379\n",
      "150/295, train_loss: 0.0437, step time: 1.0357\n",
      "151/295, train_loss: 0.3761, step time: 1.0338\n",
      "152/295, train_loss: 0.0269, step time: 1.0556\n",
      "153/295, train_loss: 0.0662, step time: 1.0587\n",
      "154/295, train_loss: 0.0440, step time: 1.0362\n",
      "155/295, train_loss: 0.0321, step time: 1.0713\n",
      "156/295, train_loss: 0.0929, step time: 1.0391\n",
      "157/295, train_loss: 0.0825, step time: 1.0392\n",
      "158/295, train_loss: 0.0449, step time: 1.0524\n",
      "159/295, train_loss: 0.0287, step time: 1.0350\n",
      "160/295, train_loss: 0.0286, step time: 1.0325\n",
      "161/295, train_loss: 0.0501, step time: 1.0815\n",
      "162/295, train_loss: 0.0485, step time: 1.0325\n",
      "163/295, train_loss: 0.0614, step time: 1.0388\n",
      "164/295, train_loss: 0.0249, step time: 1.0621\n",
      "165/295, train_loss: 0.4299, step time: 1.0841\n",
      "166/295, train_loss: 0.0538, step time: 1.0909\n",
      "167/295, train_loss: 0.0762, step time: 1.1341\n",
      "168/295, train_loss: 0.0406, step time: 1.0642\n",
      "169/295, train_loss: 0.0602, step time: 1.0367\n",
      "170/295, train_loss: 0.0478, step time: 1.0474\n",
      "171/295, train_loss: 0.0509, step time: 1.0339\n",
      "172/295, train_loss: 0.0686, step time: 1.0404\n",
      "173/295, train_loss: 0.0933, step time: 1.0397\n",
      "174/295, train_loss: 0.0364, step time: 1.0417\n",
      "175/295, train_loss: 0.0676, step time: 1.0393\n",
      "176/295, train_loss: 0.0855, step time: 1.0774\n",
      "177/295, train_loss: 0.3569, step time: 1.0627\n",
      "178/295, train_loss: 0.0499, step time: 1.0418\n",
      "179/295, train_loss: 0.0713, step time: 1.0388\n",
      "180/295, train_loss: 0.3685, step time: 1.0506\n",
      "181/295, train_loss: 0.0428, step time: 1.0341\n",
      "182/295, train_loss: 0.0805, step time: 1.0633\n",
      "183/295, train_loss: 0.0882, step time: 1.0505\n",
      "184/295, train_loss: 0.3809, step time: 1.0591\n",
      "185/295, train_loss: 0.0708, step time: 1.0551\n",
      "186/295, train_loss: 0.0765, step time: 1.0397\n",
      "187/295, train_loss: 0.0350, step time: 1.0351\n",
      "188/295, train_loss: 0.1052, step time: 1.0520\n",
      "189/295, train_loss: 0.0237, step time: 1.0643\n",
      "190/295, train_loss: 0.0972, step time: 1.0678\n",
      "191/295, train_loss: 0.0258, step time: 1.0711\n",
      "192/295, train_loss: 0.0277, step time: 1.0670\n",
      "193/295, train_loss: 0.0616, step time: 1.0332\n",
      "194/295, train_loss: 0.0325, step time: 1.0401\n",
      "195/295, train_loss: 0.1123, step time: 1.0765\n",
      "196/295, train_loss: 0.0279, step time: 1.0941\n",
      "197/295, train_loss: 0.0701, step time: 1.0513\n",
      "198/295, train_loss: 0.0241, step time: 1.0381\n",
      "199/295, train_loss: 0.0689, step time: 1.0398\n",
      "200/295, train_loss: 0.0502, step time: 1.0419\n",
      "201/295, train_loss: 0.0600, step time: 1.0601\n",
      "202/295, train_loss: 0.0310, step time: 1.0525\n",
      "203/295, train_loss: 0.0753, step time: 1.0590\n",
      "204/295, train_loss: 0.0783, step time: 1.0433\n",
      "205/295, train_loss: 0.0344, step time: 1.0713\n",
      "206/295, train_loss: 0.0641, step time: 1.0436\n",
      "207/295, train_loss: 0.0687, step time: 1.0708\n",
      "208/295, train_loss: 0.0314, step time: 1.0593\n",
      "209/295, train_loss: 0.0340, step time: 1.0374\n",
      "210/295, train_loss: 0.1016, step time: 1.0442\n",
      "211/295, train_loss: 0.0609, step time: 1.0362\n",
      "212/295, train_loss: 0.0338, step time: 1.0328\n",
      "213/295, train_loss: 0.0982, step time: 1.1093\n",
      "214/295, train_loss: 0.0492, step time: 1.0340\n",
      "215/295, train_loss: 0.0367, step time: 1.0557\n",
      "216/295, train_loss: 0.0408, step time: 1.0683\n",
      "217/295, train_loss: 0.0594, step time: 1.0376\n",
      "218/295, train_loss: 0.0519, step time: 1.0705\n",
      "219/295, train_loss: 0.3963, step time: 1.0314\n",
      "220/295, train_loss: 0.0608, step time: 1.0356\n",
      "221/295, train_loss: 0.0261, step time: 1.0375\n",
      "222/295, train_loss: 0.0373, step time: 1.0373\n",
      "223/295, train_loss: 0.0592, step time: 1.0387\n",
      "224/295, train_loss: 0.0374, step time: 1.0464\n",
      "225/295, train_loss: 0.0767, step time: 1.0551\n",
      "226/295, train_loss: 0.0774, step time: 1.0443\n",
      "227/295, train_loss: 0.0491, step time: 1.0560\n",
      "228/295, train_loss: 0.0856, step time: 1.0754\n",
      "229/295, train_loss: 0.0659, step time: 1.0500\n",
      "230/295, train_loss: 0.2094, step time: 1.1175\n",
      "231/295, train_loss: 0.0822, step time: 1.0399\n",
      "232/295, train_loss: 0.0277, step time: 1.0407\n",
      "233/295, train_loss: 0.0556, step time: 1.0615\n",
      "234/295, train_loss: 0.0558, step time: 1.0753\n",
      "235/295, train_loss: 0.0495, step time: 1.0703\n",
      "236/295, train_loss: 0.0389, step time: 1.0653\n",
      "237/295, train_loss: 0.0278, step time: 1.0397\n",
      "238/295, train_loss: 0.0507, step time: 1.0369\n",
      "239/295, train_loss: 0.0461, step time: 1.0359\n",
      "240/295, train_loss: 0.0695, step time: 1.0350\n",
      "241/295, train_loss: 0.0413, step time: 1.0409\n",
      "242/295, train_loss: 0.0473, step time: 1.0550\n",
      "243/295, train_loss: 0.3765, step time: 1.0354\n",
      "244/295, train_loss: 0.0518, step time: 1.0584\n",
      "245/295, train_loss: 0.0653, step time: 1.0362\n",
      "246/295, train_loss: 0.0395, step time: 1.0352\n",
      "247/295, train_loss: 0.0476, step time: 1.0355\n",
      "248/295, train_loss: 0.0277, step time: 1.0719\n",
      "249/295, train_loss: 0.0597, step time: 1.0390\n",
      "250/295, train_loss: 0.0267, step time: 1.0336\n",
      "251/295, train_loss: 0.0180, step time: 1.0467\n",
      "252/295, train_loss: 0.0648, step time: 1.0378\n",
      "253/295, train_loss: 0.0831, step time: 1.0423\n",
      "254/295, train_loss: 0.0547, step time: 1.0407\n",
      "255/295, train_loss: 0.0217, step time: 1.0503\n",
      "256/295, train_loss: 0.3748, step time: 1.0981\n",
      "257/295, train_loss: 0.0557, step time: 1.0358\n",
      "258/295, train_loss: 0.0263, step time: 1.0480\n",
      "259/295, train_loss: 0.0392, step time: 1.0480\n",
      "260/295, train_loss: 0.0247, step time: 1.0425\n",
      "261/295, train_loss: 0.0617, step time: 1.0333\n",
      "262/295, train_loss: 0.0635, step time: 1.0385\n",
      "263/295, train_loss: 0.0702, step time: 1.0800\n",
      "264/295, train_loss: 0.0641, step time: 1.0473\n",
      "265/295, train_loss: 0.0550, step time: 1.1219\n",
      "266/295, train_loss: 0.3609, step time: 1.0320\n",
      "267/295, train_loss: 0.0386, step time: 1.0628\n",
      "268/295, train_loss: 0.0467, step time: 1.0339\n",
      "269/295, train_loss: 0.0595, step time: 1.0442\n",
      "270/295, train_loss: 0.0934, step time: 1.1031\n",
      "271/295, train_loss: 0.3783, step time: 1.0397\n",
      "272/295, train_loss: 0.0793, step time: 1.0613\n",
      "273/295, train_loss: 0.0295, step time: 1.0359\n",
      "274/295, train_loss: 0.0545, step time: 1.0656\n",
      "275/295, train_loss: 0.0386, step time: 1.0607\n",
      "276/295, train_loss: 0.0473, step time: 1.0399\n",
      "277/295, train_loss: 0.0794, step time: 1.0537\n",
      "278/295, train_loss: 0.3583, step time: 1.0314\n",
      "279/295, train_loss: 0.0482, step time: 1.0364\n",
      "280/295, train_loss: 0.0310, step time: 1.0556\n",
      "281/295, train_loss: 0.1107, step time: 1.0319\n",
      "282/295, train_loss: 0.0784, step time: 1.0349\n",
      "283/295, train_loss: 0.3902, step time: 1.0914\n",
      "284/295, train_loss: 0.0313, step time: 1.0574\n",
      "285/295, train_loss: 0.0501, step time: 1.0550\n",
      "286/295, train_loss: 0.0314, step time: 1.0405\n",
      "287/295, train_loss: 0.0362, step time: 1.0489\n",
      "288/295, train_loss: 0.0403, step time: 1.0603\n",
      "289/295, train_loss: 0.0745, step time: 1.0291\n",
      "290/295, train_loss: 0.0310, step time: 1.0283\n",
      "291/295, train_loss: 0.0262, step time: 1.0283\n",
      "292/295, train_loss: 0.0253, step time: 1.0286\n",
      "293/295, train_loss: 0.0303, step time: 1.0301\n",
      "294/295, train_loss: 0.0944, step time: 1.0287\n",
      "295/295, train_loss: 0.0580, step time: 1.0303\n",
      "epoch 79 average loss: 0.0808\n",
      "current epoch: 79 current mean dice: 0.7515 tc: 0.6995 wt: 0.8207 et: 0.7389\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 79 is: 387.8403\n",
      "----------\n",
      "epoch 80/100\n",
      "1/295, train_loss: 0.0693, step time: 1.1035\n",
      "2/295, train_loss: 0.0536, step time: 1.0666\n",
      "3/295, train_loss: 0.0775, step time: 1.0375\n",
      "4/295, train_loss: 0.0236, step time: 1.1227\n",
      "5/295, train_loss: 0.0764, step time: 1.0833\n",
      "6/295, train_loss: 0.0564, step time: 1.0471\n",
      "7/295, train_loss: 0.3795, step time: 1.0412\n",
      "8/295, train_loss: 0.3714, step time: 1.0289\n",
      "9/295, train_loss: 0.0286, step time: 1.0432\n",
      "10/295, train_loss: 0.0361, step time: 1.0500\n",
      "11/295, train_loss: 0.0839, step time: 1.0596\n",
      "12/295, train_loss: 0.0610, step time: 1.0293\n",
      "13/295, train_loss: 0.0394, step time: 1.0311\n",
      "14/295, train_loss: 0.0543, step time: 1.0642\n",
      "15/295, train_loss: 0.0733, step time: 1.0495\n",
      "16/295, train_loss: 0.0916, step time: 1.0376\n",
      "17/295, train_loss: 0.0924, step time: 1.0465\n",
      "18/295, train_loss: 0.0441, step time: 1.0383\n",
      "19/295, train_loss: 0.0554, step time: 1.0364\n",
      "20/295, train_loss: 0.0407, step time: 1.0288\n",
      "21/295, train_loss: 0.0780, step time: 1.0414\n",
      "22/295, train_loss: 0.0564, step time: 1.0426\n",
      "23/295, train_loss: 0.0319, step time: 1.0313\n",
      "24/295, train_loss: 0.0281, step time: 1.0328\n",
      "25/295, train_loss: 0.0468, step time: 1.0410\n",
      "26/295, train_loss: 0.0481, step time: 1.0492\n",
      "27/295, train_loss: 0.0684, step time: 1.0358\n",
      "28/295, train_loss: 0.0318, step time: 1.0567\n",
      "29/295, train_loss: 0.0428, step time: 1.0354\n",
      "30/295, train_loss: 0.0449, step time: 1.0540\n",
      "31/295, train_loss: 0.0670, step time: 1.0501\n",
      "32/295, train_loss: 0.0507, step time: 1.0664\n",
      "33/295, train_loss: 0.0441, step time: 1.0697\n",
      "34/295, train_loss: 0.0578, step time: 1.0392\n",
      "35/295, train_loss: 0.0596, step time: 1.0642\n",
      "36/295, train_loss: 0.0488, step time: 1.1134\n",
      "37/295, train_loss: 0.0361, step time: 1.0717\n",
      "38/295, train_loss: 0.0243, step time: 1.1219\n",
      "39/295, train_loss: 0.0620, step time: 1.0607\n",
      "40/295, train_loss: 0.0320, step time: 1.0510\n",
      "41/295, train_loss: 0.0274, step time: 1.0440\n",
      "42/295, train_loss: 0.0652, step time: 1.0312\n",
      "43/295, train_loss: 0.0349, step time: 1.1111\n",
      "44/295, train_loss: 0.0426, step time: 1.0484\n",
      "45/295, train_loss: 0.0785, step time: 1.0429\n",
      "46/295, train_loss: 0.0317, step time: 1.0397\n",
      "47/295, train_loss: 0.1016, step time: 1.0312\n",
      "48/295, train_loss: 0.0752, step time: 1.0340\n",
      "49/295, train_loss: 0.0662, step time: 1.0842\n",
      "50/295, train_loss: 0.0260, step time: 1.0515\n",
      "51/295, train_loss: 0.0464, step time: 1.0322\n",
      "52/295, train_loss: 0.0330, step time: 1.0436\n",
      "53/295, train_loss: 0.3689, step time: 1.0662\n",
      "54/295, train_loss: 0.3713, step time: 1.0666\n",
      "55/295, train_loss: 0.0599, step time: 1.0317\n",
      "56/295, train_loss: 0.1166, step time: 1.0386\n",
      "57/295, train_loss: 0.0698, step time: 1.0491\n",
      "58/295, train_loss: 0.0405, step time: 1.0411\n",
      "59/295, train_loss: 0.0286, step time: 1.0365\n",
      "60/295, train_loss: 0.0458, step time: 1.0446\n",
      "61/295, train_loss: 0.0739, step time: 1.0391\n",
      "62/295, train_loss: 0.0583, step time: 1.0377\n",
      "63/295, train_loss: 0.0503, step time: 1.0479\n",
      "64/295, train_loss: 0.0542, step time: 1.0666\n",
      "65/295, train_loss: 0.0934, step time: 1.0334\n",
      "66/295, train_loss: 0.0432, step time: 1.0366\n",
      "67/295, train_loss: 0.0482, step time: 1.0386\n",
      "68/295, train_loss: 0.0562, step time: 1.0339\n",
      "69/295, train_loss: 0.0394, step time: 1.0908\n",
      "70/295, train_loss: 0.0393, step time: 1.0323\n",
      "71/295, train_loss: 0.3777, step time: 1.0382\n",
      "72/295, train_loss: 0.0616, step time: 1.0468\n",
      "73/295, train_loss: 0.0282, step time: 1.0358\n",
      "74/295, train_loss: 0.0379, step time: 1.0628\n",
      "75/295, train_loss: 0.0690, step time: 1.0538\n",
      "76/295, train_loss: 0.0423, step time: 1.0388\n",
      "77/295, train_loss: 0.0769, step time: 1.1113\n",
      "78/295, train_loss: 0.0576, step time: 1.0457\n",
      "79/295, train_loss: 0.0613, step time: 1.0469\n",
      "80/295, train_loss: 0.0302, step time: 1.0587\n",
      "81/295, train_loss: 0.0318, step time: 1.0413\n",
      "82/295, train_loss: 0.0767, step time: 1.0307\n",
      "83/295, train_loss: 0.0267, step time: 1.0361\n",
      "84/295, train_loss: 0.0257, step time: 1.0443\n",
      "85/295, train_loss: 0.0845, step time: 1.1115\n",
      "86/295, train_loss: 0.0583, step time: 1.0318\n",
      "87/295, train_loss: 0.3879, step time: 1.0556\n",
      "88/295, train_loss: 0.0307, step time: 1.0801\n",
      "89/295, train_loss: 0.0264, step time: 1.0565\n",
      "90/295, train_loss: 0.0617, step time: 1.0372\n",
      "91/295, train_loss: 0.0542, step time: 1.0972\n",
      "92/295, train_loss: 0.0513, step time: 1.0691\n",
      "93/295, train_loss: 0.3613, step time: 1.0719\n",
      "94/295, train_loss: 0.0456, step time: 1.0578\n",
      "95/295, train_loss: 0.0479, step time: 1.0360\n",
      "96/295, train_loss: 0.0255, step time: 1.0478\n",
      "97/295, train_loss: 0.0430, step time: 1.0492\n",
      "98/295, train_loss: 0.0746, step time: 1.0608\n",
      "99/295, train_loss: 0.0852, step time: 1.0347\n",
      "100/295, train_loss: 0.0344, step time: 1.0356\n",
      "101/295, train_loss: 0.0239, step time: 1.0467\n",
      "102/295, train_loss: 0.0469, step time: 1.0457\n",
      "103/295, train_loss: 0.1100, step time: 1.0633\n",
      "104/295, train_loss: 0.0750, step time: 1.0647\n",
      "105/295, train_loss: 0.0803, step time: 1.0422\n",
      "106/295, train_loss: 0.0774, step time: 1.0316\n",
      "107/295, train_loss: 0.0489, step time: 1.1315\n",
      "108/295, train_loss: 0.0486, step time: 1.0373\n",
      "109/295, train_loss: 0.0373, step time: 1.0503\n",
      "110/295, train_loss: 0.0408, step time: 1.0481\n",
      "111/295, train_loss: 0.0504, step time: 1.0301\n",
      "112/295, train_loss: 0.0482, step time: 1.0670\n",
      "113/295, train_loss: 0.0584, step time: 1.0401\n",
      "114/295, train_loss: 0.0239, step time: 1.0310\n",
      "115/295, train_loss: 0.0471, step time: 1.0308\n",
      "116/295, train_loss: 0.0253, step time: 1.0684\n",
      "117/295, train_loss: 0.1112, step time: 1.0686\n",
      "118/295, train_loss: 0.3948, step time: 1.0696\n",
      "119/295, train_loss: 0.0300, step time: 1.0403\n",
      "120/295, train_loss: 0.0402, step time: 1.0602\n",
      "121/295, train_loss: 0.0402, step time: 1.0487\n",
      "122/295, train_loss: 0.0343, step time: 1.0341\n",
      "123/295, train_loss: 0.3597, step time: 1.0334\n",
      "124/295, train_loss: 0.3785, step time: 1.0501\n",
      "125/295, train_loss: 0.1133, step time: 1.0306\n",
      "126/295, train_loss: 0.0747, step time: 1.0324\n",
      "127/295, train_loss: 0.3780, step time: 1.0315\n",
      "128/295, train_loss: 0.3566, step time: 1.0378\n",
      "129/295, train_loss: 0.0812, step time: 1.0398\n",
      "130/295, train_loss: 0.0565, step time: 1.0566\n",
      "131/295, train_loss: 0.0720, step time: 1.1102\n",
      "132/295, train_loss: 0.0376, step time: 1.0350\n",
      "133/295, train_loss: 0.0817, step time: 1.1113\n",
      "134/295, train_loss: 0.0297, step time: 1.0508\n",
      "135/295, train_loss: 0.0802, step time: 1.0565\n",
      "136/295, train_loss: 0.0774, step time: 1.0381\n",
      "137/295, train_loss: 0.0858, step time: 1.0520\n",
      "138/295, train_loss: 0.0556, step time: 1.0380\n",
      "139/295, train_loss: 0.0365, step time: 1.0502\n",
      "140/295, train_loss: 0.0918, step time: 1.0554\n",
      "141/295, train_loss: 0.0513, step time: 1.0424\n",
      "142/295, train_loss: 0.0634, step time: 1.0356\n",
      "143/295, train_loss: 0.1351, step time: 1.0396\n",
      "144/295, train_loss: 0.3764, step time: 1.1121\n",
      "145/295, train_loss: 0.3814, step time: 1.0340\n",
      "146/295, train_loss: 0.0764, step time: 1.0468\n",
      "147/295, train_loss: 0.0530, step time: 1.0774\n",
      "148/295, train_loss: 0.0591, step time: 1.0532\n",
      "149/295, train_loss: 0.0345, step time: 1.0478\n",
      "150/295, train_loss: 0.0840, step time: 1.0455\n",
      "151/295, train_loss: 0.3677, step time: 1.0427\n",
      "152/295, train_loss: 0.0425, step time: 1.0418\n",
      "153/295, train_loss: 0.0784, step time: 1.0725\n",
      "154/295, train_loss: 0.0496, step time: 1.0382\n",
      "155/295, train_loss: 0.0707, step time: 1.0400\n",
      "156/295, train_loss: 0.0788, step time: 1.0366\n",
      "157/295, train_loss: 0.3637, step time: 1.0480\n",
      "158/295, train_loss: 0.0518, step time: 1.0552\n",
      "159/295, train_loss: 0.0763, step time: 1.0505\n",
      "160/295, train_loss: 0.0476, step time: 1.0321\n",
      "161/295, train_loss: 0.1068, step time: 1.0359\n",
      "162/295, train_loss: 0.0585, step time: 1.0308\n",
      "163/295, train_loss: 0.0762, step time: 1.0334\n",
      "164/295, train_loss: 0.0679, step time: 1.0457\n",
      "165/295, train_loss: 0.0818, step time: 1.0409\n",
      "166/295, train_loss: 0.0391, step time: 1.0365\n",
      "167/295, train_loss: 0.0319, step time: 1.0403\n",
      "168/295, train_loss: 0.0578, step time: 1.0571\n",
      "169/295, train_loss: 0.0362, step time: 1.0315\n",
      "170/295, train_loss: 0.0696, step time: 1.1035\n",
      "171/295, train_loss: 0.0522, step time: 1.0507\n",
      "172/295, train_loss: 0.0412, step time: 1.0413\n",
      "173/295, train_loss: 0.0883, step time: 1.0419\n",
      "174/295, train_loss: 0.0301, step time: 1.0341\n",
      "175/295, train_loss: 0.0916, step time: 1.0325\n",
      "176/295, train_loss: 0.0394, step time: 1.0385\n",
      "177/295, train_loss: 0.0605, step time: 1.0628\n",
      "178/295, train_loss: 0.3800, step time: 1.0577\n",
      "179/295, train_loss: 0.0545, step time: 1.0701\n",
      "180/295, train_loss: 0.0397, step time: 1.0609\n",
      "181/295, train_loss: 0.0496, step time: 1.0401\n",
      "182/295, train_loss: 0.0411, step time: 1.0793\n",
      "183/295, train_loss: 0.0501, step time: 1.0344\n",
      "184/295, train_loss: 0.0651, step time: 1.0348\n",
      "185/295, train_loss: 0.0763, step time: 1.0321\n",
      "186/295, train_loss: 0.0579, step time: 1.0372\n",
      "187/295, train_loss: 0.0869, step time: 1.0631\n",
      "188/295, train_loss: 0.0703, step time: 1.0480\n",
      "189/295, train_loss: 0.3768, step time: 1.0414\n",
      "190/295, train_loss: 0.0352, step time: 1.0602\n",
      "191/295, train_loss: 0.1191, step time: 1.0349\n",
      "192/295, train_loss: 0.0928, step time: 1.0456\n",
      "193/295, train_loss: 0.0341, step time: 1.1022\n",
      "194/295, train_loss: 0.0608, step time: 1.0346\n",
      "195/295, train_loss: 0.0468, step time: 1.0382\n",
      "196/295, train_loss: 0.0271, step time: 1.0357\n",
      "197/295, train_loss: 0.0497, step time: 1.0434\n",
      "198/295, train_loss: 0.1226, step time: 1.0388\n",
      "199/295, train_loss: 0.0355, step time: 1.0389\n",
      "200/295, train_loss: 0.0234, step time: 1.0377\n",
      "201/295, train_loss: 0.0668, step time: 1.0371\n",
      "202/295, train_loss: 0.0550, step time: 1.0457\n",
      "203/295, train_loss: 0.0736, step time: 1.0552\n",
      "204/295, train_loss: 0.0301, step time: 1.0414\n",
      "205/295, train_loss: 0.0405, step time: 1.0412\n",
      "206/295, train_loss: 0.0441, step time: 1.0428\n",
      "207/295, train_loss: 0.0929, step time: 1.0804\n",
      "208/295, train_loss: 0.0746, step time: 1.0569\n",
      "209/295, train_loss: 0.0302, step time: 1.0346\n",
      "210/295, train_loss: 0.0629, step time: 1.0378\n",
      "211/295, train_loss: 0.0645, step time: 1.0505\n",
      "212/295, train_loss: 0.0582, step time: 1.0574\n",
      "213/295, train_loss: 0.0291, step time: 1.0454\n",
      "214/295, train_loss: 0.0771, step time: 1.0366\n",
      "215/295, train_loss: 0.0317, step time: 1.0609\n",
      "216/295, train_loss: 0.0311, step time: 1.0974\n",
      "217/295, train_loss: 0.3594, step time: 1.0349\n",
      "218/295, train_loss: 0.1041, step time: 1.0420\n",
      "219/295, train_loss: 0.0362, step time: 1.0991\n",
      "220/295, train_loss: 0.0388, step time: 1.0362\n",
      "221/295, train_loss: 0.0429, step time: 1.0460\n",
      "222/295, train_loss: 0.0268, step time: 1.0383\n",
      "223/295, train_loss: 0.0281, step time: 1.0873\n",
      "224/295, train_loss: 0.0464, step time: 1.0373\n",
      "225/295, train_loss: 0.0256, step time: 1.0473\n",
      "226/295, train_loss: 0.0974, step time: 1.0497\n",
      "227/295, train_loss: 0.3868, step time: 1.0436\n",
      "228/295, train_loss: 0.0709, step time: 1.0579\n",
      "229/295, train_loss: 0.0431, step time: 1.0509\n",
      "230/295, train_loss: 0.0731, step time: 1.0398\n",
      "231/295, train_loss: 0.0976, step time: 1.1064\n",
      "232/295, train_loss: 0.0968, step time: 1.0589\n",
      "233/295, train_loss: 0.0328, step time: 1.0342\n",
      "234/295, train_loss: 0.0278, step time: 1.0876\n",
      "235/295, train_loss: 0.0248, step time: 1.0363\n",
      "236/295, train_loss: 0.0805, step time: 1.0450\n",
      "237/295, train_loss: 0.0416, step time: 1.0396\n",
      "238/295, train_loss: 0.0388, step time: 1.0449\n",
      "239/295, train_loss: 0.0472, step time: 1.0609\n",
      "240/295, train_loss: 0.0932, step time: 1.0392\n",
      "241/295, train_loss: 0.0842, step time: 1.0461\n",
      "242/295, train_loss: 0.0647, step time: 1.0358\n",
      "243/295, train_loss: 0.1032, step time: 1.0394\n",
      "244/295, train_loss: 0.0636, step time: 1.0530\n",
      "245/295, train_loss: 0.0406, step time: 1.0592\n",
      "246/295, train_loss: 0.0444, step time: 1.0747\n",
      "247/295, train_loss: 0.0729, step time: 1.0328\n",
      "248/295, train_loss: 0.0238, step time: 1.0357\n",
      "249/295, train_loss: 0.0633, step time: 1.0344\n",
      "250/295, train_loss: 0.4293, step time: 1.0345\n",
      "251/295, train_loss: 0.0832, step time: 1.0496\n",
      "252/295, train_loss: 0.0475, step time: 1.0332\n",
      "253/295, train_loss: 0.0498, step time: 1.0430\n",
      "254/295, train_loss: 0.1316, step time: 1.0776\n",
      "255/295, train_loss: 0.0627, step time: 1.0394\n",
      "256/295, train_loss: 0.0520, step time: 1.0568\n",
      "257/295, train_loss: 0.0527, step time: 1.0500\n",
      "258/295, train_loss: 0.0493, step time: 1.0634\n",
      "259/295, train_loss: 0.0324, step time: 1.0533\n",
      "260/295, train_loss: 0.1000, step time: 1.0339\n",
      "261/295, train_loss: 0.0579, step time: 1.0353\n",
      "262/295, train_loss: 0.0514, step time: 1.0336\n",
      "263/295, train_loss: 0.0478, step time: 1.0365\n",
      "264/295, train_loss: 0.2091, step time: 1.0471\n",
      "265/295, train_loss: 0.0181, step time: 1.0401\n",
      "266/295, train_loss: 0.0497, step time: 1.0343\n",
      "267/295, train_loss: 0.0709, step time: 1.0596\n",
      "268/295, train_loss: 0.3615, step time: 1.0403\n",
      "269/295, train_loss: 0.0274, step time: 1.0733\n",
      "270/295, train_loss: 0.0309, step time: 1.1576\n",
      "271/295, train_loss: 0.0606, step time: 1.0360\n",
      "272/295, train_loss: 0.0551, step time: 1.0321\n",
      "273/295, train_loss: 0.0664, step time: 1.0325\n",
      "274/295, train_loss: 0.0209, step time: 1.0374\n",
      "275/295, train_loss: 0.0266, step time: 1.0977\n",
      "276/295, train_loss: 0.0364, step time: 1.0420\n",
      "277/295, train_loss: 0.0385, step time: 1.0374\n",
      "278/295, train_loss: 0.0406, step time: 1.0619\n",
      "279/295, train_loss: 0.0463, step time: 1.0414\n",
      "280/295, train_loss: 0.0598, step time: 1.0511\n",
      "281/295, train_loss: 0.0355, step time: 1.0662\n",
      "282/295, train_loss: 0.0800, step time: 1.0340\n",
      "283/295, train_loss: 0.0513, step time: 1.0380\n",
      "284/295, train_loss: 0.0938, step time: 1.0473\n",
      "285/295, train_loss: 0.0615, step time: 1.0413\n",
      "286/295, train_loss: 0.0651, step time: 1.0516\n",
      "287/295, train_loss: 0.0400, step time: 1.0664\n",
      "288/295, train_loss: 0.0490, step time: 1.0701\n",
      "289/295, train_loss: 0.0687, step time: 1.0303\n",
      "290/295, train_loss: 0.0501, step time: 1.0283\n",
      "291/295, train_loss: 0.0610, step time: 1.0290\n",
      "292/295, train_loss: 0.0422, step time: 1.0283\n",
      "293/295, train_loss: 0.0312, step time: 1.0290\n",
      "294/295, train_loss: 0.0739, step time: 1.0292\n",
      "295/295, train_loss: 0.0755, step time: 1.0289\n",
      "epoch 80 average loss: 0.0806\n",
      "current epoch: 80 current mean dice: 0.7728 tc: 0.7269 wt: 0.8390 et: 0.7589\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 80 is: 381.9837\n",
      "----------\n",
      "epoch 81/100\n",
      "1/295, train_loss: 0.0320, step time: 1.1031\n",
      "2/295, train_loss: 0.0407, step time: 1.0787\n",
      "3/295, train_loss: 0.0737, step time: 1.0414\n",
      "4/295, train_loss: 0.0444, step time: 1.1141\n",
      "5/295, train_loss: 0.0680, step time: 1.0520\n",
      "6/295, train_loss: 0.0970, step time: 1.0637\n",
      "7/295, train_loss: 0.0476, step time: 1.0995\n",
      "8/295, train_loss: 0.1027, step time: 1.0350\n",
      "9/295, train_loss: 0.0468, step time: 1.0542\n",
      "10/295, train_loss: 0.0764, step time: 1.1090\n",
      "11/295, train_loss: 0.0310, step time: 1.0389\n",
      "12/295, train_loss: 0.0259, step time: 1.0368\n",
      "13/295, train_loss: 0.3696, step time: 1.0360\n",
      "14/295, train_loss: 0.0579, step time: 1.0339\n",
      "15/295, train_loss: 0.0452, step time: 1.0394\n",
      "16/295, train_loss: 0.0338, step time: 1.0510\n",
      "17/295, train_loss: 0.0498, step time: 1.0822\n",
      "18/295, train_loss: 0.0583, step time: 1.0577\n",
      "19/295, train_loss: 0.0429, step time: 1.0354\n",
      "20/295, train_loss: 0.0293, step time: 1.0469\n",
      "21/295, train_loss: 0.0545, step time: 1.0592\n",
      "22/295, train_loss: 0.0559, step time: 1.0343\n",
      "23/295, train_loss: 0.0323, step time: 1.0416\n",
      "24/295, train_loss: 0.0787, step time: 1.0772\n",
      "25/295, train_loss: 0.0800, step time: 1.0720\n",
      "26/295, train_loss: 0.0530, step time: 1.1040\n",
      "27/295, train_loss: 0.0503, step time: 1.0632\n",
      "28/295, train_loss: 0.0645, step time: 1.0353\n",
      "29/295, train_loss: 0.0568, step time: 1.0430\n",
      "30/295, train_loss: 0.1073, step time: 1.0462\n",
      "31/295, train_loss: 0.0538, step time: 1.0424\n",
      "32/295, train_loss: 0.0813, step time: 1.0863\n",
      "33/295, train_loss: 0.0517, step time: 1.0465\n",
      "34/295, train_loss: 0.0767, step time: 1.0631\n",
      "35/295, train_loss: 0.0409, step time: 1.0317\n",
      "36/295, train_loss: 0.0828, step time: 1.0942\n",
      "37/295, train_loss: 0.0475, step time: 1.0461\n",
      "38/295, train_loss: 0.0556, step time: 1.0468\n",
      "39/295, train_loss: 0.0409, step time: 1.0319\n",
      "40/295, train_loss: 0.0234, step time: 1.0334\n",
      "41/295, train_loss: 0.0693, step time: 1.0503\n",
      "42/295, train_loss: 0.1098, step time: 1.0525\n",
      "43/295, train_loss: 0.0515, step time: 1.0354\n",
      "44/295, train_loss: 0.0460, step time: 1.0401\n",
      "45/295, train_loss: 0.0238, step time: 1.0950\n",
      "46/295, train_loss: 0.0499, step time: 1.0426\n",
      "47/295, train_loss: 0.0371, step time: 1.0344\n",
      "48/295, train_loss: 0.3719, step time: 1.0395\n",
      "49/295, train_loss: 0.3778, step time: 1.0587\n",
      "50/295, train_loss: 0.3776, step time: 1.0987\n",
      "51/295, train_loss: 0.0305, step time: 1.0385\n",
      "52/295, train_loss: 0.0589, step time: 1.0707\n",
      "53/295, train_loss: 0.0799, step time: 1.0375\n",
      "54/295, train_loss: 0.0254, step time: 1.0597\n",
      "55/295, train_loss: 0.3948, step time: 1.0653\n",
      "56/295, train_loss: 0.0796, step time: 1.0419\n",
      "57/295, train_loss: 0.0493, step time: 1.0330\n",
      "58/295, train_loss: 0.0617, step time: 1.0897\n",
      "59/295, train_loss: 0.0348, step time: 1.0411\n",
      "60/295, train_loss: 0.0419, step time: 1.0332\n",
      "61/295, train_loss: 0.0375, step time: 1.0365\n",
      "62/295, train_loss: 0.0755, step time: 1.0488\n",
      "63/295, train_loss: 0.0919, step time: 1.0397\n",
      "64/295, train_loss: 0.0770, step time: 1.0645\n",
      "65/295, train_loss: 0.0603, step time: 1.0417\n",
      "66/295, train_loss: 0.0313, step time: 1.0435\n",
      "67/295, train_loss: 0.0278, step time: 1.0329\n",
      "68/295, train_loss: 0.0714, step time: 1.0660\n",
      "69/295, train_loss: 0.0367, step time: 1.0539\n",
      "70/295, train_loss: 0.0274, step time: 1.0380\n",
      "71/295, train_loss: 0.0918, step time: 1.0303\n",
      "72/295, train_loss: 0.0360, step time: 1.0396\n",
      "73/295, train_loss: 0.0579, step time: 1.0467\n",
      "74/295, train_loss: 0.0236, step time: 1.0374\n",
      "75/295, train_loss: 0.0383, step time: 1.0561\n",
      "76/295, train_loss: 0.0718, step time: 1.0871\n",
      "77/295, train_loss: 0.0810, step time: 1.0334\n",
      "78/295, train_loss: 0.0434, step time: 1.0328\n",
      "79/295, train_loss: 0.0283, step time: 1.0663\n",
      "80/295, train_loss: 0.0553, step time: 1.0745\n",
      "81/295, train_loss: 0.0928, step time: 1.1027\n",
      "82/295, train_loss: 0.0683, step time: 1.0540\n",
      "83/295, train_loss: 0.0374, step time: 1.0351\n",
      "84/295, train_loss: 0.1246, step time: 1.0593\n",
      "85/295, train_loss: 0.1126, step time: 1.0415\n",
      "86/295, train_loss: 0.0253, step time: 1.0413\n",
      "87/295, train_loss: 0.0823, step time: 1.0539\n",
      "88/295, train_loss: 0.0416, step time: 1.0595\n",
      "89/295, train_loss: 0.1335, step time: 1.0419\n",
      "90/295, train_loss: 0.0363, step time: 1.0668\n",
      "91/295, train_loss: 0.0854, step time: 1.1359\n",
      "92/295, train_loss: 0.0239, step time: 1.1121\n",
      "93/295, train_loss: 0.0545, step time: 1.0514\n",
      "94/295, train_loss: 0.0440, step time: 1.0360\n",
      "95/295, train_loss: 0.0457, step time: 1.0356\n",
      "96/295, train_loss: 0.3845, step time: 1.0681\n",
      "97/295, train_loss: 0.0669, step time: 1.0473\n",
      "98/295, train_loss: 0.0619, step time: 1.0515\n",
      "99/295, train_loss: 0.0814, step time: 1.0355\n",
      "100/295, train_loss: 0.0535, step time: 1.0423\n",
      "101/295, train_loss: 0.0408, step time: 1.0641\n",
      "102/295, train_loss: 0.0910, step time: 1.0532\n",
      "103/295, train_loss: 0.0827, step time: 1.0369\n",
      "104/295, train_loss: 0.0450, step time: 1.0349\n",
      "105/295, train_loss: 0.0950, step time: 1.0552\n",
      "106/295, train_loss: 0.0316, step time: 1.0368\n",
      "107/295, train_loss: 0.0684, step time: 1.1102\n",
      "108/295, train_loss: 0.0399, step time: 1.0608\n",
      "109/295, train_loss: 0.0843, step time: 1.0334\n",
      "110/295, train_loss: 0.3617, step time: 1.0432\n",
      "111/295, train_loss: 0.0306, step time: 1.0367\n",
      "112/295, train_loss: 0.0805, step time: 1.0585\n",
      "113/295, train_loss: 0.0636, step time: 1.0372\n",
      "114/295, train_loss: 0.0536, step time: 1.0465\n",
      "115/295, train_loss: 0.0492, step time: 1.0463\n",
      "116/295, train_loss: 0.0613, step time: 1.0531\n",
      "117/295, train_loss: 0.0621, step time: 1.0429\n",
      "118/295, train_loss: 0.0379, step time: 1.0600\n",
      "119/295, train_loss: 0.3567, step time: 1.0659\n",
      "120/295, train_loss: 0.0582, step time: 1.0391\n",
      "121/295, train_loss: 0.0554, step time: 1.0674\n",
      "122/295, train_loss: 0.0428, step time: 1.0349\n",
      "123/295, train_loss: 0.0621, step time: 1.0354\n",
      "124/295, train_loss: 0.0781, step time: 1.0676\n",
      "125/295, train_loss: 0.3748, step time: 1.0414\n",
      "126/295, train_loss: 0.0390, step time: 1.0849\n",
      "127/295, train_loss: 0.0484, step time: 1.0333\n",
      "128/295, train_loss: 0.0410, step time: 1.0400\n",
      "129/295, train_loss: 0.0639, step time: 1.0430\n",
      "130/295, train_loss: 0.0257, step time: 1.0481\n",
      "131/295, train_loss: 0.0278, step time: 1.0462\n",
      "132/295, train_loss: 0.0395, step time: 1.0645\n",
      "133/295, train_loss: 0.0635, step time: 1.0436\n",
      "134/295, train_loss: 0.0703, step time: 1.0377\n",
      "135/295, train_loss: 0.0260, step time: 1.0396\n",
      "136/295, train_loss: 0.0387, step time: 1.0400\n",
      "137/295, train_loss: 0.0586, step time: 1.0969\n",
      "138/295, train_loss: 0.0855, step time: 1.0336\n",
      "139/295, train_loss: 0.0456, step time: 1.0372\n",
      "140/295, train_loss: 0.3665, step time: 1.0345\n",
      "141/295, train_loss: 0.0374, step time: 1.0407\n",
      "142/295, train_loss: 0.3892, step time: 1.0328\n",
      "143/295, train_loss: 0.0343, step time: 1.0383\n",
      "144/295, train_loss: 0.0783, step time: 1.0392\n",
      "145/295, train_loss: 0.0583, step time: 1.0511\n",
      "146/295, train_loss: 0.0301, step time: 1.0645\n",
      "147/295, train_loss: 0.0277, step time: 1.0985\n",
      "148/295, train_loss: 0.0381, step time: 1.0689\n",
      "149/295, train_loss: 0.1176, step time: 1.0555\n",
      "150/295, train_loss: 0.0364, step time: 1.0341\n",
      "151/295, train_loss: 0.0619, step time: 1.0385\n",
      "152/295, train_loss: 0.0941, step time: 1.0482\n",
      "153/295, train_loss: 0.1074, step time: 1.0515\n",
      "154/295, train_loss: 0.0262, step time: 1.0435\n",
      "155/295, train_loss: 0.0731, step time: 1.1271\n",
      "156/295, train_loss: 0.3788, step time: 1.0403\n",
      "157/295, train_loss: 0.0613, step time: 1.0359\n",
      "158/295, train_loss: 0.0491, step time: 1.1132\n",
      "159/295, train_loss: 0.3584, step time: 1.0543\n",
      "160/295, train_loss: 0.4279, step time: 1.0432\n",
      "161/295, train_loss: 0.0397, step time: 1.0518\n",
      "162/295, train_loss: 0.0471, step time: 1.0388\n",
      "163/295, train_loss: 0.0965, step time: 1.0407\n",
      "164/295, train_loss: 0.0504, step time: 1.0328\n",
      "165/295, train_loss: 0.0465, step time: 1.0713\n",
      "166/295, train_loss: 0.0491, step time: 1.0518\n",
      "167/295, train_loss: 0.0265, step time: 1.0376\n",
      "168/295, train_loss: 0.1025, step time: 1.0612\n",
      "169/295, train_loss: 0.3613, step time: 1.0489\n",
      "170/295, train_loss: 0.0316, step time: 1.0396\n",
      "171/295, train_loss: 0.0408, step time: 1.0437\n",
      "172/295, train_loss: 0.0730, step time: 1.0363\n",
      "173/295, train_loss: 0.0995, step time: 1.0371\n",
      "174/295, train_loss: 0.0178, step time: 1.0337\n",
      "175/295, train_loss: 0.0496, step time: 1.0494\n",
      "176/295, train_loss: 0.1057, step time: 1.1233\n",
      "177/295, train_loss: 0.0564, step time: 1.0687\n",
      "178/295, train_loss: 0.0694, step time: 1.0351\n",
      "179/295, train_loss: 0.0212, step time: 1.0327\n",
      "180/295, train_loss: 0.0797, step time: 1.0777\n",
      "181/295, train_loss: 0.0277, step time: 1.0383\n",
      "182/295, train_loss: 0.0402, step time: 1.0360\n",
      "183/295, train_loss: 0.0506, step time: 1.0498\n",
      "184/295, train_loss: 0.0592, step time: 1.0685\n",
      "185/295, train_loss: 0.0735, step time: 1.0415\n",
      "186/295, train_loss: 0.0774, step time: 1.0557\n",
      "187/295, train_loss: 0.0231, step time: 1.0369\n",
      "188/295, train_loss: 0.0270, step time: 1.0454\n",
      "189/295, train_loss: 0.0586, step time: 1.0623\n",
      "190/295, train_loss: 0.0706, step time: 1.0352\n",
      "191/295, train_loss: 0.0844, step time: 1.0672\n",
      "192/295, train_loss: 0.1368, step time: 1.0333\n",
      "193/295, train_loss: 0.0275, step time: 1.0425\n",
      "194/295, train_loss: 0.0328, step time: 1.0424\n",
      "195/295, train_loss: 0.0353, step time: 1.0322\n",
      "196/295, train_loss: 0.0314, step time: 1.0346\n",
      "197/295, train_loss: 0.0283, step time: 1.0394\n",
      "198/295, train_loss: 0.1177, step time: 1.0354\n",
      "199/295, train_loss: 0.0596, step time: 1.0528\n",
      "200/295, train_loss: 0.0928, step time: 1.0872\n",
      "201/295, train_loss: 0.0508, step time: 1.0373\n",
      "202/295, train_loss: 0.0338, step time: 1.0354\n",
      "203/295, train_loss: 0.3704, step time: 1.0476\n",
      "204/295, train_loss: 0.3596, step time: 1.0440\n",
      "205/295, train_loss: 0.0266, step time: 1.0631\n",
      "206/295, train_loss: 0.0738, step time: 1.0393\n",
      "207/295, train_loss: 0.3773, step time: 1.0399\n",
      "208/295, train_loss: 0.0767, step time: 1.0311\n",
      "209/295, train_loss: 0.0417, step time: 1.0469\n",
      "210/295, train_loss: 0.0827, step time: 1.0448\n",
      "211/295, train_loss: 0.0485, step time: 1.0408\n",
      "212/295, train_loss: 0.0478, step time: 1.0665\n",
      "213/295, train_loss: 0.0679, step time: 1.0323\n",
      "214/295, train_loss: 0.0395, step time: 1.0397\n",
      "215/295, train_loss: 0.0343, step time: 1.0313\n",
      "216/295, train_loss: 0.0440, step time: 1.0330\n",
      "217/295, train_loss: 0.0442, step time: 1.0634\n",
      "218/295, train_loss: 0.0501, step time: 1.0355\n",
      "219/295, train_loss: 0.2091, step time: 1.0484\n",
      "220/295, train_loss: 0.0353, step time: 1.0478\n",
      "221/295, train_loss: 0.3805, step time: 1.0615\n",
      "222/295, train_loss: 0.0516, step time: 1.0538\n",
      "223/295, train_loss: 0.0500, step time: 1.0363\n",
      "224/295, train_loss: 0.0720, step time: 1.0502\n",
      "225/295, train_loss: 0.0249, step time: 1.0602\n",
      "226/295, train_loss: 0.0246, step time: 1.0734\n",
      "227/295, train_loss: 0.0666, step time: 1.0392\n",
      "228/295, train_loss: 0.0598, step time: 1.0416\n",
      "229/295, train_loss: 0.0356, step time: 1.0336\n",
      "230/295, train_loss: 0.0392, step time: 1.0442\n",
      "231/295, train_loss: 0.0304, step time: 1.0544\n",
      "232/295, train_loss: 0.3751, step time: 1.0351\n",
      "233/295, train_loss: 0.0888, step time: 1.0375\n",
      "234/295, train_loss: 0.0507, step time: 1.0519\n",
      "235/295, train_loss: 0.0495, step time: 1.0749\n",
      "236/295, train_loss: 0.0833, step time: 1.0372\n",
      "237/295, train_loss: 0.0764, step time: 1.0354\n",
      "238/295, train_loss: 0.0571, step time: 1.0346\n",
      "239/295, train_loss: 0.0503, step time: 1.0430\n",
      "240/295, train_loss: 0.0644, step time: 1.0856\n",
      "241/295, train_loss: 0.0643, step time: 1.0372\n",
      "242/295, train_loss: 0.0432, step time: 1.0397\n",
      "243/295, train_loss: 0.0741, step time: 1.0511\n",
      "244/295, train_loss: 0.0278, step time: 1.0972\n",
      "245/295, train_loss: 0.3650, step time: 1.0390\n",
      "246/295, train_loss: 0.0322, step time: 1.0416\n",
      "247/295, train_loss: 0.0524, step time: 1.0857\n",
      "248/295, train_loss: 0.0479, step time: 1.0351\n",
      "249/295, train_loss: 0.0296, step time: 1.0417\n",
      "250/295, train_loss: 0.0423, step time: 1.0360\n",
      "251/295, train_loss: 0.0645, step time: 1.0373\n",
      "252/295, train_loss: 0.0570, step time: 1.0352\n",
      "253/295, train_loss: 0.0387, step time: 1.0667\n",
      "254/295, train_loss: 0.0360, step time: 1.0630\n",
      "255/295, train_loss: 0.0802, step time: 1.0382\n",
      "256/295, train_loss: 0.3800, step time: 1.0432\n",
      "257/295, train_loss: 0.0556, step time: 1.0403\n",
      "258/295, train_loss: 0.0360, step time: 1.0520\n",
      "259/295, train_loss: 0.0691, step time: 1.0325\n",
      "260/295, train_loss: 0.0439, step time: 1.0576\n",
      "261/295, train_loss: 0.0475, step time: 1.0602\n",
      "262/295, train_loss: 0.0642, step time: 1.0389\n",
      "263/295, train_loss: 0.0926, step time: 1.0457\n",
      "264/295, train_loss: 0.0496, step time: 1.0496\n",
      "265/295, train_loss: 0.0741, step time: 1.0413\n",
      "266/295, train_loss: 0.0434, step time: 1.0609\n",
      "267/295, train_loss: 0.0606, step time: 1.0388\n",
      "268/295, train_loss: 0.0652, step time: 1.0757\n",
      "269/295, train_loss: 0.0561, step time: 1.0325\n",
      "270/295, train_loss: 0.0466, step time: 1.0405\n",
      "271/295, train_loss: 0.0636, step time: 1.0571\n",
      "272/295, train_loss: 0.0355, step time: 1.0468\n",
      "273/295, train_loss: 0.0915, step time: 1.0376\n",
      "274/295, train_loss: 0.0786, step time: 1.0555\n",
      "275/295, train_loss: 0.0319, step time: 1.0386\n",
      "276/295, train_loss: 0.0553, step time: 1.0352\n",
      "277/295, train_loss: 0.0580, step time: 1.0367\n",
      "278/295, train_loss: 0.0656, step time: 1.0323\n",
      "279/295, train_loss: 0.0287, step time: 1.0640\n",
      "280/295, train_loss: 0.0429, step time: 1.0363\n",
      "281/295, train_loss: 0.0739, step time: 1.0653\n",
      "282/295, train_loss: 0.0648, step time: 1.0555\n",
      "283/295, train_loss: 0.0325, step time: 1.0341\n",
      "284/295, train_loss: 0.0780, step time: 1.0385\n",
      "285/295, train_loss: 0.0423, step time: 1.0477\n",
      "286/295, train_loss: 0.0415, step time: 1.0648\n",
      "287/295, train_loss: 0.0484, step time: 1.0445\n",
      "288/295, train_loss: 0.0765, step time: 1.1027\n",
      "289/295, train_loss: 0.0739, step time: 1.0289\n",
      "290/295, train_loss: 0.0312, step time: 1.0293\n",
      "291/295, train_loss: 0.0718, step time: 1.0296\n",
      "292/295, train_loss: 0.0719, step time: 1.0291\n",
      "293/295, train_loss: 0.0307, step time: 1.0295\n",
      "294/295, train_loss: 0.0566, step time: 1.0296\n",
      "295/295, train_loss: 0.0314, step time: 1.0289\n",
      "epoch 81 average loss: 0.0802\n",
      "current epoch: 81 current mean dice: 0.7595 tc: 0.7140 wt: 0.8255 et: 0.7457\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 81 is: 385.7958\n",
      "----------\n",
      "epoch 82/100\n",
      "1/295, train_loss: 0.0731, step time: 1.0574\n",
      "2/295, train_loss: 0.0356, step time: 1.0839\n",
      "3/295, train_loss: 0.2088, step time: 1.0488\n",
      "4/295, train_loss: 0.0827, step time: 1.0624\n",
      "5/295, train_loss: 0.0211, step time: 1.0424\n",
      "6/295, train_loss: 0.0374, step time: 1.0356\n",
      "7/295, train_loss: 0.0491, step time: 1.0334\n",
      "8/295, train_loss: 0.0311, step time: 1.0452\n",
      "9/295, train_loss: 0.0391, step time: 1.0704\n",
      "10/295, train_loss: 0.0456, step time: 1.0325\n",
      "11/295, train_loss: 0.0375, step time: 1.0543\n",
      "12/295, train_loss: 0.0732, step time: 1.0482\n",
      "13/295, train_loss: 0.0719, step time: 1.0610\n",
      "14/295, train_loss: 0.0584, step time: 1.0330\n",
      "15/295, train_loss: 0.0512, step time: 1.0312\n",
      "16/295, train_loss: 0.0648, step time: 1.0397\n",
      "17/295, train_loss: 0.0790, step time: 1.0418\n",
      "18/295, train_loss: 0.0350, step time: 1.0499\n",
      "19/295, train_loss: 0.0425, step time: 1.0384\n",
      "20/295, train_loss: 0.0680, step time: 1.0442\n",
      "21/295, train_loss: 0.0402, step time: 1.0372\n",
      "22/295, train_loss: 0.0282, step time: 1.0371\n",
      "23/295, train_loss: 0.3565, step time: 1.0476\n",
      "24/295, train_loss: 0.0905, step time: 1.0565\n",
      "25/295, train_loss: 0.0690, step time: 1.0755\n",
      "26/295, train_loss: 0.0277, step time: 1.0740\n",
      "27/295, train_loss: 0.0565, step time: 1.0414\n",
      "28/295, train_loss: 0.0430, step time: 1.0751\n",
      "29/295, train_loss: 0.0784, step time: 1.0360\n",
      "30/295, train_loss: 0.0915, step time: 1.0437\n",
      "31/295, train_loss: 0.0822, step time: 1.0700\n",
      "32/295, train_loss: 0.0244, step time: 1.0367\n",
      "33/295, train_loss: 0.0474, step time: 1.1049\n",
      "34/295, train_loss: 0.0601, step time: 1.0301\n",
      "35/295, train_loss: 0.1222, step time: 1.0389\n",
      "36/295, train_loss: 0.0503, step time: 1.0334\n",
      "37/295, train_loss: 0.0562, step time: 1.0408\n",
      "38/295, train_loss: 0.0398, step time: 1.0774\n",
      "39/295, train_loss: 0.0994, step time: 1.0292\n",
      "40/295, train_loss: 0.0477, step time: 1.0357\n",
      "41/295, train_loss: 0.3779, step time: 1.0446\n",
      "42/295, train_loss: 0.0623, step time: 1.0950\n",
      "43/295, train_loss: 0.0775, step time: 1.0403\n",
      "44/295, train_loss: 0.0771, step time: 1.0913\n",
      "45/295, train_loss: 0.0414, step time: 1.0546\n",
      "46/295, train_loss: 0.0527, step time: 1.0640\n",
      "47/295, train_loss: 0.0560, step time: 1.0413\n",
      "48/295, train_loss: 0.1126, step time: 1.1138\n",
      "49/295, train_loss: 0.0441, step time: 1.0363\n",
      "50/295, train_loss: 0.0708, step time: 1.0600\n",
      "51/295, train_loss: 0.0496, step time: 1.0345\n",
      "52/295, train_loss: 0.0410, step time: 1.0362\n",
      "53/295, train_loss: 0.0823, step time: 1.0451\n",
      "54/295, train_loss: 0.0786, step time: 1.0497\n",
      "55/295, train_loss: 0.0480, step time: 1.0386\n",
      "56/295, train_loss: 0.0176, step time: 1.0345\n",
      "57/295, train_loss: 0.0574, step time: 1.0616\n",
      "58/295, train_loss: 0.0577, step time: 1.0547\n",
      "59/295, train_loss: 0.0760, step time: 1.1497\n",
      "60/295, train_loss: 0.0232, step time: 1.0464\n",
      "61/295, train_loss: 0.0423, step time: 1.0461\n",
      "62/295, train_loss: 0.0752, step time: 1.0621\n",
      "63/295, train_loss: 0.0473, step time: 1.0462\n",
      "64/295, train_loss: 0.0470, step time: 1.0649\n",
      "65/295, train_loss: 0.0663, step time: 1.0339\n",
      "66/295, train_loss: 0.0339, step time: 1.0455\n",
      "67/295, train_loss: 0.0553, step time: 1.0970\n",
      "68/295, train_loss: 0.1165, step time: 1.0414\n",
      "69/295, train_loss: 0.0277, step time: 1.0302\n",
      "70/295, train_loss: 0.0234, step time: 1.0380\n",
      "71/295, train_loss: 0.0847, step time: 1.0472\n",
      "72/295, train_loss: 0.3614, step time: 1.1089\n",
      "73/295, train_loss: 0.0444, step time: 1.0312\n",
      "74/295, train_loss: 0.0404, step time: 1.0538\n",
      "75/295, train_loss: 0.0948, step time: 1.0415\n",
      "76/295, train_loss: 0.0358, step time: 1.0342\n",
      "77/295, train_loss: 0.3670, step time: 1.0413\n",
      "78/295, train_loss: 0.0841, step time: 1.0405\n",
      "79/295, train_loss: 0.0591, step time: 1.0864\n",
      "80/295, train_loss: 0.0495, step time: 1.0713\n",
      "81/295, train_loss: 0.0356, step time: 1.0731\n",
      "82/295, train_loss: 0.0469, step time: 1.0536\n",
      "83/295, train_loss: 0.0630, step time: 1.0400\n",
      "84/295, train_loss: 0.3738, step time: 1.0454\n",
      "85/295, train_loss: 0.0466, step time: 1.0549\n",
      "86/295, train_loss: 0.0379, step time: 1.0555\n",
      "87/295, train_loss: 0.0526, step time: 1.0654\n",
      "88/295, train_loss: 0.0412, step time: 1.0445\n",
      "89/295, train_loss: 0.0575, step time: 1.0891\n",
      "90/295, train_loss: 0.0646, step time: 1.0386\n",
      "91/295, train_loss: 0.0471, step time: 1.0622\n",
      "92/295, train_loss: 0.0388, step time: 1.0377\n",
      "93/295, train_loss: 0.0720, step time: 1.0956\n",
      "94/295, train_loss: 0.3770, step time: 1.0360\n",
      "95/295, train_loss: 0.0449, step time: 1.0541\n",
      "96/295, train_loss: 0.0799, step time: 1.0447\n",
      "97/295, train_loss: 0.0257, step time: 1.0319\n",
      "98/295, train_loss: 0.0762, step time: 1.0473\n",
      "99/295, train_loss: 0.0629, step time: 1.0787\n",
      "100/295, train_loss: 0.0638, step time: 1.0476\n",
      "101/295, train_loss: 0.3693, step time: 1.0367\n",
      "102/295, train_loss: 0.0404, step time: 1.0409\n",
      "103/295, train_loss: 0.0698, step time: 1.1915\n",
      "104/295, train_loss: 0.0859, step time: 1.0430\n",
      "105/295, train_loss: 0.0305, step time: 1.0641\n",
      "106/295, train_loss: 0.0932, step time: 1.0705\n",
      "107/295, train_loss: 0.0761, step time: 1.0753\n",
      "108/295, train_loss: 0.0265, step time: 1.0341\n",
      "109/295, train_loss: 0.0594, step time: 1.0387\n",
      "110/295, train_loss: 0.0944, step time: 1.0595\n",
      "111/295, train_loss: 0.1015, step time: 1.0613\n",
      "112/295, train_loss: 0.0680, step time: 1.0418\n",
      "113/295, train_loss: 0.0676, step time: 1.0363\n",
      "114/295, train_loss: 0.3792, step time: 1.0425\n",
      "115/295, train_loss: 0.0351, step time: 1.0590\n",
      "116/295, train_loss: 0.0347, step time: 1.0704\n",
      "117/295, train_loss: 0.0349, step time: 1.1271\n",
      "118/295, train_loss: 0.0489, step time: 1.0634\n",
      "119/295, train_loss: 0.0453, step time: 1.0349\n",
      "120/295, train_loss: 0.0500, step time: 1.0526\n",
      "121/295, train_loss: 0.0732, step time: 1.0390\n",
      "122/295, train_loss: 0.0283, step time: 1.0776\n",
      "123/295, train_loss: 0.0588, step time: 1.0352\n",
      "124/295, train_loss: 0.0514, step time: 1.0296\n",
      "125/295, train_loss: 0.0733, step time: 1.0615\n",
      "126/295, train_loss: 0.0756, step time: 1.0407\n",
      "127/295, train_loss: 0.0258, step time: 1.0330\n",
      "128/295, train_loss: 0.0556, step time: 1.0861\n",
      "129/295, train_loss: 0.0344, step time: 1.0987\n",
      "130/295, train_loss: 0.0282, step time: 1.0345\n",
      "131/295, train_loss: 0.0372, step time: 1.0312\n",
      "132/295, train_loss: 0.0275, step time: 1.0565\n",
      "133/295, train_loss: 0.0259, step time: 1.0610\n",
      "134/295, train_loss: 0.0598, step time: 1.0883\n",
      "135/295, train_loss: 0.0906, step time: 1.0451\n",
      "136/295, train_loss: 0.0257, step time: 1.0341\n",
      "137/295, train_loss: 0.0535, step time: 1.0389\n",
      "138/295, train_loss: 0.0381, step time: 1.0537\n",
      "139/295, train_loss: 0.0310, step time: 1.0367\n",
      "140/295, train_loss: 0.1161, step time: 1.0427\n",
      "141/295, train_loss: 0.0548, step time: 1.0406\n",
      "142/295, train_loss: 0.0465, step time: 1.0410\n",
      "143/295, train_loss: 0.0544, step time: 1.0677\n",
      "144/295, train_loss: 0.0233, step time: 1.0339\n",
      "145/295, train_loss: 0.0541, step time: 1.0465\n",
      "146/295, train_loss: 0.0761, step time: 1.0703\n",
      "147/295, train_loss: 0.1309, step time: 1.0771\n",
      "148/295, train_loss: 0.0916, step time: 1.0328\n",
      "149/295, train_loss: 0.0553, step time: 1.0406\n",
      "150/295, train_loss: 0.3636, step time: 1.1035\n",
      "151/295, train_loss: 0.0493, step time: 1.0327\n",
      "152/295, train_loss: 0.0804, step time: 1.0332\n",
      "153/295, train_loss: 0.0823, step time: 1.0631\n",
      "154/295, train_loss: 0.0560, step time: 1.0482\n",
      "155/295, train_loss: 0.0451, step time: 1.1321\n",
      "156/295, train_loss: 0.0769, step time: 1.0937\n",
      "157/295, train_loss: 0.0592, step time: 1.0554\n",
      "158/295, train_loss: 0.0621, step time: 1.1071\n",
      "159/295, train_loss: 0.0733, step time: 1.1263\n",
      "160/295, train_loss: 0.0315, step time: 1.0342\n",
      "161/295, train_loss: 0.0585, step time: 1.0588\n",
      "162/295, train_loss: 0.0770, step time: 1.0504\n",
      "163/295, train_loss: 0.0262, step time: 1.0661\n",
      "164/295, train_loss: 0.1015, step time: 1.0398\n",
      "165/295, train_loss: 0.0566, step time: 1.0539\n",
      "166/295, train_loss: 0.0965, step time: 1.0649\n",
      "167/295, train_loss: 0.0398, step time: 1.0614\n",
      "168/295, train_loss: 0.3772, step time: 1.0934\n",
      "169/295, train_loss: 0.3616, step time: 1.0365\n",
      "170/295, train_loss: 0.0238, step time: 1.0391\n",
      "171/295, train_loss: 0.0696, step time: 1.1032\n",
      "172/295, train_loss: 0.0522, step time: 1.0483\n",
      "173/295, train_loss: 0.0611, step time: 1.0495\n",
      "174/295, train_loss: 0.1069, step time: 1.0590\n",
      "175/295, train_loss: 0.0435, step time: 1.0690\n",
      "176/295, train_loss: 0.0784, step time: 1.0628\n",
      "177/295, train_loss: 0.0696, step time: 1.0528\n",
      "178/295, train_loss: 0.0314, step time: 1.0464\n",
      "179/295, train_loss: 0.0320, step time: 1.0583\n",
      "180/295, train_loss: 0.0287, step time: 1.0403\n",
      "181/295, train_loss: 0.0635, step time: 1.0564\n",
      "182/295, train_loss: 0.3581, step time: 1.0546\n",
      "183/295, train_loss: 0.0478, step time: 1.0486\n",
      "184/295, train_loss: 0.3792, step time: 1.0422\n",
      "185/295, train_loss: 0.0578, step time: 1.0357\n",
      "186/295, train_loss: 0.0322, step time: 1.0342\n",
      "187/295, train_loss: 0.0419, step time: 1.0358\n",
      "188/295, train_loss: 0.0303, step time: 1.0463\n",
      "189/295, train_loss: 0.0450, step time: 1.0557\n",
      "190/295, train_loss: 0.0512, step time: 1.0686\n",
      "191/295, train_loss: 0.0354, step time: 1.0432\n",
      "192/295, train_loss: 0.1302, step time: 1.0390\n",
      "193/295, train_loss: 0.0604, step time: 1.0420\n",
      "194/295, train_loss: 0.0378, step time: 1.0400\n",
      "195/295, train_loss: 0.0478, step time: 1.0713\n",
      "196/295, train_loss: 0.4277, step time: 1.0413\n",
      "197/295, train_loss: 0.0792, step time: 1.0371\n",
      "198/295, train_loss: 0.3756, step time: 1.0353\n",
      "199/295, train_loss: 0.0496, step time: 1.0362\n",
      "200/295, train_loss: 0.1083, step time: 1.0810\n",
      "201/295, train_loss: 0.0794, step time: 1.0366\n",
      "202/295, train_loss: 0.0633, step time: 1.0354\n",
      "203/295, train_loss: 0.0643, step time: 1.0591\n",
      "204/295, train_loss: 0.0319, step time: 1.0687\n",
      "205/295, train_loss: 0.0401, step time: 1.0402\n",
      "206/295, train_loss: 0.0726, step time: 1.0376\n",
      "207/295, train_loss: 0.0388, step time: 1.0399\n",
      "208/295, train_loss: 0.0400, step time: 1.0609\n",
      "209/295, train_loss: 0.0618, step time: 1.0337\n",
      "210/295, train_loss: 0.0311, step time: 1.0967\n",
      "211/295, train_loss: 0.3754, step time: 1.0444\n",
      "212/295, train_loss: 0.3899, step time: 1.0358\n",
      "213/295, train_loss: 0.0494, step time: 1.0451\n",
      "214/295, train_loss: 0.3595, step time: 1.0553\n",
      "215/295, train_loss: 0.0745, step time: 1.0446\n",
      "216/295, train_loss: 0.0252, step time: 1.0354\n",
      "217/295, train_loss: 0.0322, step time: 1.0427\n",
      "218/295, train_loss: 0.0337, step time: 1.0326\n",
      "219/295, train_loss: 0.0495, step time: 1.0556\n",
      "220/295, train_loss: 0.0643, step time: 1.0906\n",
      "221/295, train_loss: 0.0296, step time: 1.0355\n",
      "222/295, train_loss: 0.0480, step time: 1.0397\n",
      "223/295, train_loss: 0.0393, step time: 1.1112\n",
      "224/295, train_loss: 0.1000, step time: 1.1015\n",
      "225/295, train_loss: 0.0879, step time: 1.0325\n",
      "226/295, train_loss: 0.0725, step time: 1.0350\n",
      "227/295, train_loss: 0.0554, step time: 1.0380\n",
      "228/295, train_loss: 0.0709, step time: 1.0639\n",
      "229/295, train_loss: 0.0265, step time: 1.0566\n",
      "230/295, train_loss: 0.0483, step time: 1.0595\n",
      "231/295, train_loss: 0.0928, step time: 1.0724\n",
      "232/295, train_loss: 0.0259, step time: 1.0356\n",
      "233/295, train_loss: 0.0406, step time: 1.0397\n",
      "234/295, train_loss: 0.3846, step time: 1.0491\n",
      "235/295, train_loss: 0.0309, step time: 1.0526\n",
      "236/295, train_loss: 0.0535, step time: 1.0369\n",
      "237/295, train_loss: 0.0442, step time: 1.0307\n",
      "238/295, train_loss: 0.0448, step time: 1.0447\n",
      "239/295, train_loss: 0.0339, step time: 1.0574\n",
      "240/295, train_loss: 0.0543, step time: 1.0445\n",
      "241/295, train_loss: 0.0442, step time: 1.0592\n",
      "242/295, train_loss: 0.0428, step time: 1.0508\n",
      "243/295, train_loss: 0.0277, step time: 1.0452\n",
      "244/295, train_loss: 0.0512, step time: 1.0365\n",
      "245/295, train_loss: 0.0356, step time: 1.0676\n",
      "246/295, train_loss: 0.0486, step time: 1.0431\n",
      "247/295, train_loss: 0.0415, step time: 1.0352\n",
      "248/295, train_loss: 0.0512, step time: 1.0362\n",
      "249/295, train_loss: 0.0612, step time: 1.0575\n",
      "250/295, train_loss: 0.0308, step time: 1.0613\n",
      "251/295, train_loss: 0.3678, step time: 1.0409\n",
      "252/295, train_loss: 0.0585, step time: 1.0397\n",
      "253/295, train_loss: 0.0644, step time: 1.0773\n",
      "254/295, train_loss: 0.0684, step time: 1.0353\n",
      "255/295, train_loss: 0.0584, step time: 1.0611\n",
      "256/295, train_loss: 0.0387, step time: 1.0466\n",
      "257/295, train_loss: 0.0296, step time: 1.0371\n",
      "258/295, train_loss: 0.0711, step time: 1.0356\n",
      "259/295, train_loss: 0.0296, step time: 1.0357\n",
      "260/295, train_loss: 0.0286, step time: 1.0359\n",
      "261/295, train_loss: 0.0465, step time: 1.0389\n",
      "262/295, train_loss: 0.0730, step time: 1.0406\n",
      "263/295, train_loss: 0.0926, step time: 1.1162\n",
      "264/295, train_loss: 0.0454, step time: 1.0616\n",
      "265/295, train_loss: 0.0234, step time: 1.0481\n",
      "266/295, train_loss: 0.0519, step time: 1.0599\n",
      "267/295, train_loss: 0.0411, step time: 1.0324\n",
      "268/295, train_loss: 0.3788, step time: 1.0328\n",
      "269/295, train_loss: 0.0689, step time: 1.0433\n",
      "270/295, train_loss: 0.0689, step time: 1.0872\n",
      "271/295, train_loss: 0.0229, step time: 1.0887\n",
      "272/295, train_loss: 0.0272, step time: 1.0800\n",
      "273/295, train_loss: 0.0822, step time: 1.0325\n",
      "274/295, train_loss: 0.0732, step time: 1.0369\n",
      "275/295, train_loss: 0.0753, step time: 1.0453\n",
      "276/295, train_loss: 0.0340, step time: 1.0332\n",
      "277/295, train_loss: 0.1092, step time: 1.0435\n",
      "278/295, train_loss: 0.0624, step time: 1.0577\n",
      "279/295, train_loss: 0.0747, step time: 1.0458\n",
      "280/295, train_loss: 0.0493, step time: 1.0450\n",
      "281/295, train_loss: 0.0350, step time: 1.0399\n",
      "282/295, train_loss: 0.0926, step time: 1.0505\n",
      "283/295, train_loss: 0.0739, step time: 1.0766\n",
      "284/295, train_loss: 0.3949, step time: 1.0356\n",
      "285/295, train_loss: 0.0741, step time: 1.0795\n",
      "286/295, train_loss: 0.0426, step time: 1.0525\n",
      "287/295, train_loss: 0.0616, step time: 1.0559\n",
      "288/295, train_loss: 0.0453, step time: 1.0459\n",
      "289/295, train_loss: 0.0272, step time: 1.0328\n",
      "290/295, train_loss: 0.0367, step time: 1.0296\n",
      "291/295, train_loss: 0.0374, step time: 1.0296\n",
      "292/295, train_loss: 0.0291, step time: 1.0305\n",
      "293/295, train_loss: 0.0308, step time: 1.0286\n",
      "294/295, train_loss: 0.0585, step time: 1.0287\n",
      "295/295, train_loss: 0.0631, step time: 1.0290\n",
      "epoch 82 average loss: 0.0798\n",
      "current epoch: 82 current mean dice: 0.7671 tc: 0.7212 wt: 0.8303 et: 0.7543\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 82 is: 388.6198\n",
      "----------\n",
      "epoch 83/100\n",
      "1/295, train_loss: 0.0502, step time: 1.0771\n",
      "2/295, train_loss: 0.0747, step time: 1.1606\n",
      "3/295, train_loss: 0.0745, step time: 1.0696\n",
      "4/295, train_loss: 0.1095, step time: 1.0936\n",
      "5/295, train_loss: 0.0424, step time: 1.0424\n",
      "6/295, train_loss: 0.0642, step time: 1.0580\n",
      "7/295, train_loss: 0.0781, step time: 1.0621\n",
      "8/295, train_loss: 0.0541, step time: 1.0670\n",
      "9/295, train_loss: 0.0821, step time: 1.0440\n",
      "10/295, train_loss: 0.0485, step time: 1.0312\n",
      "11/295, train_loss: 0.0292, step time: 1.0351\n",
      "12/295, train_loss: 0.0273, step time: 1.0311\n",
      "13/295, train_loss: 0.0806, step time: 1.0346\n",
      "14/295, train_loss: 0.0245, step time: 1.0542\n",
      "15/295, train_loss: 0.0343, step time: 1.0752\n",
      "16/295, train_loss: 0.0605, step time: 1.0559\n",
      "17/295, train_loss: 0.0525, step time: 1.0425\n",
      "18/295, train_loss: 0.3657, step time: 1.0345\n",
      "19/295, train_loss: 0.0513, step time: 1.0387\n",
      "20/295, train_loss: 0.0541, step time: 1.0375\n",
      "21/295, train_loss: 0.0666, step time: 1.0408\n",
      "22/295, train_loss: 0.0339, step time: 1.0967\n",
      "23/295, train_loss: 0.3832, step time: 1.0338\n",
      "24/295, train_loss: 0.0634, step time: 1.0342\n",
      "25/295, train_loss: 0.0478, step time: 1.0439\n",
      "26/295, train_loss: 0.0337, step time: 1.0530\n",
      "27/295, train_loss: 0.0825, step time: 1.0369\n",
      "28/295, train_loss: 0.0464, step time: 1.0344\n",
      "29/295, train_loss: 0.3629, step time: 1.0328\n",
      "30/295, train_loss: 0.0505, step time: 1.0399\n",
      "31/295, train_loss: 0.0555, step time: 1.0457\n",
      "32/295, train_loss: 0.0819, step time: 1.0788\n",
      "33/295, train_loss: 0.0274, step time: 1.0723\n",
      "34/295, train_loss: 0.0784, step time: 1.0370\n",
      "35/295, train_loss: 0.0276, step time: 1.0326\n",
      "36/295, train_loss: 0.0702, step time: 1.0440\n",
      "37/295, train_loss: 0.0453, step time: 1.0515\n",
      "38/295, train_loss: 0.3781, step time: 1.0515\n",
      "39/295, train_loss: 0.0308, step time: 1.0583\n",
      "40/295, train_loss: 0.2064, step time: 1.0343\n",
      "41/295, train_loss: 0.3752, step time: 1.0469\n",
      "42/295, train_loss: 0.0732, step time: 1.0846\n",
      "43/295, train_loss: 0.0739, step time: 1.0395\n",
      "44/295, train_loss: 0.0228, step time: 1.0393\n",
      "45/295, train_loss: 0.0645, step time: 1.0465\n",
      "46/295, train_loss: 0.0739, step time: 1.0492\n",
      "47/295, train_loss: 0.0607, step time: 1.0378\n",
      "48/295, train_loss: 0.0645, step time: 1.0370\n",
      "49/295, train_loss: 0.0748, step time: 1.0544\n",
      "50/295, train_loss: 0.3562, step time: 1.0582\n",
      "51/295, train_loss: 0.0379, step time: 1.0386\n",
      "52/295, train_loss: 0.0730, step time: 1.0314\n",
      "53/295, train_loss: 0.0681, step time: 1.0387\n",
      "54/295, train_loss: 0.0617, step time: 1.0442\n",
      "55/295, train_loss: 0.0496, step time: 1.1142\n",
      "56/295, train_loss: 0.3779, step time: 1.0639\n",
      "57/295, train_loss: 0.0496, step time: 1.0332\n",
      "58/295, train_loss: 0.0366, step time: 1.0639\n",
      "59/295, train_loss: 0.3610, step time: 1.0515\n",
      "60/295, train_loss: 0.0943, step time: 1.0427\n",
      "61/295, train_loss: 0.0607, step time: 1.0441\n",
      "62/295, train_loss: 0.0434, step time: 1.0569\n",
      "63/295, train_loss: 0.0679, step time: 1.0458\n",
      "64/295, train_loss: 0.0571, step time: 1.0713\n",
      "65/295, train_loss: 0.0319, step time: 1.0389\n",
      "66/295, train_loss: 0.0486, step time: 1.0325\n",
      "67/295, train_loss: 0.0315, step time: 1.0893\n",
      "68/295, train_loss: 0.0510, step time: 1.0372\n",
      "69/295, train_loss: 0.1052, step time: 1.0430\n",
      "70/295, train_loss: 0.0629, step time: 1.0384\n",
      "71/295, train_loss: 0.1008, step time: 1.0728\n",
      "72/295, train_loss: 0.0234, step time: 1.0321\n",
      "73/295, train_loss: 0.0749, step time: 1.0381\n",
      "74/295, train_loss: 0.0275, step time: 1.0631\n",
      "75/295, train_loss: 0.0406, step time: 1.0567\n",
      "76/295, train_loss: 0.0325, step time: 1.0541\n",
      "77/295, train_loss: 0.0461, step time: 1.0386\n",
      "78/295, train_loss: 0.0309, step time: 1.0377\n",
      "79/295, train_loss: 0.0394, step time: 1.0582\n",
      "80/295, train_loss: 0.3776, step time: 1.0945\n",
      "81/295, train_loss: 0.0687, step time: 1.0481\n",
      "82/295, train_loss: 0.0703, step time: 1.0595\n",
      "83/295, train_loss: 0.0567, step time: 1.0433\n",
      "84/295, train_loss: 0.0757, step time: 1.0364\n",
      "85/295, train_loss: 0.0299, step time: 1.1020\n",
      "86/295, train_loss: 0.1064, step time: 1.0456\n",
      "87/295, train_loss: 0.0391, step time: 1.0341\n",
      "88/295, train_loss: 0.0417, step time: 1.0329\n",
      "89/295, train_loss: 0.0684, step time: 1.1150\n",
      "90/295, train_loss: 0.3781, step time: 1.1106\n",
      "91/295, train_loss: 0.0497, step time: 1.0638\n",
      "92/295, train_loss: 0.0528, step time: 1.0461\n",
      "93/295, train_loss: 0.0501, step time: 1.0379\n",
      "94/295, train_loss: 0.0597, step time: 1.0868\n",
      "95/295, train_loss: 0.0580, step time: 1.0741\n",
      "96/295, train_loss: 0.0696, step time: 1.0452\n",
      "97/295, train_loss: 0.0469, step time: 1.0869\n",
      "98/295, train_loss: 0.0485, step time: 1.0359\n",
      "99/295, train_loss: 0.0547, step time: 1.0357\n",
      "100/295, train_loss: 0.0972, step time: 1.0637\n",
      "101/295, train_loss: 0.3740, step time: 1.0361\n",
      "102/295, train_loss: 0.0481, step time: 1.0351\n",
      "103/295, train_loss: 0.0270, step time: 1.0470\n",
      "104/295, train_loss: 0.0715, step time: 1.0532\n",
      "105/295, train_loss: 0.0583, step time: 1.0518\n",
      "106/295, train_loss: 0.0252, step time: 1.0463\n",
      "107/295, train_loss: 0.0729, step time: 1.0529\n",
      "108/295, train_loss: 0.0233, step time: 1.0607\n",
      "109/295, train_loss: 0.0482, step time: 1.0478\n",
      "110/295, train_loss: 0.0473, step time: 1.0387\n",
      "111/295, train_loss: 0.0249, step time: 1.0395\n",
      "112/295, train_loss: 0.0294, step time: 1.0334\n",
      "113/295, train_loss: 0.0686, step time: 1.0690\n",
      "114/295, train_loss: 0.0606, step time: 1.0574\n",
      "115/295, train_loss: 0.0209, step time: 1.0372\n",
      "116/295, train_loss: 0.4236, step time: 1.0327\n",
      "117/295, train_loss: 0.0303, step time: 1.0416\n",
      "118/295, train_loss: 0.0293, step time: 1.0406\n",
      "119/295, train_loss: 0.0368, step time: 1.0792\n",
      "120/295, train_loss: 0.0798, step time: 1.0514\n",
      "121/295, train_loss: 0.0471, step time: 1.0370\n",
      "122/295, train_loss: 0.0258, step time: 1.0702\n",
      "123/295, train_loss: 0.0796, step time: 1.0336\n",
      "124/295, train_loss: 0.0490, step time: 1.0315\n",
      "125/295, train_loss: 0.0476, step time: 1.0356\n",
      "126/295, train_loss: 0.0527, step time: 1.0329\n",
      "127/295, train_loss: 0.0261, step time: 1.0372\n",
      "128/295, train_loss: 0.0412, step time: 1.0615\n",
      "129/295, train_loss: 0.0431, step time: 1.0662\n",
      "130/295, train_loss: 0.0546, step time: 1.0405\n",
      "131/295, train_loss: 0.0576, step time: 1.0381\n",
      "132/295, train_loss: 0.0317, step time: 1.0770\n",
      "133/295, train_loss: 0.0687, step time: 1.0758\n",
      "134/295, train_loss: 0.1300, step time: 1.0322\n",
      "135/295, train_loss: 0.0252, step time: 1.0344\n",
      "136/295, train_loss: 0.1002, step time: 1.0569\n",
      "137/295, train_loss: 0.0548, step time: 1.0467\n",
      "138/295, train_loss: 0.0513, step time: 1.0378\n",
      "139/295, train_loss: 0.0350, step time: 1.0595\n",
      "140/295, train_loss: 0.0361, step time: 1.0525\n",
      "141/295, train_loss: 0.0620, step time: 1.0329\n",
      "142/295, train_loss: 0.0756, step time: 1.0551\n",
      "143/295, train_loss: 0.0602, step time: 1.0793\n",
      "144/295, train_loss: 0.0340, step time: 1.0757\n",
      "145/295, train_loss: 0.0935, step time: 1.0397\n",
      "146/295, train_loss: 0.0716, step time: 1.0397\n",
      "147/295, train_loss: 0.0563, step time: 1.0422\n",
      "148/295, train_loss: 0.0821, step time: 1.0805\n",
      "149/295, train_loss: 0.0478, step time: 1.0458\n",
      "150/295, train_loss: 0.0498, step time: 1.0381\n",
      "151/295, train_loss: 0.0432, step time: 1.0324\n",
      "152/295, train_loss: 0.3665, step time: 1.0420\n",
      "153/295, train_loss: 0.0521, step time: 1.0651\n",
      "154/295, train_loss: 0.1186, step time: 1.0410\n",
      "155/295, train_loss: 0.0449, step time: 1.0610\n",
      "156/295, train_loss: 0.0373, step time: 1.0520\n",
      "157/295, train_loss: 0.0381, step time: 1.0480\n",
      "158/295, train_loss: 0.0506, step time: 1.0358\n",
      "159/295, train_loss: 0.0916, step time: 1.0533\n",
      "160/295, train_loss: 0.0377, step time: 1.0692\n",
      "161/295, train_loss: 0.0733, step time: 1.0790\n",
      "162/295, train_loss: 0.0565, step time: 1.0352\n",
      "163/295, train_loss: 0.0572, step time: 1.0674\n",
      "164/295, train_loss: 0.1171, step time: 1.0382\n",
      "165/295, train_loss: 0.0629, step time: 1.0488\n",
      "166/295, train_loss: 0.0913, step time: 1.0344\n",
      "167/295, train_loss: 0.0792, step time: 1.0354\n",
      "168/295, train_loss: 0.0277, step time: 1.0408\n",
      "169/295, train_loss: 0.0433, step time: 1.0339\n",
      "170/295, train_loss: 0.0388, step time: 1.0459\n",
      "171/295, train_loss: 0.0414, step time: 1.0373\n",
      "172/295, train_loss: 0.0391, step time: 1.0406\n",
      "173/295, train_loss: 0.0413, step time: 1.1057\n",
      "174/295, train_loss: 0.0908, step time: 1.0362\n",
      "175/295, train_loss: 0.0727, step time: 1.0450\n",
      "176/295, train_loss: 0.0678, step time: 1.2206\n",
      "177/295, train_loss: 0.0529, step time: 1.0664\n",
      "178/295, train_loss: 0.0641, step time: 1.0678\n",
      "179/295, train_loss: 0.0342, step time: 1.0668\n",
      "180/295, train_loss: 0.0724, step time: 1.0606\n",
      "181/295, train_loss: 0.1120, step time: 1.0380\n",
      "182/295, train_loss: 0.0552, step time: 1.0373\n",
      "183/295, train_loss: 0.0436, step time: 1.0363\n",
      "184/295, train_loss: 0.0726, step time: 1.0345\n",
      "185/295, train_loss: 0.0560, step time: 1.0499\n",
      "186/295, train_loss: 0.0309, step time: 1.0336\n",
      "187/295, train_loss: 0.0626, step time: 1.0491\n",
      "188/295, train_loss: 0.0473, step time: 1.0732\n",
      "189/295, train_loss: 0.0481, step time: 1.0599\n",
      "190/295, train_loss: 0.0786, step time: 1.0400\n",
      "191/295, train_loss: 0.0952, step time: 1.0836\n",
      "192/295, train_loss: 0.0717, step time: 1.0537\n",
      "193/295, train_loss: 0.0566, step time: 1.0579\n",
      "194/295, train_loss: 0.0335, step time: 1.0344\n",
      "195/295, train_loss: 0.0635, step time: 1.0424\n",
      "196/295, train_loss: 0.0464, step time: 1.0483\n",
      "197/295, train_loss: 0.0661, step time: 1.0346\n",
      "198/295, train_loss: 0.0795, step time: 1.0418\n",
      "199/295, train_loss: 0.0410, step time: 1.0562\n",
      "200/295, train_loss: 0.0583, step time: 1.0441\n",
      "201/295, train_loss: 0.3789, step time: 1.0414\n",
      "202/295, train_loss: 0.3757, step time: 1.0473\n",
      "203/295, train_loss: 0.0404, step time: 1.0363\n",
      "204/295, train_loss: 0.0345, step time: 1.0374\n",
      "205/295, train_loss: 0.0997, step time: 1.0409\n",
      "206/295, train_loss: 0.0399, step time: 1.0972\n",
      "207/295, train_loss: 0.0256, step time: 1.0378\n",
      "208/295, train_loss: 0.0434, step time: 1.0357\n",
      "209/295, train_loss: 0.3575, step time: 1.0335\n",
      "210/295, train_loss: 0.0455, step time: 1.0347\n",
      "211/295, train_loss: 0.0638, step time: 1.0720\n",
      "212/295, train_loss: 0.0871, step time: 1.0612\n",
      "213/295, train_loss: 0.0303, step time: 1.1135\n",
      "214/295, train_loss: 0.3950, step time: 1.0469\n",
      "215/295, train_loss: 0.0376, step time: 1.0354\n",
      "216/295, train_loss: 0.0834, step time: 1.0448\n",
      "217/295, train_loss: 0.0284, step time: 1.0347\n",
      "218/295, train_loss: 0.0339, step time: 1.0503\n",
      "219/295, train_loss: 0.0299, step time: 1.0416\n",
      "220/295, train_loss: 0.0800, step time: 1.0435\n",
      "221/295, train_loss: 0.0527, step time: 1.0403\n",
      "222/295, train_loss: 0.3602, step time: 1.0565\n",
      "223/295, train_loss: 0.0271, step time: 1.0374\n",
      "224/295, train_loss: 0.0479, step time: 1.0353\n",
      "225/295, train_loss: 0.0291, step time: 1.0334\n",
      "226/295, train_loss: 0.0910, step time: 1.0385\n",
      "227/295, train_loss: 0.0383, step time: 1.0643\n",
      "228/295, train_loss: 0.0265, step time: 1.0621\n",
      "229/295, train_loss: 0.0766, step time: 1.0375\n",
      "230/295, train_loss: 0.0577, step time: 1.0350\n",
      "231/295, train_loss: 0.0314, step time: 1.0347\n",
      "232/295, train_loss: 0.0542, step time: 1.0494\n",
      "233/295, train_loss: 0.0254, step time: 1.0387\n",
      "234/295, train_loss: 0.0358, step time: 1.0363\n",
      "235/295, train_loss: 0.0452, step time: 1.0709\n",
      "236/295, train_loss: 0.0472, step time: 1.1017\n",
      "237/295, train_loss: 0.3610, step time: 1.0554\n",
      "238/295, train_loss: 0.3699, step time: 1.0927\n",
      "239/295, train_loss: 0.1147, step time: 1.0509\n",
      "240/295, train_loss: 0.0577, step time: 1.0393\n",
      "241/295, train_loss: 0.0447, step time: 1.0409\n",
      "242/295, train_loss: 0.0764, step time: 1.0426\n",
      "243/295, train_loss: 0.0181, step time: 1.0434\n",
      "244/295, train_loss: 0.0308, step time: 1.0394\n",
      "245/295, train_loss: 0.0903, step time: 1.0374\n",
      "246/295, train_loss: 0.0349, step time: 1.0393\n",
      "247/295, train_loss: 0.0715, step time: 1.0381\n",
      "248/295, train_loss: 0.0750, step time: 1.0404\n",
      "249/295, train_loss: 0.0494, step time: 1.0601\n",
      "250/295, train_loss: 0.0393, step time: 1.0711\n",
      "251/295, train_loss: 0.0903, step time: 1.0348\n",
      "252/295, train_loss: 0.0922, step time: 1.0672\n",
      "253/295, train_loss: 0.0325, step time: 1.0374\n",
      "254/295, train_loss: 0.0230, step time: 1.0346\n",
      "255/295, train_loss: 0.0355, step time: 1.0632\n",
      "256/295, train_loss: 0.0352, step time: 1.0436\n",
      "257/295, train_loss: 0.0430, step time: 1.0425\n",
      "258/295, train_loss: 0.0427, step time: 1.0374\n",
      "259/295, train_loss: 0.0260, step time: 1.0381\n",
      "260/295, train_loss: 0.1305, step time: 1.0365\n",
      "261/295, train_loss: 0.0274, step time: 1.0633\n",
      "262/295, train_loss: 0.0356, step time: 1.0355\n",
      "263/295, train_loss: 0.0521, step time: 1.0311\n",
      "264/295, train_loss: 0.0757, step time: 1.0368\n",
      "265/295, train_loss: 0.3872, step time: 1.0640\n",
      "266/295, train_loss: 0.0234, step time: 1.0445\n",
      "267/295, train_loss: 0.0268, step time: 1.0460\n",
      "268/295, train_loss: 0.0407, step time: 1.0845\n",
      "269/295, train_loss: 0.0402, step time: 1.0358\n",
      "270/295, train_loss: 0.0438, step time: 1.0396\n",
      "271/295, train_loss: 0.0500, step time: 1.0378\n",
      "272/295, train_loss: 0.0384, step time: 1.0602\n",
      "273/295, train_loss: 0.0477, step time: 1.0390\n",
      "274/295, train_loss: 0.3748, step time: 1.0397\n",
      "275/295, train_loss: 0.0405, step time: 1.0414\n",
      "276/295, train_loss: 0.0632, step time: 1.0467\n",
      "277/295, train_loss: 0.0558, step time: 1.0333\n",
      "278/295, train_loss: 0.0926, step time: 1.0539\n",
      "279/295, train_loss: 0.0837, step time: 1.0606\n",
      "280/295, train_loss: 0.0588, step time: 1.0369\n",
      "281/295, train_loss: 0.0696, step time: 1.0378\n",
      "282/295, train_loss: 0.0316, step time: 1.0399\n",
      "283/295, train_loss: 0.0616, step time: 1.0797\n",
      "284/295, train_loss: 0.0430, step time: 1.0655\n",
      "285/295, train_loss: 0.0717, step time: 1.0720\n",
      "286/295, train_loss: 0.0585, step time: 1.0724\n",
      "287/295, train_loss: 0.0365, step time: 1.0687\n",
      "288/295, train_loss: 0.0664, step time: 1.0637\n",
      "289/295, train_loss: 0.0351, step time: 1.0308\n",
      "290/295, train_loss: 0.0240, step time: 1.0298\n",
      "291/295, train_loss: 0.0712, step time: 1.0300\n",
      "292/295, train_loss: 0.0743, step time: 1.0291\n",
      "293/295, train_loss: 0.0782, step time: 1.0292\n",
      "294/295, train_loss: 0.0399, step time: 1.0302\n",
      "295/295, train_loss: 0.0472, step time: 1.0303\n",
      "epoch 83 average loss: 0.0792\n",
      "current epoch: 83 current mean dice: 0.7616 tc: 0.7115 wt: 0.8277 et: 0.7492\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 83 is: 383.0652\n",
      "----------\n",
      "epoch 84/100\n",
      "1/295, train_loss: 0.0593, step time: 1.0912\n",
      "2/295, train_loss: 0.0294, step time: 1.0454\n",
      "3/295, train_loss: 0.0509, step time: 1.0517\n",
      "4/295, train_loss: 0.0203, step time: 1.0340\n",
      "5/295, train_loss: 0.0250, step time: 1.1310\n",
      "6/295, train_loss: 0.0353, step time: 1.0865\n",
      "7/295, train_loss: 0.0712, step time: 1.0323\n",
      "8/295, train_loss: 0.0663, step time: 1.0330\n",
      "9/295, train_loss: 0.0599, step time: 1.0304\n",
      "10/295, train_loss: 0.0256, step time: 1.0370\n",
      "11/295, train_loss: 0.0502, step time: 1.0312\n",
      "12/295, train_loss: 0.0407, step time: 1.0760\n",
      "13/295, train_loss: 0.0578, step time: 1.0611\n",
      "14/295, train_loss: 0.0996, step time: 1.0739\n",
      "15/295, train_loss: 0.3572, step time: 1.0472\n",
      "16/295, train_loss: 0.0358, step time: 1.0333\n",
      "17/295, train_loss: 0.0268, step time: 1.0411\n",
      "18/295, train_loss: 0.0339, step time: 1.1361\n",
      "19/295, train_loss: 0.0319, step time: 1.0391\n",
      "20/295, train_loss: 0.0542, step time: 1.0316\n",
      "21/295, train_loss: 0.0471, step time: 1.0324\n",
      "22/295, train_loss: 0.0271, step time: 1.1060\n",
      "23/295, train_loss: 0.0681, step time: 1.0396\n",
      "24/295, train_loss: 0.0741, step time: 1.0396\n",
      "25/295, train_loss: 0.0353, step time: 1.0549\n",
      "26/295, train_loss: 0.0442, step time: 1.0405\n",
      "27/295, train_loss: 0.0434, step time: 1.0522\n",
      "28/295, train_loss: 0.0228, step time: 1.0356\n",
      "29/295, train_loss: 0.0271, step time: 1.0485\n",
      "30/295, train_loss: 0.0770, step time: 1.0363\n",
      "31/295, train_loss: 0.0753, step time: 1.0359\n",
      "32/295, train_loss: 0.0477, step time: 1.0442\n",
      "33/295, train_loss: 0.0575, step time: 1.0367\n",
      "34/295, train_loss: 0.0334, step time: 1.0546\n",
      "35/295, train_loss: 0.0895, step time: 1.0466\n",
      "36/295, train_loss: 0.0504, step time: 1.0413\n",
      "37/295, train_loss: 0.0893, step time: 1.0308\n",
      "38/295, train_loss: 0.0828, step time: 1.0373\n",
      "39/295, train_loss: 0.1067, step time: 1.0345\n",
      "40/295, train_loss: 0.0370, step time: 1.0335\n",
      "41/295, train_loss: 0.0896, step time: 1.0540\n",
      "42/295, train_loss: 0.0872, step time: 1.0405\n",
      "43/295, train_loss: 0.0343, step time: 1.0387\n",
      "44/295, train_loss: 0.0638, step time: 1.0547\n",
      "45/295, train_loss: 0.0573, step time: 1.0782\n",
      "46/295, train_loss: 0.0537, step time: 1.0663\n",
      "47/295, train_loss: 0.0712, step time: 1.0389\n",
      "48/295, train_loss: 0.0472, step time: 1.0390\n",
      "49/295, train_loss: 0.0685, step time: 1.0360\n",
      "50/295, train_loss: 0.0493, step time: 1.0540\n",
      "51/295, train_loss: 0.3728, step time: 1.0324\n",
      "52/295, train_loss: 0.0719, step time: 1.0396\n",
      "53/295, train_loss: 0.0512, step time: 1.0867\n",
      "54/295, train_loss: 0.0305, step time: 1.0556\n",
      "55/295, train_loss: 0.0297, step time: 1.0355\n",
      "56/295, train_loss: 0.0346, step time: 1.0412\n",
      "57/295, train_loss: 0.0323, step time: 1.0420\n",
      "58/295, train_loss: 0.0302, step time: 1.0483\n",
      "59/295, train_loss: 0.0593, step time: 1.0597\n",
      "60/295, train_loss: 0.0610, step time: 1.0568\n",
      "61/295, train_loss: 0.0466, step time: 1.0729\n",
      "62/295, train_loss: 0.0410, step time: 1.0590\n",
      "63/295, train_loss: 0.3659, step time: 1.0384\n",
      "64/295, train_loss: 0.0522, step time: 1.0514\n",
      "65/295, train_loss: 0.0562, step time: 1.0912\n",
      "66/295, train_loss: 0.0691, step time: 1.0329\n",
      "67/295, train_loss: 0.0695, step time: 1.0406\n",
      "68/295, train_loss: 0.0668, step time: 1.0412\n",
      "69/295, train_loss: 0.0557, step time: 1.0329\n",
      "70/295, train_loss: 0.0642, step time: 1.0357\n",
      "71/295, train_loss: 0.0385, step time: 1.0330\n",
      "72/295, train_loss: 0.0395, step time: 1.0637\n",
      "73/295, train_loss: 0.0335, step time: 1.0665\n",
      "74/295, train_loss: 0.0575, step time: 1.0306\n",
      "75/295, train_loss: 0.0429, step time: 1.0311\n",
      "76/295, train_loss: 0.0780, step time: 1.0333\n",
      "77/295, train_loss: 0.0592, step time: 1.0515\n",
      "78/295, train_loss: 0.0255, step time: 1.0475\n",
      "79/295, train_loss: 0.0490, step time: 1.0302\n",
      "80/295, train_loss: 0.0431, step time: 1.0412\n",
      "81/295, train_loss: 0.0691, step time: 1.0388\n",
      "82/295, train_loss: 0.0521, step time: 1.0558\n",
      "83/295, train_loss: 0.0320, step time: 1.0644\n",
      "84/295, train_loss: 0.3557, step time: 1.0326\n",
      "85/295, train_loss: 0.0516, step time: 1.0380\n",
      "86/295, train_loss: 0.0631, step time: 1.0338\n",
      "87/295, train_loss: 0.0820, step time: 1.0745\n",
      "88/295, train_loss: 0.0987, step time: 1.0418\n",
      "89/295, train_loss: 0.0261, step time: 1.0347\n",
      "90/295, train_loss: 0.0585, step time: 1.0622\n",
      "91/295, train_loss: 0.0577, step time: 1.0575\n",
      "92/295, train_loss: 0.0918, step time: 1.0570\n",
      "93/295, train_loss: 0.0551, step time: 1.0613\n",
      "94/295, train_loss: 0.0597, step time: 1.0778\n",
      "95/295, train_loss: 0.0313, step time: 1.0388\n",
      "96/295, train_loss: 0.1154, step time: 1.0292\n",
      "97/295, train_loss: 0.0709, step time: 1.0425\n",
      "98/295, train_loss: 0.0500, step time: 1.0325\n",
      "99/295, train_loss: 0.0335, step time: 1.0702\n",
      "100/295, train_loss: 0.1283, step time: 1.0359\n",
      "101/295, train_loss: 0.0701, step time: 1.0329\n",
      "102/295, train_loss: 0.0234, step time: 1.0409\n",
      "103/295, train_loss: 0.0470, step time: 1.0797\n",
      "104/295, train_loss: 0.0428, step time: 1.0379\n",
      "105/295, train_loss: 0.0310, step time: 1.0535\n",
      "106/295, train_loss: 0.0486, step time: 1.0362\n",
      "107/295, train_loss: 0.0556, step time: 1.0351\n",
      "108/295, train_loss: 0.0402, step time: 1.0434\n",
      "109/295, train_loss: 0.0293, step time: 1.0964\n",
      "110/295, train_loss: 0.1124, step time: 1.0413\n",
      "111/295, train_loss: 0.0764, step time: 1.0474\n",
      "112/295, train_loss: 0.0625, step time: 1.0448\n",
      "113/295, train_loss: 0.0290, step time: 1.0622\n",
      "114/295, train_loss: 0.0451, step time: 1.0343\n",
      "115/295, train_loss: 0.0794, step time: 1.0371\n",
      "116/295, train_loss: 0.0564, step time: 1.0439\n",
      "117/295, train_loss: 0.0752, step time: 1.0734\n",
      "118/295, train_loss: 0.0296, step time: 1.0558\n",
      "119/295, train_loss: 0.0176, step time: 1.0505\n",
      "120/295, train_loss: 0.0408, step time: 1.0438\n",
      "121/295, train_loss: 0.0347, step time: 1.0633\n",
      "122/295, train_loss: 0.0471, step time: 1.0379\n",
      "123/295, train_loss: 0.0256, step time: 1.0401\n",
      "124/295, train_loss: 0.3779, step time: 1.0657\n",
      "125/295, train_loss: 0.0584, step time: 1.0932\n",
      "126/295, train_loss: 0.3766, step time: 1.0307\n",
      "127/295, train_loss: 0.0445, step time: 1.0381\n",
      "128/295, train_loss: 0.0809, step time: 1.0354\n",
      "129/295, train_loss: 0.0307, step time: 1.0399\n",
      "130/295, train_loss: 0.0409, step time: 1.0710\n",
      "131/295, train_loss: 0.0428, step time: 1.0541\n",
      "132/295, train_loss: 0.0534, step time: 1.0342\n",
      "133/295, train_loss: 0.1145, step time: 1.0444\n",
      "134/295, train_loss: 0.0601, step time: 1.0507\n",
      "135/295, train_loss: 0.3734, step time: 1.1288\n",
      "136/295, train_loss: 0.3630, step time: 1.0364\n",
      "137/295, train_loss: 0.0750, step time: 1.0674\n",
      "138/295, train_loss: 0.0413, step time: 1.0397\n",
      "139/295, train_loss: 0.0518, step time: 1.0358\n",
      "140/295, train_loss: 0.0465, step time: 1.0622\n",
      "141/295, train_loss: 0.0476, step time: 1.0323\n",
      "142/295, train_loss: 0.0608, step time: 1.0362\n",
      "143/295, train_loss: 0.0501, step time: 1.0892\n",
      "144/295, train_loss: 0.0309, step time: 1.0597\n",
      "145/295, train_loss: 0.0487, step time: 1.0531\n",
      "146/295, train_loss: 0.0483, step time: 1.0447\n",
      "147/295, train_loss: 0.0583, step time: 1.0374\n",
      "148/295, train_loss: 0.0617, step time: 1.0456\n",
      "149/295, train_loss: 0.0277, step time: 1.0637\n",
      "150/295, train_loss: 0.0825, step time: 1.0549\n",
      "151/295, train_loss: 0.0630, step time: 1.0490\n",
      "152/295, train_loss: 0.0743, step time: 1.0397\n",
      "153/295, train_loss: 0.0453, step time: 1.0401\n",
      "154/295, train_loss: 0.0436, step time: 1.0590\n",
      "155/295, train_loss: 0.0544, step time: 1.1041\n",
      "156/295, train_loss: 0.0380, step time: 1.0378\n",
      "157/295, train_loss: 0.0350, step time: 1.0495\n",
      "158/295, train_loss: 0.0400, step time: 1.0385\n",
      "159/295, train_loss: 0.0722, step time: 1.0374\n",
      "160/295, train_loss: 0.0812, step time: 1.0383\n",
      "161/295, train_loss: 0.0378, step time: 1.0826\n",
      "162/295, train_loss: 0.0601, step time: 1.0409\n",
      "163/295, train_loss: 0.0446, step time: 1.0801\n",
      "164/295, train_loss: 0.0749, step time: 1.0464\n",
      "165/295, train_loss: 0.0688, step time: 1.0778\n",
      "166/295, train_loss: 0.0316, step time: 1.0359\n",
      "167/295, train_loss: 0.0650, step time: 1.0536\n",
      "168/295, train_loss: 0.0794, step time: 1.0912\n",
      "169/295, train_loss: 0.0261, step time: 1.0783\n",
      "170/295, train_loss: 0.0405, step time: 1.0319\n",
      "171/295, train_loss: 0.3604, step time: 1.0399\n",
      "172/295, train_loss: 0.0370, step time: 1.0347\n",
      "173/295, train_loss: 0.0917, step time: 1.0358\n",
      "174/295, train_loss: 0.0707, step time: 1.0467\n",
      "175/295, train_loss: 0.0227, step time: 1.0358\n",
      "176/295, train_loss: 0.3947, step time: 1.0414\n",
      "177/295, train_loss: 0.0244, step time: 1.0860\n",
      "178/295, train_loss: 0.0394, step time: 1.0523\n",
      "179/295, train_loss: 0.0632, step time: 1.0527\n",
      "180/295, train_loss: 0.1283, step time: 1.0391\n",
      "181/295, train_loss: 0.0937, step time: 1.0432\n",
      "182/295, train_loss: 0.4232, step time: 1.0477\n",
      "183/295, train_loss: 0.0454, step time: 1.0638\n",
      "184/295, train_loss: 0.0921, step time: 1.0322\n",
      "185/295, train_loss: 0.0786, step time: 1.0398\n",
      "186/295, train_loss: 0.0656, step time: 1.0651\n",
      "187/295, train_loss: 0.0823, step time: 1.0389\n",
      "188/295, train_loss: 0.0464, step time: 1.0312\n",
      "189/295, train_loss: 0.0787, step time: 1.0389\n",
      "190/295, train_loss: 0.0755, step time: 1.0372\n",
      "191/295, train_loss: 0.0571, step time: 1.0653\n",
      "192/295, train_loss: 0.0543, step time: 1.0671\n",
      "193/295, train_loss: 0.0978, step time: 1.0394\n",
      "194/295, train_loss: 0.0616, step time: 1.0421\n",
      "195/295, train_loss: 0.3872, step time: 1.0371\n",
      "196/295, train_loss: 0.0250, step time: 1.0409\n",
      "197/295, train_loss: 0.0366, step time: 1.0407\n",
      "198/295, train_loss: 0.0429, step time: 1.0517\n",
      "199/295, train_loss: 0.1166, step time: 1.0424\n",
      "200/295, train_loss: 0.0669, step time: 1.0349\n",
      "201/295, train_loss: 0.0384, step time: 1.0360\n",
      "202/295, train_loss: 0.0311, step time: 1.0340\n",
      "203/295, train_loss: 0.0898, step time: 1.0367\n",
      "204/295, train_loss: 0.2067, step time: 1.0396\n",
      "205/295, train_loss: 0.0539, step time: 1.1071\n",
      "206/295, train_loss: 0.0707, step time: 1.0369\n",
      "207/295, train_loss: 0.0392, step time: 1.0351\n",
      "208/295, train_loss: 0.0551, step time: 1.0419\n",
      "209/295, train_loss: 0.1074, step time: 1.0439\n",
      "210/295, train_loss: 0.0240, step time: 1.0645\n",
      "211/295, train_loss: 0.0347, step time: 1.0627\n",
      "212/295, train_loss: 0.0784, step time: 1.0418\n",
      "213/295, train_loss: 0.0504, step time: 1.0834\n",
      "214/295, train_loss: 0.0431, step time: 1.0440\n",
      "215/295, train_loss: 0.0479, step time: 1.0441\n",
      "216/295, train_loss: 0.0669, step time: 1.0551\n",
      "217/295, train_loss: 0.0267, step time: 1.0328\n",
      "218/295, train_loss: 0.0452, step time: 1.0331\n",
      "219/295, train_loss: 0.3703, step time: 1.0474\n",
      "220/295, train_loss: 0.0522, step time: 1.0482\n",
      "221/295, train_loss: 0.0389, step time: 1.0542\n",
      "222/295, train_loss: 0.0427, step time: 1.0447\n",
      "223/295, train_loss: 0.0685, step time: 1.0536\n",
      "224/295, train_loss: 0.0533, step time: 1.0565\n",
      "225/295, train_loss: 0.0383, step time: 1.0328\n",
      "226/295, train_loss: 0.0646, step time: 1.0614\n",
      "227/295, train_loss: 0.0720, step time: 1.0420\n",
      "228/295, train_loss: 0.0711, step time: 1.0336\n",
      "229/295, train_loss: 0.0318, step time: 1.0353\n",
      "230/295, train_loss: 0.0909, step time: 1.0649\n",
      "231/295, train_loss: 0.0735, step time: 1.0316\n",
      "232/295, train_loss: 0.3839, step time: 1.0740\n",
      "233/295, train_loss: 0.0467, step time: 1.0351\n",
      "234/295, train_loss: 0.0840, step time: 1.0329\n",
      "235/295, train_loss: 0.0575, step time: 1.0349\n",
      "236/295, train_loss: 0.3690, step time: 1.0325\n",
      "237/295, train_loss: 0.0360, step time: 1.0414\n",
      "238/295, train_loss: 0.0752, step time: 1.0422\n",
      "239/295, train_loss: 0.0342, step time: 1.0408\n",
      "240/295, train_loss: 0.0641, step time: 1.0584\n",
      "241/295, train_loss: 0.0377, step time: 1.0703\n",
      "242/295, train_loss: 0.3744, step time: 1.0350\n",
      "243/295, train_loss: 0.0397, step time: 1.0355\n",
      "244/295, train_loss: 0.0500, step time: 1.0462\n",
      "245/295, train_loss: 0.0351, step time: 1.0631\n",
      "246/295, train_loss: 0.0488, step time: 1.0693\n",
      "247/295, train_loss: 0.1002, step time: 1.0477\n",
      "248/295, train_loss: 0.0494, step time: 1.0427\n",
      "249/295, train_loss: 0.0917, step time: 1.0476\n",
      "250/295, train_loss: 0.0463, step time: 1.0413\n",
      "251/295, train_loss: 0.0272, step time: 1.0364\n",
      "252/295, train_loss: 0.3786, step time: 1.0339\n",
      "253/295, train_loss: 0.0538, step time: 1.0430\n",
      "254/295, train_loss: 0.0740, step time: 1.0373\n",
      "255/295, train_loss: 0.0504, step time: 1.0319\n",
      "256/295, train_loss: 0.0618, step time: 1.0593\n",
      "257/295, train_loss: 0.0676, step time: 1.0983\n",
      "258/295, train_loss: 0.0252, step time: 1.0342\n",
      "259/295, train_loss: 0.0310, step time: 1.0356\n",
      "260/295, train_loss: 0.0312, step time: 1.0414\n",
      "261/295, train_loss: 0.0375, step time: 1.0448\n",
      "262/295, train_loss: 0.0440, step time: 1.0401\n",
      "263/295, train_loss: 0.0225, step time: 1.0702\n",
      "264/295, train_loss: 0.0237, step time: 1.0602\n",
      "265/295, train_loss: 0.0274, step time: 1.0770\n",
      "266/295, train_loss: 0.0412, step time: 1.0345\n",
      "267/295, train_loss: 0.0771, step time: 1.0497\n",
      "268/295, train_loss: 0.3790, step time: 1.0569\n",
      "269/295, train_loss: 0.0333, step time: 1.0507\n",
      "270/295, train_loss: 0.0726, step time: 1.0586\n",
      "271/295, train_loss: 0.0277, step time: 1.0578\n",
      "272/295, train_loss: 0.0480, step time: 1.0396\n",
      "273/295, train_loss: 0.1043, step time: 1.0382\n",
      "274/295, train_loss: 0.0742, step time: 1.0392\n",
      "275/295, train_loss: 0.3607, step time: 1.0808\n",
      "276/295, train_loss: 0.0781, step time: 1.0366\n",
      "277/295, train_loss: 0.0371, step time: 1.0459\n",
      "278/295, train_loss: 0.3596, step time: 1.0396\n",
      "279/295, train_loss: 0.0623, step time: 1.0662\n",
      "280/295, train_loss: 0.0478, step time: 1.0354\n",
      "281/295, train_loss: 0.0279, step time: 1.0374\n",
      "282/295, train_loss: 0.0951, step time: 1.0441\n",
      "283/295, train_loss: 0.0427, step time: 1.0566\n",
      "284/295, train_loss: 0.0273, step time: 1.0365\n",
      "285/295, train_loss: 0.3784, step time: 1.0612\n",
      "286/295, train_loss: 0.0691, step time: 1.0582\n",
      "287/295, train_loss: 0.0470, step time: 1.0398\n",
      "288/295, train_loss: 0.3757, step time: 1.0453\n",
      "289/295, train_loss: 0.0425, step time: 1.0284\n",
      "290/295, train_loss: 0.0460, step time: 1.0295\n",
      "291/295, train_loss: 0.0492, step time: 1.0300\n",
      "292/295, train_loss: 0.0730, step time: 1.0299\n",
      "293/295, train_loss: 0.0298, step time: 1.0297\n",
      "294/295, train_loss: 0.0251, step time: 1.0291\n",
      "295/295, train_loss: 0.0743, step time: 1.0282\n",
      "epoch 84 average loss: 0.0789\n",
      "current epoch: 84 current mean dice: 0.7552 tc: 0.7080 wt: 0.8265 et: 0.7372\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 84 is: 380.8152\n",
      "----------\n",
      "epoch 85/100\n",
      "1/295, train_loss: 0.0279, step time: 1.0673\n",
      "2/295, train_loss: 0.0403, step time: 1.0652\n",
      "3/295, train_loss: 0.0783, step time: 1.0435\n",
      "4/295, train_loss: 0.1057, step time: 1.0582\n",
      "5/295, train_loss: 0.0248, step time: 1.0515\n",
      "6/295, train_loss: 0.0506, step time: 1.0308\n",
      "7/295, train_loss: 0.0497, step time: 1.0360\n",
      "8/295, train_loss: 0.0222, step time: 1.0355\n",
      "9/295, train_loss: 0.0451, step time: 1.0377\n",
      "10/295, train_loss: 0.0333, step time: 1.0409\n",
      "11/295, train_loss: 0.0313, step time: 1.0328\n",
      "12/295, train_loss: 0.0463, step time: 1.0375\n",
      "13/295, train_loss: 0.0293, step time: 1.0304\n",
      "14/295, train_loss: 0.3778, step time: 1.1069\n",
      "15/295, train_loss: 0.0424, step time: 1.0386\n",
      "16/295, train_loss: 0.0434, step time: 1.0316\n",
      "17/295, train_loss: 0.0731, step time: 1.0478\n",
      "18/295, train_loss: 0.0269, step time: 1.1127\n",
      "19/295, train_loss: 0.0599, step time: 1.0486\n",
      "20/295, train_loss: 0.0500, step time: 1.0593\n",
      "21/295, train_loss: 0.0434, step time: 1.0528\n",
      "22/295, train_loss: 0.0632, step time: 1.0340\n",
      "23/295, train_loss: 0.0365, step time: 1.0424\n",
      "24/295, train_loss: 0.0800, step time: 1.0651\n",
      "25/295, train_loss: 0.0351, step time: 1.0619\n",
      "26/295, train_loss: 0.0612, step time: 1.0395\n",
      "27/295, train_loss: 0.0923, step time: 1.1019\n",
      "28/295, train_loss: 0.0530, step time: 1.0414\n",
      "29/295, train_loss: 0.0592, step time: 1.0687\n",
      "30/295, train_loss: 0.0805, step time: 1.0473\n",
      "31/295, train_loss: 0.0493, step time: 1.0379\n",
      "32/295, train_loss: 0.0714, step time: 1.0374\n",
      "33/295, train_loss: 0.0752, step time: 1.0337\n",
      "34/295, train_loss: 0.0440, step time: 1.0333\n",
      "35/295, train_loss: 0.0712, step time: 1.0485\n",
      "36/295, train_loss: 0.0772, step time: 1.0737\n",
      "37/295, train_loss: 0.0507, step time: 1.0376\n",
      "38/295, train_loss: 0.0317, step time: 1.0393\n",
      "39/295, train_loss: 0.3679, step time: 1.0373\n",
      "40/295, train_loss: 0.0270, step time: 1.0568\n",
      "41/295, train_loss: 0.0435, step time: 1.0584\n",
      "42/295, train_loss: 0.0408, step time: 1.0523\n",
      "43/295, train_loss: 0.0313, step time: 1.0388\n",
      "44/295, train_loss: 0.0308, step time: 1.0555\n",
      "45/295, train_loss: 0.0280, step time: 1.0391\n",
      "46/295, train_loss: 0.3609, step time: 1.0589\n",
      "47/295, train_loss: 0.0348, step time: 1.0645\n",
      "48/295, train_loss: 0.0369, step time: 1.0356\n",
      "49/295, train_loss: 0.4243, step time: 1.0394\n",
      "50/295, train_loss: 0.0542, step time: 1.0362\n",
      "51/295, train_loss: 0.0743, step time: 1.0602\n",
      "52/295, train_loss: 0.0374, step time: 1.0423\n",
      "53/295, train_loss: 0.1147, step time: 1.0345\n",
      "54/295, train_loss: 0.0921, step time: 1.0547\n",
      "55/295, train_loss: 0.0448, step time: 1.1454\n",
      "56/295, train_loss: 0.0742, step time: 1.0788\n",
      "57/295, train_loss: 0.0488, step time: 1.0312\n",
      "58/295, train_loss: 0.0320, step time: 1.0423\n",
      "59/295, train_loss: 0.3787, step time: 1.0574\n",
      "60/295, train_loss: 0.0246, step time: 1.0420\n",
      "61/295, train_loss: 0.0671, step time: 1.0473\n",
      "62/295, train_loss: 0.0579, step time: 1.0400\n",
      "63/295, train_loss: 0.3943, step time: 1.0443\n",
      "64/295, train_loss: 0.0520, step time: 1.0390\n",
      "65/295, train_loss: 0.0230, step time: 1.0349\n",
      "66/295, train_loss: 0.0466, step time: 1.0461\n",
      "67/295, train_loss: 0.0792, step time: 1.0375\n",
      "68/295, train_loss: 0.0638, step time: 1.0417\n",
      "69/295, train_loss: 0.0782, step time: 1.0496\n",
      "70/295, train_loss: 0.0399, step time: 1.0391\n",
      "71/295, train_loss: 0.0636, step time: 1.0521\n",
      "72/295, train_loss: 0.0307, step time: 1.0434\n",
      "73/295, train_loss: 0.3592, step time: 1.0320\n",
      "74/295, train_loss: 0.0294, step time: 1.0356\n",
      "75/295, train_loss: 0.0639, step time: 1.0368\n",
      "76/295, train_loss: 0.0422, step time: 1.1376\n",
      "77/295, train_loss: 0.0673, step time: 1.0908\n",
      "78/295, train_loss: 0.0604, step time: 1.0541\n",
      "79/295, train_loss: 0.0271, step time: 1.0352\n",
      "80/295, train_loss: 0.0909, step time: 1.1019\n",
      "81/295, train_loss: 0.0258, step time: 1.0483\n",
      "82/295, train_loss: 0.0328, step time: 1.0470\n",
      "83/295, train_loss: 0.0353, step time: 1.0697\n",
      "84/295, train_loss: 0.1289, step time: 1.0418\n",
      "85/295, train_loss: 0.0476, step time: 1.0311\n",
      "86/295, train_loss: 0.0609, step time: 1.0605\n",
      "87/295, train_loss: 0.0537, step time: 1.0485\n",
      "88/295, train_loss: 0.0670, step time: 1.0645\n",
      "89/295, train_loss: 0.0342, step time: 1.0438\n",
      "90/295, train_loss: 0.3744, step time: 1.0610\n",
      "91/295, train_loss: 0.0379, step time: 1.0512\n",
      "92/295, train_loss: 0.0337, step time: 1.0636\n",
      "93/295, train_loss: 0.0741, step time: 1.0411\n",
      "94/295, train_loss: 0.0407, step time: 1.0640\n",
      "95/295, train_loss: 0.0798, step time: 1.0606\n",
      "96/295, train_loss: 0.0207, step time: 1.0499\n",
      "97/295, train_loss: 0.3776, step time: 1.0384\n",
      "98/295, train_loss: 0.0483, step time: 1.0407\n",
      "99/295, train_loss: 0.0814, step time: 1.0532\n",
      "100/295, train_loss: 0.0539, step time: 1.0453\n",
      "101/295, train_loss: 0.0619, step time: 1.0591\n",
      "102/295, train_loss: 0.0352, step time: 1.0479\n",
      "103/295, train_loss: 0.0540, step time: 1.0435\n",
      "104/295, train_loss: 0.0700, step time: 1.0556\n",
      "105/295, train_loss: 0.0414, step time: 1.0398\n",
      "106/295, train_loss: 0.0685, step time: 1.0602\n",
      "107/295, train_loss: 0.0680, step time: 1.0652\n",
      "108/295, train_loss: 0.0766, step time: 1.0521\n",
      "109/295, train_loss: 0.0380, step time: 1.0857\n",
      "110/295, train_loss: 0.0361, step time: 1.0531\n",
      "111/295, train_loss: 0.0569, step time: 1.0313\n",
      "112/295, train_loss: 0.3707, step time: 1.0491\n",
      "113/295, train_loss: 0.0502, step time: 1.0333\n",
      "114/295, train_loss: 0.0302, step time: 1.0605\n",
      "115/295, train_loss: 0.0251, step time: 1.0322\n",
      "116/295, train_loss: 0.0733, step time: 1.0416\n",
      "117/295, train_loss: 0.0428, step time: 1.0438\n",
      "118/295, train_loss: 0.0708, step time: 1.0636\n",
      "119/295, train_loss: 0.0688, step time: 1.0379\n",
      "120/295, train_loss: 0.0816, step time: 1.0659\n",
      "121/295, train_loss: 0.3756, step time: 1.0570\n",
      "122/295, train_loss: 0.0998, step time: 1.0649\n",
      "123/295, train_loss: 0.0989, step time: 1.0353\n",
      "124/295, train_loss: 0.0260, step time: 1.0596\n",
      "125/295, train_loss: 0.0545, step time: 1.0768\n",
      "126/295, train_loss: 0.0367, step time: 1.0358\n",
      "127/295, train_loss: 0.0481, step time: 1.0394\n",
      "128/295, train_loss: 0.0465, step time: 1.0313\n",
      "129/295, train_loss: 0.0413, step time: 1.0631\n",
      "130/295, train_loss: 0.1001, step time: 1.0378\n",
      "131/295, train_loss: 0.0470, step time: 1.0617\n",
      "132/295, train_loss: 0.0572, step time: 1.0738\n",
      "133/295, train_loss: 0.0530, step time: 1.0309\n",
      "134/295, train_loss: 0.0753, step time: 1.0363\n",
      "135/295, train_loss: 0.0622, step time: 1.0827\n",
      "136/295, train_loss: 0.0334, step time: 1.0371\n",
      "137/295, train_loss: 0.0228, step time: 1.0551\n",
      "138/295, train_loss: 0.0650, step time: 1.0747\n",
      "139/295, train_loss: 0.0252, step time: 1.0513\n",
      "140/295, train_loss: 0.0366, step time: 1.0360\n",
      "141/295, train_loss: 0.3759, step time: 1.0717\n",
      "142/295, train_loss: 0.0294, step time: 1.0402\n",
      "143/295, train_loss: 0.0519, step time: 1.0321\n",
      "144/295, train_loss: 0.1117, step time: 1.0634\n",
      "145/295, train_loss: 0.0892, step time: 1.0500\n",
      "146/295, train_loss: 0.0427, step time: 1.0539\n",
      "147/295, train_loss: 0.0276, step time: 1.0418\n",
      "148/295, train_loss: 0.0314, step time: 1.0956\n",
      "149/295, train_loss: 0.0236, step time: 1.0747\n",
      "150/295, train_loss: 0.0716, step time: 1.0495\n",
      "151/295, train_loss: 0.0781, step time: 1.0572\n",
      "152/295, train_loss: 0.0341, step time: 1.0360\n",
      "153/295, train_loss: 0.0669, step time: 1.0653\n",
      "154/295, train_loss: 0.0355, step time: 1.0383\n",
      "155/295, train_loss: 0.0752, step time: 1.0411\n",
      "156/295, train_loss: 0.0563, step time: 1.0353\n",
      "157/295, train_loss: 0.0409, step time: 1.1203\n",
      "158/295, train_loss: 0.0611, step time: 1.1302\n",
      "159/295, train_loss: 0.0311, step time: 1.0366\n",
      "160/295, train_loss: 0.0284, step time: 1.0317\n",
      "161/295, train_loss: 0.0689, step time: 1.0375\n",
      "162/295, train_loss: 0.0459, step time: 1.0365\n",
      "163/295, train_loss: 0.0391, step time: 1.0363\n",
      "164/295, train_loss: 0.0512, step time: 1.0655\n",
      "165/295, train_loss: 0.0303, step time: 1.0669\n",
      "166/295, train_loss: 0.3781, step time: 1.0408\n",
      "167/295, train_loss: 0.0333, step time: 1.0521\n",
      "168/295, train_loss: 0.0622, step time: 1.0382\n",
      "169/295, train_loss: 0.0343, step time: 1.0373\n",
      "170/295, train_loss: 0.0825, step time: 1.0408\n",
      "171/295, train_loss: 0.0490, step time: 1.0333\n",
      "172/295, train_loss: 0.0804, step time: 1.0411\n",
      "173/295, train_loss: 0.0411, step time: 1.0558\n",
      "174/295, train_loss: 0.0524, step time: 1.0488\n",
      "175/295, train_loss: 0.0590, step time: 1.0624\n",
      "176/295, train_loss: 0.0682, step time: 1.0381\n",
      "177/295, train_loss: 0.3736, step time: 1.0314\n",
      "178/295, train_loss: 0.0889, step time: 1.1109\n",
      "179/295, train_loss: 0.0376, step time: 1.0415\n",
      "180/295, train_loss: 0.1153, step time: 1.0340\n",
      "181/295, train_loss: 0.3561, step time: 1.0385\n",
      "182/295, train_loss: 0.0553, step time: 1.0511\n",
      "183/295, train_loss: 0.0253, step time: 1.0385\n",
      "184/295, train_loss: 0.0943, step time: 1.0901\n",
      "185/295, train_loss: 0.0290, step time: 1.0650\n",
      "186/295, train_loss: 0.0734, step time: 1.0724\n",
      "187/295, train_loss: 0.0424, step time: 1.0944\n",
      "188/295, train_loss: 0.0575, step time: 1.0347\n",
      "189/295, train_loss: 0.0611, step time: 1.0376\n",
      "190/295, train_loss: 0.0554, step time: 1.0335\n",
      "191/295, train_loss: 0.0379, step time: 1.0376\n",
      "192/295, train_loss: 0.0247, step time: 1.0460\n",
      "193/295, train_loss: 0.0305, step time: 1.0584\n",
      "194/295, train_loss: 0.0398, step time: 1.0584\n",
      "195/295, train_loss: 0.0470, step time: 1.0384\n",
      "196/295, train_loss: 0.0235, step time: 1.0369\n",
      "197/295, train_loss: 0.0748, step time: 1.0522\n",
      "198/295, train_loss: 0.0484, step time: 1.0365\n",
      "199/295, train_loss: 0.0575, step time: 1.0378\n",
      "200/295, train_loss: 0.0548, step time: 1.0397\n",
      "201/295, train_loss: 0.0523, step time: 1.0923\n",
      "202/295, train_loss: 0.0446, step time: 1.0941\n",
      "203/295, train_loss: 0.3629, step time: 1.0342\n",
      "204/295, train_loss: 0.0628, step time: 1.0468\n",
      "205/295, train_loss: 0.0496, step time: 1.0913\n",
      "206/295, train_loss: 0.0457, step time: 1.0402\n",
      "207/295, train_loss: 0.0506, step time: 1.0381\n",
      "208/295, train_loss: 0.0501, step time: 1.0569\n",
      "209/295, train_loss: 0.0463, step time: 1.0387\n",
      "210/295, train_loss: 0.0393, step time: 1.0416\n",
      "211/295, train_loss: 0.0579, step time: 1.0774\n",
      "212/295, train_loss: 0.0833, step time: 1.0848\n",
      "213/295, train_loss: 0.0661, step time: 1.0529\n",
      "214/295, train_loss: 0.0686, step time: 1.0350\n",
      "215/295, train_loss: 0.0302, step time: 1.0311\n",
      "216/295, train_loss: 0.0557, step time: 1.0513\n",
      "217/295, train_loss: 0.0572, step time: 1.0340\n",
      "218/295, train_loss: 0.3658, step time: 1.0505\n",
      "219/295, train_loss: 0.0391, step time: 1.1311\n",
      "220/295, train_loss: 0.0549, step time: 1.0332\n",
      "221/295, train_loss: 0.0471, step time: 1.1139\n",
      "222/295, train_loss: 0.0311, step time: 1.0492\n",
      "223/295, train_loss: 0.0448, step time: 1.0383\n",
      "224/295, train_loss: 0.0564, step time: 1.0425\n",
      "225/295, train_loss: 0.0530, step time: 1.0650\n",
      "226/295, train_loss: 0.2061, step time: 1.0571\n",
      "227/295, train_loss: 0.0501, step time: 1.0862\n",
      "228/295, train_loss: 0.3604, step time: 1.0752\n",
      "229/295, train_loss: 0.0321, step time: 1.0455\n",
      "230/295, train_loss: 0.0176, step time: 1.0392\n",
      "231/295, train_loss: 0.0352, step time: 1.0353\n",
      "232/295, train_loss: 0.0470, step time: 1.0339\n",
      "233/295, train_loss: 0.3872, step time: 1.0404\n",
      "234/295, train_loss: 0.0460, step time: 1.0379\n",
      "235/295, train_loss: 0.0383, step time: 1.0624\n",
      "236/295, train_loss: 0.0243, step time: 1.0672\n",
      "237/295, train_loss: 0.0541, step time: 1.0534\n",
      "238/295, train_loss: 0.0919, step time: 1.0522\n",
      "239/295, train_loss: 0.0357, step time: 1.0379\n",
      "240/295, train_loss: 0.0469, step time: 1.0575\n",
      "241/295, train_loss: 0.0489, step time: 1.0326\n",
      "242/295, train_loss: 0.0722, step time: 1.0393\n",
      "243/295, train_loss: 0.0984, step time: 1.0563\n",
      "244/295, train_loss: 0.0424, step time: 1.0397\n",
      "245/295, train_loss: 0.0593, step time: 1.0565\n",
      "246/295, train_loss: 0.0465, step time: 1.0536\n",
      "247/295, train_loss: 0.0392, step time: 1.0398\n",
      "248/295, train_loss: 0.0399, step time: 1.0598\n",
      "249/295, train_loss: 0.0291, step time: 1.0315\n",
      "250/295, train_loss: 0.0374, step time: 1.0328\n",
      "251/295, train_loss: 0.0576, step time: 1.0330\n",
      "252/295, train_loss: 0.0628, step time: 1.0613\n",
      "253/295, train_loss: 0.0739, step time: 1.1035\n",
      "254/295, train_loss: 0.0746, step time: 1.0434\n",
      "255/295, train_loss: 0.0721, step time: 1.0685\n",
      "256/295, train_loss: 0.0254, step time: 1.0322\n",
      "257/295, train_loss: 0.0890, step time: 1.0383\n",
      "258/295, train_loss: 0.0775, step time: 1.0507\n",
      "259/295, train_loss: 0.0274, step time: 1.0794\n",
      "260/295, train_loss: 0.0596, step time: 1.0382\n",
      "261/295, train_loss: 0.0708, step time: 1.0747\n",
      "262/295, train_loss: 0.1147, step time: 1.0852\n",
      "263/295, train_loss: 0.0866, step time: 1.0547\n",
      "264/295, train_loss: 0.0271, step time: 1.0432\n",
      "265/295, train_loss: 0.0412, step time: 1.0408\n",
      "266/295, train_loss: 0.0416, step time: 1.0342\n",
      "267/295, train_loss: 0.0687, step time: 1.0376\n",
      "268/295, train_loss: 0.0702, step time: 1.0538\n",
      "269/295, train_loss: 0.0664, step time: 1.0373\n",
      "270/295, train_loss: 0.0497, step time: 1.0574\n",
      "271/295, train_loss: 0.3829, step time: 1.0728\n",
      "272/295, train_loss: 0.0600, step time: 1.0463\n",
      "273/295, train_loss: 0.0716, step time: 1.0525\n",
      "274/295, train_loss: 0.0393, step time: 1.0362\n",
      "275/295, train_loss: 0.0774, step time: 1.0360\n",
      "276/295, train_loss: 0.0454, step time: 1.0647\n",
      "277/295, train_loss: 0.3728, step time: 1.0891\n",
      "278/295, train_loss: 0.1039, step time: 1.1130\n",
      "279/295, train_loss: 0.0224, step time: 1.0570\n",
      "280/295, train_loss: 0.0920, step time: 1.0676\n",
      "281/295, train_loss: 0.0467, step time: 1.0390\n",
      "282/295, train_loss: 0.0415, step time: 1.0393\n",
      "283/295, train_loss: 0.3583, step time: 1.0432\n",
      "284/295, train_loss: 0.0734, step time: 1.0967\n",
      "285/295, train_loss: 0.1244, step time: 1.0335\n",
      "286/295, train_loss: 0.0901, step time: 1.0413\n",
      "287/295, train_loss: 0.0496, step time: 1.0302\n",
      "288/295, train_loss: 0.0754, step time: 1.0284\n",
      "289/295, train_loss: 0.0597, step time: 1.0288\n",
      "290/295, train_loss: 0.0345, step time: 1.0298\n",
      "291/295, train_loss: 0.0920, step time: 1.0291\n",
      "292/295, train_loss: 0.1088, step time: 1.0297\n",
      "293/295, train_loss: 0.0687, step time: 1.0299\n",
      "294/295, train_loss: 0.0682, step time: 1.0288\n",
      "295/295, train_loss: 0.0273, step time: 1.0278\n",
      "epoch 85 average loss: 0.0786\n",
      "current epoch: 85 current mean dice: 0.7676 tc: 0.7234 wt: 0.8322 et: 0.7530\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 85 is: 388.8291\n",
      "----------\n",
      "epoch 86/100\n",
      "1/295, train_loss: 0.0349, step time: 1.1666\n",
      "2/295, train_loss: 0.0488, step time: 1.0757\n",
      "3/295, train_loss: 0.0540, step time: 1.0829\n",
      "4/295, train_loss: 0.0517, step time: 1.0547\n",
      "5/295, train_loss: 0.0718, step time: 1.0470\n",
      "6/295, train_loss: 0.0493, step time: 1.0521\n",
      "7/295, train_loss: 0.0224, step time: 1.0565\n",
      "8/295, train_loss: 0.3733, step time: 1.0554\n",
      "9/295, train_loss: 0.1270, step time: 1.0565\n",
      "10/295, train_loss: 0.1019, step time: 1.0331\n",
      "11/295, train_loss: 0.0508, step time: 1.0307\n",
      "12/295, train_loss: 0.3610, step time: 1.0394\n",
      "13/295, train_loss: 0.0262, step time: 1.0356\n",
      "14/295, train_loss: 0.1075, step time: 1.0430\n",
      "15/295, train_loss: 0.3650, step time: 1.0459\n",
      "16/295, train_loss: 0.0481, step time: 1.0834\n",
      "17/295, train_loss: 0.0536, step time: 1.0464\n",
      "18/295, train_loss: 0.0611, step time: 1.0964\n",
      "19/295, train_loss: 0.0697, step time: 1.0612\n",
      "20/295, train_loss: 0.0380, step time: 1.0307\n",
      "21/295, train_loss: 0.0263, step time: 1.0435\n",
      "22/295, train_loss: 0.0508, step time: 1.1019\n",
      "23/295, train_loss: 0.0277, step time: 1.0303\n",
      "24/295, train_loss: 0.0888, step time: 1.0397\n",
      "25/295, train_loss: 0.0362, step time: 1.0308\n",
      "26/295, train_loss: 0.0433, step time: 1.0341\n",
      "27/295, train_loss: 0.0352, step time: 1.0508\n",
      "28/295, train_loss: 0.0611, step time: 1.0775\n",
      "29/295, train_loss: 0.0294, step time: 1.0409\n",
      "30/295, train_loss: 0.0499, step time: 1.1099\n",
      "31/295, train_loss: 0.0452, step time: 1.0388\n",
      "32/295, train_loss: 0.0668, step time: 1.0387\n",
      "33/295, train_loss: 0.0244, step time: 1.0550\n",
      "34/295, train_loss: 0.0821, step time: 1.1179\n",
      "35/295, train_loss: 0.0606, step time: 1.0335\n",
      "36/295, train_loss: 0.0629, step time: 1.0351\n",
      "37/295, train_loss: 0.0709, step time: 1.0379\n",
      "38/295, train_loss: 0.0364, step time: 1.0553\n",
      "39/295, train_loss: 0.0314, step time: 1.0477\n",
      "40/295, train_loss: 0.0656, step time: 1.0330\n",
      "41/295, train_loss: 0.0224, step time: 1.0534\n",
      "42/295, train_loss: 0.0740, step time: 1.0565\n",
      "43/295, train_loss: 0.0781, step time: 1.0434\n",
      "44/295, train_loss: 0.0429, step time: 1.0411\n",
      "45/295, train_loss: 0.0465, step time: 1.0725\n",
      "46/295, train_loss: 0.0588, step time: 1.0687\n",
      "47/295, train_loss: 0.0464, step time: 1.0354\n",
      "48/295, train_loss: 0.0442, step time: 1.0350\n",
      "49/295, train_loss: 0.0320, step time: 1.0506\n",
      "50/295, train_loss: 0.0333, step time: 1.0611\n",
      "51/295, train_loss: 0.0625, step time: 1.0358\n",
      "52/295, train_loss: 0.0406, step time: 1.0408\n",
      "53/295, train_loss: 0.0625, step time: 1.0422\n",
      "54/295, train_loss: 0.0896, step time: 1.0570\n",
      "55/295, train_loss: 0.1108, step time: 1.0541\n",
      "56/295, train_loss: 0.3935, step time: 1.0476\n",
      "57/295, train_loss: 0.0305, step time: 1.0497\n",
      "58/295, train_loss: 0.0689, step time: 1.0591\n",
      "59/295, train_loss: 0.0807, step time: 1.0682\n",
      "60/295, train_loss: 0.0462, step time: 1.0558\n",
      "61/295, train_loss: 0.0728, step time: 1.0437\n",
      "62/295, train_loss: 0.0908, step time: 1.0733\n",
      "63/295, train_loss: 0.0471, step time: 1.0373\n",
      "64/295, train_loss: 0.0256, step time: 1.0438\n",
      "65/295, train_loss: 0.0507, step time: 1.0546\n",
      "66/295, train_loss: 0.0544, step time: 1.0411\n",
      "67/295, train_loss: 0.0758, step time: 1.0372\n",
      "68/295, train_loss: 0.0559, step time: 1.0457\n",
      "69/295, train_loss: 0.0424, step time: 1.0584\n",
      "70/295, train_loss: 0.0294, step time: 1.0417\n",
      "71/295, train_loss: 0.0339, step time: 1.0374\n",
      "72/295, train_loss: 0.0313, step time: 1.0557\n",
      "73/295, train_loss: 0.0709, step time: 1.0355\n",
      "74/295, train_loss: 0.0750, step time: 1.0409\n",
      "75/295, train_loss: 0.0534, step time: 1.0710\n",
      "76/295, train_loss: 0.3784, step time: 1.0368\n",
      "77/295, train_loss: 0.0669, step time: 1.0462\n",
      "78/295, train_loss: 0.0500, step time: 1.0852\n",
      "79/295, train_loss: 0.0400, step time: 1.0340\n",
      "80/295, train_loss: 0.0683, step time: 1.0570\n",
      "81/295, train_loss: 0.0347, step time: 1.0560\n",
      "82/295, train_loss: 0.0479, step time: 1.0366\n",
      "83/295, train_loss: 0.0630, step time: 1.0605\n",
      "84/295, train_loss: 0.0399, step time: 1.0680\n",
      "85/295, train_loss: 0.0518, step time: 1.0351\n",
      "86/295, train_loss: 0.0480, step time: 1.0302\n",
      "87/295, train_loss: 0.0500, step time: 1.0323\n",
      "88/295, train_loss: 0.0914, step time: 1.0384\n",
      "89/295, train_loss: 0.0577, step time: 1.0780\n",
      "90/295, train_loss: 0.3750, step time: 1.0438\n",
      "91/295, train_loss: 0.0911, step time: 1.0560\n",
      "92/295, train_loss: 0.3730, step time: 1.0412\n",
      "93/295, train_loss: 0.0446, step time: 1.0453\n",
      "94/295, train_loss: 0.0730, step time: 1.0514\n",
      "95/295, train_loss: 0.0561, step time: 1.0450\n",
      "96/295, train_loss: 0.0678, step time: 1.0613\n",
      "97/295, train_loss: 0.0447, step time: 1.0540\n",
      "98/295, train_loss: 0.0702, step time: 1.1077\n",
      "99/295, train_loss: 0.0400, step time: 1.0370\n",
      "100/295, train_loss: 0.0367, step time: 1.0413\n",
      "101/295, train_loss: 0.0231, step time: 1.0404\n",
      "102/295, train_loss: 0.3786, step time: 1.0358\n",
      "103/295, train_loss: 0.0269, step time: 1.0533\n",
      "104/295, train_loss: 0.0702, step time: 1.0585\n",
      "105/295, train_loss: 0.0307, step time: 1.0582\n",
      "106/295, train_loss: 0.0819, step time: 1.0758\n",
      "107/295, train_loss: 0.0554, step time: 1.0377\n",
      "108/295, train_loss: 0.0467, step time: 1.0496\n",
      "109/295, train_loss: 0.0754, step time: 1.0359\n",
      "110/295, train_loss: 0.1118, step time: 1.0318\n",
      "111/295, train_loss: 0.0697, step time: 1.0607\n",
      "112/295, train_loss: 0.1245, step time: 1.0536\n",
      "113/295, train_loss: 0.0361, step time: 1.0339\n",
      "114/295, train_loss: 0.0641, step time: 1.0644\n",
      "115/295, train_loss: 0.0895, step time: 1.0387\n",
      "116/295, train_loss: 0.0773, step time: 1.0673\n",
      "117/295, train_loss: 0.0604, step time: 1.0570\n",
      "118/295, train_loss: 0.0381, step time: 1.0565\n",
      "119/295, train_loss: 0.0729, step time: 1.0370\n",
      "120/295, train_loss: 0.0673, step time: 1.0400\n",
      "121/295, train_loss: 0.0709, step time: 1.0347\n",
      "122/295, train_loss: 0.0279, step time: 1.0380\n",
      "123/295, train_loss: 0.0316, step time: 1.0569\n",
      "124/295, train_loss: 0.0685, step time: 1.0397\n",
      "125/295, train_loss: 0.0309, step time: 1.0329\n",
      "126/295, train_loss: 0.3556, step time: 1.0334\n",
      "127/295, train_loss: 0.0865, step time: 1.0426\n",
      "128/295, train_loss: 0.0373, step time: 1.0390\n",
      "129/295, train_loss: 0.0769, step time: 1.0674\n",
      "130/295, train_loss: 0.0772, step time: 1.0457\n",
      "131/295, train_loss: 0.0456, step time: 1.0306\n",
      "132/295, train_loss: 0.2063, step time: 1.0441\n",
      "133/295, train_loss: 0.0743, step time: 1.0679\n",
      "134/295, train_loss: 0.0250, step time: 1.0382\n",
      "135/295, train_loss: 0.0344, step time: 1.0352\n",
      "136/295, train_loss: 0.0542, step time: 1.0373\n",
      "137/295, train_loss: 0.0403, step time: 1.0576\n",
      "138/295, train_loss: 0.0558, step time: 1.0523\n",
      "139/295, train_loss: 0.0467, step time: 1.0637\n",
      "140/295, train_loss: 0.0332, step time: 1.0366\n",
      "141/295, train_loss: 0.0227, step time: 1.1512\n",
      "142/295, train_loss: 0.0270, step time: 1.0599\n",
      "143/295, train_loss: 0.1142, step time: 1.0464\n",
      "144/295, train_loss: 0.0441, step time: 1.0404\n",
      "145/295, train_loss: 0.0413, step time: 1.0352\n",
      "146/295, train_loss: 0.0253, step time: 1.0381\n",
      "147/295, train_loss: 0.0576, step time: 1.0541\n",
      "148/295, train_loss: 0.0688, step time: 1.0954\n",
      "149/295, train_loss: 0.0757, step time: 1.0439\n",
      "150/295, train_loss: 0.0499, step time: 1.0540\n",
      "151/295, train_loss: 0.0301, step time: 1.0420\n",
      "152/295, train_loss: 0.0821, step time: 1.0949\n",
      "153/295, train_loss: 0.0468, step time: 1.0389\n",
      "154/295, train_loss: 0.0357, step time: 1.0373\n",
      "155/295, train_loss: 0.0590, step time: 1.0755\n",
      "156/295, train_loss: 0.0255, step time: 1.0367\n",
      "157/295, train_loss: 0.0269, step time: 1.0383\n",
      "158/295, train_loss: 0.0446, step time: 1.0386\n",
      "159/295, train_loss: 0.0348, step time: 1.0458\n",
      "160/295, train_loss: 0.0955, step time: 1.0746\n",
      "161/295, train_loss: 0.0387, step time: 1.0368\n",
      "162/295, train_loss: 0.3589, step time: 1.0383\n",
      "163/295, train_loss: 0.0494, step time: 1.0380\n",
      "164/295, train_loss: 0.0523, step time: 1.0723\n",
      "165/295, train_loss: 0.0536, step time: 1.0593\n",
      "166/295, train_loss: 0.0762, step time: 1.0394\n",
      "167/295, train_loss: 0.0493, step time: 1.0309\n",
      "168/295, train_loss: 0.0770, step time: 1.0508\n",
      "169/295, train_loss: 0.0221, step time: 1.0398\n",
      "170/295, train_loss: 0.0365, step time: 1.0697\n",
      "171/295, train_loss: 0.0573, step time: 1.0543\n",
      "172/295, train_loss: 0.0491, step time: 1.0357\n",
      "173/295, train_loss: 0.0289, step time: 1.0389\n",
      "174/295, train_loss: 0.0575, step time: 1.0686\n",
      "175/295, train_loss: 0.0656, step time: 1.0467\n",
      "176/295, train_loss: 0.3574, step time: 1.0586\n",
      "177/295, train_loss: 0.0375, step time: 1.0495\n",
      "178/295, train_loss: 0.0274, step time: 1.0472\n",
      "179/295, train_loss: 0.0769, step time: 1.0334\n",
      "180/295, train_loss: 0.0493, step time: 1.0344\n",
      "181/295, train_loss: 0.0412, step time: 1.0609\n",
      "182/295, train_loss: 0.0398, step time: 1.0611\n",
      "183/295, train_loss: 0.0378, step time: 1.0394\n",
      "184/295, train_loss: 0.3751, step time: 1.0678\n",
      "185/295, train_loss: 0.0613, step time: 1.0421\n",
      "186/295, train_loss: 0.0422, step time: 1.0374\n",
      "187/295, train_loss: 0.0942, step time: 1.0458\n",
      "188/295, train_loss: 0.0736, step time: 1.1088\n",
      "189/295, train_loss: 0.0412, step time: 1.0315\n",
      "190/295, train_loss: 0.0745, step time: 1.0496\n",
      "191/295, train_loss: 0.3632, step time: 1.0477\n",
      "192/295, train_loss: 0.0308, step time: 1.0372\n",
      "193/295, train_loss: 0.0540, step time: 1.0384\n",
      "194/295, train_loss: 0.0405, step time: 1.0368\n",
      "195/295, train_loss: 0.0494, step time: 1.0821\n",
      "196/295, train_loss: 0.0357, step time: 1.0966\n",
      "197/295, train_loss: 0.0248, step time: 1.0421\n",
      "198/295, train_loss: 0.0593, step time: 1.0444\n",
      "199/295, train_loss: 0.0566, step time: 1.0356\n",
      "200/295, train_loss: 0.3671, step time: 1.0534\n",
      "201/295, train_loss: 0.0294, step time: 1.0475\n",
      "202/295, train_loss: 0.0577, step time: 1.0374\n",
      "203/295, train_loss: 0.0602, step time: 1.0447\n",
      "204/295, train_loss: 0.0794, step time: 1.0354\n",
      "205/295, train_loss: 0.0271, step time: 1.0432\n",
      "206/295, train_loss: 0.0455, step time: 1.0860\n",
      "207/295, train_loss: 0.0423, step time: 1.1146\n",
      "208/295, train_loss: 0.0363, step time: 1.0661\n",
      "209/295, train_loss: 0.0535, step time: 1.0527\n",
      "210/295, train_loss: 0.0434, step time: 1.0538\n",
      "211/295, train_loss: 0.0348, step time: 1.0318\n",
      "212/295, train_loss: 0.0391, step time: 1.0331\n",
      "213/295, train_loss: 0.0248, step time: 1.0339\n",
      "214/295, train_loss: 0.0267, step time: 1.0535\n",
      "215/295, train_loss: 0.0636, step time: 1.1402\n",
      "216/295, train_loss: 0.0396, step time: 1.0399\n",
      "217/295, train_loss: 0.0899, step time: 1.0406\n",
      "218/295, train_loss: 0.0451, step time: 1.0388\n",
      "219/295, train_loss: 0.0785, step time: 1.0569\n",
      "220/295, train_loss: 0.0447, step time: 1.0437\n",
      "221/295, train_loss: 0.0344, step time: 1.0467\n",
      "222/295, train_loss: 0.0423, step time: 1.0373\n",
      "223/295, train_loss: 0.0536, step time: 1.0398\n",
      "224/295, train_loss: 0.0486, step time: 1.0341\n",
      "225/295, train_loss: 0.0390, step time: 1.0432\n",
      "226/295, train_loss: 0.4256, step time: 1.0394\n",
      "227/295, train_loss: 0.0282, step time: 1.0631\n",
      "228/295, train_loss: 0.0206, step time: 1.0361\n",
      "229/295, train_loss: 0.0174, step time: 1.0390\n",
      "230/295, train_loss: 0.0378, step time: 1.0380\n",
      "231/295, train_loss: 0.0625, step time: 1.0318\n",
      "232/295, train_loss: 0.0990, step time: 1.0309\n",
      "233/295, train_loss: 0.0583, step time: 1.0803\n",
      "234/295, train_loss: 0.3696, step time: 1.0784\n",
      "235/295, train_loss: 0.0415, step time: 1.0341\n",
      "236/295, train_loss: 0.0805, step time: 1.0465\n",
      "237/295, train_loss: 0.0989, step time: 1.1499\n",
      "238/295, train_loss: 0.0510, step time: 1.0382\n",
      "239/295, train_loss: 0.1053, step time: 1.0377\n",
      "240/295, train_loss: 0.1003, step time: 1.0502\n",
      "241/295, train_loss: 0.0494, step time: 1.0393\n",
      "242/295, train_loss: 0.0306, step time: 1.0346\n",
      "243/295, train_loss: 0.0642, step time: 1.0499\n",
      "244/295, train_loss: 0.0461, step time: 1.0519\n",
      "245/295, train_loss: 0.0563, step time: 1.0384\n",
      "246/295, train_loss: 0.0277, step time: 1.0435\n",
      "247/295, train_loss: 0.0666, step time: 1.0743\n",
      "248/295, train_loss: 0.0344, step time: 1.0320\n",
      "249/295, train_loss: 0.0742, step time: 1.0423\n",
      "250/295, train_loss: 0.0683, step time: 1.0457\n",
      "251/295, train_loss: 0.0469, step time: 1.0622\n",
      "252/295, train_loss: 0.0414, step time: 1.0358\n",
      "253/295, train_loss: 0.0400, step time: 1.0443\n",
      "254/295, train_loss: 0.0801, step time: 1.0364\n",
      "255/295, train_loss: 0.0914, step time: 1.0391\n",
      "256/295, train_loss: 0.1140, step time: 1.0475\n",
      "257/295, train_loss: 0.3776, step time: 1.0383\n",
      "258/295, train_loss: 0.0390, step time: 1.0416\n",
      "259/295, train_loss: 0.0922, step time: 1.0441\n",
      "260/295, train_loss: 0.0338, step time: 1.0605\n",
      "261/295, train_loss: 0.0892, step time: 1.0757\n",
      "262/295, train_loss: 0.0435, step time: 1.0381\n",
      "263/295, train_loss: 0.0726, step time: 1.0356\n",
      "264/295, train_loss: 0.0250, step time: 1.0409\n",
      "265/295, train_loss: 0.0642, step time: 1.0430\n",
      "266/295, train_loss: 0.0410, step time: 1.0528\n",
      "267/295, train_loss: 0.0491, step time: 1.0480\n",
      "268/295, train_loss: 0.0474, step time: 1.0396\n",
      "269/295, train_loss: 0.3819, step time: 1.0612\n",
      "270/295, train_loss: 0.0599, step time: 1.0550\n",
      "271/295, train_loss: 0.0711, step time: 1.0507\n",
      "272/295, train_loss: 0.0316, step time: 1.0357\n",
      "273/295, train_loss: 0.0573, step time: 1.0397\n",
      "274/295, train_loss: 0.0588, step time: 1.0472\n",
      "275/295, train_loss: 0.3722, step time: 1.0470\n",
      "276/295, train_loss: 0.0650, step time: 1.0364\n",
      "277/295, train_loss: 0.0347, step time: 1.0417\n",
      "278/295, train_loss: 0.0676, step time: 1.0481\n",
      "279/295, train_loss: 0.0542, step time: 1.0447\n",
      "280/295, train_loss: 0.0321, step time: 1.0461\n",
      "281/295, train_loss: 0.0658, step time: 1.0342\n",
      "282/295, train_loss: 0.0710, step time: 1.0349\n",
      "283/295, train_loss: 0.0628, step time: 1.0342\n",
      "284/295, train_loss: 0.3606, step time: 1.0484\n",
      "285/295, train_loss: 0.0569, step time: 1.0519\n",
      "286/295, train_loss: 0.0256, step time: 1.0430\n",
      "287/295, train_loss: 0.0231, step time: 1.0421\n",
      "288/295, train_loss: 0.0301, step time: 1.0407\n",
      "289/295, train_loss: 0.0807, step time: 1.0323\n",
      "290/295, train_loss: 0.0751, step time: 1.0290\n",
      "291/295, train_loss: 0.3866, step time: 1.0290\n",
      "292/295, train_loss: 0.0466, step time: 1.0295\n",
      "293/295, train_loss: 0.0293, step time: 1.0293\n",
      "294/295, train_loss: 0.0297, step time: 1.0299\n",
      "295/295, train_loss: 0.3776, step time: 1.0295\n",
      "epoch 86 average loss: 0.0783\n",
      "current epoch: 86 current mean dice: 0.7726 tc: 0.7238 wt: 0.8380 et: 0.7598\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 86 is: 384.5500\n",
      "----------\n",
      "epoch 87/100\n",
      "1/295, train_loss: 0.0334, step time: 1.1023\n",
      "2/295, train_loss: 0.0698, step time: 1.0669\n",
      "3/295, train_loss: 0.0350, step time: 1.0566\n",
      "4/295, train_loss: 0.0521, step time: 1.0631\n",
      "5/295, train_loss: 0.3653, step time: 1.0369\n",
      "6/295, train_loss: 0.0459, step time: 1.0402\n",
      "7/295, train_loss: 0.0344, step time: 1.0384\n",
      "8/295, train_loss: 0.0437, step time: 1.0594\n",
      "9/295, train_loss: 0.0891, step time: 1.0392\n",
      "10/295, train_loss: 0.0201, step time: 1.0489\n",
      "11/295, train_loss: 0.0250, step time: 1.1098\n",
      "12/295, train_loss: 0.0407, step time: 1.0434\n",
      "13/295, train_loss: 0.0365, step time: 1.0346\n",
      "14/295, train_loss: 0.0479, step time: 1.0431\n",
      "15/295, train_loss: 0.0372, step time: 1.0452\n",
      "16/295, train_loss: 0.0416, step time: 1.0440\n",
      "17/295, train_loss: 0.0749, step time: 1.0298\n",
      "18/295, train_loss: 0.2057, step time: 1.0395\n",
      "19/295, train_loss: 0.0522, step time: 1.0726\n",
      "20/295, train_loss: 0.0573, step time: 1.0522\n",
      "21/295, train_loss: 0.0347, step time: 1.0338\n",
      "22/295, train_loss: 0.0626, step time: 1.0994\n",
      "23/295, train_loss: 0.0778, step time: 1.0329\n",
      "24/295, train_loss: 0.0459, step time: 1.0326\n",
      "25/295, train_loss: 0.0484, step time: 1.0427\n",
      "26/295, train_loss: 0.0406, step time: 1.0316\n",
      "27/295, train_loss: 0.0172, step time: 1.0351\n",
      "28/295, train_loss: 0.0345, step time: 1.0548\n",
      "29/295, train_loss: 0.3860, step time: 1.0685\n",
      "30/295, train_loss: 0.0480, step time: 1.0427\n",
      "31/295, train_loss: 0.1136, step time: 1.0435\n",
      "32/295, train_loss: 0.0892, step time: 1.0340\n",
      "33/295, train_loss: 0.1229, step time: 1.0375\n",
      "34/295, train_loss: 0.3570, step time: 1.0626\n",
      "35/295, train_loss: 0.0393, step time: 1.0652\n",
      "36/295, train_loss: 0.0414, step time: 1.0403\n",
      "37/295, train_loss: 0.0646, step time: 1.0350\n",
      "38/295, train_loss: 0.0488, step time: 1.0361\n",
      "39/295, train_loss: 0.0738, step time: 1.0352\n",
      "40/295, train_loss: 0.0674, step time: 1.0381\n",
      "41/295, train_loss: 0.0335, step time: 1.0360\n",
      "42/295, train_loss: 0.0713, step time: 1.0706\n",
      "43/295, train_loss: 0.0452, step time: 1.0813\n",
      "44/295, train_loss: 0.0593, step time: 1.0327\n",
      "45/295, train_loss: 0.0384, step time: 1.0564\n",
      "46/295, train_loss: 0.0633, step time: 1.0346\n",
      "47/295, train_loss: 0.0715, step time: 1.0620\n",
      "48/295, train_loss: 0.0757, step time: 1.0411\n",
      "49/295, train_loss: 0.0218, step time: 1.0391\n",
      "50/295, train_loss: 0.0978, step time: 1.0353\n",
      "51/295, train_loss: 0.1075, step time: 1.0416\n",
      "52/295, train_loss: 0.0495, step time: 1.0393\n",
      "53/295, train_loss: 0.0953, step time: 1.0572\n",
      "54/295, train_loss: 0.0396, step time: 1.0319\n",
      "55/295, train_loss: 0.3733, step time: 1.0436\n",
      "56/295, train_loss: 0.0531, step time: 1.0347\n",
      "57/295, train_loss: 0.0339, step time: 1.0615\n",
      "58/295, train_loss: 0.0724, step time: 1.0894\n",
      "59/295, train_loss: 0.0540, step time: 1.0999\n",
      "60/295, train_loss: 0.0637, step time: 1.0609\n",
      "61/295, train_loss: 0.0287, step time: 1.0400\n",
      "62/295, train_loss: 0.0268, step time: 1.0344\n",
      "63/295, train_loss: 0.0571, step time: 1.0644\n",
      "64/295, train_loss: 0.0243, step time: 1.0601\n",
      "65/295, train_loss: 0.3816, step time: 1.0349\n",
      "66/295, train_loss: 0.0315, step time: 1.0485\n",
      "67/295, train_loss: 0.0305, step time: 1.0446\n",
      "68/295, train_loss: 0.0910, step time: 1.0999\n",
      "69/295, train_loss: 0.0282, step time: 1.0805\n",
      "70/295, train_loss: 0.4211, step time: 1.0398\n",
      "71/295, train_loss: 0.0490, step time: 1.0372\n",
      "72/295, train_loss: 0.0701, step time: 1.0464\n",
      "73/295, train_loss: 0.0531, step time: 1.0386\n",
      "74/295, train_loss: 0.0321, step time: 1.0392\n",
      "75/295, train_loss: 0.0664, step time: 1.0910\n",
      "76/295, train_loss: 0.0502, step time: 1.0489\n",
      "77/295, train_loss: 0.0384, step time: 1.0460\n",
      "78/295, train_loss: 0.3603, step time: 1.0378\n",
      "79/295, train_loss: 0.0519, step time: 1.0316\n",
      "80/295, train_loss: 0.0305, step time: 1.0361\n",
      "81/295, train_loss: 0.0571, step time: 1.0325\n",
      "82/295, train_loss: 0.0609, step time: 1.0474\n",
      "83/295, train_loss: 0.0665, step time: 1.0595\n",
      "84/295, train_loss: 0.0560, step time: 1.0364\n",
      "85/295, train_loss: 0.0590, step time: 1.0644\n",
      "86/295, train_loss: 0.0618, step time: 1.1097\n",
      "87/295, train_loss: 0.0788, step time: 1.0732\n",
      "88/295, train_loss: 0.0676, step time: 1.0457\n",
      "89/295, train_loss: 0.3729, step time: 1.0330\n",
      "90/295, train_loss: 0.0749, step time: 1.0350\n",
      "91/295, train_loss: 0.0402, step time: 1.0452\n",
      "92/295, train_loss: 0.0469, step time: 1.0470\n",
      "93/295, train_loss: 0.0539, step time: 1.0340\n",
      "94/295, train_loss: 0.0578, step time: 1.0580\n",
      "95/295, train_loss: 0.0629, step time: 1.0892\n",
      "96/295, train_loss: 0.0499, step time: 1.0367\n",
      "97/295, train_loss: 0.0443, step time: 1.0442\n",
      "98/295, train_loss: 0.0333, step time: 1.0935\n",
      "99/295, train_loss: 0.0265, step time: 1.0537\n",
      "100/295, train_loss: 0.0599, step time: 1.0390\n",
      "101/295, train_loss: 0.0244, step time: 1.0432\n",
      "102/295, train_loss: 0.0469, step time: 1.0549\n",
      "103/295, train_loss: 0.0460, step time: 1.0544\n",
      "104/295, train_loss: 0.0499, step time: 1.0372\n",
      "105/295, train_loss: 0.0732, step time: 1.0409\n",
      "106/295, train_loss: 0.0538, step time: 1.0419\n",
      "107/295, train_loss: 0.0777, step time: 1.0443\n",
      "108/295, train_loss: 0.3752, step time: 1.0809\n",
      "109/295, train_loss: 0.0444, step time: 1.0513\n",
      "110/295, train_loss: 0.0764, step time: 1.0424\n",
      "111/295, train_loss: 0.0659, step time: 1.0406\n",
      "112/295, train_loss: 0.0371, step time: 1.0370\n",
      "113/295, train_loss: 0.0464, step time: 1.0491\n",
      "114/295, train_loss: 0.3554, step time: 1.0703\n",
      "115/295, train_loss: 0.0457, step time: 1.0338\n",
      "116/295, train_loss: 0.0388, step time: 1.0370\n",
      "117/295, train_loss: 0.3773, step time: 1.0402\n",
      "118/295, train_loss: 0.0340, step time: 1.0673\n",
      "119/295, train_loss: 0.0618, step time: 1.0445\n",
      "120/295, train_loss: 0.0648, step time: 1.0400\n",
      "121/295, train_loss: 0.0301, step time: 1.0551\n",
      "122/295, train_loss: 0.0678, step time: 1.0597\n",
      "123/295, train_loss: 0.0500, step time: 1.0429\n",
      "124/295, train_loss: 0.0261, step time: 1.0587\n",
      "125/295, train_loss: 0.0279, step time: 1.0346\n",
      "126/295, train_loss: 0.3743, step time: 1.0361\n",
      "127/295, train_loss: 0.0423, step time: 1.0350\n",
      "128/295, train_loss: 0.0227, step time: 1.0597\n",
      "129/295, train_loss: 0.0555, step time: 1.0382\n",
      "130/295, train_loss: 0.0235, step time: 1.0439\n",
      "131/295, train_loss: 0.0419, step time: 1.0444\n",
      "132/295, train_loss: 0.0496, step time: 1.0753\n",
      "133/295, train_loss: 0.0691, step time: 1.0519\n",
      "134/295, train_loss: 0.0305, step time: 1.0570\n",
      "135/295, train_loss: 0.0377, step time: 1.0336\n",
      "136/295, train_loss: 0.0571, step time: 1.0344\n",
      "137/295, train_loss: 0.0601, step time: 1.0321\n",
      "138/295, train_loss: 0.0463, step time: 1.0440\n",
      "139/295, train_loss: 0.0486, step time: 1.0399\n",
      "140/295, train_loss: 0.0427, step time: 1.0429\n",
      "141/295, train_loss: 0.0676, step time: 1.0345\n",
      "142/295, train_loss: 0.0435, step time: 1.0421\n",
      "143/295, train_loss: 0.0609, step time: 1.0536\n",
      "144/295, train_loss: 0.0375, step time: 1.0386\n",
      "145/295, train_loss: 0.0876, step time: 1.0632\n",
      "146/295, train_loss: 0.0304, step time: 1.0780\n",
      "147/295, train_loss: 0.0397, step time: 1.0476\n",
      "148/295, train_loss: 0.0293, step time: 1.0428\n",
      "149/295, train_loss: 0.0554, step time: 1.1074\n",
      "150/295, train_loss: 0.0684, step time: 1.0386\n",
      "151/295, train_loss: 0.0820, step time: 1.0417\n",
      "152/295, train_loss: 0.0347, step time: 1.0392\n",
      "153/295, train_loss: 0.0764, step time: 1.0626\n",
      "154/295, train_loss: 0.0360, step time: 1.0362\n",
      "155/295, train_loss: 0.0800, step time: 1.0496\n",
      "156/295, train_loss: 0.0300, step time: 1.0486\n",
      "157/295, train_loss: 0.0286, step time: 1.0345\n",
      "158/295, train_loss: 0.0292, step time: 1.1408\n",
      "159/295, train_loss: 0.0725, step time: 1.0622\n",
      "160/295, train_loss: 0.0920, step time: 1.0368\n",
      "161/295, train_loss: 0.0514, step time: 1.0407\n",
      "162/295, train_loss: 0.0221, step time: 1.0852\n",
      "163/295, train_loss: 0.3714, step time: 1.0408\n",
      "164/295, train_loss: 0.0861, step time: 1.0399\n",
      "165/295, train_loss: 0.0382, step time: 1.0338\n",
      "166/295, train_loss: 0.0594, step time: 1.0415\n",
      "167/295, train_loss: 0.0356, step time: 1.0704\n",
      "168/295, train_loss: 0.0318, step time: 1.0672\n",
      "169/295, train_loss: 0.0778, step time: 1.0418\n",
      "170/295, train_loss: 0.0521, step time: 1.0387\n",
      "171/295, train_loss: 0.0754, step time: 1.0663\n",
      "172/295, train_loss: 0.0379, step time: 1.0377\n",
      "173/295, train_loss: 0.0385, step time: 1.0416\n",
      "174/295, train_loss: 0.0595, step time: 1.0554\n",
      "175/295, train_loss: 0.0412, step time: 1.0430\n",
      "176/295, train_loss: 0.0698, step time: 1.0395\n",
      "177/295, train_loss: 0.0733, step time: 1.0359\n",
      "178/295, train_loss: 0.0479, step time: 1.0366\n",
      "179/295, train_loss: 0.0248, step time: 1.0369\n",
      "180/295, train_loss: 0.0231, step time: 1.0350\n",
      "181/295, train_loss: 0.0267, step time: 1.0409\n",
      "182/295, train_loss: 0.0257, step time: 1.0409\n",
      "183/295, train_loss: 0.3665, step time: 1.0371\n",
      "184/295, train_loss: 0.0652, step time: 1.0330\n",
      "185/295, train_loss: 0.0766, step time: 1.0367\n",
      "186/295, train_loss: 0.0269, step time: 1.0450\n",
      "187/295, train_loss: 0.0445, step time: 1.0594\n",
      "188/295, train_loss: 0.0381, step time: 1.1036\n",
      "189/295, train_loss: 0.0536, step time: 1.0692\n",
      "190/295, train_loss: 0.0439, step time: 1.0370\n",
      "191/295, train_loss: 0.0796, step time: 1.0413\n",
      "192/295, train_loss: 0.1134, step time: 1.0861\n",
      "193/295, train_loss: 0.0366, step time: 1.0421\n",
      "194/295, train_loss: 0.1043, step time: 1.0392\n",
      "195/295, train_loss: 0.1277, step time: 1.0429\n",
      "196/295, train_loss: 0.0313, step time: 1.0352\n",
      "197/295, train_loss: 0.0260, step time: 1.0344\n",
      "198/295, train_loss: 0.0930, step time: 1.0416\n",
      "199/295, train_loss: 0.0746, step time: 1.0463\n",
      "200/295, train_loss: 0.0676, step time: 1.0493\n",
      "201/295, train_loss: 0.1142, step time: 1.0441\n",
      "202/295, train_loss: 0.0276, step time: 1.0376\n",
      "203/295, train_loss: 0.0341, step time: 1.0360\n",
      "204/295, train_loss: 0.0739, step time: 1.0876\n",
      "205/295, train_loss: 0.0699, step time: 1.0914\n",
      "206/295, train_loss: 0.0319, step time: 1.0386\n",
      "207/295, train_loss: 0.0554, step time: 1.0356\n",
      "208/295, train_loss: 0.0906, step time: 1.0313\n",
      "209/295, train_loss: 0.0610, step time: 1.0466\n",
      "210/295, train_loss: 0.0545, step time: 1.0358\n",
      "211/295, train_loss: 0.0435, step time: 1.0411\n",
      "212/295, train_loss: 0.0364, step time: 1.1263\n",
      "213/295, train_loss: 0.0524, step time: 1.0368\n",
      "214/295, train_loss: 0.0506, step time: 1.0623\n",
      "215/295, train_loss: 0.0254, step time: 1.0348\n",
      "216/295, train_loss: 0.0738, step time: 1.0433\n",
      "217/295, train_loss: 0.0477, step time: 1.0386\n",
      "218/295, train_loss: 0.0817, step time: 1.0444\n",
      "219/295, train_loss: 0.0939, step time: 1.0457\n",
      "220/295, train_loss: 0.0351, step time: 1.0366\n",
      "221/295, train_loss: 0.0555, step time: 1.0474\n",
      "222/295, train_loss: 0.3786, step time: 1.0377\n",
      "223/295, train_loss: 0.3775, step time: 1.0375\n",
      "224/295, train_loss: 0.0811, step time: 1.0348\n",
      "225/295, train_loss: 0.0491, step time: 1.0383\n",
      "226/295, train_loss: 0.0450, step time: 1.0341\n",
      "227/295, train_loss: 0.0589, step time: 1.0421\n",
      "228/295, train_loss: 0.0474, step time: 1.1344\n",
      "229/295, train_loss: 0.0773, step time: 1.0330\n",
      "230/295, train_loss: 0.0678, step time: 1.0448\n",
      "231/295, train_loss: 0.0292, step time: 1.0332\n",
      "232/295, train_loss: 0.3934, step time: 1.0435\n",
      "233/295, train_loss: 0.0765, step time: 1.0744\n",
      "234/295, train_loss: 0.0320, step time: 1.0382\n",
      "235/295, train_loss: 0.0251, step time: 1.0640\n",
      "236/295, train_loss: 0.0593, step time: 1.0391\n",
      "237/295, train_loss: 0.1023, step time: 1.0734\n",
      "238/295, train_loss: 0.0409, step time: 1.0449\n",
      "239/295, train_loss: 0.0481, step time: 1.0330\n",
      "240/295, train_loss: 0.0883, step time: 1.0485\n",
      "241/295, train_loss: 0.0496, step time: 1.1115\n",
      "242/295, train_loss: 0.0637, step time: 1.0481\n",
      "243/295, train_loss: 0.0987, step time: 1.0417\n",
      "244/295, train_loss: 0.0720, step time: 1.0343\n",
      "245/295, train_loss: 0.0667, step time: 1.0328\n",
      "246/295, train_loss: 0.3626, step time: 1.0410\n",
      "247/295, train_loss: 0.0296, step time: 1.0433\n",
      "248/295, train_loss: 0.0354, step time: 1.0546\n",
      "249/295, train_loss: 0.3777, step time: 1.0373\n",
      "250/295, train_loss: 0.0465, step time: 1.0459\n",
      "251/295, train_loss: 0.0294, step time: 1.0383\n",
      "252/295, train_loss: 0.0637, step time: 1.0723\n",
      "253/295, train_loss: 0.1001, step time: 1.1810\n",
      "254/295, train_loss: 0.0626, step time: 1.0445\n",
      "255/295, train_loss: 0.1116, step time: 1.0693\n",
      "256/295, train_loss: 0.0402, step time: 1.0416\n",
      "257/295, train_loss: 0.0374, step time: 1.0517\n",
      "258/295, train_loss: 0.0494, step time: 1.0343\n",
      "259/295, train_loss: 0.0521, step time: 1.0362\n",
      "260/295, train_loss: 0.0896, step time: 1.0723\n",
      "261/295, train_loss: 0.0402, step time: 1.0383\n",
      "262/295, train_loss: 0.0571, step time: 1.0527\n",
      "263/295, train_loss: 0.0265, step time: 1.0464\n",
      "264/295, train_loss: 0.0227, step time: 1.0498\n",
      "265/295, train_loss: 0.0342, step time: 1.0442\n",
      "266/295, train_loss: 0.0278, step time: 1.0340\n",
      "267/295, train_loss: 0.0409, step time: 1.0387\n",
      "268/295, train_loss: 0.0436, step time: 1.0546\n",
      "269/295, train_loss: 0.0401, step time: 1.0930\n",
      "270/295, train_loss: 0.0338, step time: 1.0704\n",
      "271/295, train_loss: 0.3687, step time: 1.0423\n",
      "272/295, train_loss: 0.0815, step time: 1.0467\n",
      "273/295, train_loss: 0.0271, step time: 1.0373\n",
      "274/295, train_loss: 0.3586, step time: 1.0422\n",
      "275/295, train_loss: 0.0466, step time: 1.0811\n",
      "276/295, train_loss: 0.0805, step time: 1.0497\n",
      "277/295, train_loss: 0.0757, step time: 1.0509\n",
      "278/295, train_loss: 0.0444, step time: 1.0397\n",
      "279/295, train_loss: 0.0606, step time: 1.0697\n",
      "280/295, train_loss: 0.0570, step time: 1.0331\n",
      "281/295, train_loss: 0.0428, step time: 1.0333\n",
      "282/295, train_loss: 0.3604, step time: 1.0385\n",
      "283/295, train_loss: 0.0583, step time: 1.0369\n",
      "284/295, train_loss: 0.0731, step time: 1.0339\n",
      "285/295, train_loss: 0.0251, step time: 1.0332\n",
      "286/295, train_loss: 0.0662, step time: 1.0352\n",
      "287/295, train_loss: 0.0418, step time: 1.0581\n",
      "288/295, train_loss: 0.0447, step time: 1.0604\n",
      "289/295, train_loss: 0.0454, step time: 1.0287\n",
      "290/295, train_loss: 0.0569, step time: 1.0290\n",
      "291/295, train_loss: 0.0704, step time: 1.0288\n",
      "292/295, train_loss: 0.0305, step time: 1.0296\n",
      "293/295, train_loss: 0.0474, step time: 1.0283\n",
      "294/295, train_loss: 0.0898, step time: 1.0295\n",
      "295/295, train_loss: 0.0713, step time: 1.0287\n",
      "epoch 87 average loss: 0.0781\n",
      "current epoch: 87 current mean dice: 0.7802 tc: 0.7308 wt: 0.8500 et: 0.7649\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 87 is: 383.2057\n",
      "----------\n",
      "epoch 88/100\n",
      "1/295, train_loss: 0.0411, step time: 1.1159\n",
      "2/295, train_loss: 0.0449, step time: 1.0458\n",
      "3/295, train_loss: 0.0570, step time: 1.0661\n",
      "4/295, train_loss: 0.0768, step time: 1.1165\n",
      "5/295, train_loss: 0.2057, step time: 1.0622\n",
      "6/295, train_loss: 0.3725, step time: 1.0448\n",
      "7/295, train_loss: 0.0482, step time: 1.0386\n",
      "8/295, train_loss: 0.0547, step time: 1.0583\n",
      "9/295, train_loss: 0.0463, step time: 1.0350\n",
      "10/295, train_loss: 0.0959, step time: 1.0734\n",
      "11/295, train_loss: 0.0398, step time: 1.0636\n",
      "12/295, train_loss: 0.0538, step time: 1.0390\n",
      "13/295, train_loss: 0.3770, step time: 1.0704\n",
      "14/295, train_loss: 0.3684, step time: 1.1090\n",
      "15/295, train_loss: 0.0753, step time: 1.0333\n",
      "16/295, train_loss: 0.0687, step time: 1.0527\n",
      "17/295, train_loss: 0.0672, step time: 1.0809\n",
      "18/295, train_loss: 0.0493, step time: 1.0367\n",
      "19/295, train_loss: 0.0742, step time: 1.0341\n",
      "20/295, train_loss: 0.0929, step time: 1.0379\n",
      "21/295, train_loss: 0.0269, step time: 1.0381\n",
      "22/295, train_loss: 0.0795, step time: 1.0548\n",
      "23/295, train_loss: 0.0712, step time: 1.0415\n",
      "24/295, train_loss: 0.0460, step time: 1.0334\n",
      "25/295, train_loss: 0.0703, step time: 1.0438\n",
      "26/295, train_loss: 0.0662, step time: 1.0447\n",
      "27/295, train_loss: 0.0307, step time: 1.0355\n",
      "28/295, train_loss: 0.0516, step time: 1.1033\n",
      "29/295, train_loss: 0.0254, step time: 1.0433\n",
      "30/295, train_loss: 0.0341, step time: 1.0415\n",
      "31/295, train_loss: 0.1012, step time: 1.0661\n",
      "32/295, train_loss: 0.0279, step time: 1.0755\n",
      "33/295, train_loss: 0.0478, step time: 1.0482\n",
      "34/295, train_loss: 0.0379, step time: 1.0373\n",
      "35/295, train_loss: 0.0357, step time: 1.0613\n",
      "36/295, train_loss: 0.0368, step time: 1.0855\n",
      "37/295, train_loss: 0.0578, step time: 1.0458\n",
      "38/295, train_loss: 0.0976, step time: 1.0448\n",
      "39/295, train_loss: 0.0302, step time: 1.0830\n",
      "40/295, train_loss: 0.0445, step time: 1.0375\n",
      "41/295, train_loss: 0.3585, step time: 1.0366\n",
      "42/295, train_loss: 0.0268, step time: 1.0603\n",
      "43/295, train_loss: 0.0337, step time: 1.0738\n",
      "44/295, train_loss: 0.0609, step time: 1.0319\n",
      "45/295, train_loss: 0.0589, step time: 1.0452\n",
      "46/295, train_loss: 0.0432, step time: 1.0390\n",
      "47/295, train_loss: 0.0608, step time: 1.0395\n",
      "48/295, train_loss: 0.0651, step time: 1.0568\n",
      "49/295, train_loss: 0.0755, step time: 1.0382\n",
      "50/295, train_loss: 0.0625, step time: 1.0355\n",
      "51/295, train_loss: 0.0488, step time: 1.0571\n",
      "52/295, train_loss: 0.0532, step time: 1.0502\n",
      "53/295, train_loss: 0.3777, step time: 1.0359\n",
      "54/295, train_loss: 0.0458, step time: 1.0383\n",
      "55/295, train_loss: 0.0526, step time: 1.0880\n",
      "56/295, train_loss: 0.3572, step time: 1.0378\n",
      "57/295, train_loss: 0.0365, step time: 1.0335\n",
      "58/295, train_loss: 0.0344, step time: 1.0344\n",
      "59/295, train_loss: 0.0503, step time: 1.0382\n",
      "60/295, train_loss: 0.0748, step time: 1.0581\n",
      "61/295, train_loss: 0.0626, step time: 1.0525\n",
      "62/295, train_loss: 0.0293, step time: 1.0395\n",
      "63/295, train_loss: 0.0377, step time: 1.0399\n",
      "64/295, train_loss: 0.0658, step time: 1.1063\n",
      "65/295, train_loss: 0.0226, step time: 1.0350\n",
      "66/295, train_loss: 0.4201, step time: 1.0716\n",
      "67/295, train_loss: 0.0645, step time: 1.0690\n",
      "68/295, train_loss: 0.0867, step time: 1.0299\n",
      "69/295, train_loss: 0.0439, step time: 1.0573\n",
      "70/295, train_loss: 0.0433, step time: 1.0805\n",
      "71/295, train_loss: 0.0488, step time: 1.0398\n",
      "72/295, train_loss: 0.0573, step time: 1.0403\n",
      "73/295, train_loss: 0.0680, step time: 1.0406\n",
      "74/295, train_loss: 0.0473, step time: 1.0358\n",
      "75/295, train_loss: 0.1116, step time: 1.0391\n",
      "76/295, train_loss: 0.0291, step time: 1.0594\n",
      "77/295, train_loss: 0.0624, step time: 1.0372\n",
      "78/295, train_loss: 0.0809, step time: 1.0417\n",
      "79/295, train_loss: 0.0524, step time: 1.0375\n",
      "80/295, train_loss: 0.0888, step time: 1.0388\n",
      "81/295, train_loss: 0.0469, step time: 1.0339\n",
      "82/295, train_loss: 0.0826, step time: 1.0336\n",
      "83/295, train_loss: 0.0266, step time: 1.0739\n",
      "84/295, train_loss: 0.3710, step time: 1.0377\n",
      "85/295, train_loss: 0.0597, step time: 1.1087\n",
      "86/295, train_loss: 0.0370, step time: 1.0468\n",
      "87/295, train_loss: 0.0298, step time: 1.0744\n",
      "88/295, train_loss: 0.0408, step time: 1.0375\n",
      "89/295, train_loss: 0.0337, step time: 1.0469\n",
      "90/295, train_loss: 0.0630, step time: 1.0569\n",
      "91/295, train_loss: 0.0482, step time: 1.0889\n",
      "92/295, train_loss: 0.0677, step time: 1.0461\n",
      "93/295, train_loss: 0.0773, step time: 1.0643\n",
      "94/295, train_loss: 0.3664, step time: 1.0705\n",
      "95/295, train_loss: 0.0671, step time: 1.0364\n",
      "96/295, train_loss: 0.0445, step time: 1.0491\n",
      "97/295, train_loss: 0.0351, step time: 1.0567\n",
      "98/295, train_loss: 0.0393, step time: 1.0959\n",
      "99/295, train_loss: 0.0915, step time: 1.0562\n",
      "100/295, train_loss: 0.0255, step time: 1.0514\n",
      "101/295, train_loss: 0.0369, step time: 1.0330\n",
      "102/295, train_loss: 0.0987, step time: 1.0549\n",
      "103/295, train_loss: 0.0532, step time: 1.0399\n",
      "104/295, train_loss: 0.0494, step time: 1.0505\n",
      "105/295, train_loss: 0.0768, step time: 1.1161\n",
      "106/295, train_loss: 0.0424, step time: 1.0587\n",
      "107/295, train_loss: 0.0579, step time: 1.0347\n",
      "108/295, train_loss: 0.0271, step time: 1.0450\n",
      "109/295, train_loss: 0.0579, step time: 1.0583\n",
      "110/295, train_loss: 0.0292, step time: 1.0732\n",
      "111/295, train_loss: 0.0553, step time: 1.0356\n",
      "112/295, train_loss: 0.0485, step time: 1.0654\n",
      "113/295, train_loss: 0.0910, step time: 1.1051\n",
      "114/295, train_loss: 0.0582, step time: 1.0327\n",
      "115/295, train_loss: 0.0201, step time: 1.0380\n",
      "116/295, train_loss: 0.3804, step time: 1.0384\n",
      "117/295, train_loss: 0.0387, step time: 1.0387\n",
      "118/295, train_loss: 0.0665, step time: 1.0329\n",
      "119/295, train_loss: 0.0557, step time: 1.0428\n",
      "120/295, train_loss: 0.0596, step time: 1.0705\n",
      "121/295, train_loss: 0.0682, step time: 1.0695\n",
      "122/295, train_loss: 0.0651, step time: 1.0615\n",
      "123/295, train_loss: 0.0567, step time: 1.0589\n",
      "124/295, train_loss: 0.3602, step time: 1.0413\n",
      "125/295, train_loss: 0.0268, step time: 1.0489\n",
      "126/295, train_loss: 0.0496, step time: 1.0316\n",
      "127/295, train_loss: 0.0592, step time: 1.0721\n",
      "128/295, train_loss: 0.0312, step time: 1.0321\n",
      "129/295, train_loss: 0.3745, step time: 1.0476\n",
      "130/295, train_loss: 0.1130, step time: 1.0418\n",
      "131/295, train_loss: 0.0556, step time: 1.0410\n",
      "132/295, train_loss: 0.0770, step time: 1.0514\n",
      "133/295, train_loss: 0.3621, step time: 1.0360\n",
      "134/295, train_loss: 0.0385, step time: 1.0616\n",
      "135/295, train_loss: 0.0624, step time: 1.0595\n",
      "136/295, train_loss: 0.0439, step time: 1.0330\n",
      "137/295, train_loss: 0.0744, step time: 1.0446\n",
      "138/295, train_loss: 0.0520, step time: 1.0874\n",
      "139/295, train_loss: 0.0345, step time: 1.0356\n",
      "140/295, train_loss: 0.0332, step time: 1.0404\n",
      "141/295, train_loss: 0.0533, step time: 1.0908\n",
      "142/295, train_loss: 0.0224, step time: 1.0792\n",
      "143/295, train_loss: 0.0385, step time: 1.0977\n",
      "144/295, train_loss: 0.0454, step time: 1.0379\n",
      "145/295, train_loss: 0.1069, step time: 1.0322\n",
      "146/295, train_loss: 0.0401, step time: 1.0397\n",
      "147/295, train_loss: 0.0342, step time: 1.0338\n",
      "148/295, train_loss: 0.0305, step time: 1.0392\n",
      "149/295, train_loss: 0.0532, step time: 1.0532\n",
      "150/295, train_loss: 0.0916, step time: 1.0883\n",
      "151/295, train_loss: 0.0377, step time: 1.0397\n",
      "152/295, train_loss: 0.3652, step time: 1.0905\n",
      "153/295, train_loss: 0.0632, step time: 1.0366\n",
      "154/295, train_loss: 0.0305, step time: 1.0405\n",
      "155/295, train_loss: 0.0369, step time: 1.0577\n",
      "156/295, train_loss: 0.0320, step time: 1.0406\n",
      "157/295, train_loss: 0.0537, step time: 1.0398\n",
      "158/295, train_loss: 0.0487, step time: 1.0369\n",
      "159/295, train_loss: 0.3779, step time: 1.0465\n",
      "160/295, train_loss: 0.0565, step time: 1.0648\n",
      "161/295, train_loss: 0.0225, step time: 1.0466\n",
      "162/295, train_loss: 0.3557, step time: 1.0321\n",
      "163/295, train_loss: 0.3725, step time: 1.0333\n",
      "164/295, train_loss: 0.0599, step time: 1.0416\n",
      "165/295, train_loss: 0.0351, step time: 1.0414\n",
      "166/295, train_loss: 0.0449, step time: 1.0354\n",
      "167/295, train_loss: 0.0466, step time: 1.0409\n",
      "168/295, train_loss: 0.0290, step time: 1.0405\n",
      "169/295, train_loss: 0.0610, step time: 1.0635\n",
      "170/295, train_loss: 0.0307, step time: 1.0401\n",
      "171/295, train_loss: 0.0727, step time: 1.0415\n",
      "172/295, train_loss: 0.0551, step time: 1.0531\n",
      "173/295, train_loss: 0.0464, step time: 1.0732\n",
      "174/295, train_loss: 0.0466, step time: 1.0319\n",
      "175/295, train_loss: 0.0426, step time: 1.0454\n",
      "176/295, train_loss: 0.0230, step time: 1.0474\n",
      "177/295, train_loss: 0.0427, step time: 1.0884\n",
      "178/295, train_loss: 0.0386, step time: 1.0458\n",
      "179/295, train_loss: 0.0708, step time: 1.0792\n",
      "180/295, train_loss: 0.0312, step time: 1.0405\n",
      "181/295, train_loss: 0.0698, step time: 1.0510\n",
      "182/295, train_loss: 0.0715, step time: 1.0447\n",
      "183/295, train_loss: 0.0734, step time: 1.0529\n",
      "184/295, train_loss: 0.0402, step time: 1.0347\n",
      "185/295, train_loss: 0.0660, step time: 1.0382\n",
      "186/295, train_loss: 0.0590, step time: 1.0351\n",
      "187/295, train_loss: 0.0702, step time: 1.0387\n",
      "188/295, train_loss: 0.0445, step time: 1.1172\n",
      "189/295, train_loss: 0.0514, step time: 1.0364\n",
      "190/295, train_loss: 0.0370, step time: 1.0927\n",
      "191/295, train_loss: 0.0749, step time: 1.0402\n",
      "192/295, train_loss: 0.0172, step time: 1.0939\n",
      "193/295, train_loss: 0.0249, step time: 1.0461\n",
      "194/295, train_loss: 0.0270, step time: 1.0392\n",
      "195/295, train_loss: 0.0349, step time: 1.0480\n",
      "196/295, train_loss: 0.1035, step time: 1.0667\n",
      "197/295, train_loss: 0.0408, step time: 1.0371\n",
      "198/295, train_loss: 0.0244, step time: 1.1078\n",
      "199/295, train_loss: 0.0387, step time: 1.0634\n",
      "200/295, train_loss: 0.0696, step time: 1.0312\n",
      "201/295, train_loss: 0.0269, step time: 1.0320\n",
      "202/295, train_loss: 0.0780, step time: 1.0555\n",
      "203/295, train_loss: 0.0433, step time: 1.0373\n",
      "204/295, train_loss: 0.0642, step time: 1.0409\n",
      "205/295, train_loss: 0.0857, step time: 1.0388\n",
      "206/295, train_loss: 0.0719, step time: 1.0542\n",
      "207/295, train_loss: 0.0420, step time: 1.0429\n",
      "208/295, train_loss: 0.0817, step time: 1.0406\n",
      "209/295, train_loss: 0.0731, step time: 1.0406\n",
      "210/295, train_loss: 0.0570, step time: 1.0439\n",
      "211/295, train_loss: 0.3863, step time: 1.0797\n",
      "212/295, train_loss: 0.0730, step time: 1.0425\n",
      "213/295, train_loss: 0.0891, step time: 1.1130\n",
      "214/295, train_loss: 0.0215, step time: 1.0363\n",
      "215/295, train_loss: 0.0395, step time: 1.0615\n",
      "216/295, train_loss: 0.0713, step time: 1.0358\n",
      "217/295, train_loss: 0.0440, step time: 1.0461\n",
      "218/295, train_loss: 0.0245, step time: 1.0375\n",
      "219/295, train_loss: 0.0452, step time: 1.0372\n",
      "220/295, train_loss: 0.0464, step time: 1.0386\n",
      "221/295, train_loss: 0.0759, step time: 1.0697\n",
      "222/295, train_loss: 0.0492, step time: 1.0737\n",
      "223/295, train_loss: 0.0362, step time: 1.0321\n",
      "224/295, train_loss: 0.0370, step time: 1.0611\n",
      "225/295, train_loss: 0.0905, step time: 1.0549\n",
      "226/295, train_loss: 0.0263, step time: 1.0401\n",
      "227/295, train_loss: 0.0491, step time: 1.0598\n",
      "228/295, train_loss: 0.1125, step time: 1.0354\n",
      "229/295, train_loss: 0.0417, step time: 1.0365\n",
      "230/295, train_loss: 0.0240, step time: 1.0364\n",
      "231/295, train_loss: 0.0572, step time: 1.0361\n",
      "232/295, train_loss: 0.3600, step time: 1.0501\n",
      "233/295, train_loss: 0.0737, step time: 1.0630\n",
      "234/295, train_loss: 0.0338, step time: 1.0496\n",
      "235/295, train_loss: 0.0539, step time: 1.0372\n",
      "236/295, train_loss: 0.3742, step time: 1.0524\n",
      "237/295, train_loss: 0.0786, step time: 1.0416\n",
      "238/295, train_loss: 0.0893, step time: 1.0406\n",
      "239/295, train_loss: 0.0261, step time: 1.0408\n",
      "240/295, train_loss: 0.0447, step time: 1.0369\n",
      "241/295, train_loss: 0.0417, step time: 1.0439\n",
      "242/295, train_loss: 0.0683, step time: 1.0707\n",
      "243/295, train_loss: 0.0410, step time: 1.0558\n",
      "244/295, train_loss: 0.0640, step time: 1.0393\n",
      "245/295, train_loss: 0.0887, step time: 1.0324\n",
      "246/295, train_loss: 0.0599, step time: 1.0390\n",
      "247/295, train_loss: 0.0453, step time: 1.0518\n",
      "248/295, train_loss: 0.0791, step time: 1.0368\n",
      "249/295, train_loss: 0.0510, step time: 1.0342\n",
      "250/295, train_loss: 0.0513, step time: 1.0391\n",
      "251/295, train_loss: 0.0316, step time: 1.0352\n",
      "252/295, train_loss: 0.0799, step time: 1.0351\n",
      "253/295, train_loss: 0.0616, step time: 1.0426\n",
      "254/295, train_loss: 0.0729, step time: 1.0820\n",
      "255/295, train_loss: 0.0280, step time: 1.0730\n",
      "256/295, train_loss: 0.0317, step time: 1.0377\n",
      "257/295, train_loss: 0.0317, step time: 1.0316\n",
      "258/295, train_loss: 0.0349, step time: 1.0420\n",
      "259/295, train_loss: 0.0398, step time: 1.0740\n",
      "260/295, train_loss: 0.0341, step time: 1.0374\n",
      "261/295, train_loss: 0.0711, step time: 1.0328\n",
      "262/295, train_loss: 0.0254, step time: 1.0637\n",
      "263/295, train_loss: 0.0733, step time: 1.0365\n",
      "264/295, train_loss: 0.3937, step time: 1.1288\n",
      "265/295, train_loss: 0.0310, step time: 1.0405\n",
      "266/295, train_loss: 0.1134, step time: 1.0361\n",
      "267/295, train_loss: 0.0521, step time: 1.0661\n",
      "268/295, train_loss: 0.0744, step time: 1.0589\n",
      "269/295, train_loss: 0.0356, step time: 1.0704\n",
      "270/295, train_loss: 0.0497, step time: 1.0732\n",
      "271/295, train_loss: 0.0809, step time: 1.0447\n",
      "272/295, train_loss: 0.0330, step time: 1.0349\n",
      "273/295, train_loss: 0.0470, step time: 1.0335\n",
      "274/295, train_loss: 0.0874, step time: 1.0666\n",
      "275/295, train_loss: 0.0497, step time: 1.0406\n",
      "276/295, train_loss: 0.0257, step time: 1.0342\n",
      "277/295, train_loss: 0.0497, step time: 1.0337\n",
      "278/295, train_loss: 0.0411, step time: 1.0446\n",
      "279/295, train_loss: 0.0287, step time: 1.0709\n",
      "280/295, train_loss: 0.0407, step time: 1.1000\n",
      "281/295, train_loss: 0.0497, step time: 1.0366\n",
      "282/295, train_loss: 0.3782, step time: 1.0312\n",
      "283/295, train_loss: 0.0635, step time: 1.0552\n",
      "284/295, train_loss: 0.0671, step time: 1.0334\n",
      "285/295, train_loss: 0.1272, step time: 1.0394\n",
      "286/295, train_loss: 0.0294, step time: 1.0322\n",
      "287/295, train_loss: 0.0989, step time: 1.0386\n",
      "288/295, train_loss: 0.0250, step time: 1.0301\n",
      "289/295, train_loss: 0.0684, step time: 1.0293\n",
      "290/295, train_loss: 0.0277, step time: 1.0292\n",
      "291/295, train_loss: 0.0227, step time: 1.0317\n",
      "292/295, train_loss: 0.0572, step time: 1.0289\n",
      "293/295, train_loss: 0.0393, step time: 1.0307\n",
      "294/295, train_loss: 0.0476, step time: 1.0291\n",
      "295/295, train_loss: 0.1237, step time: 1.0283\n",
      "epoch 88 average loss: 0.0779\n",
      "current epoch: 88 current mean dice: 0.7393 tc: 0.6856 wt: 0.8132 et: 0.7257\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 88 is: 388.1929\n",
      "----------\n",
      "epoch 89/100\n",
      "1/295, train_loss: 0.0596, step time: 1.1700\n",
      "2/295, train_loss: 0.0297, step time: 1.0852\n",
      "3/295, train_loss: 0.0268, step time: 1.0609\n",
      "4/295, train_loss: 0.0575, step time: 1.0462\n",
      "5/295, train_loss: 0.0606, step time: 1.0708\n",
      "6/295, train_loss: 0.0268, step time: 1.0440\n",
      "7/295, train_loss: 0.3746, step time: 1.0889\n",
      "8/295, train_loss: 0.0423, step time: 1.1180\n",
      "9/295, train_loss: 0.0352, step time: 1.0295\n",
      "10/295, train_loss: 0.0252, step time: 1.0339\n",
      "11/295, train_loss: 0.0301, step time: 1.0350\n",
      "12/295, train_loss: 0.0936, step time: 1.0365\n",
      "13/295, train_loss: 0.0255, step time: 1.0346\n",
      "14/295, train_loss: 0.0457, step time: 1.0619\n",
      "15/295, train_loss: 0.0580, step time: 1.0825\n",
      "16/295, train_loss: 0.2054, step time: 1.0655\n",
      "17/295, train_loss: 0.0912, step time: 1.0600\n",
      "18/295, train_loss: 0.0650, step time: 1.0845\n",
      "19/295, train_loss: 0.0270, step time: 1.0453\n",
      "20/295, train_loss: 0.0524, step time: 1.0393\n",
      "21/295, train_loss: 0.0399, step time: 1.0592\n",
      "22/295, train_loss: 0.0289, step time: 1.0633\n",
      "23/295, train_loss: 0.0788, step time: 1.0326\n",
      "24/295, train_loss: 0.0407, step time: 1.0456\n",
      "25/295, train_loss: 0.0722, step time: 1.0379\n",
      "26/295, train_loss: 0.0686, step time: 1.0315\n",
      "27/295, train_loss: 0.0370, step time: 1.0356\n",
      "28/295, train_loss: 0.0621, step time: 1.0746\n",
      "29/295, train_loss: 0.0361, step time: 1.0593\n",
      "30/295, train_loss: 0.0974, step time: 1.0383\n",
      "31/295, train_loss: 0.0498, step time: 1.0533\n",
      "32/295, train_loss: 0.0287, step time: 1.0897\n",
      "33/295, train_loss: 0.0875, step time: 1.0355\n",
      "34/295, train_loss: 0.0909, step time: 1.0494\n",
      "35/295, train_loss: 0.3583, step time: 1.0519\n",
      "36/295, train_loss: 0.0344, step time: 1.0380\n",
      "37/295, train_loss: 0.0317, step time: 1.0410\n",
      "38/295, train_loss: 0.0305, step time: 1.0406\n",
      "39/295, train_loss: 0.0276, step time: 1.0402\n",
      "40/295, train_loss: 0.0528, step time: 1.1302\n",
      "41/295, train_loss: 0.0217, step time: 1.0359\n",
      "42/295, train_loss: 0.0438, step time: 1.0364\n",
      "43/295, train_loss: 0.0416, step time: 1.0999\n",
      "44/295, train_loss: 0.0884, step time: 1.0513\n",
      "45/295, train_loss: 0.0371, step time: 1.0350\n",
      "46/295, train_loss: 0.0374, step time: 1.0400\n",
      "47/295, train_loss: 0.0534, step time: 1.0924\n",
      "48/295, train_loss: 0.4193, step time: 1.0344\n",
      "49/295, train_loss: 0.0478, step time: 1.0329\n",
      "50/295, train_loss: 0.0246, step time: 1.0395\n",
      "51/295, train_loss: 0.0345, step time: 1.0403\n",
      "52/295, train_loss: 0.0313, step time: 1.0446\n",
      "53/295, train_loss: 0.0596, step time: 1.0348\n",
      "54/295, train_loss: 0.0578, step time: 1.0625\n",
      "55/295, train_loss: 0.0251, step time: 1.0367\n",
      "56/295, train_loss: 0.0569, step time: 1.0319\n",
      "57/295, train_loss: 0.0399, step time: 1.0441\n",
      "58/295, train_loss: 0.0360, step time: 1.0352\n",
      "59/295, train_loss: 0.0590, step time: 1.0512\n",
      "60/295, train_loss: 0.0725, step time: 1.0663\n",
      "61/295, train_loss: 0.0263, step time: 1.0538\n",
      "62/295, train_loss: 0.0585, step time: 1.0489\n",
      "63/295, train_loss: 0.3777, step time: 1.1274\n",
      "64/295, train_loss: 0.0716, step time: 1.0386\n",
      "65/295, train_loss: 0.0477, step time: 1.0349\n",
      "66/295, train_loss: 0.0734, step time: 1.0647\n",
      "67/295, train_loss: 0.1135, step time: 1.0334\n",
      "68/295, train_loss: 0.0491, step time: 1.0400\n",
      "69/295, train_loss: 0.0679, step time: 1.0319\n",
      "70/295, train_loss: 0.0704, step time: 1.0426\n",
      "71/295, train_loss: 0.0660, step time: 1.0563\n",
      "72/295, train_loss: 0.0447, step time: 1.0870\n",
      "73/295, train_loss: 0.3605, step time: 1.0340\n",
      "74/295, train_loss: 0.0802, step time: 1.0352\n",
      "75/295, train_loss: 0.3662, step time: 1.0311\n",
      "76/295, train_loss: 0.0398, step time: 1.1052\n",
      "77/295, train_loss: 0.0364, step time: 1.0380\n",
      "78/295, train_loss: 0.0566, step time: 1.0536\n",
      "79/295, train_loss: 0.0969, step time: 1.0386\n",
      "80/295, train_loss: 0.0744, step time: 1.0331\n",
      "81/295, train_loss: 0.0450, step time: 1.0391\n",
      "82/295, train_loss: 0.0575, step time: 1.0536\n",
      "83/295, train_loss: 0.1063, step time: 1.0356\n",
      "84/295, train_loss: 0.0615, step time: 1.0556\n",
      "85/295, train_loss: 0.0768, step time: 1.0437\n",
      "86/295, train_loss: 0.0517, step time: 1.0361\n",
      "87/295, train_loss: 0.0463, step time: 1.0460\n",
      "88/295, train_loss: 0.0372, step time: 1.0402\n",
      "89/295, train_loss: 0.1136, step time: 1.0559\n",
      "90/295, train_loss: 0.0330, step time: 1.0333\n",
      "91/295, train_loss: 0.0336, step time: 1.0565\n",
      "92/295, train_loss: 0.0789, step time: 1.0631\n",
      "93/295, train_loss: 0.0644, step time: 1.0368\n",
      "94/295, train_loss: 0.0462, step time: 1.0451\n",
      "95/295, train_loss: 0.0423, step time: 1.0644\n",
      "96/295, train_loss: 0.3722, step time: 1.0482\n",
      "97/295, train_loss: 0.0623, step time: 1.0317\n",
      "98/295, train_loss: 0.0232, step time: 1.0300\n",
      "99/295, train_loss: 0.0339, step time: 1.0444\n",
      "100/295, train_loss: 0.0228, step time: 1.0350\n",
      "101/295, train_loss: 0.0822, step time: 1.0376\n",
      "102/295, train_loss: 0.3776, step time: 1.0617\n",
      "103/295, train_loss: 0.0511, step time: 1.0346\n",
      "104/295, train_loss: 0.0674, step time: 1.0460\n",
      "105/295, train_loss: 0.0253, step time: 1.0463\n",
      "106/295, train_loss: 0.0788, step time: 1.0504\n",
      "107/295, train_loss: 0.0273, step time: 1.0329\n",
      "108/295, train_loss: 0.3614, step time: 1.0371\n",
      "109/295, train_loss: 0.0543, step time: 1.0617\n",
      "110/295, train_loss: 0.0506, step time: 1.0378\n",
      "111/295, train_loss: 0.0462, step time: 1.0357\n",
      "112/295, train_loss: 0.0488, step time: 1.0410\n",
      "113/295, train_loss: 0.0478, step time: 1.0343\n",
      "114/295, train_loss: 0.0305, step time: 1.0339\n",
      "115/295, train_loss: 0.0304, step time: 1.0587\n",
      "116/295, train_loss: 0.0359, step time: 1.0611\n",
      "117/295, train_loss: 0.0383, step time: 1.0544\n",
      "118/295, train_loss: 0.0266, step time: 1.0383\n",
      "119/295, train_loss: 0.0486, step time: 1.0424\n",
      "120/295, train_loss: 0.0436, step time: 1.0417\n",
      "121/295, train_loss: 0.0431, step time: 1.0695\n",
      "122/295, train_loss: 0.0381, step time: 1.0879\n",
      "123/295, train_loss: 0.0700, step time: 1.0440\n",
      "124/295, train_loss: 0.0493, step time: 1.0387\n",
      "125/295, train_loss: 0.0303, step time: 1.0403\n",
      "126/295, train_loss: 0.0604, step time: 1.0400\n",
      "127/295, train_loss: 0.0765, step time: 1.0614\n",
      "128/295, train_loss: 0.0377, step time: 1.0451\n",
      "129/295, train_loss: 0.0248, step time: 1.0364\n",
      "130/295, train_loss: 0.3622, step time: 1.0450\n",
      "131/295, train_loss: 0.0596, step time: 1.0450\n",
      "132/295, train_loss: 0.0400, step time: 1.0644\n",
      "133/295, train_loss: 0.0601, step time: 1.0584\n",
      "134/295, train_loss: 0.0732, step time: 1.0383\n",
      "135/295, train_loss: 0.0249, step time: 1.0382\n",
      "136/295, train_loss: 0.0291, step time: 1.0449\n",
      "137/295, train_loss: 0.0269, step time: 1.0453\n",
      "138/295, train_loss: 0.0515, step time: 1.1126\n",
      "139/295, train_loss: 0.0664, step time: 1.0326\n",
      "140/295, train_loss: 0.0444, step time: 1.0338\n",
      "141/295, train_loss: 0.0614, step time: 1.0447\n",
      "142/295, train_loss: 0.0555, step time: 1.0778\n",
      "143/295, train_loss: 0.0645, step time: 1.0482\n",
      "144/295, train_loss: 0.0556, step time: 1.0434\n",
      "145/295, train_loss: 0.0785, step time: 1.0381\n",
      "146/295, train_loss: 0.0637, step time: 1.0573\n",
      "147/295, train_loss: 0.0561, step time: 1.0359\n",
      "148/295, train_loss: 0.0672, step time: 1.0316\n",
      "149/295, train_loss: 0.0465, step time: 1.0458\n",
      "150/295, train_loss: 0.0472, step time: 1.0471\n",
      "151/295, train_loss: 0.0386, step time: 1.0381\n",
      "152/295, train_loss: 0.0203, step time: 1.0775\n",
      "153/295, train_loss: 0.0507, step time: 1.0355\n",
      "154/295, train_loss: 0.0623, step time: 1.0439\n",
      "155/295, train_loss: 0.3713, step time: 1.0890\n",
      "156/295, train_loss: 0.0492, step time: 1.0461\n",
      "157/295, train_loss: 0.0382, step time: 1.0443\n",
      "158/295, train_loss: 0.0465, step time: 1.0339\n",
      "159/295, train_loss: 0.0432, step time: 1.0455\n",
      "160/295, train_loss: 0.0328, step time: 1.0522\n",
      "161/295, train_loss: 0.0533, step time: 1.0334\n",
      "162/295, train_loss: 0.0632, step time: 1.0363\n",
      "163/295, train_loss: 0.0505, step time: 1.0481\n",
      "164/295, train_loss: 0.0331, step time: 1.0449\n",
      "165/295, train_loss: 0.0290, step time: 1.0370\n",
      "166/295, train_loss: 0.0493, step time: 1.0538\n",
      "167/295, train_loss: 0.0660, step time: 1.0434\n",
      "168/295, train_loss: 0.0411, step time: 1.0514\n",
      "169/295, train_loss: 0.0247, step time: 1.0354\n",
      "170/295, train_loss: 0.0662, step time: 1.0333\n",
      "171/295, train_loss: 0.3558, step time: 1.0362\n",
      "172/295, train_loss: 0.0695, step time: 1.0416\n",
      "173/295, train_loss: 0.0319, step time: 1.0658\n",
      "174/295, train_loss: 0.0359, step time: 1.0592\n",
      "175/295, train_loss: 0.0815, step time: 1.0378\n",
      "176/295, train_loss: 0.0755, step time: 1.0545\n",
      "177/295, train_loss: 0.3774, step time: 1.0346\n",
      "178/295, train_loss: 0.0448, step time: 1.0388\n",
      "179/295, train_loss: 0.0462, step time: 1.0579\n",
      "180/295, train_loss: 0.0541, step time: 1.0573\n",
      "181/295, train_loss: 0.0682, step time: 1.0391\n",
      "182/295, train_loss: 0.0260, step time: 1.0375\n",
      "183/295, train_loss: 0.0243, step time: 1.0715\n",
      "184/295, train_loss: 0.1091, step time: 1.0431\n",
      "185/295, train_loss: 0.0279, step time: 1.0386\n",
      "186/295, train_loss: 0.3769, step time: 1.0553\n",
      "187/295, train_loss: 0.0285, step time: 1.0374\n",
      "188/295, train_loss: 0.0628, step time: 1.1245\n",
      "189/295, train_loss: 0.0292, step time: 1.0551\n",
      "190/295, train_loss: 0.0553, step time: 1.0407\n",
      "191/295, train_loss: 0.0900, step time: 1.0332\n",
      "192/295, train_loss: 0.0490, step time: 1.0387\n",
      "193/295, train_loss: 0.0517, step time: 1.0373\n",
      "194/295, train_loss: 0.0341, step time: 1.0356\n",
      "195/295, train_loss: 0.0308, step time: 1.0383\n",
      "196/295, train_loss: 0.0692, step time: 1.0545\n",
      "197/295, train_loss: 0.0174, step time: 1.0457\n",
      "198/295, train_loss: 0.0491, step time: 1.0588\n",
      "199/295, train_loss: 0.0744, step time: 1.0416\n",
      "200/295, train_loss: 0.1020, step time: 1.0566\n",
      "201/295, train_loss: 0.3748, step time: 1.1053\n",
      "202/295, train_loss: 0.0736, step time: 1.0384\n",
      "203/295, train_loss: 0.0482, step time: 1.0548\n",
      "204/295, train_loss: 0.0461, step time: 1.0873\n",
      "205/295, train_loss: 0.0912, step time: 1.0380\n",
      "206/295, train_loss: 0.0413, step time: 1.0402\n",
      "207/295, train_loss: 0.0415, step time: 1.0510\n",
      "208/295, train_loss: 0.0759, step time: 1.0530\n",
      "209/295, train_loss: 0.0587, step time: 1.0933\n",
      "210/295, train_loss: 0.0536, step time: 1.0968\n",
      "211/295, train_loss: 0.0389, step time: 1.0479\n",
      "212/295, train_loss: 0.0345, step time: 1.0313\n",
      "213/295, train_loss: 0.0530, step time: 1.0353\n",
      "214/295, train_loss: 0.0275, step time: 1.0671\n",
      "215/295, train_loss: 0.0337, step time: 1.0357\n",
      "216/295, train_loss: 0.0458, step time: 1.0346\n",
      "217/295, train_loss: 0.0463, step time: 1.0405\n",
      "218/295, train_loss: 0.0656, step time: 1.0581\n",
      "219/295, train_loss: 0.0503, step time: 1.0552\n",
      "220/295, train_loss: 0.0401, step time: 1.0417\n",
      "221/295, train_loss: 0.0685, step time: 1.0519\n",
      "222/295, train_loss: 0.0778, step time: 1.0436\n",
      "223/295, train_loss: 0.0351, step time: 1.0704\n",
      "224/295, train_loss: 0.0905, step time: 1.0426\n",
      "225/295, train_loss: 0.0493, step time: 1.0628\n",
      "226/295, train_loss: 0.3801, step time: 1.0424\n",
      "227/295, train_loss: 0.0770, step time: 1.0412\n",
      "228/295, train_loss: 0.0720, step time: 1.0749\n",
      "229/295, train_loss: 0.0679, step time: 1.0837\n",
      "230/295, train_loss: 0.0713, step time: 1.0382\n",
      "231/295, train_loss: 0.0572, step time: 1.0391\n",
      "232/295, train_loss: 0.0447, step time: 1.0603\n",
      "233/295, train_loss: 0.0405, step time: 1.0710\n",
      "234/295, train_loss: 0.0856, step time: 1.0663\n",
      "235/295, train_loss: 0.0438, step time: 1.0362\n",
      "236/295, train_loss: 0.0224, step time: 1.0369\n",
      "237/295, train_loss: 0.0726, step time: 1.0712\n",
      "238/295, train_loss: 0.0867, step time: 1.0774\n",
      "239/295, train_loss: 0.0406, step time: 1.0473\n",
      "240/295, train_loss: 0.0731, step time: 1.0454\n",
      "241/295, train_loss: 0.0707, step time: 1.0607\n",
      "242/295, train_loss: 0.0225, step time: 1.0408\n",
      "243/295, train_loss: 0.3694, step time: 1.0395\n",
      "244/295, train_loss: 0.0576, step time: 1.0590\n",
      "245/295, train_loss: 0.0662, step time: 1.0334\n",
      "246/295, train_loss: 0.0491, step time: 1.0406\n",
      "247/295, train_loss: 0.0225, step time: 1.0632\n",
      "248/295, train_loss: 0.0892, step time: 1.0372\n",
      "249/295, train_loss: 0.0348, step time: 1.0407\n",
      "250/295, train_loss: 0.0871, step time: 1.0391\n",
      "251/295, train_loss: 0.0454, step time: 1.0675\n",
      "252/295, train_loss: 0.0547, step time: 1.0392\n",
      "253/295, train_loss: 0.1029, step time: 1.0528\n",
      "254/295, train_loss: 0.0790, step time: 1.0366\n",
      "255/295, train_loss: 0.0718, step time: 1.0337\n",
      "256/295, train_loss: 0.0386, step time: 1.0376\n",
      "257/295, train_loss: 0.0744, step time: 1.0344\n",
      "258/295, train_loss: 0.0560, step time: 1.0362\n",
      "259/295, train_loss: 0.0369, step time: 1.0361\n",
      "260/295, train_loss: 0.0650, step time: 1.0409\n",
      "261/295, train_loss: 0.0439, step time: 1.0330\n",
      "262/295, train_loss: 0.0583, step time: 1.0424\n",
      "263/295, train_loss: 0.0710, step time: 1.0419\n",
      "264/295, train_loss: 0.0381, step time: 1.0376\n",
      "265/295, train_loss: 0.0771, step time: 1.0433\n",
      "266/295, train_loss: 0.0329, step time: 1.0975\n",
      "267/295, train_loss: 0.0605, step time: 1.0668\n",
      "268/295, train_loss: 0.1276, step time: 1.0591\n",
      "269/295, train_loss: 0.0943, step time: 1.0714\n",
      "270/295, train_loss: 0.3858, step time: 1.0462\n",
      "271/295, train_loss: 0.0335, step time: 1.0530\n",
      "272/295, train_loss: 0.0333, step time: 1.0427\n",
      "273/295, train_loss: 0.3722, step time: 1.0375\n",
      "274/295, train_loss: 0.0671, step time: 1.0358\n",
      "275/295, train_loss: 0.0312, step time: 1.0530\n",
      "276/295, train_loss: 0.0523, step time: 1.0601\n",
      "277/295, train_loss: 0.1233, step time: 1.0329\n",
      "278/295, train_loss: 0.0473, step time: 1.1048\n",
      "279/295, train_loss: 0.0420, step time: 1.0479\n",
      "280/295, train_loss: 0.0738, step time: 1.0503\n",
      "281/295, train_loss: 0.0537, step time: 1.0398\n",
      "282/295, train_loss: 0.0452, step time: 1.0435\n",
      "283/295, train_loss: 0.0349, step time: 1.1224\n",
      "284/295, train_loss: 0.3571, step time: 1.0362\n",
      "285/295, train_loss: 0.0270, step time: 1.0414\n",
      "286/295, train_loss: 0.0763, step time: 1.0412\n",
      "287/295, train_loss: 0.0404, step time: 1.0433\n",
      "288/295, train_loss: 0.1116, step time: 1.0345\n",
      "289/295, train_loss: 0.0407, step time: 1.0312\n",
      "290/295, train_loss: 0.3931, step time: 1.0299\n",
      "291/295, train_loss: 0.0977, step time: 1.0294\n",
      "292/295, train_loss: 0.3649, step time: 1.0290\n",
      "293/295, train_loss: 0.0429, step time: 1.0295\n",
      "294/295, train_loss: 0.0318, step time: 1.0315\n",
      "295/295, train_loss: 0.0627, step time: 1.0306\n",
      "epoch 89 average loss: 0.0777\n",
      "current epoch: 89 current mean dice: 0.7753 tc: 0.7315 wt: 0.8353 et: 0.7618\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 89 is: 385.4184\n",
      "----------\n",
      "epoch 90/100\n",
      "1/295, train_loss: 0.0460, step time: 1.1038\n",
      "2/295, train_loss: 0.0881, step time: 1.0442\n",
      "3/295, train_loss: 0.0245, step time: 1.0982\n",
      "4/295, train_loss: 0.0888, step time: 1.1213\n",
      "5/295, train_loss: 0.0530, step time: 1.1104\n",
      "6/295, train_loss: 0.0535, step time: 1.0521\n",
      "7/295, train_loss: 0.0542, step time: 1.0737\n",
      "8/295, train_loss: 0.0497, step time: 1.0386\n",
      "9/295, train_loss: 0.0334, step time: 1.0350\n",
      "10/295, train_loss: 0.0366, step time: 1.0695\n",
      "11/295, train_loss: 0.0512, step time: 1.0519\n",
      "12/295, train_loss: 0.0500, step time: 1.0412\n",
      "13/295, train_loss: 0.0342, step time: 1.0395\n",
      "14/295, train_loss: 0.0267, step time: 1.0568\n",
      "15/295, train_loss: 0.0314, step time: 1.0677\n",
      "16/295, train_loss: 0.3774, step time: 1.0333\n",
      "17/295, train_loss: 0.0725, step time: 1.0579\n",
      "18/295, train_loss: 0.0470, step time: 1.0611\n",
      "19/295, train_loss: 0.3740, step time: 1.0394\n",
      "20/295, train_loss: 0.0610, step time: 1.0357\n",
      "21/295, train_loss: 0.0767, step time: 1.0352\n",
      "22/295, train_loss: 0.0291, step time: 1.1530\n",
      "23/295, train_loss: 0.0290, step time: 1.0407\n",
      "24/295, train_loss: 0.0399, step time: 1.0340\n",
      "25/295, train_loss: 0.0559, step time: 1.0395\n",
      "26/295, train_loss: 0.0659, step time: 1.0872\n",
      "27/295, train_loss: 0.0813, step time: 1.0437\n",
      "28/295, train_loss: 0.0242, step time: 1.0346\n",
      "29/295, train_loss: 0.3876, step time: 1.0448\n",
      "30/295, train_loss: 0.0290, step time: 1.0520\n",
      "31/295, train_loss: 0.0382, step time: 1.0729\n",
      "32/295, train_loss: 0.0666, step time: 1.0809\n",
      "33/295, train_loss: 0.0574, step time: 1.0590\n",
      "34/295, train_loss: 0.0240, step time: 1.0330\n",
      "35/295, train_loss: 0.0607, step time: 1.1158\n",
      "36/295, train_loss: 0.0528, step time: 1.0311\n",
      "37/295, train_loss: 0.0494, step time: 1.0400\n",
      "38/295, train_loss: 0.0223, step time: 1.0421\n",
      "39/295, train_loss: 0.0411, step time: 1.0487\n",
      "40/295, train_loss: 0.0302, step time: 1.0332\n",
      "41/295, train_loss: 0.3567, step time: 1.0559\n",
      "42/295, train_loss: 0.0266, step time: 1.0813\n",
      "43/295, train_loss: 0.0487, step time: 1.0298\n",
      "44/295, train_loss: 0.3703, step time: 1.0297\n",
      "45/295, train_loss: 0.0646, step time: 1.0532\n",
      "46/295, train_loss: 0.3606, step time: 1.0580\n",
      "47/295, train_loss: 0.0431, step time: 1.0704\n",
      "48/295, train_loss: 0.0586, step time: 1.0467\n",
      "49/295, train_loss: 0.0537, step time: 1.0402\n",
      "50/295, train_loss: 0.0638, step time: 1.0462\n",
      "51/295, train_loss: 0.0767, step time: 1.1048\n",
      "52/295, train_loss: 0.0973, step time: 1.0615\n",
      "53/295, train_loss: 0.0461, step time: 1.0476\n",
      "54/295, train_loss: 0.0375, step time: 1.0384\n",
      "55/295, train_loss: 0.0571, step time: 1.0361\n",
      "56/295, train_loss: 0.0817, step time: 1.0307\n",
      "57/295, train_loss: 0.0381, step time: 1.0665\n",
      "58/295, train_loss: 0.0896, step time: 1.0558\n",
      "59/295, train_loss: 0.0453, step time: 1.0392\n",
      "60/295, train_loss: 0.0912, step time: 1.0408\n",
      "61/295, train_loss: 0.0459, step time: 1.0396\n",
      "62/295, train_loss: 0.0610, step time: 1.0356\n",
      "63/295, train_loss: 0.0696, step time: 1.0415\n",
      "64/295, train_loss: 0.0402, step time: 1.0393\n",
      "65/295, train_loss: 0.0277, step time: 1.0543\n",
      "66/295, train_loss: 0.0512, step time: 1.0434\n",
      "67/295, train_loss: 0.0685, step time: 1.0427\n",
      "68/295, train_loss: 0.0395, step time: 1.0376\n",
      "69/295, train_loss: 0.0530, step time: 1.1108\n",
      "70/295, train_loss: 0.0712, step time: 1.1663\n",
      "71/295, train_loss: 0.0473, step time: 1.0336\n",
      "72/295, train_loss: 0.0390, step time: 1.0578\n",
      "73/295, train_loss: 0.0613, step time: 1.0298\n",
      "74/295, train_loss: 0.3587, step time: 1.0317\n",
      "75/295, train_loss: 0.0457, step time: 1.0330\n",
      "76/295, train_loss: 0.0904, step time: 1.0537\n",
      "77/295, train_loss: 0.0480, step time: 1.0307\n",
      "78/295, train_loss: 0.0533, step time: 1.0792\n",
      "79/295, train_loss: 0.0570, step time: 1.0634\n",
      "80/295, train_loss: 0.0443, step time: 1.0359\n",
      "81/295, train_loss: 0.0303, step time: 1.0498\n",
      "82/295, train_loss: 0.0701, step time: 1.0320\n",
      "83/295, train_loss: 0.3772, step time: 1.0490\n",
      "84/295, train_loss: 0.0930, step time: 1.0836\n",
      "85/295, train_loss: 0.0379, step time: 1.0375\n",
      "86/295, train_loss: 0.0626, step time: 1.0691\n",
      "87/295, train_loss: 0.0757, step time: 1.0426\n",
      "88/295, train_loss: 0.0547, step time: 1.0341\n",
      "89/295, train_loss: 0.0223, step time: 1.0435\n",
      "90/295, train_loss: 0.1010, step time: 1.0693\n",
      "91/295, train_loss: 0.0430, step time: 1.0370\n",
      "92/295, train_loss: 0.0623, step time: 1.0376\n",
      "93/295, train_loss: 0.0464, step time: 1.0611\n",
      "94/295, train_loss: 0.0943, step time: 1.0464\n",
      "95/295, train_loss: 0.0686, step time: 1.1485\n",
      "96/295, train_loss: 0.3927, step time: 1.0431\n",
      "97/295, train_loss: 0.0525, step time: 1.0441\n",
      "98/295, train_loss: 0.0684, step time: 1.0676\n",
      "99/295, train_loss: 0.0407, step time: 1.0457\n",
      "100/295, train_loss: 0.1115, step time: 1.0376\n",
      "101/295, train_loss: 0.0560, step time: 1.0452\n",
      "102/295, train_loss: 0.0968, step time: 1.0369\n",
      "103/295, train_loss: 0.0419, step time: 1.0434\n",
      "104/295, train_loss: 0.0437, step time: 1.0628\n",
      "105/295, train_loss: 0.0760, step time: 1.0603\n",
      "106/295, train_loss: 0.0506, step time: 1.0652\n",
      "107/295, train_loss: 0.0343, step time: 1.0507\n",
      "108/295, train_loss: 0.0209, step time: 1.0424\n",
      "109/295, train_loss: 0.0787, step time: 1.0348\n",
      "110/295, train_loss: 0.0338, step time: 1.0551\n",
      "111/295, train_loss: 0.0304, step time: 1.0334\n",
      "112/295, train_loss: 0.0464, step time: 1.0447\n",
      "113/295, train_loss: 0.0406, step time: 1.0376\n",
      "114/295, train_loss: 0.0263, step time: 1.0763\n",
      "115/295, train_loss: 0.3627, step time: 1.1055\n",
      "116/295, train_loss: 0.3804, step time: 1.0511\n",
      "117/295, train_loss: 0.0308, step time: 1.0327\n",
      "118/295, train_loss: 0.1225, step time: 1.0377\n",
      "119/295, train_loss: 0.0746, step time: 1.0669\n",
      "120/295, train_loss: 0.0440, step time: 1.0472\n",
      "121/295, train_loss: 0.0459, step time: 1.1119\n",
      "122/295, train_loss: 0.0435, step time: 1.0332\n",
      "123/295, train_loss: 0.0253, step time: 1.0632\n",
      "124/295, train_loss: 0.0846, step time: 1.0606\n",
      "125/295, train_loss: 0.0594, step time: 1.0557\n",
      "126/295, train_loss: 0.0380, step time: 1.0428\n",
      "127/295, train_loss: 0.0900, step time: 1.0368\n",
      "128/295, train_loss: 0.3765, step time: 1.0324\n",
      "129/295, train_loss: 0.0264, step time: 1.0579\n",
      "130/295, train_loss: 0.0441, step time: 1.0332\n",
      "131/295, train_loss: 0.0752, step time: 1.0374\n",
      "132/295, train_loss: 0.0605, step time: 1.0342\n",
      "133/295, train_loss: 0.0582, step time: 1.0358\n",
      "134/295, train_loss: 0.0775, step time: 1.0366\n",
      "135/295, train_loss: 0.3736, step time: 1.0441\n",
      "136/295, train_loss: 0.0714, step time: 1.0356\n",
      "137/295, train_loss: 0.0573, step time: 1.0507\n",
      "138/295, train_loss: 0.3751, step time: 1.0318\n",
      "139/295, train_loss: 0.3772, step time: 1.0374\n",
      "140/295, train_loss: 0.0491, step time: 1.0343\n",
      "141/295, train_loss: 0.0339, step time: 1.0523\n",
      "142/295, train_loss: 0.0525, step time: 1.0372\n",
      "143/295, train_loss: 0.0434, step time: 1.0382\n",
      "144/295, train_loss: 0.0515, step time: 1.0329\n",
      "145/295, train_loss: 0.0488, step time: 1.0333\n",
      "146/295, train_loss: 0.0643, step time: 1.0463\n",
      "147/295, train_loss: 0.0621, step time: 1.0507\n",
      "148/295, train_loss: 0.0486, step time: 1.0677\n",
      "149/295, train_loss: 0.0604, step time: 1.0682\n",
      "150/295, train_loss: 0.0366, step time: 1.0621\n",
      "151/295, train_loss: 0.3553, step time: 1.0517\n",
      "152/295, train_loss: 0.0347, step time: 1.0560\n",
      "153/295, train_loss: 0.0635, step time: 1.0591\n",
      "154/295, train_loss: 0.0801, step time: 1.0321\n",
      "155/295, train_loss: 0.0402, step time: 1.0489\n",
      "156/295, train_loss: 0.0363, step time: 1.0385\n",
      "157/295, train_loss: 0.0320, step time: 1.0382\n",
      "158/295, train_loss: 0.0267, step time: 1.0341\n",
      "159/295, train_loss: 0.0396, step time: 1.0355\n",
      "160/295, train_loss: 0.0513, step time: 1.0434\n",
      "161/295, train_loss: 0.0283, step time: 1.0708\n",
      "162/295, train_loss: 0.0583, step time: 1.0375\n",
      "163/295, train_loss: 0.0886, step time: 1.0362\n",
      "164/295, train_loss: 0.0674, step time: 1.0418\n",
      "165/295, train_loss: 0.0339, step time: 1.0349\n",
      "166/295, train_loss: 0.0477, step time: 1.0395\n",
      "167/295, train_loss: 0.0373, step time: 1.0374\n",
      "168/295, train_loss: 0.0391, step time: 1.0495\n",
      "169/295, train_loss: 0.4191, step time: 1.0384\n",
      "170/295, train_loss: 0.0490, step time: 1.0683\n",
      "171/295, train_loss: 0.0314, step time: 1.0423\n",
      "172/295, train_loss: 0.1135, step time: 1.0370\n",
      "173/295, train_loss: 0.0504, step time: 1.0319\n",
      "174/295, train_loss: 0.0317, step time: 1.0571\n",
      "175/295, train_loss: 0.2052, step time: 1.0593\n",
      "176/295, train_loss: 0.0267, step time: 1.0530\n",
      "177/295, train_loss: 0.0652, step time: 1.0487\n",
      "178/295, train_loss: 0.0231, step time: 1.0915\n",
      "179/295, train_loss: 0.0339, step time: 1.0595\n",
      "180/295, train_loss: 0.0359, step time: 1.0750\n",
      "181/295, train_loss: 0.0493, step time: 1.0700\n",
      "182/295, train_loss: 0.0378, step time: 1.0905\n",
      "183/295, train_loss: 0.0457, step time: 1.0502\n",
      "184/295, train_loss: 0.0371, step time: 1.0352\n",
      "185/295, train_loss: 0.0506, step time: 1.0460\n",
      "186/295, train_loss: 0.0482, step time: 1.0373\n",
      "187/295, train_loss: 0.0348, step time: 1.0324\n",
      "188/295, train_loss: 0.0222, step time: 1.0482\n",
      "189/295, train_loss: 0.0676, step time: 1.0339\n",
      "190/295, train_loss: 0.0744, step time: 1.0487\n",
      "191/295, train_loss: 0.0579, step time: 1.0350\n",
      "192/295, train_loss: 0.0786, step time: 1.0434\n",
      "193/295, train_loss: 0.3723, step time: 1.0773\n",
      "194/295, train_loss: 0.0293, step time: 1.0376\n",
      "195/295, train_loss: 0.0593, step time: 1.0488\n",
      "196/295, train_loss: 0.0394, step time: 1.0604\n",
      "197/295, train_loss: 0.0470, step time: 1.0488\n",
      "198/295, train_loss: 0.0295, step time: 1.0516\n",
      "199/295, train_loss: 0.0250, step time: 1.0369\n",
      "200/295, train_loss: 0.0331, step time: 1.0616\n",
      "201/295, train_loss: 0.0405, step time: 1.0385\n",
      "202/295, train_loss: 0.0631, step time: 1.0410\n",
      "203/295, train_loss: 0.3599, step time: 1.0616\n",
      "204/295, train_loss: 0.0744, step time: 1.0376\n",
      "205/295, train_loss: 0.0230, step time: 1.0389\n",
      "206/295, train_loss: 0.0659, step time: 1.0404\n",
      "207/295, train_loss: 0.0409, step time: 1.0413\n",
      "208/295, train_loss: 0.0277, step time: 1.1075\n",
      "209/295, train_loss: 0.0554, step time: 1.0640\n",
      "210/295, train_loss: 0.0749, step time: 1.0393\n",
      "211/295, train_loss: 0.0426, step time: 1.0411\n",
      "212/295, train_loss: 0.0302, step time: 1.0501\n",
      "213/295, train_loss: 0.0712, step time: 1.0404\n",
      "214/295, train_loss: 0.0305, step time: 1.0358\n",
      "215/295, train_loss: 0.0721, step time: 1.0390\n",
      "216/295, train_loss: 0.0786, step time: 1.0387\n",
      "217/295, train_loss: 0.0856, step time: 1.0370\n",
      "218/295, train_loss: 0.0672, step time: 1.0450\n",
      "219/295, train_loss: 0.0661, step time: 1.0466\n",
      "220/295, train_loss: 0.0447, step time: 1.0469\n",
      "221/295, train_loss: 0.0265, step time: 1.0349\n",
      "222/295, train_loss: 0.1059, step time: 1.0435\n",
      "223/295, train_loss: 0.0432, step time: 1.0588\n",
      "224/295, train_loss: 0.0495, step time: 1.0423\n",
      "225/295, train_loss: 0.0865, step time: 1.0397\n",
      "226/295, train_loss: 0.0471, step time: 1.0974\n",
      "227/295, train_loss: 0.0633, step time: 1.0411\n",
      "228/295, train_loss: 0.0571, step time: 1.0377\n",
      "229/295, train_loss: 0.0695, step time: 1.0434\n",
      "230/295, train_loss: 0.0717, step time: 1.0390\n",
      "231/295, train_loss: 0.0346, step time: 1.0403\n",
      "232/295, train_loss: 0.0418, step time: 1.0707\n",
      "233/295, train_loss: 0.0519, step time: 1.0632\n",
      "234/295, train_loss: 0.0345, step time: 1.0363\n",
      "235/295, train_loss: 0.0353, step time: 1.0408\n",
      "236/295, train_loss: 0.0686, step time: 1.1047\n",
      "237/295, train_loss: 0.0314, step time: 1.0475\n",
      "238/295, train_loss: 0.0250, step time: 1.0702\n",
      "239/295, train_loss: 0.1131, step time: 1.0812\n",
      "240/295, train_loss: 0.0721, step time: 1.0525\n",
      "241/295, train_loss: 0.0706, step time: 1.0387\n",
      "242/295, train_loss: 0.0547, step time: 1.0432\n",
      "243/295, train_loss: 0.0459, step time: 1.0349\n",
      "244/295, train_loss: 0.0719, step time: 1.0393\n",
      "245/295, train_loss: 0.0881, step time: 1.0384\n",
      "246/295, train_loss: 0.0338, step time: 1.0435\n",
      "247/295, train_loss: 0.0486, step time: 1.0406\n",
      "248/295, train_loss: 0.0725, step time: 1.0380\n",
      "249/295, train_loss: 0.0255, step time: 1.0623\n",
      "250/295, train_loss: 0.3648, step time: 1.0425\n",
      "251/295, train_loss: 0.0272, step time: 1.0455\n",
      "252/295, train_loss: 0.0670, step time: 1.0342\n",
      "253/295, train_loss: 0.1085, step time: 1.0552\n",
      "254/295, train_loss: 0.0452, step time: 1.0369\n",
      "255/295, train_loss: 0.0572, step time: 1.0337\n",
      "256/295, train_loss: 0.0568, step time: 1.0350\n",
      "257/295, train_loss: 0.0364, step time: 1.0699\n",
      "258/295, train_loss: 0.0776, step time: 1.0325\n",
      "259/295, train_loss: 0.0440, step time: 1.0358\n",
      "260/295, train_loss: 0.0562, step time: 1.0611\n",
      "261/295, train_loss: 0.0314, step time: 1.0590\n",
      "262/295, train_loss: 0.1026, step time: 1.0451\n",
      "263/295, train_loss: 0.0272, step time: 1.0766\n",
      "264/295, train_loss: 0.0171, step time: 1.0541\n",
      "265/295, train_loss: 0.0362, step time: 1.0306\n",
      "266/295, train_loss: 0.0260, step time: 1.0523\n",
      "267/295, train_loss: 0.0737, step time: 1.0758\n",
      "268/295, train_loss: 0.0424, step time: 1.0406\n",
      "269/295, train_loss: 0.0599, step time: 1.0331\n",
      "270/295, train_loss: 0.0644, step time: 1.0841\n",
      "271/295, train_loss: 0.0497, step time: 1.0589\n",
      "272/295, train_loss: 0.0687, step time: 1.0350\n",
      "273/295, train_loss: 0.0204, step time: 1.0448\n",
      "274/295, train_loss: 0.0250, step time: 1.0396\n",
      "275/295, train_loss: 0.0613, step time: 1.0341\n",
      "276/295, train_loss: 0.0403, step time: 1.0390\n",
      "277/295, train_loss: 0.0979, step time: 1.0869\n",
      "278/295, train_loss: 0.0739, step time: 1.0539\n",
      "279/295, train_loss: 0.0392, step time: 1.0339\n",
      "280/295, train_loss: 0.0385, step time: 1.0604\n",
      "281/295, train_loss: 0.0784, step time: 1.0733\n",
      "282/295, train_loss: 0.0301, step time: 1.0584\n",
      "283/295, train_loss: 0.0332, step time: 1.0374\n",
      "284/295, train_loss: 0.0735, step time: 1.0358\n",
      "285/295, train_loss: 0.1269, step time: 1.0351\n",
      "286/295, train_loss: 0.0448, step time: 1.0425\n",
      "287/295, train_loss: 0.0662, step time: 1.0334\n",
      "288/295, train_loss: 0.0328, step time: 1.0334\n",
      "289/295, train_loss: 0.0588, step time: 1.0290\n",
      "290/295, train_loss: 0.0249, step time: 1.0294\n",
      "291/295, train_loss: 0.0781, step time: 1.0352\n",
      "292/295, train_loss: 0.0760, step time: 1.0286\n",
      "293/295, train_loss: 0.3660, step time: 1.0296\n",
      "294/295, train_loss: 0.0409, step time: 1.0286\n",
      "295/295, train_loss: 0.3686, step time: 1.0285\n",
      "epoch 90 average loss: 0.0775\n",
      "current epoch: 90 current mean dice: 0.7447 tc: 0.6969 wt: 0.8115 et: 0.7320\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 90 is: 382.9438\n",
      "----------\n",
      "epoch 91/100\n",
      "1/295, train_loss: 0.0380, step time: 1.1446\n",
      "2/295, train_loss: 0.0720, step time: 1.0776\n",
      "3/295, train_loss: 0.0378, step time: 1.0697\n",
      "4/295, train_loss: 0.0413, step time: 1.0450\n",
      "5/295, train_loss: 0.0245, step time: 1.0388\n",
      "6/295, train_loss: 0.0452, step time: 1.0453\n",
      "7/295, train_loss: 0.3601, step time: 1.0364\n",
      "8/295, train_loss: 0.0973, step time: 1.0380\n",
      "9/295, train_loss: 0.0973, step time: 1.0349\n",
      "10/295, train_loss: 0.0668, step time: 1.0772\n",
      "11/295, train_loss: 0.0888, step time: 1.0294\n",
      "12/295, train_loss: 0.0395, step time: 1.0392\n",
      "13/295, train_loss: 0.0632, step time: 1.0572\n",
      "14/295, train_loss: 0.0798, step time: 1.0392\n",
      "15/295, train_loss: 0.0603, step time: 1.0479\n",
      "16/295, train_loss: 0.3621, step time: 1.0344\n",
      "17/295, train_loss: 0.0455, step time: 1.0323\n",
      "18/295, train_loss: 0.0337, step time: 1.0304\n",
      "19/295, train_loss: 0.0490, step time: 1.0318\n",
      "20/295, train_loss: 0.0580, step time: 1.0609\n",
      "21/295, train_loss: 0.0716, step time: 1.0568\n",
      "22/295, train_loss: 0.0698, step time: 1.0412\n",
      "23/295, train_loss: 0.3646, step time: 1.0461\n",
      "24/295, train_loss: 0.0331, step time: 1.0442\n",
      "25/295, train_loss: 0.3584, step time: 1.0597\n",
      "26/295, train_loss: 0.0470, step time: 1.0327\n",
      "27/295, train_loss: 0.0251, step time: 1.0449\n",
      "28/295, train_loss: 0.0406, step time: 1.0338\n",
      "29/295, train_loss: 0.0222, step time: 1.0599\n",
      "30/295, train_loss: 0.0260, step time: 1.0903\n",
      "31/295, train_loss: 0.0683, step time: 1.0389\n",
      "32/295, train_loss: 0.0469, step time: 1.0636\n",
      "33/295, train_loss: 0.0349, step time: 1.0400\n",
      "34/295, train_loss: 0.0319, step time: 1.0449\n",
      "35/295, train_loss: 0.0453, step time: 1.0620\n",
      "36/295, train_loss: 0.0662, step time: 1.0610\n",
      "37/295, train_loss: 0.0732, step time: 1.0328\n",
      "38/295, train_loss: 0.0679, step time: 1.0326\n",
      "39/295, train_loss: 0.3552, step time: 1.0494\n",
      "40/295, train_loss: 0.0286, step time: 1.0470\n",
      "41/295, train_loss: 0.0553, step time: 1.0438\n",
      "42/295, train_loss: 0.0304, step time: 1.0399\n",
      "43/295, train_loss: 0.0741, step time: 1.1114\n",
      "44/295, train_loss: 0.0312, step time: 1.0555\n",
      "45/295, train_loss: 0.0497, step time: 1.0356\n",
      "46/295, train_loss: 0.0577, step time: 1.1296\n",
      "47/295, train_loss: 0.0340, step time: 1.0369\n",
      "48/295, train_loss: 0.0337, step time: 1.0673\n",
      "49/295, train_loss: 0.0360, step time: 1.0807\n",
      "50/295, train_loss: 0.0416, step time: 1.0346\n",
      "51/295, train_loss: 0.3684, step time: 1.0329\n",
      "52/295, train_loss: 0.0736, step time: 1.0800\n",
      "53/295, train_loss: 0.0536, step time: 1.0472\n",
      "54/295, train_loss: 0.0329, step time: 1.0456\n",
      "55/295, train_loss: 0.0943, step time: 1.0435\n",
      "56/295, train_loss: 0.0585, step time: 1.0358\n",
      "57/295, train_loss: 0.0267, step time: 1.0330\n",
      "58/295, train_loss: 0.0908, step time: 1.0371\n",
      "59/295, train_loss: 0.0785, step time: 1.0585\n",
      "60/295, train_loss: 0.0501, step time: 1.0569\n",
      "61/295, train_loss: 0.0360, step time: 1.0400\n",
      "62/295, train_loss: 0.0256, step time: 1.0664\n",
      "63/295, train_loss: 0.0305, step time: 1.0382\n",
      "64/295, train_loss: 0.2055, step time: 1.0584\n",
      "65/295, train_loss: 0.0422, step time: 1.0344\n",
      "66/295, train_loss: 0.0335, step time: 1.0366\n",
      "67/295, train_loss: 0.0455, step time: 1.0462\n",
      "68/295, train_loss: 0.0534, step time: 1.0313\n",
      "69/295, train_loss: 0.0750, step time: 1.1026\n",
      "70/295, train_loss: 0.0522, step time: 1.0419\n",
      "71/295, train_loss: 0.0779, step time: 1.0621\n",
      "72/295, train_loss: 0.0852, step time: 1.0582\n",
      "73/295, train_loss: 0.3799, step time: 1.0442\n",
      "74/295, train_loss: 0.0253, step time: 1.0321\n",
      "75/295, train_loss: 0.0609, step time: 1.0496\n",
      "76/295, train_loss: 0.0898, step time: 1.0378\n",
      "77/295, train_loss: 0.0713, step time: 1.0391\n",
      "78/295, train_loss: 0.0535, step time: 1.0391\n",
      "79/295, train_loss: 0.0290, step time: 1.0715\n",
      "80/295, train_loss: 0.0444, step time: 1.0332\n",
      "81/295, train_loss: 0.0275, step time: 1.0904\n",
      "82/295, train_loss: 0.0649, step time: 1.0349\n",
      "83/295, train_loss: 0.1081, step time: 1.0455\n",
      "84/295, train_loss: 0.3771, step time: 1.0605\n",
      "85/295, train_loss: 0.0445, step time: 1.0689\n",
      "86/295, train_loss: 0.0812, step time: 1.0322\n",
      "87/295, train_loss: 0.0710, step time: 1.0443\n",
      "88/295, train_loss: 0.0510, step time: 1.0327\n",
      "89/295, train_loss: 0.0490, step time: 1.0383\n",
      "90/295, train_loss: 0.0342, step time: 1.0380\n",
      "91/295, train_loss: 0.0549, step time: 1.0833\n",
      "92/295, train_loss: 0.0679, step time: 1.0378\n",
      "93/295, train_loss: 0.0286, step time: 1.0438\n",
      "94/295, train_loss: 0.0376, step time: 1.0642\n",
      "95/295, train_loss: 0.0417, step time: 1.0317\n",
      "96/295, train_loss: 0.0317, step time: 1.0434\n",
      "97/295, train_loss: 0.0454, step time: 1.0402\n",
      "98/295, train_loss: 0.0221, step time: 1.0406\n",
      "99/295, train_loss: 0.0380, step time: 1.0328\n",
      "100/295, train_loss: 0.0528, step time: 1.0392\n",
      "101/295, train_loss: 0.0572, step time: 1.0325\n",
      "102/295, train_loss: 0.3770, step time: 1.0370\n",
      "103/295, train_loss: 0.0663, step time: 1.0361\n",
      "104/295, train_loss: 0.0764, step time: 1.0588\n",
      "105/295, train_loss: 0.0310, step time: 1.0337\n",
      "106/295, train_loss: 0.0724, step time: 1.0356\n",
      "107/295, train_loss: 0.0664, step time: 1.0338\n",
      "108/295, train_loss: 0.0251, step time: 1.0359\n",
      "109/295, train_loss: 0.0466, step time: 1.0787\n",
      "110/295, train_loss: 0.0489, step time: 1.0312\n",
      "111/295, train_loss: 0.0485, step time: 1.0362\n",
      "112/295, train_loss: 0.1128, step time: 1.0385\n",
      "113/295, train_loss: 0.0277, step time: 1.0354\n",
      "114/295, train_loss: 0.0303, step time: 1.0423\n",
      "115/295, train_loss: 0.0491, step time: 1.0548\n",
      "116/295, train_loss: 0.0665, step time: 1.0310\n",
      "117/295, train_loss: 0.0351, step time: 1.0355\n",
      "118/295, train_loss: 0.3727, step time: 1.0379\n",
      "119/295, train_loss: 0.0746, step time: 1.0364\n",
      "120/295, train_loss: 0.0561, step time: 1.0378\n",
      "121/295, train_loss: 0.0470, step time: 1.0448\n",
      "122/295, train_loss: 0.0396, step time: 1.0599\n",
      "123/295, train_loss: 0.0658, step time: 1.0393\n",
      "124/295, train_loss: 0.0814, step time: 1.0411\n",
      "125/295, train_loss: 0.0302, step time: 1.0451\n",
      "126/295, train_loss: 0.0271, step time: 1.1113\n",
      "127/295, train_loss: 0.0455, step time: 1.0507\n",
      "128/295, train_loss: 0.0343, step time: 1.0670\n",
      "129/295, train_loss: 0.0603, step time: 1.0450\n",
      "130/295, train_loss: 0.0305, step time: 1.0340\n",
      "131/295, train_loss: 0.0458, step time: 1.0534\n",
      "132/295, train_loss: 0.0570, step time: 1.0830\n",
      "133/295, train_loss: 0.0208, step time: 1.0430\n",
      "134/295, train_loss: 0.0371, step time: 1.0437\n",
      "135/295, train_loss: 0.0485, step time: 1.0401\n",
      "136/295, train_loss: 0.0569, step time: 1.0338\n",
      "137/295, train_loss: 0.3769, step time: 1.0534\n",
      "138/295, train_loss: 0.0531, step time: 1.0389\n",
      "139/295, train_loss: 0.0452, step time: 1.0550\n",
      "140/295, train_loss: 0.0719, step time: 1.0595\n",
      "141/295, train_loss: 0.0460, step time: 1.0559\n",
      "142/295, train_loss: 0.0596, step time: 1.0345\n",
      "143/295, train_loss: 0.0400, step time: 1.0334\n",
      "144/295, train_loss: 0.4183, step time: 1.0445\n",
      "145/295, train_loss: 0.1216, step time: 1.0395\n",
      "146/295, train_loss: 0.0275, step time: 1.0415\n",
      "147/295, train_loss: 0.0930, step time: 1.1026\n",
      "148/295, train_loss: 0.0686, step time: 1.0365\n",
      "149/295, train_loss: 0.0707, step time: 1.0440\n",
      "150/295, train_loss: 0.0245, step time: 1.0432\n",
      "151/295, train_loss: 0.0579, step time: 1.0560\n",
      "152/295, train_loss: 0.0411, step time: 1.0904\n",
      "153/295, train_loss: 0.0862, step time: 1.0565\n",
      "154/295, train_loss: 0.0684, step time: 1.0345\n",
      "155/295, train_loss: 0.0332, step time: 1.0312\n",
      "156/295, train_loss: 0.0429, step time: 1.0583\n",
      "157/295, train_loss: 0.0549, step time: 1.0308\n",
      "158/295, train_loss: 0.0346, step time: 1.0344\n",
      "159/295, train_loss: 0.0558, step time: 1.0417\n",
      "160/295, train_loss: 0.0886, step time: 1.0671\n",
      "161/295, train_loss: 0.0632, step time: 1.1002\n",
      "162/295, train_loss: 0.3739, step time: 1.0579\n",
      "163/295, train_loss: 0.3774, step time: 1.0389\n",
      "164/295, train_loss: 0.3929, step time: 1.0380\n",
      "165/295, train_loss: 0.0404, step time: 1.0389\n",
      "166/295, train_loss: 0.0223, step time: 1.0330\n",
      "167/295, train_loss: 0.3575, step time: 1.0369\n",
      "168/295, train_loss: 0.0487, step time: 1.0355\n",
      "169/295, train_loss: 0.3599, step time: 1.0606\n",
      "170/295, train_loss: 0.0712, step time: 1.0347\n",
      "171/295, train_loss: 0.0385, step time: 1.0362\n",
      "172/295, train_loss: 0.0783, step time: 1.0399\n",
      "173/295, train_loss: 0.0287, step time: 1.0451\n",
      "174/295, train_loss: 0.3706, step time: 1.0574\n",
      "175/295, train_loss: 0.0269, step time: 1.0355\n",
      "176/295, train_loss: 0.0876, step time: 1.0314\n",
      "177/295, train_loss: 0.0483, step time: 1.0347\n",
      "178/295, train_loss: 0.0573, step time: 1.0467\n",
      "179/295, train_loss: 0.0699, step time: 1.0358\n",
      "180/295, train_loss: 0.0725, step time: 1.0389\n",
      "181/295, train_loss: 0.0725, step time: 1.0568\n",
      "182/295, train_loss: 0.0784, step time: 1.0358\n",
      "183/295, train_loss: 0.0305, step time: 1.0647\n",
      "184/295, train_loss: 0.0517, step time: 1.0341\n",
      "185/295, train_loss: 0.0445, step time: 1.0432\n",
      "186/295, train_loss: 0.0431, step time: 1.0583\n",
      "187/295, train_loss: 0.0391, step time: 1.0584\n",
      "188/295, train_loss: 0.1258, step time: 1.0652\n",
      "189/295, train_loss: 0.1129, step time: 1.0644\n",
      "190/295, train_loss: 0.0686, step time: 1.0406\n",
      "191/295, train_loss: 0.0743, step time: 1.0329\n",
      "192/295, train_loss: 0.0505, step time: 1.0322\n",
      "193/295, train_loss: 0.0397, step time: 1.0469\n",
      "194/295, train_loss: 0.0225, step time: 1.0710\n",
      "195/295, train_loss: 0.0614, step time: 1.0817\n",
      "196/295, train_loss: 0.0461, step time: 1.0381\n",
      "197/295, train_loss: 0.0199, step time: 1.0396\n",
      "198/295, train_loss: 0.0619, step time: 1.0423\n",
      "199/295, train_loss: 0.0513, step time: 1.0431\n",
      "200/295, train_loss: 0.0410, step time: 1.0357\n",
      "201/295, train_loss: 0.0379, step time: 1.0442\n",
      "202/295, train_loss: 0.3863, step time: 1.0348\n",
      "203/295, train_loss: 0.0747, step time: 1.0319\n",
      "204/295, train_loss: 0.0338, step time: 1.0852\n",
      "205/295, train_loss: 0.0776, step time: 1.0340\n",
      "206/295, train_loss: 0.3738, step time: 1.0587\n",
      "207/295, train_loss: 0.0357, step time: 1.0681\n",
      "208/295, train_loss: 0.0313, step time: 1.0472\n",
      "209/295, train_loss: 0.0602, step time: 1.0621\n",
      "210/295, train_loss: 0.0403, step time: 1.0711\n",
      "211/295, train_loss: 0.0291, step time: 1.0500\n",
      "212/295, train_loss: 0.0464, step time: 1.0940\n",
      "213/295, train_loss: 0.0778, step time: 1.0371\n",
      "214/295, train_loss: 0.0369, step time: 1.0577\n",
      "215/295, train_loss: 0.0715, step time: 1.0341\n",
      "216/295, train_loss: 0.0783, step time: 1.0432\n",
      "217/295, train_loss: 0.0296, step time: 1.0543\n",
      "218/295, train_loss: 0.0502, step time: 1.0637\n",
      "219/295, train_loss: 0.0517, step time: 1.0317\n",
      "220/295, train_loss: 0.0619, step time: 1.0472\n",
      "221/295, train_loss: 0.0438, step time: 1.0425\n",
      "222/295, train_loss: 0.0978, step time: 1.0403\n",
      "223/295, train_loss: 0.0245, step time: 1.0545\n",
      "224/295, train_loss: 0.0659, step time: 1.0360\n",
      "225/295, train_loss: 0.3672, step time: 1.0345\n",
      "226/295, train_loss: 0.0904, step time: 1.0425\n",
      "227/295, train_loss: 0.0901, step time: 1.0473\n",
      "228/295, train_loss: 0.0604, step time: 1.0430\n",
      "229/295, train_loss: 0.0550, step time: 1.0461\n",
      "230/295, train_loss: 0.0312, step time: 1.0399\n",
      "231/295, train_loss: 0.0434, step time: 1.0487\n",
      "232/295, train_loss: 0.0433, step time: 1.0316\n",
      "233/295, train_loss: 0.0768, step time: 1.0357\n",
      "234/295, train_loss: 0.0330, step time: 1.0421\n",
      "235/295, train_loss: 0.0399, step time: 1.1306\n",
      "236/295, train_loss: 0.0703, step time: 1.0411\n",
      "237/295, train_loss: 0.0649, step time: 1.0490\n",
      "238/295, train_loss: 0.0515, step time: 1.0965\n",
      "239/295, train_loss: 0.1012, step time: 1.1107\n",
      "240/295, train_loss: 0.0760, step time: 1.0430\n",
      "241/295, train_loss: 0.0892, step time: 1.1389\n",
      "242/295, train_loss: 0.0439, step time: 1.0386\n",
      "243/295, train_loss: 0.0270, step time: 1.0455\n",
      "244/295, train_loss: 0.0766, step time: 1.0322\n",
      "245/295, train_loss: 0.0229, step time: 1.0516\n",
      "246/295, train_loss: 0.0490, step time: 1.0353\n",
      "247/295, train_loss: 0.0361, step time: 1.0404\n",
      "248/295, train_loss: 0.1023, step time: 1.1225\n",
      "249/295, train_loss: 0.0635, step time: 1.0907\n",
      "250/295, train_loss: 0.0173, step time: 1.0395\n",
      "251/295, train_loss: 0.0495, step time: 1.0602\n",
      "252/295, train_loss: 0.1113, step time: 1.0387\n",
      "253/295, train_loss: 0.0442, step time: 1.0542\n",
      "254/295, train_loss: 0.0256, step time: 1.0389\n",
      "255/295, train_loss: 0.0400, step time: 1.0363\n",
      "256/295, train_loss: 0.3720, step time: 1.0337\n",
      "257/295, train_loss: 0.0683, step time: 1.0334\n",
      "258/295, train_loss: 0.0641, step time: 1.0388\n",
      "259/295, train_loss: 0.0589, step time: 1.0500\n",
      "260/295, train_loss: 0.0478, step time: 1.0575\n",
      "261/295, train_loss: 0.0569, step time: 1.1356\n",
      "262/295, train_loss: 0.0636, step time: 1.0743\n",
      "263/295, train_loss: 0.0485, step time: 1.0339\n",
      "264/295, train_loss: 0.0602, step time: 1.0375\n",
      "265/295, train_loss: 0.0288, step time: 1.0432\n",
      "266/295, train_loss: 0.0565, step time: 1.0428\n",
      "267/295, train_loss: 0.0391, step time: 1.0376\n",
      "268/295, train_loss: 0.0477, step time: 1.0931\n",
      "269/295, train_loss: 0.0667, step time: 1.0621\n",
      "270/295, train_loss: 0.0267, step time: 1.0634\n",
      "271/295, train_loss: 0.0847, step time: 1.1274\n",
      "272/295, train_loss: 0.0619, step time: 1.0436\n",
      "273/295, train_loss: 0.0490, step time: 1.0804\n",
      "274/295, train_loss: 0.0338, step time: 1.0667\n",
      "275/295, train_loss: 0.0422, step time: 1.0336\n",
      "276/295, train_loss: 0.0404, step time: 1.0407\n",
      "277/295, train_loss: 0.0368, step time: 1.0352\n",
      "278/295, train_loss: 0.0426, step time: 1.0392\n",
      "279/295, train_loss: 0.0248, step time: 1.0711\n",
      "280/295, train_loss: 0.0387, step time: 1.0351\n",
      "281/295, train_loss: 0.0267, step time: 1.0324\n",
      "282/295, train_loss: 0.0440, step time: 1.0356\n",
      "283/295, train_loss: 0.0594, step time: 1.0354\n",
      "284/295, train_loss: 0.1066, step time: 1.0435\n",
      "285/295, train_loss: 0.0337, step time: 1.0361\n",
      "286/295, train_loss: 0.0501, step time: 1.0422\n",
      "287/295, train_loss: 0.0375, step time: 1.0499\n",
      "288/295, train_loss: 0.0528, step time: 1.0320\n",
      "289/295, train_loss: 0.0240, step time: 1.0341\n",
      "290/295, train_loss: 0.0621, step time: 1.0285\n",
      "291/295, train_loss: 0.0551, step time: 1.0284\n",
      "292/295, train_loss: 0.0629, step time: 1.0285\n",
      "293/295, train_loss: 0.0572, step time: 1.0286\n",
      "294/295, train_loss: 0.0266, step time: 1.0333\n",
      "295/295, train_loss: 0.0365, step time: 1.0289\n",
      "epoch 91 average loss: 0.0774\n",
      "current epoch: 91 current mean dice: 0.7220 tc: 0.6593 wt: 0.8118 et: 0.7022\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 91 is: 386.1198\n",
      "----------\n",
      "epoch 92/100\n",
      "1/295, train_loss: 0.0703, step time: 1.0750\n",
      "2/295, train_loss: 0.0892, step time: 1.1811\n",
      "3/295, train_loss: 0.0565, step time: 1.0727\n",
      "4/295, train_loss: 0.0464, step time: 1.0677\n",
      "5/295, train_loss: 0.0574, step time: 1.0319\n",
      "6/295, train_loss: 0.3716, step time: 1.0836\n",
      "7/295, train_loss: 0.0544, step time: 1.0752\n",
      "8/295, train_loss: 0.0762, step time: 1.0637\n",
      "9/295, train_loss: 0.0470, step time: 1.0370\n",
      "10/295, train_loss: 0.0397, step time: 1.0742\n",
      "11/295, train_loss: 0.0470, step time: 1.1133\n",
      "12/295, train_loss: 0.0433, step time: 1.0692\n",
      "13/295, train_loss: 0.0209, step time: 1.0634\n",
      "14/295, train_loss: 0.0392, step time: 1.0407\n",
      "15/295, train_loss: 0.0339, step time: 1.0466\n",
      "16/295, train_loss: 0.3767, step time: 1.1107\n",
      "17/295, train_loss: 0.0695, step time: 1.0304\n",
      "18/295, train_loss: 0.0171, step time: 1.0314\n",
      "19/295, train_loss: 0.0577, step time: 1.0346\n",
      "20/295, train_loss: 0.3702, step time: 1.0375\n",
      "21/295, train_loss: 0.3555, step time: 1.0385\n",
      "22/295, train_loss: 0.0698, step time: 1.0396\n",
      "23/295, train_loss: 0.3797, step time: 1.0442\n",
      "24/295, train_loss: 0.0844, step time: 1.0659\n",
      "25/295, train_loss: 0.0389, step time: 1.1187\n",
      "26/295, train_loss: 0.0271, step time: 1.0501\n",
      "27/295, train_loss: 0.0450, step time: 1.0324\n",
      "28/295, train_loss: 0.0417, step time: 1.0516\n",
      "29/295, train_loss: 0.0658, step time: 1.0338\n",
      "30/295, train_loss: 0.0406, step time: 1.0504\n",
      "31/295, train_loss: 0.0395, step time: 1.0991\n",
      "32/295, train_loss: 0.0224, step time: 1.0615\n",
      "33/295, train_loss: 0.3599, step time: 1.0382\n",
      "34/295, train_loss: 0.0434, step time: 1.0328\n",
      "35/295, train_loss: 0.0744, step time: 1.0362\n",
      "36/295, train_loss: 0.0444, step time: 1.0700\n",
      "37/295, train_loss: 0.0889, step time: 1.0491\n",
      "38/295, train_loss: 0.0221, step time: 1.0593\n",
      "39/295, train_loss: 0.0311, step time: 1.0361\n",
      "40/295, train_loss: 0.0445, step time: 1.0487\n",
      "41/295, train_loss: 0.0317, step time: 1.0395\n",
      "42/295, train_loss: 0.0382, step time: 1.0325\n",
      "43/295, train_loss: 0.0296, step time: 1.0316\n",
      "44/295, train_loss: 0.0319, step time: 1.0389\n",
      "45/295, train_loss: 0.0536, step time: 1.0321\n",
      "46/295, train_loss: 0.0872, step time: 1.1216\n",
      "47/295, train_loss: 0.0346, step time: 1.0295\n",
      "48/295, train_loss: 0.0407, step time: 1.0546\n",
      "49/295, train_loss: 0.0423, step time: 1.0401\n",
      "50/295, train_loss: 0.0813, step time: 1.0670\n",
      "51/295, train_loss: 0.0726, step time: 1.0609\n",
      "52/295, train_loss: 0.0886, step time: 1.0538\n",
      "53/295, train_loss: 0.0405, step time: 1.0331\n",
      "54/295, train_loss: 0.0247, step time: 1.0981\n",
      "55/295, train_loss: 0.0346, step time: 1.1012\n",
      "56/295, train_loss: 0.0602, step time: 1.0644\n",
      "57/295, train_loss: 0.0484, step time: 1.0407\n",
      "58/295, train_loss: 0.0290, step time: 1.0327\n",
      "59/295, train_loss: 0.0639, step time: 1.0397\n",
      "60/295, train_loss: 0.0304, step time: 1.1171\n",
      "61/295, train_loss: 0.0420, step time: 1.0715\n",
      "62/295, train_loss: 0.0484, step time: 1.1317\n",
      "63/295, train_loss: 0.0389, step time: 1.0473\n",
      "64/295, train_loss: 0.0241, step time: 1.0297\n",
      "65/295, train_loss: 0.0666, step time: 1.0419\n",
      "66/295, train_loss: 0.0338, step time: 1.0516\n",
      "67/295, train_loss: 0.0782, step time: 1.0604\n",
      "68/295, train_loss: 0.0324, step time: 1.0339\n",
      "69/295, train_loss: 0.0505, step time: 1.0345\n",
      "70/295, train_loss: 0.3920, step time: 1.0418\n",
      "71/295, train_loss: 0.1253, step time: 1.0513\n",
      "72/295, train_loss: 0.0760, step time: 1.0324\n",
      "73/295, train_loss: 0.0360, step time: 1.0439\n",
      "74/295, train_loss: 0.0451, step time: 1.0422\n",
      "75/295, train_loss: 0.0753, step time: 1.0497\n",
      "76/295, train_loss: 0.0451, step time: 1.0524\n",
      "77/295, train_loss: 0.0897, step time: 1.0700\n",
      "78/295, train_loss: 0.1060, step time: 1.0369\n",
      "79/295, train_loss: 0.0764, step time: 1.0462\n",
      "80/295, train_loss: 0.0596, step time: 1.0302\n",
      "81/295, train_loss: 0.0259, step time: 1.0830\n",
      "82/295, train_loss: 0.0504, step time: 1.0443\n",
      "83/295, train_loss: 0.0410, step time: 1.0404\n",
      "84/295, train_loss: 0.0673, step time: 1.0601\n",
      "85/295, train_loss: 0.0727, step time: 1.0418\n",
      "86/295, train_loss: 0.0584, step time: 1.0332\n",
      "87/295, train_loss: 0.0971, step time: 1.0569\n",
      "88/295, train_loss: 0.0373, step time: 1.0530\n",
      "89/295, train_loss: 0.3654, step time: 1.0442\n",
      "90/295, train_loss: 0.0227, step time: 1.0311\n",
      "91/295, train_loss: 0.0699, step time: 1.0467\n",
      "92/295, train_loss: 0.0567, step time: 1.1558\n",
      "93/295, train_loss: 0.0591, step time: 1.0406\n",
      "94/295, train_loss: 0.0332, step time: 1.0933\n",
      "95/295, train_loss: 0.0288, step time: 1.0606\n",
      "96/295, train_loss: 0.0744, step time: 1.0386\n",
      "97/295, train_loss: 0.0426, step time: 1.0536\n",
      "98/295, train_loss: 0.0246, step time: 1.0338\n",
      "99/295, train_loss: 0.0527, step time: 1.0577\n",
      "100/295, train_loss: 0.0405, step time: 1.0464\n",
      "101/295, train_loss: 0.0362, step time: 1.0347\n",
      "102/295, train_loss: 0.0715, step time: 1.0338\n",
      "103/295, train_loss: 0.0635, step time: 1.0448\n",
      "104/295, train_loss: 0.0262, step time: 1.0971\n",
      "105/295, train_loss: 0.0482, step time: 1.0521\n",
      "106/295, train_loss: 0.0759, step time: 1.0348\n",
      "107/295, train_loss: 0.0774, step time: 1.0370\n",
      "108/295, train_loss: 0.0441, step time: 1.0458\n",
      "109/295, train_loss: 0.0372, step time: 1.0343\n",
      "110/295, train_loss: 0.0485, step time: 1.0462\n",
      "111/295, train_loss: 0.0712, step time: 1.0412\n",
      "112/295, train_loss: 0.0500, step time: 1.0457\n",
      "113/295, train_loss: 0.3683, step time: 1.0504\n",
      "114/295, train_loss: 0.0620, step time: 1.0403\n",
      "115/295, train_loss: 0.0369, step time: 1.0350\n",
      "116/295, train_loss: 0.0385, step time: 1.1153\n",
      "117/295, train_loss: 0.0608, step time: 1.0565\n",
      "118/295, train_loss: 0.0389, step time: 1.0375\n",
      "119/295, train_loss: 0.0432, step time: 1.0587\n",
      "120/295, train_loss: 0.0394, step time: 1.0433\n",
      "121/295, train_loss: 0.0616, step time: 1.0455\n",
      "122/295, train_loss: 0.0774, step time: 1.0774\n",
      "123/295, train_loss: 0.0681, step time: 1.1079\n",
      "124/295, train_loss: 0.0512, step time: 1.0442\n",
      "125/295, train_loss: 0.1006, step time: 1.0478\n",
      "126/295, train_loss: 0.3776, step time: 1.0572\n",
      "127/295, train_loss: 0.0464, step time: 1.0299\n",
      "128/295, train_loss: 0.0386, step time: 1.0352\n",
      "129/295, train_loss: 0.0517, step time: 1.0478\n",
      "130/295, train_loss: 0.0220, step time: 1.0334\n",
      "131/295, train_loss: 0.0674, step time: 1.0723\n",
      "132/295, train_loss: 0.0459, step time: 1.0465\n",
      "133/295, train_loss: 0.0423, step time: 1.0357\n",
      "134/295, train_loss: 0.0620, step time: 1.0326\n",
      "135/295, train_loss: 0.0285, step time: 1.0741\n",
      "136/295, train_loss: 0.0313, step time: 1.0599\n",
      "137/295, train_loss: 0.0773, step time: 1.1467\n",
      "138/295, train_loss: 0.0358, step time: 1.0358\n",
      "139/295, train_loss: 0.0449, step time: 1.0369\n",
      "140/295, train_loss: 0.4182, step time: 1.0395\n",
      "141/295, train_loss: 0.0340, step time: 1.0348\n",
      "142/295, train_loss: 0.0568, step time: 1.0402\n",
      "143/295, train_loss: 0.1131, step time: 1.0416\n",
      "144/295, train_loss: 0.0734, step time: 1.0368\n",
      "145/295, train_loss: 0.0247, step time: 1.0355\n",
      "146/295, train_loss: 0.0311, step time: 1.0549\n",
      "147/295, train_loss: 0.0498, step time: 1.0587\n",
      "148/295, train_loss: 0.0549, step time: 1.1061\n",
      "149/295, train_loss: 0.0675, step time: 1.0722\n",
      "150/295, train_loss: 0.0925, step time: 1.0424\n",
      "151/295, train_loss: 0.0441, step time: 1.1175\n",
      "152/295, train_loss: 0.0241, step time: 1.0362\n",
      "153/295, train_loss: 0.0559, step time: 1.0548\n",
      "154/295, train_loss: 0.0608, step time: 1.0397\n",
      "155/295, train_loss: 0.0301, step time: 1.0670\n",
      "156/295, train_loss: 0.3762, step time: 1.0470\n",
      "157/295, train_loss: 0.0570, step time: 1.0565\n",
      "158/295, train_loss: 0.0437, step time: 1.0387\n",
      "159/295, train_loss: 0.0493, step time: 1.0358\n",
      "160/295, train_loss: 0.0934, step time: 1.0667\n",
      "161/295, train_loss: 0.0260, step time: 1.0495\n",
      "162/295, train_loss: 0.0596, step time: 1.0402\n",
      "163/295, train_loss: 0.0532, step time: 1.0595\n",
      "164/295, train_loss: 0.0718, step time: 1.0379\n",
      "165/295, train_loss: 0.0440, step time: 1.0330\n",
      "166/295, train_loss: 0.0600, step time: 1.0359\n",
      "167/295, train_loss: 0.0713, step time: 1.0367\n",
      "168/295, train_loss: 0.0581, step time: 1.0557\n",
      "169/295, train_loss: 0.0364, step time: 1.0365\n",
      "170/295, train_loss: 0.0517, step time: 1.0344\n",
      "171/295, train_loss: 0.0532, step time: 1.0395\n",
      "172/295, train_loss: 0.0898, step time: 1.0644\n",
      "173/295, train_loss: 0.0298, step time: 1.0532\n",
      "174/295, train_loss: 0.1206, step time: 1.0572\n",
      "175/295, train_loss: 0.0407, step time: 1.1010\n",
      "176/295, train_loss: 0.0354, step time: 1.1453\n",
      "177/295, train_loss: 0.0957, step time: 1.0584\n",
      "178/295, train_loss: 0.0721, step time: 1.0634\n",
      "179/295, train_loss: 0.0489, step time: 1.0461\n",
      "180/295, train_loss: 0.3658, step time: 1.0723\n",
      "181/295, train_loss: 0.0333, step time: 1.1049\n",
      "182/295, train_loss: 0.0910, step time: 1.0689\n",
      "183/295, train_loss: 0.0298, step time: 1.0541\n",
      "184/295, train_loss: 0.0283, step time: 1.0635\n",
      "185/295, train_loss: 0.0461, step time: 1.0364\n",
      "186/295, train_loss: 0.0650, step time: 1.0340\n",
      "187/295, train_loss: 0.0801, step time: 1.0520\n",
      "188/295, train_loss: 0.0721, step time: 1.0744\n",
      "189/295, train_loss: 0.0595, step time: 1.0696\n",
      "190/295, train_loss: 0.0289, step time: 1.0352\n",
      "191/295, train_loss: 0.0852, step time: 1.0364\n",
      "192/295, train_loss: 0.0662, step time: 1.0332\n",
      "193/295, train_loss: 0.0547, step time: 1.0434\n",
      "194/295, train_loss: 0.0666, step time: 1.0423\n",
      "195/295, train_loss: 0.0587, step time: 1.1127\n",
      "196/295, train_loss: 0.0265, step time: 1.0404\n",
      "197/295, train_loss: 0.0266, step time: 1.0648\n",
      "198/295, train_loss: 0.0504, step time: 1.0352\n",
      "199/295, train_loss: 0.0350, step time: 1.0531\n",
      "200/295, train_loss: 0.3584, step time: 1.0649\n",
      "201/295, train_loss: 0.0488, step time: 1.0526\n",
      "202/295, train_loss: 0.0277, step time: 1.0549\n",
      "203/295, train_loss: 0.0625, step time: 1.0861\n",
      "204/295, train_loss: 0.0263, step time: 1.0450\n",
      "205/295, train_loss: 0.3853, step time: 1.0737\n",
      "206/295, train_loss: 0.0659, step time: 1.0404\n",
      "207/295, train_loss: 0.0716, step time: 1.0891\n",
      "208/295, train_loss: 0.0886, step time: 1.0476\n",
      "209/295, train_loss: 0.0674, step time: 1.0892\n",
      "210/295, train_loss: 0.0392, step time: 1.0414\n",
      "211/295, train_loss: 0.0372, step time: 1.0761\n",
      "212/295, train_loss: 0.2053, step time: 1.0604\n",
      "213/295, train_loss: 0.0708, step time: 1.0320\n",
      "214/295, train_loss: 0.0487, step time: 1.0392\n",
      "215/295, train_loss: 0.0784, step time: 1.0372\n",
      "216/295, train_loss: 0.0431, step time: 1.0425\n",
      "217/295, train_loss: 0.0359, step time: 1.0544\n",
      "218/295, train_loss: 0.1111, step time: 1.0409\n",
      "219/295, train_loss: 0.0454, step time: 1.1080\n",
      "220/295, train_loss: 0.0493, step time: 1.0734\n",
      "221/295, train_loss: 0.1020, step time: 1.0367\n",
      "222/295, train_loss: 0.0339, step time: 1.0404\n",
      "223/295, train_loss: 0.0652, step time: 1.0338\n",
      "224/295, train_loss: 0.0506, step time: 1.0414\n",
      "225/295, train_loss: 0.0454, step time: 1.0433\n",
      "226/295, train_loss: 0.0272, step time: 1.0355\n",
      "227/295, train_loss: 0.0530, step time: 1.0440\n",
      "228/295, train_loss: 0.0643, step time: 1.0455\n",
      "229/295, train_loss: 0.0306, step time: 1.0480\n",
      "230/295, train_loss: 0.3768, step time: 1.0367\n",
      "231/295, train_loss: 0.0609, step time: 1.0396\n",
      "232/295, train_loss: 0.3718, step time: 1.0723\n",
      "233/295, train_loss: 0.0268, step time: 1.0416\n",
      "234/295, train_loss: 0.0463, step time: 1.0359\n",
      "235/295, train_loss: 0.0511, step time: 1.0565\n",
      "236/295, train_loss: 0.0702, step time: 1.0385\n",
      "237/295, train_loss: 0.0793, step time: 1.0428\n",
      "238/295, train_loss: 0.0966, step time: 1.0890\n",
      "239/295, train_loss: 0.0683, step time: 1.0727\n",
      "240/295, train_loss: 0.0537, step time: 1.0439\n",
      "241/295, train_loss: 0.0342, step time: 1.0698\n",
      "242/295, train_loss: 0.0543, step time: 1.0384\n",
      "243/295, train_loss: 0.0658, step time: 1.0327\n",
      "244/295, train_loss: 0.0281, step time: 1.0471\n",
      "245/295, train_loss: 0.3598, step time: 1.0664\n",
      "246/295, train_loss: 0.1132, step time: 1.0366\n",
      "247/295, train_loss: 0.0355, step time: 1.0505\n",
      "248/295, train_loss: 0.0636, step time: 1.0369\n",
      "249/295, train_loss: 0.0637, step time: 1.0458\n",
      "250/295, train_loss: 0.0399, step time: 1.0342\n",
      "251/295, train_loss: 0.0612, step time: 1.0427\n",
      "252/295, train_loss: 0.0527, step time: 1.0368\n",
      "253/295, train_loss: 0.3738, step time: 1.0509\n",
      "254/295, train_loss: 0.0381, step time: 1.0538\n",
      "255/295, train_loss: 0.0631, step time: 1.0340\n",
      "256/295, train_loss: 0.1068, step time: 1.0638\n",
      "257/295, train_loss: 0.0312, step time: 1.0496\n",
      "258/295, train_loss: 0.0196, step time: 1.0481\n",
      "259/295, train_loss: 0.0413, step time: 1.0880\n",
      "260/295, train_loss: 0.0693, step time: 1.0366\n",
      "261/295, train_loss: 0.0779, step time: 1.0306\n",
      "262/295, train_loss: 0.0813, step time: 1.0543\n",
      "263/295, train_loss: 0.0466, step time: 1.0403\n",
      "264/295, train_loss: 0.3623, step time: 1.0770\n",
      "265/295, train_loss: 0.0564, step time: 1.0625\n",
      "266/295, train_loss: 0.0493, step time: 1.0482\n",
      "267/295, train_loss: 0.0845, step time: 1.0412\n",
      "268/295, train_loss: 0.0524, step time: 1.0400\n",
      "269/295, train_loss: 0.0478, step time: 1.0660\n",
      "270/295, train_loss: 0.0245, step time: 1.0581\n",
      "271/295, train_loss: 0.0378, step time: 1.0401\n",
      "272/295, train_loss: 0.0336, step time: 1.0443\n",
      "273/295, train_loss: 0.0340, step time: 1.0542\n",
      "274/295, train_loss: 0.0305, step time: 1.1097\n",
      "275/295, train_loss: 0.0254, step time: 1.0559\n",
      "276/295, train_loss: 0.0379, step time: 1.0361\n",
      "277/295, train_loss: 0.0461, step time: 1.0397\n",
      "278/295, train_loss: 0.0571, step time: 1.0429\n",
      "279/295, train_loss: 0.0563, step time: 1.0390\n",
      "280/295, train_loss: 0.3567, step time: 1.0337\n",
      "281/295, train_loss: 0.0741, step time: 1.0736\n",
      "282/295, train_loss: 0.0745, step time: 1.0454\n",
      "283/295, train_loss: 0.0330, step time: 1.0355\n",
      "284/295, train_loss: 0.3738, step time: 1.0377\n",
      "285/295, train_loss: 0.0250, step time: 1.0564\n",
      "286/295, train_loss: 0.0332, step time: 1.0448\n",
      "287/295, train_loss: 0.0555, step time: 1.0704\n",
      "288/295, train_loss: 0.0264, step time: 1.0324\n",
      "289/295, train_loss: 0.0462, step time: 1.0298\n",
      "290/295, train_loss: 0.0475, step time: 1.0288\n",
      "291/295, train_loss: 0.0470, step time: 1.0287\n",
      "292/295, train_loss: 0.0305, step time: 1.0293\n",
      "293/295, train_loss: 0.0732, step time: 1.0308\n",
      "294/295, train_loss: 0.0251, step time: 1.0291\n",
      "295/295, train_loss: 0.0224, step time: 1.0294\n",
      "epoch 92 average loss: 0.0772\n",
      "current epoch: 92 current mean dice: 0.7554 tc: 0.7043 wt: 0.8283 et: 0.7392\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 92 is: 385.8984\n",
      "----------\n",
      "epoch 93/100\n",
      "1/295, train_loss: 0.0908, step time: 1.1704\n",
      "2/295, train_loss: 0.0484, step time: 1.0425\n",
      "3/295, train_loss: 0.0430, step time: 1.0709\n",
      "4/295, train_loss: 0.0426, step time: 1.0871\n",
      "5/295, train_loss: 0.0528, step time: 1.0464\n",
      "6/295, train_loss: 0.0302, step time: 1.1121\n",
      "7/295, train_loss: 0.1013, step time: 1.0779\n",
      "8/295, train_loss: 0.0595, step time: 1.0445\n",
      "9/295, train_loss: 0.0466, step time: 1.0610\n",
      "10/295, train_loss: 0.0884, step time: 1.0351\n",
      "11/295, train_loss: 0.0402, step time: 1.0408\n",
      "12/295, train_loss: 0.0524, step time: 1.0365\n",
      "13/295, train_loss: 0.0530, step time: 1.0533\n",
      "14/295, train_loss: 0.0367, step time: 1.0558\n",
      "15/295, train_loss: 0.0581, step time: 1.0426\n",
      "16/295, train_loss: 0.0544, step time: 1.0599\n",
      "17/295, train_loss: 0.0394, step time: 1.0302\n",
      "18/295, train_loss: 0.0456, step time: 1.0304\n",
      "19/295, train_loss: 0.0290, step time: 1.0617\n",
      "20/295, train_loss: 0.3923, step time: 1.0405\n",
      "21/295, train_loss: 0.0488, step time: 1.0444\n",
      "22/295, train_loss: 0.0604, step time: 1.0306\n",
      "23/295, train_loss: 0.0618, step time: 1.0307\n",
      "24/295, train_loss: 0.0965, step time: 1.0340\n",
      "25/295, train_loss: 0.0964, step time: 1.0592\n",
      "26/295, train_loss: 0.0223, step time: 1.0349\n",
      "27/295, train_loss: 0.3566, step time: 1.0390\n",
      "28/295, train_loss: 0.1056, step time: 1.0326\n",
      "29/295, train_loss: 0.0469, step time: 1.0399\n",
      "30/295, train_loss: 0.0222, step time: 1.1082\n",
      "31/295, train_loss: 0.0665, step time: 1.0866\n",
      "32/295, train_loss: 0.3660, step time: 1.0408\n",
      "33/295, train_loss: 0.0399, step time: 1.0358\n",
      "34/295, train_loss: 0.0377, step time: 1.0756\n",
      "35/295, train_loss: 0.0358, step time: 1.1267\n",
      "36/295, train_loss: 0.0812, step time: 1.0579\n",
      "37/295, train_loss: 0.0436, step time: 1.0314\n",
      "38/295, train_loss: 0.0477, step time: 1.0468\n",
      "39/295, train_loss: 0.0281, step time: 1.0415\n",
      "40/295, train_loss: 0.1056, step time: 1.0522\n",
      "41/295, train_loss: 0.3646, step time: 1.0339\n",
      "42/295, train_loss: 0.0694, step time: 1.0458\n",
      "43/295, train_loss: 0.0843, step time: 1.0843\n",
      "44/295, train_loss: 0.0878, step time: 1.0543\n",
      "45/295, train_loss: 0.0586, step time: 1.0602\n",
      "46/295, train_loss: 0.3738, step time: 1.0533\n",
      "47/295, train_loss: 0.0742, step time: 1.0666\n",
      "48/295, train_loss: 0.0198, step time: 1.0551\n",
      "49/295, train_loss: 0.0432, step time: 1.0376\n",
      "50/295, train_loss: 0.0937, step time: 1.0350\n",
      "51/295, train_loss: 0.0897, step time: 1.0546\n",
      "52/295, train_loss: 0.0552, step time: 1.0832\n",
      "53/295, train_loss: 0.0489, step time: 1.0573\n",
      "54/295, train_loss: 0.0242, step time: 1.0385\n",
      "55/295, train_loss: 0.0413, step time: 1.0368\n",
      "56/295, train_loss: 0.0526, step time: 1.0447\n",
      "57/295, train_loss: 0.3598, step time: 1.0656\n",
      "58/295, train_loss: 0.0503, step time: 1.0428\n",
      "59/295, train_loss: 0.0556, step time: 1.0459\n",
      "60/295, train_loss: 0.0311, step time: 1.0480\n",
      "61/295, train_loss: 0.0854, step time: 1.0928\n",
      "62/295, train_loss: 0.0463, step time: 1.1277\n",
      "63/295, train_loss: 0.0669, step time: 1.1244\n",
      "64/295, train_loss: 0.0626, step time: 1.0696\n",
      "65/295, train_loss: 0.0266, step time: 1.0390\n",
      "66/295, train_loss: 0.0262, step time: 1.0395\n",
      "67/295, train_loss: 0.0288, step time: 1.0313\n",
      "68/295, train_loss: 0.3765, step time: 1.0520\n",
      "69/295, train_loss: 0.0741, step time: 1.0328\n",
      "70/295, train_loss: 0.0333, step time: 1.0317\n",
      "71/295, train_loss: 0.0451, step time: 1.0607\n",
      "72/295, train_loss: 0.0608, step time: 1.0366\n",
      "73/295, train_loss: 0.0377, step time: 1.0372\n",
      "74/295, train_loss: 0.0643, step time: 1.0397\n",
      "75/295, train_loss: 0.0484, step time: 1.0387\n",
      "76/295, train_loss: 0.3714, step time: 1.0458\n",
      "77/295, train_loss: 0.0471, step time: 1.0477\n",
      "78/295, train_loss: 0.0472, step time: 1.0361\n",
      "79/295, train_loss: 0.0557, step time: 1.0351\n",
      "80/295, train_loss: 0.0577, step time: 1.0386\n",
      "81/295, train_loss: 0.0349, step time: 1.0560\n",
      "82/295, train_loss: 0.0301, step time: 1.0532\n",
      "83/295, train_loss: 0.0634, step time: 1.0307\n",
      "84/295, train_loss: 0.0304, step time: 1.0847\n",
      "85/295, train_loss: 0.0568, step time: 1.0341\n",
      "86/295, train_loss: 0.1000, step time: 1.1032\n",
      "87/295, train_loss: 0.0441, step time: 1.0358\n",
      "88/295, train_loss: 0.0703, step time: 1.0341\n",
      "89/295, train_loss: 0.0329, step time: 1.0830\n",
      "90/295, train_loss: 0.0742, step time: 1.0469\n",
      "91/295, train_loss: 0.0576, step time: 1.0457\n",
      "92/295, train_loss: 0.0639, step time: 1.1081\n",
      "93/295, train_loss: 0.0463, step time: 1.0905\n",
      "94/295, train_loss: 0.0707, step time: 1.0314\n",
      "95/295, train_loss: 0.0694, step time: 1.0614\n",
      "96/295, train_loss: 0.0338, step time: 1.0314\n",
      "97/295, train_loss: 0.0564, step time: 1.0679\n",
      "98/295, train_loss: 0.0548, step time: 1.0410\n",
      "99/295, train_loss: 0.0522, step time: 1.0449\n",
      "100/295, train_loss: 0.0225, step time: 1.0855\n",
      "101/295, train_loss: 0.0571, step time: 1.0378\n",
      "102/295, train_loss: 0.0386, step time: 1.0428\n",
      "103/295, train_loss: 0.0172, step time: 1.0428\n",
      "104/295, train_loss: 0.0228, step time: 1.0991\n",
      "105/295, train_loss: 0.0353, step time: 1.0354\n",
      "106/295, train_loss: 0.0382, step time: 1.0339\n",
      "107/295, train_loss: 0.0412, step time: 1.0308\n",
      "108/295, train_loss: 0.0394, step time: 1.0538\n",
      "109/295, train_loss: 0.0923, step time: 1.0322\n",
      "110/295, train_loss: 0.0766, step time: 1.0390\n",
      "111/295, train_loss: 0.0338, step time: 1.0518\n",
      "112/295, train_loss: 0.0251, step time: 1.0528\n",
      "113/295, train_loss: 0.0544, step time: 1.0592\n",
      "114/295, train_loss: 0.0592, step time: 1.0460\n",
      "115/295, train_loss: 0.0309, step time: 1.0402\n",
      "116/295, train_loss: 0.0495, step time: 1.0470\n",
      "117/295, train_loss: 0.0209, step time: 1.0374\n",
      "118/295, train_loss: 0.0470, step time: 1.0396\n",
      "119/295, train_loss: 0.0722, step time: 1.0670\n",
      "120/295, train_loss: 0.0337, step time: 1.0442\n",
      "121/295, train_loss: 0.0336, step time: 1.0561\n",
      "122/295, train_loss: 0.0484, step time: 1.0635\n",
      "123/295, train_loss: 0.0570, step time: 1.0433\n",
      "124/295, train_loss: 0.0805, step time: 1.0716\n",
      "125/295, train_loss: 0.0341, step time: 1.0348\n",
      "126/295, train_loss: 0.0515, step time: 1.0473\n",
      "127/295, train_loss: 0.0273, step time: 1.0591\n",
      "128/295, train_loss: 0.0249, step time: 1.0623\n",
      "129/295, train_loss: 0.3684, step time: 1.0615\n",
      "130/295, train_loss: 0.0265, step time: 1.0333\n",
      "131/295, train_loss: 0.0715, step time: 1.0334\n",
      "132/295, train_loss: 0.0624, step time: 1.0573\n",
      "133/295, train_loss: 0.3585, step time: 1.0502\n",
      "134/295, train_loss: 0.0304, step time: 1.0648\n",
      "135/295, train_loss: 0.0290, step time: 1.0336\n",
      "136/295, train_loss: 0.0699, step time: 1.0318\n",
      "137/295, train_loss: 0.3721, step time: 1.0387\n",
      "138/295, train_loss: 0.0292, step time: 1.0557\n",
      "139/295, train_loss: 0.0442, step time: 1.0324\n",
      "140/295, train_loss: 0.0777, step time: 1.0438\n",
      "141/295, train_loss: 0.0415, step time: 1.0334\n",
      "142/295, train_loss: 0.0485, step time: 1.0327\n",
      "143/295, train_loss: 0.0313, step time: 1.0508\n",
      "144/295, train_loss: 0.0367, step time: 1.0537\n",
      "145/295, train_loss: 0.0359, step time: 1.0491\n",
      "146/295, train_loss: 0.0678, step time: 1.0501\n",
      "147/295, train_loss: 0.0376, step time: 1.0620\n",
      "148/295, train_loss: 0.0580, step time: 1.0518\n",
      "149/295, train_loss: 0.3555, step time: 1.0628\n",
      "150/295, train_loss: 0.0336, step time: 1.0395\n",
      "151/295, train_loss: 0.0554, step time: 1.0559\n",
      "152/295, train_loss: 0.0763, step time: 1.0574\n",
      "153/295, train_loss: 0.0660, step time: 1.0510\n",
      "154/295, train_loss: 0.0566, step time: 1.0835\n",
      "155/295, train_loss: 0.0500, step time: 1.0414\n",
      "156/295, train_loss: 0.0771, step time: 1.0454\n",
      "157/295, train_loss: 0.0395, step time: 1.0565\n",
      "158/295, train_loss: 0.0390, step time: 1.0857\n",
      "159/295, train_loss: 0.0332, step time: 1.0470\n",
      "160/295, train_loss: 0.0428, step time: 1.0681\n",
      "161/295, train_loss: 0.0436, step time: 1.0741\n",
      "162/295, train_loss: 0.0555, step time: 1.0446\n",
      "163/295, train_loss: 0.0381, step time: 1.0346\n",
      "164/295, train_loss: 0.3722, step time: 1.0840\n",
      "165/295, train_loss: 0.0298, step time: 1.0513\n",
      "166/295, train_loss: 0.0343, step time: 1.0738\n",
      "167/295, train_loss: 0.0399, step time: 1.0398\n",
      "168/295, train_loss: 0.0607, step time: 1.0355\n",
      "169/295, train_loss: 0.0720, step time: 1.0419\n",
      "170/295, train_loss: 0.0452, step time: 1.0378\n",
      "171/295, train_loss: 0.0421, step time: 1.0436\n",
      "172/295, train_loss: 0.0733, step time: 1.0336\n",
      "173/295, train_loss: 0.0613, step time: 1.0390\n",
      "174/295, train_loss: 0.0513, step time: 1.0493\n",
      "175/295, train_loss: 0.0286, step time: 1.0703\n",
      "176/295, train_loss: 0.0964, step time: 1.0372\n",
      "177/295, train_loss: 0.0346, step time: 1.0329\n",
      "178/295, train_loss: 0.0359, step time: 1.0514\n",
      "179/295, train_loss: 0.0246, step time: 1.0411\n",
      "180/295, train_loss: 0.0604, step time: 1.0398\n",
      "181/295, train_loss: 0.0367, step time: 1.0712\n",
      "182/295, train_loss: 0.0480, step time: 1.0396\n",
      "183/295, train_loss: 0.0592, step time: 1.0398\n",
      "184/295, train_loss: 0.0431, step time: 1.0326\n",
      "185/295, train_loss: 0.0763, step time: 1.0415\n",
      "186/295, train_loss: 0.0331, step time: 1.0373\n",
      "187/295, train_loss: 0.0619, step time: 1.0359\n",
      "188/295, train_loss: 0.0462, step time: 1.0345\n",
      "189/295, train_loss: 0.0375, step time: 1.0369\n",
      "190/295, train_loss: 0.1253, step time: 1.0511\n",
      "191/295, train_loss: 0.0656, step time: 1.0735\n",
      "192/295, train_loss: 0.4178, step time: 1.1012\n",
      "193/295, train_loss: 0.0896, step time: 1.0393\n",
      "194/295, train_loss: 0.0336, step time: 1.0360\n",
      "195/295, train_loss: 0.3595, step time: 1.0327\n",
      "196/295, train_loss: 0.0672, step time: 1.0330\n",
      "197/295, train_loss: 0.0269, step time: 1.0379\n",
      "198/295, train_loss: 0.0883, step time: 1.0327\n",
      "199/295, train_loss: 0.0404, step time: 1.0347\n",
      "200/295, train_loss: 0.0786, step time: 1.0331\n",
      "201/295, train_loss: 0.0883, step time: 1.0314\n",
      "202/295, train_loss: 0.0734, step time: 1.0563\n",
      "203/295, train_loss: 0.3772, step time: 1.0324\n",
      "204/295, train_loss: 0.0500, step time: 1.0366\n",
      "205/295, train_loss: 0.0346, step time: 1.0445\n",
      "206/295, train_loss: 0.0394, step time: 1.0584\n",
      "207/295, train_loss: 0.0667, step time: 1.0378\n",
      "208/295, train_loss: 0.0714, step time: 1.0463\n",
      "209/295, train_loss: 0.0673, step time: 1.0617\n",
      "210/295, train_loss: 0.0513, step time: 1.0638\n",
      "211/295, train_loss: 0.0406, step time: 1.0727\n",
      "212/295, train_loss: 0.0255, step time: 1.0437\n",
      "213/295, train_loss: 0.3798, step time: 1.0328\n",
      "214/295, train_loss: 0.0259, step time: 1.0597\n",
      "215/295, train_loss: 0.0602, step time: 1.0429\n",
      "216/295, train_loss: 0.0560, step time: 1.0364\n",
      "217/295, train_loss: 0.0434, step time: 1.0373\n",
      "218/295, train_loss: 0.0458, step time: 1.0391\n",
      "219/295, train_loss: 0.0492, step time: 1.0510\n",
      "220/295, train_loss: 0.0762, step time: 1.0402\n",
      "221/295, train_loss: 0.0407, step time: 1.0385\n",
      "222/295, train_loss: 0.1126, step time: 1.0333\n",
      "223/295, train_loss: 0.0752, step time: 1.0960\n",
      "224/295, train_loss: 0.0890, step time: 1.0747\n",
      "225/295, train_loss: 0.0494, step time: 1.0359\n",
      "226/295, train_loss: 0.1208, step time: 1.0363\n",
      "227/295, train_loss: 0.0631, step time: 1.0502\n",
      "228/295, train_loss: 0.0512, step time: 1.0348\n",
      "229/295, train_loss: 0.0708, step time: 1.0431\n",
      "230/295, train_loss: 0.0503, step time: 1.0371\n",
      "231/295, train_loss: 0.0384, step time: 1.0360\n",
      "232/295, train_loss: 0.0737, step time: 1.0579\n",
      "233/295, train_loss: 0.0312, step time: 1.0345\n",
      "234/295, train_loss: 0.0276, step time: 1.0554\n",
      "235/295, train_loss: 0.0592, step time: 1.0488\n",
      "236/295, train_loss: 0.0311, step time: 1.0642\n",
      "237/295, train_loss: 0.0777, step time: 1.0343\n",
      "238/295, train_loss: 0.0303, step time: 1.0425\n",
      "239/295, train_loss: 0.0407, step time: 1.0503\n",
      "240/295, train_loss: 0.0719, step time: 1.0654\n",
      "241/295, train_loss: 0.0249, step time: 1.0392\n",
      "242/295, train_loss: 0.0488, step time: 1.0390\n",
      "243/295, train_loss: 0.0356, step time: 1.0407\n",
      "244/295, train_loss: 0.0679, step time: 1.0336\n",
      "245/295, train_loss: 0.0786, step time: 1.0318\n",
      "246/295, train_loss: 0.0265, step time: 1.0435\n",
      "247/295, train_loss: 0.0249, step time: 1.0463\n",
      "248/295, train_loss: 0.0269, step time: 1.0528\n",
      "249/295, train_loss: 0.0656, step time: 1.0383\n",
      "250/295, train_loss: 0.0277, step time: 1.0347\n",
      "251/295, train_loss: 0.0360, step time: 1.0354\n",
      "252/295, train_loss: 0.2048, step time: 1.0327\n",
      "253/295, train_loss: 0.0700, step time: 1.0341\n",
      "254/295, train_loss: 0.0534, step time: 1.0335\n",
      "255/295, train_loss: 0.3623, step time: 1.0359\n",
      "256/295, train_loss: 0.0711, step time: 1.0434\n",
      "257/295, train_loss: 0.0714, step time: 1.0617\n",
      "258/295, train_loss: 0.0445, step time: 1.0335\n",
      "259/295, train_loss: 0.0457, step time: 1.0445\n",
      "260/295, train_loss: 0.0422, step time: 1.0368\n",
      "261/295, train_loss: 0.0784, step time: 1.0588\n",
      "262/295, train_loss: 0.1113, step time: 1.0410\n",
      "263/295, train_loss: 0.0640, step time: 1.0413\n",
      "264/295, train_loss: 0.0503, step time: 1.0391\n",
      "265/295, train_loss: 0.0389, step time: 1.0370\n",
      "266/295, train_loss: 0.0657, step time: 1.0397\n",
      "267/295, train_loss: 0.3768, step time: 1.0534\n",
      "268/295, train_loss: 0.0459, step time: 1.0313\n",
      "269/295, train_loss: 0.0755, step time: 1.0400\n",
      "270/295, train_loss: 0.0378, step time: 1.0487\n",
      "271/295, train_loss: 0.0443, step time: 1.0599\n",
      "272/295, train_loss: 0.0221, step time: 1.0602\n",
      "273/295, train_loss: 0.0568, step time: 1.0341\n",
      "274/295, train_loss: 0.0657, step time: 1.0334\n",
      "275/295, train_loss: 0.0723, step time: 1.0476\n",
      "276/295, train_loss: 0.1120, step time: 1.0360\n",
      "277/295, train_loss: 0.0319, step time: 1.0381\n",
      "278/295, train_loss: 0.0247, step time: 1.0599\n",
      "279/295, train_loss: 0.0250, step time: 1.0791\n",
      "280/295, train_loss: 0.0597, step time: 1.0339\n",
      "281/295, train_loss: 0.0843, step time: 1.0352\n",
      "282/295, train_loss: 0.0295, step time: 1.0442\n",
      "283/295, train_loss: 0.0458, step time: 1.0361\n",
      "284/295, train_loss: 0.0264, step time: 1.0471\n",
      "285/295, train_loss: 0.0795, step time: 1.1008\n",
      "286/295, train_loss: 0.0637, step time: 1.0767\n",
      "287/295, train_loss: 0.0239, step time: 1.0366\n",
      "288/295, train_loss: 0.3766, step time: 1.0397\n",
      "289/295, train_loss: 0.0450, step time: 1.0294\n",
      "290/295, train_loss: 0.3745, step time: 1.0280\n",
      "291/295, train_loss: 0.0649, step time: 1.0289\n",
      "292/295, train_loss: 0.0691, step time: 1.0312\n",
      "293/295, train_loss: 0.3864, step time: 1.0291\n",
      "294/295, train_loss: 0.0459, step time: 1.0290\n",
      "295/295, train_loss: 0.0448, step time: 1.0287\n",
      "epoch 93 average loss: 0.0771\n",
      "current epoch: 93 current mean dice: 0.7713 tc: 0.7229 wt: 0.8374 et: 0.7567\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 93 is: 383.2153\n",
      "----------\n",
      "epoch 94/100\n",
      "1/295, train_loss: 0.0247, step time: 1.0567\n",
      "2/295, train_loss: 0.0172, step time: 1.1324\n",
      "3/295, train_loss: 0.0566, step time: 1.0713\n",
      "4/295, train_loss: 0.0439, step time: 1.1003\n",
      "5/295, train_loss: 0.0305, step time: 1.1015\n",
      "6/295, train_loss: 0.3767, step time: 1.0459\n",
      "7/295, train_loss: 0.0439, step time: 1.0389\n",
      "8/295, train_loss: 0.3685, step time: 1.0729\n",
      "9/295, train_loss: 0.0376, step time: 1.0476\n",
      "10/295, train_loss: 0.0761, step time: 1.0860\n",
      "11/295, train_loss: 0.0687, step time: 1.0568\n",
      "12/295, train_loss: 0.0432, step time: 1.0448\n",
      "13/295, train_loss: 0.0653, step time: 1.0365\n",
      "14/295, train_loss: 0.0639, step time: 1.0503\n",
      "15/295, train_loss: 0.0329, step time: 1.0327\n",
      "16/295, train_loss: 0.0339, step time: 1.0517\n",
      "17/295, train_loss: 0.0469, step time: 1.0704\n",
      "18/295, train_loss: 0.0533, step time: 1.0614\n",
      "19/295, train_loss: 0.0567, step time: 1.0475\n",
      "20/295, train_loss: 0.0770, step time: 1.0460\n",
      "21/295, train_loss: 0.0373, step time: 1.0439\n",
      "22/295, train_loss: 0.0432, step time: 1.0330\n",
      "23/295, train_loss: 0.0214, step time: 1.0471\n",
      "24/295, train_loss: 0.0632, step time: 1.0583\n",
      "25/295, train_loss: 0.0259, step time: 1.0389\n",
      "26/295, train_loss: 0.0303, step time: 1.0317\n",
      "27/295, train_loss: 0.0721, step time: 1.0566\n",
      "28/295, train_loss: 0.0642, step time: 1.0452\n",
      "29/295, train_loss: 0.3646, step time: 1.0332\n",
      "30/295, train_loss: 0.0263, step time: 1.0451\n",
      "31/295, train_loss: 0.0276, step time: 1.0688\n",
      "32/295, train_loss: 0.0710, step time: 1.1059\n",
      "33/295, train_loss: 0.0313, step time: 1.0400\n",
      "34/295, train_loss: 0.4170, step time: 1.0649\n",
      "35/295, train_loss: 0.0760, step time: 1.0421\n",
      "36/295, train_loss: 0.0815, step time: 1.0401\n",
      "37/295, train_loss: 0.0439, step time: 1.0648\n",
      "38/295, train_loss: 0.0696, step time: 1.0554\n",
      "39/295, train_loss: 0.0456, step time: 1.0487\n",
      "40/295, train_loss: 0.0357, step time: 1.0383\n",
      "41/295, train_loss: 0.0675, step time: 1.0398\n",
      "42/295, train_loss: 0.0420, step time: 1.0632\n",
      "43/295, train_loss: 0.0328, step time: 1.0381\n",
      "44/295, train_loss: 0.0398, step time: 1.0352\n",
      "45/295, train_loss: 0.0572, step time: 1.0309\n",
      "46/295, train_loss: 0.2047, step time: 1.0713\n",
      "47/295, train_loss: 0.1124, step time: 1.0325\n",
      "48/295, train_loss: 0.0367, step time: 1.0844\n",
      "49/295, train_loss: 0.0483, step time: 1.0452\n",
      "50/295, train_loss: 0.0468, step time: 1.0348\n",
      "51/295, train_loss: 0.0757, step time: 1.0436\n",
      "52/295, train_loss: 0.0780, step time: 1.0491\n",
      "53/295, train_loss: 0.0908, step time: 1.0677\n",
      "54/295, train_loss: 0.0621, step time: 1.0474\n",
      "55/295, train_loss: 0.0889, step time: 1.0536\n",
      "56/295, train_loss: 0.0431, step time: 1.0357\n",
      "57/295, train_loss: 0.0240, step time: 1.0513\n",
      "58/295, train_loss: 0.3715, step time: 1.0449\n",
      "59/295, train_loss: 0.3654, step time: 1.0420\n",
      "60/295, train_loss: 0.0618, step time: 1.0372\n",
      "61/295, train_loss: 0.0280, step time: 1.0877\n",
      "62/295, train_loss: 0.0252, step time: 1.0482\n",
      "63/295, train_loss: 0.0842, step time: 1.0472\n",
      "64/295, train_loss: 0.0586, step time: 1.0386\n",
      "65/295, train_loss: 0.0356, step time: 1.0472\n",
      "66/295, train_loss: 0.0511, step time: 1.0380\n",
      "67/295, train_loss: 0.0268, step time: 1.0885\n",
      "68/295, train_loss: 0.0227, step time: 1.0542\n",
      "69/295, train_loss: 0.0390, step time: 1.0405\n",
      "70/295, train_loss: 0.0255, step time: 1.0483\n",
      "71/295, train_loss: 0.0430, step time: 1.0634\n",
      "72/295, train_loss: 0.0561, step time: 1.0436\n",
      "73/295, train_loss: 0.3767, step time: 1.0362\n",
      "74/295, train_loss: 0.0529, step time: 1.0616\n",
      "75/295, train_loss: 0.0491, step time: 1.0360\n",
      "76/295, train_loss: 0.0389, step time: 1.0350\n",
      "77/295, train_loss: 0.0342, step time: 1.0655\n",
      "78/295, train_loss: 0.0598, step time: 1.0510\n",
      "79/295, train_loss: 0.0608, step time: 1.0445\n",
      "80/295, train_loss: 0.0416, step time: 1.0422\n",
      "81/295, train_loss: 0.0221, step time: 1.0468\n",
      "82/295, train_loss: 0.0885, step time: 1.0384\n",
      "83/295, train_loss: 0.0559, step time: 1.0466\n",
      "84/295, train_loss: 0.0286, step time: 1.0511\n",
      "85/295, train_loss: 0.0447, step time: 1.0904\n",
      "86/295, train_loss: 0.0293, step time: 1.0354\n",
      "87/295, train_loss: 0.0221, step time: 1.0397\n",
      "88/295, train_loss: 0.0334, step time: 1.0348\n",
      "89/295, train_loss: 0.0304, step time: 1.0425\n",
      "90/295, train_loss: 0.0709, step time: 1.0755\n",
      "91/295, train_loss: 0.0507, step time: 1.0422\n",
      "92/295, train_loss: 0.0698, step time: 1.0577\n",
      "93/295, train_loss: 0.0483, step time: 1.0388\n",
      "94/295, train_loss: 0.0240, step time: 1.0608\n",
      "95/295, train_loss: 0.0674, step time: 1.0311\n",
      "96/295, train_loss: 0.0333, step time: 1.0363\n",
      "97/295, train_loss: 0.0480, step time: 1.0604\n",
      "98/295, train_loss: 0.0376, step time: 1.0390\n",
      "99/295, train_loss: 0.0459, step time: 1.0369\n",
      "100/295, train_loss: 0.0453, step time: 1.0380\n",
      "101/295, train_loss: 0.0395, step time: 1.0683\n",
      "102/295, train_loss: 0.0720, step time: 1.0311\n",
      "103/295, train_loss: 0.0770, step time: 1.0331\n",
      "104/295, train_loss: 0.1005, step time: 1.0314\n",
      "105/295, train_loss: 0.0713, step time: 1.0374\n",
      "106/295, train_loss: 0.3923, step time: 1.0339\n",
      "107/295, train_loss: 0.0732, step time: 1.0506\n",
      "108/295, train_loss: 0.0592, step time: 1.0497\n",
      "109/295, train_loss: 0.0633, step time: 1.0565\n",
      "110/295, train_loss: 0.0249, step time: 1.0569\n",
      "111/295, train_loss: 0.0664, step time: 1.0311\n",
      "112/295, train_loss: 0.0476, step time: 1.0394\n",
      "113/295, train_loss: 0.0420, step time: 1.0323\n",
      "114/295, train_loss: 0.0390, step time: 1.1153\n",
      "115/295, train_loss: 0.0452, step time: 1.0544\n",
      "116/295, train_loss: 0.0627, step time: 1.0419\n",
      "117/295, train_loss: 0.0264, step time: 1.0542\n",
      "118/295, train_loss: 0.0740, step time: 1.0570\n",
      "119/295, train_loss: 0.0300, step time: 1.0355\n",
      "120/295, train_loss: 0.0547, step time: 1.1318\n",
      "121/295, train_loss: 0.0457, step time: 1.0617\n",
      "122/295, train_loss: 0.0478, step time: 1.0670\n",
      "123/295, train_loss: 0.0196, step time: 1.0441\n",
      "124/295, train_loss: 0.0783, step time: 1.0316\n",
      "125/295, train_loss: 0.0492, step time: 1.0331\n",
      "126/295, train_loss: 0.0459, step time: 1.0457\n",
      "127/295, train_loss: 0.0632, step time: 1.0441\n",
      "128/295, train_loss: 0.3738, step time: 1.0385\n",
      "129/295, train_loss: 0.0657, step time: 1.0391\n",
      "130/295, train_loss: 0.0355, step time: 1.0703\n",
      "131/295, train_loss: 0.0517, step time: 1.0382\n",
      "132/295, train_loss: 0.0495, step time: 1.0537\n",
      "133/295, train_loss: 0.0747, step time: 1.0330\n",
      "134/295, train_loss: 0.0451, step time: 1.0353\n",
      "135/295, train_loss: 0.0251, step time: 1.0454\n",
      "136/295, train_loss: 0.0970, step time: 1.0504\n",
      "137/295, train_loss: 0.1204, step time: 1.0386\n",
      "138/295, train_loss: 0.0780, step time: 1.0366\n",
      "139/295, train_loss: 0.0335, step time: 1.0366\n",
      "140/295, train_loss: 0.3765, step time: 1.0371\n",
      "141/295, train_loss: 0.0339, step time: 1.0365\n",
      "142/295, train_loss: 0.0398, step time: 1.0324\n",
      "143/295, train_loss: 0.1126, step time: 1.0396\n",
      "144/295, train_loss: 0.0463, step time: 1.0355\n",
      "145/295, train_loss: 0.3601, step time: 1.0691\n",
      "146/295, train_loss: 0.0451, step time: 1.0458\n",
      "147/295, train_loss: 0.0887, step time: 1.0627\n",
      "148/295, train_loss: 0.0604, step time: 1.0566\n",
      "149/295, train_loss: 0.0431, step time: 1.0365\n",
      "150/295, train_loss: 0.0524, step time: 1.0451\n",
      "151/295, train_loss: 0.0312, step time: 1.0710\n",
      "152/295, train_loss: 0.0923, step time: 1.0697\n",
      "153/295, train_loss: 0.0852, step time: 1.0389\n",
      "154/295, train_loss: 0.0406, step time: 1.0471\n",
      "155/295, train_loss: 0.0417, step time: 1.0561\n",
      "156/295, train_loss: 0.0717, step time: 1.0431\n",
      "157/295, train_loss: 0.0345, step time: 1.0378\n",
      "158/295, train_loss: 0.0502, step time: 1.0462\n",
      "159/295, train_loss: 0.0370, step time: 1.0503\n",
      "160/295, train_loss: 0.0398, step time: 1.0368\n",
      "161/295, train_loss: 0.0396, step time: 1.0319\n",
      "162/295, train_loss: 0.0398, step time: 1.0680\n",
      "163/295, train_loss: 0.3567, step time: 1.0338\n",
      "164/295, train_loss: 0.0661, step time: 1.0454\n",
      "165/295, train_loss: 0.0474, step time: 1.0332\n",
      "166/295, train_loss: 0.0398, step time: 1.0529\n",
      "167/295, train_loss: 0.0307, step time: 1.0411\n",
      "168/295, train_loss: 0.0304, step time: 1.0490\n",
      "169/295, train_loss: 0.0322, step time: 1.0524\n",
      "170/295, train_loss: 0.0224, step time: 1.0604\n",
      "171/295, train_loss: 0.0252, step time: 1.0362\n",
      "172/295, train_loss: 0.0422, step time: 1.0384\n",
      "173/295, train_loss: 0.0269, step time: 1.0308\n",
      "174/295, train_loss: 0.0460, step time: 1.0388\n",
      "175/295, train_loss: 0.3764, step time: 1.0701\n",
      "176/295, train_loss: 0.1018, step time: 1.0417\n",
      "177/295, train_loss: 0.0590, step time: 1.0531\n",
      "178/295, train_loss: 0.3622, step time: 1.0721\n",
      "179/295, train_loss: 0.0579, step time: 1.0407\n",
      "180/295, train_loss: 0.0292, step time: 1.0339\n",
      "181/295, train_loss: 0.0614, step time: 1.0447\n",
      "182/295, train_loss: 0.0457, step time: 1.0562\n",
      "183/295, train_loss: 0.0560, step time: 1.0422\n",
      "184/295, train_loss: 0.0656, step time: 1.0396\n",
      "185/295, train_loss: 0.0263, step time: 1.0446\n",
      "186/295, train_loss: 0.0503, step time: 1.0493\n",
      "187/295, train_loss: 0.0883, step time: 1.0372\n",
      "188/295, train_loss: 0.0421, step time: 1.0366\n",
      "189/295, train_loss: 0.3793, step time: 1.0338\n",
      "190/295, train_loss: 0.0378, step time: 1.0387\n",
      "191/295, train_loss: 0.0311, step time: 1.0635\n",
      "192/295, train_loss: 0.0355, step time: 1.0390\n",
      "193/295, train_loss: 0.0775, step time: 1.0571\n",
      "194/295, train_loss: 0.0632, step time: 1.0360\n",
      "195/295, train_loss: 0.0496, step time: 1.0327\n",
      "196/295, train_loss: 0.0285, step time: 1.0368\n",
      "197/295, train_loss: 0.1254, step time: 1.0378\n",
      "198/295, train_loss: 0.0567, step time: 1.0594\n",
      "199/295, train_loss: 0.0511, step time: 1.0670\n",
      "200/295, train_loss: 0.3555, step time: 1.0429\n",
      "201/295, train_loss: 0.0854, step time: 1.0347\n",
      "202/295, train_loss: 0.0561, step time: 1.0497\n",
      "203/295, train_loss: 0.0315, step time: 1.0480\n",
      "204/295, train_loss: 0.0707, step time: 1.0394\n",
      "205/295, train_loss: 0.0679, step time: 1.0507\n",
      "206/295, train_loss: 0.0403, step time: 1.0330\n",
      "207/295, train_loss: 0.1111, step time: 1.0337\n",
      "208/295, train_loss: 0.0382, step time: 1.0610\n",
      "209/295, train_loss: 0.0511, step time: 1.0357\n",
      "210/295, train_loss: 0.0430, step time: 1.0319\n",
      "211/295, train_loss: 0.1059, step time: 1.0420\n",
      "212/295, train_loss: 0.0486, step time: 1.0529\n",
      "213/295, train_loss: 0.0225, step time: 1.0476\n",
      "214/295, train_loss: 0.1048, step time: 1.0378\n",
      "215/295, train_loss: 0.0365, step time: 1.0372\n",
      "216/295, train_loss: 0.0783, step time: 1.0364\n",
      "217/295, train_loss: 0.0554, step time: 1.0357\n",
      "218/295, train_loss: 0.0268, step time: 1.0422\n",
      "219/295, train_loss: 0.0389, step time: 1.0344\n",
      "220/295, train_loss: 0.0608, step time: 1.0423\n",
      "221/295, train_loss: 0.0406, step time: 1.0347\n",
      "222/295, train_loss: 0.0675, step time: 1.0452\n",
      "223/295, train_loss: 0.0621, step time: 1.0531\n",
      "224/295, train_loss: 0.0874, step time: 1.0436\n",
      "225/295, train_loss: 0.0600, step time: 1.0454\n",
      "226/295, train_loss: 0.3587, step time: 1.0391\n",
      "227/295, train_loss: 0.0538, step time: 1.0566\n",
      "228/295, train_loss: 0.0739, step time: 1.0379\n",
      "229/295, train_loss: 0.0243, step time: 1.0823\n",
      "230/295, train_loss: 0.0246, step time: 1.0379\n",
      "231/295, train_loss: 0.0559, step time: 1.0736\n",
      "232/295, train_loss: 0.0359, step time: 1.0371\n",
      "233/295, train_loss: 0.0892, step time: 1.0319\n",
      "234/295, train_loss: 0.0575, step time: 1.0444\n",
      "235/295, train_loss: 0.0380, step time: 1.0379\n",
      "236/295, train_loss: 0.3723, step time: 1.0445\n",
      "237/295, train_loss: 0.0316, step time: 1.0347\n",
      "238/295, train_loss: 0.0722, step time: 1.0741\n",
      "239/295, train_loss: 0.0544, step time: 1.0378\n",
      "240/295, train_loss: 0.0581, step time: 1.0465\n",
      "241/295, train_loss: 0.0533, step time: 1.0329\n",
      "242/295, train_loss: 0.0763, step time: 1.0460\n",
      "243/295, train_loss: 0.0340, step time: 1.0554\n",
      "244/295, train_loss: 0.0596, step time: 1.0373\n",
      "245/295, train_loss: 0.0507, step time: 1.0428\n",
      "246/295, train_loss: 0.0737, step time: 1.0528\n",
      "247/295, train_loss: 0.0707, step time: 1.0698\n",
      "248/295, train_loss: 0.0275, step time: 1.0425\n",
      "249/295, train_loss: 0.0797, step time: 1.0344\n",
      "250/295, train_loss: 0.0439, step time: 1.0396\n",
      "251/295, train_loss: 0.0263, step time: 1.0509\n",
      "252/295, train_loss: 0.0685, step time: 1.0363\n",
      "253/295, train_loss: 0.0295, step time: 1.0343\n",
      "254/295, train_loss: 0.0487, step time: 1.0332\n",
      "255/295, train_loss: 0.0485, step time: 1.0379\n",
      "256/295, train_loss: 0.0808, step time: 1.0378\n",
      "257/295, train_loss: 0.0625, step time: 1.0382\n",
      "258/295, train_loss: 0.3855, step time: 1.0519\n",
      "259/295, train_loss: 0.0750, step time: 1.0983\n",
      "260/295, train_loss: 0.0406, step time: 1.0338\n",
      "261/295, train_loss: 0.0340, step time: 1.0331\n",
      "262/295, train_loss: 0.0442, step time: 1.0352\n",
      "263/295, train_loss: 0.0678, step time: 1.0342\n",
      "264/295, train_loss: 0.0612, step time: 1.0376\n",
      "265/295, train_loss: 0.0470, step time: 1.0467\n",
      "266/295, train_loss: 0.0735, step time: 1.0465\n",
      "267/295, train_loss: 0.0335, step time: 1.0393\n",
      "268/295, train_loss: 0.0593, step time: 1.0717\n",
      "269/295, train_loss: 0.3700, step time: 1.0317\n",
      "270/295, train_loss: 0.0567, step time: 1.0357\n",
      "271/295, train_loss: 0.0657, step time: 1.0548\n",
      "272/295, train_loss: 0.0286, step time: 1.0408\n",
      "273/295, train_loss: 0.0526, step time: 1.0348\n",
      "274/295, train_loss: 0.0365, step time: 1.0532\n",
      "275/295, train_loss: 0.0952, step time: 1.0367\n",
      "276/295, train_loss: 0.0902, step time: 1.0484\n",
      "277/295, train_loss: 0.0715, step time: 1.0414\n",
      "278/295, train_loss: 0.0485, step time: 1.0355\n",
      "279/295, train_loss: 0.0656, step time: 1.0438\n",
      "280/295, train_loss: 0.3599, step time: 1.0322\n",
      "281/295, train_loss: 0.0348, step time: 1.0716\n",
      "282/295, train_loss: 0.0271, step time: 1.0367\n",
      "283/295, train_loss: 0.0533, step time: 1.0375\n",
      "284/295, train_loss: 0.0485, step time: 1.0377\n",
      "285/295, train_loss: 0.0310, step time: 1.0469\n",
      "286/295, train_loss: 0.0407, step time: 1.0418\n",
      "287/295, train_loss: 0.0338, step time: 1.0510\n",
      "288/295, train_loss: 0.0693, step time: 1.0311\n",
      "289/295, train_loss: 0.0925, step time: 1.0286\n",
      "290/295, train_loss: 0.0972, step time: 1.0284\n",
      "291/295, train_loss: 0.0530, step time: 1.0298\n",
      "292/295, train_loss: 0.0369, step time: 1.0295\n",
      "293/295, train_loss: 0.0528, step time: 1.0287\n",
      "294/295, train_loss: 0.0697, step time: 1.0295\n",
      "295/295, train_loss: 0.3738, step time: 1.0286\n",
      "epoch 94 average loss: 0.0770\n",
      "current epoch: 94 current mean dice: 0.7509 tc: 0.7016 wt: 0.8231 et: 0.7340\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 94 is: 383.1860\n",
      "----------\n",
      "epoch 95/100\n",
      "1/295, train_loss: 0.0592, step time: 1.0789\n",
      "2/295, train_loss: 0.1065, step time: 1.0714\n",
      "3/295, train_loss: 0.0598, step time: 1.1318\n",
      "4/295, train_loss: 0.0372, step time: 1.0563\n",
      "5/295, train_loss: 0.0408, step time: 1.0388\n",
      "6/295, train_loss: 0.1110, step time: 1.0779\n",
      "7/295, train_loss: 0.0386, step time: 1.0371\n",
      "8/295, train_loss: 0.0731, step time: 1.0335\n",
      "9/295, train_loss: 0.3700, step time: 1.0747\n",
      "10/295, train_loss: 0.0461, step time: 1.1029\n",
      "11/295, train_loss: 0.0528, step time: 1.0353\n",
      "12/295, train_loss: 0.0224, step time: 1.0533\n",
      "13/295, train_loss: 0.0265, step time: 1.0541\n",
      "14/295, train_loss: 0.3567, step time: 1.0344\n",
      "15/295, train_loss: 0.0881, step time: 1.0462\n",
      "16/295, train_loss: 0.0481, step time: 1.0519\n",
      "17/295, train_loss: 0.0545, step time: 1.0459\n",
      "18/295, train_loss: 0.0508, step time: 1.0389\n",
      "19/295, train_loss: 0.0576, step time: 1.0403\n",
      "20/295, train_loss: 0.0557, step time: 1.0520\n",
      "21/295, train_loss: 0.0431, step time: 1.0325\n",
      "22/295, train_loss: 0.0720, step time: 1.0376\n",
      "23/295, train_loss: 0.0256, step time: 1.0373\n",
      "24/295, train_loss: 0.0456, step time: 1.0725\n",
      "25/295, train_loss: 0.0392, step time: 1.0345\n",
      "26/295, train_loss: 0.0524, step time: 1.0616\n",
      "27/295, train_loss: 0.0493, step time: 1.0352\n",
      "28/295, train_loss: 0.0411, step time: 1.0557\n",
      "29/295, train_loss: 0.4171, step time: 1.0695\n",
      "30/295, train_loss: 0.0498, step time: 1.0457\n",
      "31/295, train_loss: 0.0267, step time: 1.0408\n",
      "32/295, train_loss: 0.0382, step time: 1.0698\n",
      "33/295, train_loss: 0.0330, step time: 1.0309\n",
      "34/295, train_loss: 0.0328, step time: 1.1229\n",
      "35/295, train_loss: 0.0892, step time: 1.0346\n",
      "36/295, train_loss: 0.0780, step time: 1.0847\n",
      "37/295, train_loss: 0.0892, step time: 1.0368\n",
      "38/295, train_loss: 0.0490, step time: 1.0419\n",
      "39/295, train_loss: 0.0337, step time: 1.0385\n",
      "40/295, train_loss: 0.0491, step time: 1.0620\n",
      "41/295, train_loss: 0.0598, step time: 1.0300\n",
      "42/295, train_loss: 0.0226, step time: 1.0371\n",
      "43/295, train_loss: 0.0700, step time: 1.0583\n",
      "44/295, train_loss: 0.0697, step time: 1.0340\n",
      "45/295, train_loss: 0.0592, step time: 1.0369\n",
      "46/295, train_loss: 0.0509, step time: 1.0556\n",
      "47/295, train_loss: 0.0637, step time: 1.0958\n",
      "48/295, train_loss: 0.0758, step time: 1.0509\n",
      "49/295, train_loss: 0.0585, step time: 1.1319\n",
      "50/295, train_loss: 0.0419, step time: 1.0310\n",
      "51/295, train_loss: 0.0881, step time: 1.0338\n",
      "52/295, train_loss: 0.0839, step time: 1.0400\n",
      "53/295, train_loss: 0.0336, step time: 1.0993\n",
      "54/295, train_loss: 0.0616, step time: 1.0309\n",
      "55/295, train_loss: 0.0691, step time: 1.0350\n",
      "56/295, train_loss: 0.0486, step time: 1.0390\n",
      "57/295, train_loss: 0.0656, step time: 1.0554\n",
      "58/295, train_loss: 0.0461, step time: 1.0439\n",
      "59/295, train_loss: 0.0356, step time: 1.0607\n",
      "60/295, train_loss: 0.0440, step time: 1.0627\n",
      "61/295, train_loss: 0.0265, step time: 1.0409\n",
      "62/295, train_loss: 0.0380, step time: 1.0932\n",
      "63/295, train_loss: 0.0436, step time: 1.0461\n",
      "64/295, train_loss: 0.0638, step time: 1.0361\n",
      "65/295, train_loss: 0.0745, step time: 1.0807\n",
      "66/295, train_loss: 0.0532, step time: 1.0940\n",
      "67/295, train_loss: 0.0394, step time: 1.0316\n",
      "68/295, train_loss: 0.0765, step time: 1.0303\n",
      "69/295, train_loss: 0.0771, step time: 1.0351\n",
      "70/295, train_loss: 0.0468, step time: 1.0368\n",
      "71/295, train_loss: 0.0674, step time: 1.0486\n",
      "72/295, train_loss: 0.0619, step time: 1.0346\n",
      "73/295, train_loss: 0.0309, step time: 1.0393\n",
      "74/295, train_loss: 0.0634, step time: 1.0459\n",
      "75/295, train_loss: 0.0493, step time: 1.0572\n",
      "76/295, train_loss: 0.0482, step time: 1.0698\n",
      "77/295, train_loss: 0.0345, step time: 1.0381\n",
      "78/295, train_loss: 0.0421, step time: 1.0369\n",
      "79/295, train_loss: 0.3718, step time: 1.0356\n",
      "80/295, train_loss: 0.0364, step time: 1.0384\n",
      "81/295, train_loss: 0.0247, step time: 1.0637\n",
      "82/295, train_loss: 0.0517, step time: 1.0587\n",
      "83/295, train_loss: 0.0375, step time: 1.0389\n",
      "84/295, train_loss: 0.0575, step time: 1.0394\n",
      "85/295, train_loss: 0.0954, step time: 1.0498\n",
      "86/295, train_loss: 0.0309, step time: 1.0401\n",
      "87/295, train_loss: 0.0368, step time: 1.0488\n",
      "88/295, train_loss: 0.0605, step time: 1.0759\n",
      "89/295, train_loss: 0.0358, step time: 1.0949\n",
      "90/295, train_loss: 0.0759, step time: 1.0408\n",
      "91/295, train_loss: 0.0326, step time: 1.0407\n",
      "92/295, train_loss: 0.0507, step time: 1.0459\n",
      "93/295, train_loss: 0.0197, step time: 1.1051\n",
      "94/295, train_loss: 0.0526, step time: 1.0330\n",
      "95/295, train_loss: 0.0553, step time: 1.0531\n",
      "96/295, train_loss: 0.0670, step time: 1.0562\n",
      "97/295, train_loss: 0.0713, step time: 1.0349\n",
      "98/295, train_loss: 0.0398, step time: 1.0381\n",
      "99/295, train_loss: 0.0647, step time: 1.0350\n",
      "100/295, train_loss: 0.0336, step time: 1.0370\n",
      "101/295, train_loss: 0.0454, step time: 1.0532\n",
      "102/295, train_loss: 0.0458, step time: 1.0335\n",
      "103/295, train_loss: 0.0393, step time: 1.0437\n",
      "104/295, train_loss: 0.0543, step time: 1.0388\n",
      "105/295, train_loss: 0.0403, step time: 1.0334\n",
      "106/295, train_loss: 0.0524, step time: 1.0697\n",
      "107/295, train_loss: 0.0343, step time: 1.0583\n",
      "108/295, train_loss: 0.0298, step time: 1.0595\n",
      "109/295, train_loss: 0.3622, step time: 1.0626\n",
      "110/295, train_loss: 0.0305, step time: 1.0379\n",
      "111/295, train_loss: 0.0721, step time: 1.0524\n",
      "112/295, train_loss: 0.3767, step time: 1.0708\n",
      "113/295, train_loss: 0.0222, step time: 1.0567\n",
      "114/295, train_loss: 0.0432, step time: 1.1116\n",
      "115/295, train_loss: 0.0383, step time: 1.0376\n",
      "116/295, train_loss: 0.0465, step time: 1.0703\n",
      "117/295, train_loss: 0.0339, step time: 1.0562\n",
      "118/295, train_loss: 0.0275, step time: 1.0466\n",
      "119/295, train_loss: 0.0487, step time: 1.0388\n",
      "120/295, train_loss: 0.0280, step time: 1.0530\n",
      "121/295, train_loss: 0.0332, step time: 1.0477\n",
      "122/295, train_loss: 0.0349, step time: 1.0399\n",
      "123/295, train_loss: 0.0655, step time: 1.0619\n",
      "124/295, train_loss: 0.0487, step time: 1.0365\n",
      "125/295, train_loss: 0.0376, step time: 1.0399\n",
      "126/295, train_loss: 0.2049, step time: 1.0346\n",
      "127/295, train_loss: 0.0509, step time: 1.0410\n",
      "128/295, train_loss: 0.0566, step time: 1.0978\n",
      "129/295, train_loss: 0.0629, step time: 1.0376\n",
      "130/295, train_loss: 0.0357, step time: 1.0618\n",
      "131/295, train_loss: 0.3597, step time: 1.0692\n",
      "132/295, train_loss: 0.1199, step time: 1.0343\n",
      "133/295, train_loss: 0.0457, step time: 1.0714\n",
      "134/295, train_loss: 0.0453, step time: 1.0699\n",
      "135/295, train_loss: 0.0425, step time: 1.0856\n",
      "136/295, train_loss: 0.0285, step time: 1.0345\n",
      "137/295, train_loss: 0.0391, step time: 1.0426\n",
      "138/295, train_loss: 0.0730, step time: 1.0396\n",
      "139/295, train_loss: 0.0220, step time: 1.0537\n",
      "140/295, train_loss: 0.0303, step time: 1.0429\n",
      "141/295, train_loss: 0.1015, step time: 1.0557\n",
      "142/295, train_loss: 0.0452, step time: 1.0806\n",
      "143/295, train_loss: 0.3859, step time: 1.0594\n",
      "144/295, train_loss: 0.0365, step time: 1.0365\n",
      "145/295, train_loss: 0.3793, step time: 1.0378\n",
      "146/295, train_loss: 0.0471, step time: 1.0657\n",
      "147/295, train_loss: 0.0968, step time: 1.0422\n",
      "148/295, train_loss: 0.0887, step time: 1.0534\n",
      "149/295, train_loss: 0.0241, step time: 1.0453\n",
      "150/295, train_loss: 0.0959, step time: 1.0403\n",
      "151/295, train_loss: 0.1005, step time: 1.0472\n",
      "152/295, train_loss: 0.0418, step time: 1.0487\n",
      "153/295, train_loss: 0.0599, step time: 1.0383\n",
      "154/295, train_loss: 0.0710, step time: 1.0575\n",
      "155/295, train_loss: 0.0421, step time: 1.0353\n",
      "156/295, train_loss: 0.0849, step time: 1.0432\n",
      "157/295, train_loss: 0.0618, step time: 1.0494\n",
      "158/295, train_loss: 0.0807, step time: 1.0470\n",
      "159/295, train_loss: 0.0204, step time: 1.0671\n",
      "160/295, train_loss: 0.0773, step time: 1.0499\n",
      "161/295, train_loss: 0.0568, step time: 1.0455\n",
      "162/295, train_loss: 0.0316, step time: 1.1386\n",
      "163/295, train_loss: 0.0236, step time: 1.0456\n",
      "164/295, train_loss: 0.0456, step time: 1.0888\n",
      "165/295, train_loss: 0.0436, step time: 1.0537\n",
      "166/295, train_loss: 0.0367, step time: 1.0572\n",
      "167/295, train_loss: 0.3649, step time: 1.0446\n",
      "168/295, train_loss: 0.0718, step time: 1.0454\n",
      "169/295, train_loss: 0.3918, step time: 1.0428\n",
      "170/295, train_loss: 0.0566, step time: 1.0426\n",
      "171/295, train_loss: 0.0698, step time: 1.0385\n",
      "172/295, train_loss: 0.0679, step time: 1.0389\n",
      "173/295, train_loss: 0.0259, step time: 1.0615\n",
      "174/295, train_loss: 0.0243, step time: 1.1109\n",
      "175/295, train_loss: 0.0472, step time: 1.0362\n",
      "176/295, train_loss: 0.3736, step time: 1.0575\n",
      "177/295, train_loss: 0.0613, step time: 1.0460\n",
      "178/295, train_loss: 0.0628, step time: 1.0455\n",
      "179/295, train_loss: 0.0355, step time: 1.0781\n",
      "180/295, train_loss: 0.0566, step time: 1.1141\n",
      "181/295, train_loss: 0.0358, step time: 1.0401\n",
      "182/295, train_loss: 0.0245, step time: 1.0667\n",
      "183/295, train_loss: 0.0407, step time: 1.0407\n",
      "184/295, train_loss: 0.0708, step time: 1.0386\n",
      "185/295, train_loss: 0.0431, step time: 1.0353\n",
      "186/295, train_loss: 0.0296, step time: 1.0381\n",
      "187/295, train_loss: 0.0590, step time: 1.0391\n",
      "188/295, train_loss: 0.0749, step time: 1.0351\n",
      "189/295, train_loss: 0.0676, step time: 1.0380\n",
      "190/295, train_loss: 0.0286, step time: 1.0536\n",
      "191/295, train_loss: 0.0923, step time: 1.0629\n",
      "192/295, train_loss: 0.1248, step time: 1.0382\n",
      "193/295, train_loss: 0.0285, step time: 1.1234\n",
      "194/295, train_loss: 0.0752, step time: 1.0395\n",
      "195/295, train_loss: 0.0305, step time: 1.0445\n",
      "196/295, train_loss: 0.0432, step time: 1.0335\n",
      "197/295, train_loss: 0.0660, step time: 1.0330\n",
      "198/295, train_loss: 0.0767, step time: 1.0364\n",
      "199/295, train_loss: 0.0459, step time: 1.0469\n",
      "200/295, train_loss: 0.0258, step time: 1.0996\n",
      "201/295, train_loss: 0.0410, step time: 1.0486\n",
      "202/295, train_loss: 0.1057, step time: 1.0597\n",
      "203/295, train_loss: 0.0671, step time: 1.0418\n",
      "204/295, train_loss: 0.0365, step time: 1.0368\n",
      "205/295, train_loss: 0.3684, step time: 1.0393\n",
      "206/295, train_loss: 0.0288, step time: 1.0358\n",
      "207/295, train_loss: 0.0220, step time: 1.0360\n",
      "208/295, train_loss: 0.3739, step time: 1.0450\n",
      "209/295, train_loss: 0.0626, step time: 1.0805\n",
      "210/295, train_loss: 0.0797, step time: 1.0684\n",
      "211/295, train_loss: 0.0304, step time: 1.0540\n",
      "212/295, train_loss: 0.0395, step time: 1.0410\n",
      "213/295, train_loss: 0.3656, step time: 1.0362\n",
      "214/295, train_loss: 0.0606, step time: 1.0358\n",
      "215/295, train_loss: 0.0535, step time: 1.0443\n",
      "216/295, train_loss: 0.0572, step time: 1.0562\n",
      "217/295, train_loss: 0.0606, step time: 1.0342\n",
      "218/295, train_loss: 0.0775, step time: 1.0385\n",
      "219/295, train_loss: 0.0386, step time: 1.0392\n",
      "220/295, train_loss: 0.0887, step time: 1.0472\n",
      "221/295, train_loss: 0.0565, step time: 1.0718\n",
      "222/295, train_loss: 0.0810, step time: 1.0477\n",
      "223/295, train_loss: 0.3769, step time: 1.0374\n",
      "224/295, train_loss: 0.0909, step time: 1.0355\n",
      "225/295, train_loss: 0.0607, step time: 1.0430\n",
      "226/295, train_loss: 0.3601, step time: 1.0376\n",
      "227/295, train_loss: 0.0275, step time: 1.0403\n",
      "228/295, train_loss: 0.0248, step time: 1.0485\n",
      "229/295, train_loss: 0.0536, step time: 1.0657\n",
      "230/295, train_loss: 0.0653, step time: 1.0574\n",
      "231/295, train_loss: 0.0930, step time: 1.0402\n",
      "232/295, train_loss: 0.0340, step time: 1.0353\n",
      "233/295, train_loss: 0.0561, step time: 1.0320\n",
      "234/295, train_loss: 0.0440, step time: 1.0511\n",
      "235/295, train_loss: 0.0399, step time: 1.0600\n",
      "236/295, train_loss: 0.0312, step time: 1.0444\n",
      "237/295, train_loss: 0.0482, step time: 1.0403\n",
      "238/295, train_loss: 0.0653, step time: 1.0825\n",
      "239/295, train_loss: 0.0526, step time: 1.0366\n",
      "240/295, train_loss: 0.0428, step time: 1.0649\n",
      "241/295, train_loss: 0.0581, step time: 1.0365\n",
      "242/295, train_loss: 0.0689, step time: 1.0488\n",
      "243/295, train_loss: 0.0337, step time: 1.0415\n",
      "244/295, train_loss: 0.0581, step time: 1.0796\n",
      "245/295, train_loss: 0.0734, step time: 1.0432\n",
      "246/295, train_loss: 0.0466, step time: 1.0520\n",
      "247/295, train_loss: 0.0505, step time: 1.0426\n",
      "248/295, train_loss: 0.0486, step time: 1.0902\n",
      "249/295, train_loss: 0.0708, step time: 1.0662\n",
      "250/295, train_loss: 0.0686, step time: 1.0339\n",
      "251/295, train_loss: 0.0716, step time: 1.0539\n",
      "252/295, train_loss: 0.0405, step time: 1.0348\n",
      "253/295, train_loss: 0.0315, step time: 1.0358\n",
      "254/295, train_loss: 0.0376, step time: 1.0640\n",
      "255/295, train_loss: 0.1123, step time: 1.0646\n",
      "256/295, train_loss: 0.3765, step time: 1.0485\n",
      "257/295, train_loss: 0.0711, step time: 1.0350\n",
      "258/295, train_loss: 0.3717, step time: 1.0417\n",
      "259/295, train_loss: 0.0267, step time: 1.0601\n",
      "260/295, train_loss: 0.0299, step time: 1.0396\n",
      "261/295, train_loss: 0.0478, step time: 1.0892\n",
      "262/295, train_loss: 0.0343, step time: 1.0374\n",
      "263/295, train_loss: 0.0547, step time: 1.0390\n",
      "264/295, train_loss: 0.0266, step time: 1.0402\n",
      "265/295, train_loss: 0.0267, step time: 1.0342\n",
      "266/295, train_loss: 0.0710, step time: 1.0505\n",
      "267/295, train_loss: 0.0444, step time: 1.0555\n",
      "268/295, train_loss: 0.0498, step time: 1.0531\n",
      "269/295, train_loss: 0.0773, step time: 1.0592\n",
      "270/295, train_loss: 0.0870, step time: 1.0384\n",
      "271/295, train_loss: 0.0450, step time: 1.0394\n",
      "272/295, train_loss: 0.3769, step time: 1.0375\n",
      "273/295, train_loss: 0.0739, step time: 1.0321\n",
      "274/295, train_loss: 0.0655, step time: 1.0320\n",
      "275/295, train_loss: 0.0769, step time: 1.0378\n",
      "276/295, train_loss: 0.0442, step time: 1.0346\n",
      "277/295, train_loss: 0.0331, step time: 1.0366\n",
      "278/295, train_loss: 0.0255, step time: 1.0406\n",
      "279/295, train_loss: 0.0447, step time: 1.0381\n",
      "280/295, train_loss: 0.0619, step time: 1.0674\n",
      "281/295, train_loss: 0.0741, step time: 1.0386\n",
      "282/295, train_loss: 0.1125, step time: 1.0623\n",
      "283/295, train_loss: 0.0261, step time: 1.0381\n",
      "284/295, train_loss: 0.0395, step time: 1.0463\n",
      "285/295, train_loss: 0.0654, step time: 1.0389\n",
      "286/295, train_loss: 0.0251, step time: 1.0419\n",
      "287/295, train_loss: 0.0262, step time: 1.0568\n",
      "288/295, train_loss: 0.0292, step time: 1.0290\n",
      "289/295, train_loss: 0.3552, step time: 1.0276\n",
      "290/295, train_loss: 0.0543, step time: 1.0280\n",
      "291/295, train_loss: 0.0510, step time: 1.0290\n",
      "292/295, train_loss: 0.0171, step time: 1.0274\n",
      "293/295, train_loss: 0.0842, step time: 1.0275\n",
      "294/295, train_loss: 0.0311, step time: 1.0280\n",
      "295/295, train_loss: 0.3583, step time: 1.0295\n",
      "epoch 95 average loss: 0.0770\n",
      "current epoch: 95 current mean dice: 0.7663 tc: 0.7162 wt: 0.8346 et: 0.7520\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 95 is: 386.3197\n",
      "----------\n",
      "epoch 96/100\n",
      "1/295, train_loss: 0.0509, step time: 1.0769\n",
      "2/295, train_loss: 0.0882, step time: 1.0891\n",
      "3/295, train_loss: 0.0297, step time: 1.2152\n",
      "4/295, train_loss: 0.0392, step time: 1.0492\n",
      "5/295, train_loss: 0.0497, step time: 1.0736\n",
      "6/295, train_loss: 0.0301, step time: 1.0506\n",
      "7/295, train_loss: 0.0335, step time: 1.1046\n",
      "8/295, train_loss: 0.0395, step time: 1.0339\n",
      "9/295, train_loss: 0.0574, step time: 1.0294\n",
      "10/295, train_loss: 0.0312, step time: 1.0425\n",
      "11/295, train_loss: 0.0594, step time: 1.0437\n",
      "12/295, train_loss: 0.0590, step time: 1.0339\n",
      "13/295, train_loss: 0.0702, step time: 1.0362\n",
      "14/295, train_loss: 0.0336, step time: 1.0593\n",
      "15/295, train_loss: 0.0448, step time: 1.0445\n",
      "16/295, train_loss: 0.0382, step time: 1.0476\n",
      "17/295, train_loss: 0.0263, step time: 1.0333\n",
      "18/295, train_loss: 0.0302, step time: 1.0504\n",
      "19/295, train_loss: 0.0263, step time: 1.0514\n",
      "20/295, train_loss: 0.0334, step time: 1.0596\n",
      "21/295, train_loss: 0.0724, step time: 1.0300\n",
      "22/295, train_loss: 0.0666, step time: 1.0433\n",
      "23/295, train_loss: 0.1247, step time: 1.0302\n",
      "24/295, train_loss: 0.3644, step time: 1.0555\n",
      "25/295, train_loss: 0.0744, step time: 1.0361\n",
      "26/295, train_loss: 0.0224, step time: 1.0532\n",
      "27/295, train_loss: 0.0273, step time: 1.1104\n",
      "28/295, train_loss: 0.0492, step time: 1.0804\n",
      "29/295, train_loss: 0.3598, step time: 1.0671\n",
      "30/295, train_loss: 0.0438, step time: 1.0553\n",
      "31/295, train_loss: 0.0464, step time: 1.0457\n",
      "32/295, train_loss: 0.0355, step time: 1.0707\n",
      "33/295, train_loss: 0.0598, step time: 1.0551\n",
      "34/295, train_loss: 0.0908, step time: 1.0741\n",
      "35/295, train_loss: 0.0547, step time: 1.0614\n",
      "36/295, train_loss: 0.0340, step time: 1.0326\n",
      "37/295, train_loss: 0.0507, step time: 1.0487\n",
      "38/295, train_loss: 0.0720, step time: 1.0318\n",
      "39/295, train_loss: 0.0711, step time: 1.0451\n",
      "40/295, train_loss: 0.3717, step time: 1.0305\n",
      "41/295, train_loss: 0.0808, step time: 1.0362\n",
      "42/295, train_loss: 0.0520, step time: 1.0624\n",
      "43/295, train_loss: 0.0611, step time: 1.0514\n",
      "44/295, train_loss: 0.0655, step time: 1.0299\n",
      "45/295, train_loss: 0.0625, step time: 1.0404\n",
      "46/295, train_loss: 0.0713, step time: 1.0381\n",
      "47/295, train_loss: 0.3553, step time: 1.0417\n",
      "48/295, train_loss: 0.0289, step time: 1.0994\n",
      "49/295, train_loss: 0.0221, step time: 1.0409\n",
      "50/295, train_loss: 0.0681, step time: 1.0715\n",
      "51/295, train_loss: 0.0403, step time: 1.1373\n",
      "52/295, train_loss: 0.0794, step time: 1.0331\n",
      "53/295, train_loss: 0.0289, step time: 1.0599\n",
      "54/295, train_loss: 0.0351, step time: 1.0541\n",
      "55/295, train_loss: 0.0479, step time: 1.0402\n",
      "56/295, train_loss: 0.0619, step time: 1.0425\n",
      "57/295, train_loss: 0.0286, step time: 1.0612\n",
      "58/295, train_loss: 0.0580, step time: 1.0774\n",
      "59/295, train_loss: 0.3683, step time: 1.0498\n",
      "60/295, train_loss: 0.0358, step time: 1.0422\n",
      "61/295, train_loss: 0.0544, step time: 1.0482\n",
      "62/295, train_loss: 0.0653, step time: 1.0462\n",
      "63/295, train_loss: 0.1049, step time: 1.0947\n",
      "64/295, train_loss: 0.0579, step time: 1.0552\n",
      "65/295, train_loss: 0.3714, step time: 1.0331\n",
      "66/295, train_loss: 0.0484, step time: 1.0393\n",
      "67/295, train_loss: 0.0471, step time: 1.0384\n",
      "68/295, train_loss: 0.0349, step time: 1.0361\n",
      "69/295, train_loss: 0.0241, step time: 1.0313\n",
      "70/295, train_loss: 0.0480, step time: 1.0981\n",
      "71/295, train_loss: 0.0383, step time: 1.0457\n",
      "72/295, train_loss: 0.0924, step time: 1.0983\n",
      "73/295, train_loss: 0.0334, step time: 1.0546\n",
      "74/295, train_loss: 0.0391, step time: 1.0358\n",
      "75/295, train_loss: 0.0604, step time: 1.0567\n",
      "76/295, train_loss: 0.0883, step time: 1.0348\n",
      "77/295, train_loss: 0.0302, step time: 1.0515\n",
      "78/295, train_loss: 0.0374, step time: 1.0316\n",
      "79/295, train_loss: 0.3735, step time: 1.0342\n",
      "80/295, train_loss: 0.1196, step time: 1.0415\n",
      "81/295, train_loss: 0.0705, step time: 1.0342\n",
      "82/295, train_loss: 0.0665, step time: 1.0472\n",
      "83/295, train_loss: 0.0652, step time: 1.1017\n",
      "84/295, train_loss: 0.0691, step time: 1.0406\n",
      "85/295, train_loss: 0.0334, step time: 1.0502\n",
      "86/295, train_loss: 0.0486, step time: 1.0356\n",
      "87/295, train_loss: 0.0302, step time: 1.0456\n",
      "88/295, train_loss: 0.0343, step time: 1.1019\n",
      "89/295, train_loss: 0.0956, step time: 1.0581\n",
      "90/295, train_loss: 0.0376, step time: 1.0464\n",
      "91/295, train_loss: 0.0667, step time: 1.0541\n",
      "92/295, train_loss: 0.0422, step time: 1.0404\n",
      "93/295, train_loss: 0.0375, step time: 1.0341\n",
      "94/295, train_loss: 0.0204, step time: 1.0363\n",
      "95/295, train_loss: 0.0240, step time: 1.0383\n",
      "96/295, train_loss: 0.0395, step time: 1.0535\n",
      "97/295, train_loss: 0.0486, step time: 1.0340\n",
      "98/295, train_loss: 0.0589, step time: 1.0597\n",
      "99/295, train_loss: 0.4169, step time: 1.0742\n",
      "100/295, train_loss: 0.0601, step time: 1.0981\n",
      "101/295, train_loss: 0.0486, step time: 1.0383\n",
      "102/295, train_loss: 0.0734, step time: 1.0352\n",
      "103/295, train_loss: 0.0424, step time: 1.0358\n",
      "104/295, train_loss: 0.0776, step time: 1.0348\n",
      "105/295, train_loss: 0.0600, step time: 1.0378\n",
      "106/295, train_loss: 0.0451, step time: 1.0374\n",
      "107/295, train_loss: 0.0873, step time: 1.0345\n",
      "108/295, train_loss: 0.0633, step time: 1.0431\n",
      "109/295, train_loss: 0.0694, step time: 1.0342\n",
      "110/295, train_loss: 0.0431, step time: 1.0409\n",
      "111/295, train_loss: 0.3596, step time: 1.0635\n",
      "112/295, train_loss: 0.0720, step time: 1.0334\n",
      "113/295, train_loss: 0.0237, step time: 1.0373\n",
      "114/295, train_loss: 0.0768, step time: 1.0855\n",
      "115/295, train_loss: 0.0433, step time: 1.0521\n",
      "116/295, train_loss: 0.0460, step time: 1.0334\n",
      "117/295, train_loss: 0.0472, step time: 1.0479\n",
      "118/295, train_loss: 0.0195, step time: 1.0565\n",
      "119/295, train_loss: 0.0563, step time: 1.0971\n",
      "120/295, train_loss: 0.3697, step time: 1.0450\n",
      "121/295, train_loss: 0.0515, step time: 1.0634\n",
      "122/295, train_loss: 0.1012, step time: 1.0503\n",
      "123/295, train_loss: 0.0759, step time: 1.0569\n",
      "124/295, train_loss: 0.0219, step time: 1.0776\n",
      "125/295, train_loss: 0.0260, step time: 1.0464\n",
      "126/295, train_loss: 0.0305, step time: 1.1236\n",
      "127/295, train_loss: 0.0708, step time: 1.0814\n",
      "128/295, train_loss: 0.3738, step time: 1.0378\n",
      "129/295, train_loss: 0.0377, step time: 1.0480\n",
      "130/295, train_loss: 0.0412, step time: 1.0643\n",
      "131/295, train_loss: 0.3860, step time: 1.0324\n",
      "132/295, train_loss: 0.0249, step time: 1.0394\n",
      "133/295, train_loss: 0.0437, step time: 1.0347\n",
      "134/295, train_loss: 0.0269, step time: 1.0310\n",
      "135/295, train_loss: 0.0309, step time: 1.0891\n",
      "136/295, train_loss: 0.0891, step time: 1.0366\n",
      "137/295, train_loss: 0.0603, step time: 1.0386\n",
      "138/295, train_loss: 0.0591, step time: 1.0580\n",
      "139/295, train_loss: 0.3918, step time: 1.0327\n",
      "140/295, train_loss: 0.0410, step time: 1.0472\n",
      "141/295, train_loss: 0.0268, step time: 1.0971\n",
      "142/295, train_loss: 0.1124, step time: 1.0335\n",
      "143/295, train_loss: 0.3656, step time: 1.0497\n",
      "144/295, train_loss: 0.0416, step time: 1.1227\n",
      "145/295, train_loss: 0.0560, step time: 1.0793\n",
      "146/295, train_loss: 0.0733, step time: 1.0714\n",
      "147/295, train_loss: 0.0499, step time: 1.0420\n",
      "148/295, train_loss: 0.0851, step time: 1.0625\n",
      "149/295, train_loss: 0.0965, step time: 1.0408\n",
      "150/295, train_loss: 0.0532, step time: 1.0471\n",
      "151/295, train_loss: 0.3762, step time: 1.0476\n",
      "152/295, train_loss: 0.0459, step time: 1.0370\n",
      "153/295, train_loss: 0.0735, step time: 1.0534\n",
      "154/295, train_loss: 0.0312, step time: 1.0344\n",
      "155/295, train_loss: 0.0655, step time: 1.0369\n",
      "156/295, train_loss: 0.0772, step time: 1.0546\n",
      "157/295, train_loss: 0.0447, step time: 1.0655\n",
      "158/295, train_loss: 0.0482, step time: 1.0603\n",
      "159/295, train_loss: 0.0633, step time: 1.0338\n",
      "160/295, train_loss: 0.0294, step time: 1.0380\n",
      "161/295, train_loss: 0.0249, step time: 1.0356\n",
      "162/295, train_loss: 0.0405, step time: 1.0331\n",
      "163/295, train_loss: 0.0398, step time: 1.0601\n",
      "164/295, train_loss: 0.0265, step time: 1.0319\n",
      "165/295, train_loss: 0.0858, step time: 1.0375\n",
      "166/295, train_loss: 0.0416, step time: 1.0377\n",
      "167/295, train_loss: 0.0375, step time: 1.0398\n",
      "168/295, train_loss: 0.0706, step time: 1.0959\n",
      "169/295, train_loss: 0.0539, step time: 1.0344\n",
      "170/295, train_loss: 0.0563, step time: 1.0375\n",
      "171/295, train_loss: 0.0329, step time: 1.0903\n",
      "172/295, train_loss: 0.0493, step time: 1.0360\n",
      "173/295, train_loss: 0.0925, step time: 1.0358\n",
      "174/295, train_loss: 0.0727, step time: 1.0462\n",
      "175/295, train_loss: 0.0537, step time: 1.0827\n",
      "176/295, train_loss: 0.1000, step time: 1.0958\n",
      "177/295, train_loss: 0.0633, step time: 1.0536\n",
      "178/295, train_loss: 0.0248, step time: 1.0628\n",
      "179/295, train_loss: 0.2047, step time: 1.0352\n",
      "180/295, train_loss: 0.3765, step time: 1.0462\n",
      "181/295, train_loss: 0.0409, step time: 1.1104\n",
      "182/295, train_loss: 0.0778, step time: 1.0422\n",
      "183/295, train_loss: 0.0744, step time: 1.0475\n",
      "184/295, train_loss: 0.0395, step time: 1.0471\n",
      "185/295, train_loss: 0.0362, step time: 1.0430\n",
      "186/295, train_loss: 0.3619, step time: 1.0532\n",
      "187/295, train_loss: 0.0362, step time: 1.0619\n",
      "188/295, train_loss: 0.0783, step time: 1.0432\n",
      "189/295, train_loss: 0.0450, step time: 1.0339\n",
      "190/295, train_loss: 0.0314, step time: 1.0348\n",
      "191/295, train_loss: 0.0675, step time: 1.0353\n",
      "192/295, train_loss: 0.0455, step time: 1.0455\n",
      "193/295, train_loss: 0.0283, step time: 1.0639\n",
      "194/295, train_loss: 0.0809, step time: 1.0373\n",
      "195/295, train_loss: 0.0226, step time: 1.0513\n",
      "196/295, train_loss: 0.3763, step time: 1.0686\n",
      "197/295, train_loss: 0.0695, step time: 1.0346\n",
      "198/295, train_loss: 0.0316, step time: 1.0364\n",
      "199/295, train_loss: 0.0262, step time: 1.0358\n",
      "200/295, train_loss: 0.0705, step time: 1.0705\n",
      "201/295, train_loss: 0.0439, step time: 1.0648\n",
      "202/295, train_loss: 0.0760, step time: 1.0340\n",
      "203/295, train_loss: 0.0894, step time: 1.0817\n",
      "204/295, train_loss: 0.0459, step time: 1.0443\n",
      "205/295, train_loss: 0.0594, step time: 1.0505\n",
      "206/295, train_loss: 0.0246, step time: 1.0516\n",
      "207/295, train_loss: 0.0380, step time: 1.0481\n",
      "208/295, train_loss: 0.0170, step time: 1.0392\n",
      "209/295, train_loss: 0.0283, step time: 1.0365\n",
      "210/295, train_loss: 0.0488, step time: 1.0379\n",
      "211/295, train_loss: 0.0432, step time: 1.0382\n",
      "212/295, train_loss: 0.0268, step time: 1.0363\n",
      "213/295, train_loss: 0.0453, step time: 1.0329\n",
      "214/295, train_loss: 0.0477, step time: 1.0391\n",
      "215/295, train_loss: 0.3795, step time: 1.0416\n",
      "216/295, train_loss: 0.0456, step time: 1.0366\n",
      "217/295, train_loss: 0.0392, step time: 1.0361\n",
      "218/295, train_loss: 0.0638, step time: 1.0325\n",
      "219/295, train_loss: 0.0964, step time: 1.0501\n",
      "220/295, train_loss: 0.0274, step time: 1.0379\n",
      "221/295, train_loss: 0.0754, step time: 1.0731\n",
      "222/295, train_loss: 0.3567, step time: 1.0331\n",
      "223/295, train_loss: 0.0331, step time: 1.0347\n",
      "224/295, train_loss: 0.0443, step time: 1.0515\n",
      "225/295, train_loss: 0.0420, step time: 1.0352\n",
      "226/295, train_loss: 0.0416, step time: 1.0391\n",
      "227/295, train_loss: 0.0256, step time: 1.0643\n",
      "228/295, train_loss: 0.0263, step time: 1.0375\n",
      "229/295, train_loss: 0.0255, step time: 1.0607\n",
      "230/295, train_loss: 0.0365, step time: 1.0497\n",
      "231/295, train_loss: 0.0565, step time: 1.0596\n",
      "232/295, train_loss: 0.0447, step time: 1.0683\n",
      "233/295, train_loss: 0.0443, step time: 1.0326\n",
      "234/295, train_loss: 0.0498, step time: 1.0449\n",
      "235/295, train_loss: 0.0368, step time: 1.0479\n",
      "236/295, train_loss: 0.1111, step time: 1.0635\n",
      "237/295, train_loss: 0.1111, step time: 1.0789\n",
      "238/295, train_loss: 0.0739, step time: 1.0582\n",
      "239/295, train_loss: 0.0611, step time: 1.0385\n",
      "240/295, train_loss: 0.0471, step time: 1.0345\n",
      "241/295, train_loss: 0.0432, step time: 1.0391\n",
      "242/295, train_loss: 0.0759, step time: 1.0364\n",
      "243/295, train_loss: 0.0871, step time: 1.0502\n",
      "244/295, train_loss: 0.0655, step time: 1.0375\n",
      "245/295, train_loss: 0.1051, step time: 1.0352\n",
      "246/295, train_loss: 0.0451, step time: 1.0341\n",
      "247/295, train_loss: 0.0617, step time: 1.0373\n",
      "248/295, train_loss: 0.0525, step time: 1.0645\n",
      "249/295, train_loss: 0.0366, step time: 1.0726\n",
      "250/295, train_loss: 0.3584, step time: 1.0345\n",
      "251/295, train_loss: 0.0608, step time: 1.0366\n",
      "252/295, train_loss: 0.0708, step time: 1.0645\n",
      "253/295, train_loss: 0.0497, step time: 1.0608\n",
      "254/295, train_loss: 0.0332, step time: 1.0380\n",
      "255/295, train_loss: 0.0646, step time: 1.0395\n",
      "256/295, train_loss: 0.0560, step time: 1.0406\n",
      "257/295, train_loss: 0.0772, step time: 1.0550\n",
      "258/295, train_loss: 0.0582, step time: 1.0341\n",
      "259/295, train_loss: 0.0760, step time: 1.0418\n",
      "260/295, train_loss: 0.0399, step time: 1.0751\n",
      "261/295, train_loss: 0.0344, step time: 1.0349\n",
      "262/295, train_loss: 0.0555, step time: 1.0582\n",
      "263/295, train_loss: 0.0669, step time: 1.0378\n",
      "264/295, train_loss: 0.0680, step time: 1.0349\n",
      "265/295, train_loss: 0.0357, step time: 1.0418\n",
      "266/295, train_loss: 0.0530, step time: 1.0445\n",
      "267/295, train_loss: 0.0526, step time: 1.0516\n",
      "268/295, train_loss: 0.0523, step time: 1.0448\n",
      "269/295, train_loss: 0.0329, step time: 1.0420\n",
      "270/295, train_loss: 0.0340, step time: 1.0613\n",
      "271/295, train_loss: 0.0405, step time: 1.0376\n",
      "272/295, train_loss: 0.0884, step time: 1.1046\n",
      "273/295, train_loss: 0.0353, step time: 1.0372\n",
      "274/295, train_loss: 0.0655, step time: 1.0443\n",
      "275/295, train_loss: 0.0624, step time: 1.0482\n",
      "276/295, train_loss: 0.0563, step time: 1.0443\n",
      "277/295, train_loss: 0.3767, step time: 1.0659\n",
      "278/295, train_loss: 0.0465, step time: 1.0380\n",
      "279/295, train_loss: 0.0567, step time: 1.0432\n",
      "280/295, train_loss: 0.0388, step time: 1.0379\n",
      "281/295, train_loss: 0.0294, step time: 1.0409\n",
      "282/295, train_loss: 0.0219, step time: 1.0448\n",
      "283/295, train_loss: 0.0465, step time: 1.0399\n",
      "284/295, train_loss: 0.0576, step time: 1.0333\n",
      "285/295, train_loss: 0.0483, step time: 1.0322\n",
      "286/295, train_loss: 0.0503, step time: 1.0504\n",
      "287/295, train_loss: 0.0840, step time: 1.0338\n",
      "288/295, train_loss: 0.0507, step time: 1.0404\n",
      "289/295, train_loss: 0.0544, step time: 1.0285\n",
      "290/295, train_loss: 0.0309, step time: 1.0282\n",
      "291/295, train_loss: 0.0627, step time: 1.0282\n",
      "292/295, train_loss: 0.0248, step time: 1.0294\n",
      "293/295, train_loss: 0.0562, step time: 1.0285\n",
      "294/295, train_loss: 0.0321, step time: 1.0281\n",
      "295/295, train_loss: 0.0683, step time: 1.0280\n",
      "epoch 96 average loss: 0.0769\n",
      "current epoch: 96 current mean dice: 0.7465 tc: 0.6947 wt: 0.8194 et: 0.7309\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 96 is: 383.6063\n",
      "----------\n",
      "epoch 97/100\n",
      "1/295, train_loss: 0.0564, step time: 1.1267\n",
      "2/295, train_loss: 0.0686, step time: 1.0681\n",
      "3/295, train_loss: 0.0588, step time: 1.0908\n",
      "4/295, train_loss: 0.3738, step time: 1.0731\n",
      "5/295, train_loss: 0.0412, step time: 1.0401\n",
      "6/295, train_loss: 0.0695, step time: 1.0629\n",
      "7/295, train_loss: 0.0471, step time: 1.0751\n",
      "8/295, train_loss: 0.3854, step time: 1.0528\n",
      "9/295, train_loss: 0.0590, step time: 1.0401\n",
      "10/295, train_loss: 0.0284, step time: 1.0336\n",
      "11/295, train_loss: 0.0839, step time: 1.0327\n",
      "12/295, train_loss: 0.0340, step time: 1.0649\n",
      "13/295, train_loss: 0.0498, step time: 1.0492\n",
      "14/295, train_loss: 0.0308, step time: 1.0390\n",
      "15/295, train_loss: 0.0353, step time: 1.0379\n",
      "16/295, train_loss: 0.0250, step time: 1.0347\n",
      "17/295, train_loss: 0.0708, step time: 1.0351\n",
      "18/295, train_loss: 0.0525, step time: 1.0946\n",
      "19/295, train_loss: 0.0698, step time: 1.0513\n",
      "20/295, train_loss: 0.0356, step time: 1.0775\n",
      "21/295, train_loss: 0.0414, step time: 1.0392\n",
      "22/295, train_loss: 0.0250, step time: 1.0615\n",
      "23/295, train_loss: 0.0361, step time: 1.0359\n",
      "24/295, train_loss: 0.0477, step time: 1.0338\n",
      "25/295, train_loss: 0.3760, step time: 1.0715\n",
      "26/295, train_loss: 0.0288, step time: 1.0380\n",
      "27/295, train_loss: 0.0617, step time: 1.0290\n",
      "28/295, train_loss: 0.0476, step time: 1.0316\n",
      "29/295, train_loss: 0.0318, step time: 1.0402\n",
      "30/295, train_loss: 0.0445, step time: 1.0321\n",
      "31/295, train_loss: 0.0654, step time: 1.0756\n",
      "32/295, train_loss: 0.0382, step time: 1.0570\n",
      "33/295, train_loss: 0.0884, step time: 1.0522\n",
      "34/295, train_loss: 0.0656, step time: 1.0884\n",
      "35/295, train_loss: 0.0436, step time: 1.0727\n",
      "36/295, train_loss: 0.0758, step time: 1.0335\n",
      "37/295, train_loss: 0.0455, step time: 1.0628\n",
      "38/295, train_loss: 0.0806, step time: 1.0738\n",
      "39/295, train_loss: 0.0274, step time: 1.0321\n",
      "40/295, train_loss: 0.0377, step time: 1.0371\n",
      "41/295, train_loss: 0.0453, step time: 1.0326\n",
      "42/295, train_loss: 0.0281, step time: 1.0311\n",
      "43/295, train_loss: 0.0268, step time: 1.1554\n",
      "44/295, train_loss: 0.0419, step time: 1.0470\n",
      "45/295, train_loss: 0.0482, step time: 1.0389\n",
      "46/295, train_loss: 0.0447, step time: 1.0412\n",
      "47/295, train_loss: 0.0204, step time: 1.0441\n",
      "48/295, train_loss: 0.3643, step time: 1.0362\n",
      "49/295, train_loss: 0.0388, step time: 1.0368\n",
      "50/295, train_loss: 0.0733, step time: 1.0345\n",
      "51/295, train_loss: 0.0333, step time: 1.0461\n",
      "52/295, train_loss: 0.0842, step time: 1.0665\n",
      "53/295, train_loss: 0.0276, step time: 1.0937\n",
      "54/295, train_loss: 0.0783, step time: 1.0643\n",
      "55/295, train_loss: 0.0925, step time: 1.0559\n",
      "56/295, train_loss: 0.0567, step time: 1.0359\n",
      "57/295, train_loss: 0.0240, step time: 1.0591\n",
      "58/295, train_loss: 0.0221, step time: 1.0896\n",
      "59/295, train_loss: 0.1128, step time: 1.0347\n",
      "60/295, train_loss: 0.0635, step time: 1.0763\n",
      "61/295, train_loss: 0.0402, step time: 1.0554\n",
      "62/295, train_loss: 0.0407, step time: 1.1049\n",
      "63/295, train_loss: 0.0335, step time: 1.0534\n",
      "64/295, train_loss: 0.0268, step time: 1.0408\n",
      "65/295, train_loss: 0.0795, step time: 1.0699\n",
      "66/295, train_loss: 0.0714, step time: 1.0609\n",
      "67/295, train_loss: 0.0398, step time: 1.0409\n",
      "68/295, train_loss: 0.3695, step time: 1.0406\n",
      "69/295, train_loss: 0.0226, step time: 1.0405\n",
      "70/295, train_loss: 0.0650, step time: 1.0426\n",
      "71/295, train_loss: 0.0522, step time: 1.0706\n",
      "72/295, train_loss: 0.0378, step time: 1.0547\n",
      "73/295, train_loss: 0.0659, step time: 1.0401\n",
      "74/295, train_loss: 0.0356, step time: 1.1230\n",
      "75/295, train_loss: 0.0366, step time: 1.0440\n",
      "76/295, train_loss: 0.0243, step time: 1.0569\n",
      "77/295, train_loss: 0.0637, step time: 1.0428\n",
      "78/295, train_loss: 0.0756, step time: 1.0410\n",
      "79/295, train_loss: 0.0395, step time: 1.0370\n",
      "80/295, train_loss: 0.0675, step time: 1.0386\n",
      "81/295, train_loss: 0.0404, step time: 1.0520\n",
      "82/295, train_loss: 0.0480, step time: 1.0900\n",
      "83/295, train_loss: 0.0584, step time: 1.0965\n",
      "84/295, train_loss: 0.0284, step time: 1.0408\n",
      "85/295, train_loss: 0.0953, step time: 1.0422\n",
      "86/295, train_loss: 0.3595, step time: 1.0423\n",
      "87/295, train_loss: 0.0375, step time: 1.0442\n",
      "88/295, train_loss: 0.0310, step time: 1.0455\n",
      "89/295, train_loss: 0.0195, step time: 1.0441\n",
      "90/295, train_loss: 0.0550, step time: 1.1483\n",
      "91/295, train_loss: 0.0509, step time: 1.0521\n",
      "92/295, train_loss: 0.0260, step time: 1.0349\n",
      "93/295, train_loss: 0.0331, step time: 1.0366\n",
      "94/295, train_loss: 0.0359, step time: 1.0639\n",
      "95/295, train_loss: 0.0508, step time: 1.0369\n",
      "96/295, train_loss: 0.0468, step time: 1.0510\n",
      "97/295, train_loss: 0.0608, step time: 1.0374\n",
      "98/295, train_loss: 0.0487, step time: 1.0343\n",
      "99/295, train_loss: 0.0893, step time: 1.0324\n",
      "100/295, train_loss: 0.0340, step time: 1.0698\n",
      "101/295, train_loss: 0.0568, step time: 1.0349\n",
      "102/295, train_loss: 0.0786, step time: 1.0582\n",
      "103/295, train_loss: 0.0456, step time: 1.0959\n",
      "104/295, train_loss: 0.0620, step time: 1.0837\n",
      "105/295, train_loss: 0.0481, step time: 1.0387\n",
      "106/295, train_loss: 0.0769, step time: 1.0875\n",
      "107/295, train_loss: 0.0437, step time: 1.0597\n",
      "108/295, train_loss: 0.1044, step time: 1.0476\n",
      "109/295, train_loss: 0.0760, step time: 1.0404\n",
      "110/295, train_loss: 0.0433, step time: 1.0314\n",
      "111/295, train_loss: 0.0332, step time: 1.0599\n",
      "112/295, train_loss: 0.3763, step time: 1.0307\n",
      "113/295, train_loss: 0.0502, step time: 1.0353\n",
      "114/295, train_loss: 0.0392, step time: 1.0708\n",
      "115/295, train_loss: 0.0266, step time: 1.0608\n",
      "116/295, train_loss: 0.0728, step time: 1.0439\n",
      "117/295, train_loss: 0.0714, step time: 1.0447\n",
      "118/295, train_loss: 0.0417, step time: 1.0457\n",
      "119/295, train_loss: 0.0311, step time: 1.0358\n",
      "120/295, train_loss: 0.0326, step time: 1.0532\n",
      "121/295, train_loss: 0.0883, step time: 1.0538\n",
      "122/295, train_loss: 0.0619, step time: 1.0568\n",
      "123/295, train_loss: 0.0261, step time: 1.0595\n",
      "124/295, train_loss: 0.0543, step time: 1.0590\n",
      "125/295, train_loss: 0.0653, step time: 1.0534\n",
      "126/295, train_loss: 0.2046, step time: 1.1120\n",
      "127/295, train_loss: 0.0602, step time: 1.0337\n",
      "128/295, train_loss: 0.0503, step time: 1.0441\n",
      "129/295, train_loss: 0.0490, step time: 1.0781\n",
      "130/295, train_loss: 0.0430, step time: 1.0510\n",
      "131/295, train_loss: 0.0265, step time: 1.0323\n",
      "132/295, train_loss: 0.0295, step time: 1.0521\n",
      "133/295, train_loss: 0.0223, step time: 1.1038\n",
      "134/295, train_loss: 0.0545, step time: 1.0328\n",
      "135/295, train_loss: 0.0670, step time: 1.1225\n",
      "136/295, train_loss: 0.0559, step time: 1.0488\n",
      "137/295, train_loss: 0.0563, step time: 1.0467\n",
      "138/295, train_loss: 0.0462, step time: 1.0384\n",
      "139/295, train_loss: 0.1047, step time: 1.0393\n",
      "140/295, train_loss: 0.0352, step time: 1.0892\n",
      "141/295, train_loss: 0.0710, step time: 1.0403\n",
      "142/295, train_loss: 0.0220, step time: 1.0319\n",
      "143/295, train_loss: 0.3768, step time: 1.0989\n",
      "144/295, train_loss: 0.3621, step time: 1.0823\n",
      "145/295, train_loss: 0.0405, step time: 1.0338\n",
      "146/295, train_loss: 0.0921, step time: 1.0602\n",
      "147/295, train_loss: 0.0697, step time: 1.0353\n",
      "148/295, train_loss: 0.0301, step time: 1.0451\n",
      "149/295, train_loss: 0.0603, step time: 1.0550\n",
      "150/295, train_loss: 0.0742, step time: 1.0470\n",
      "151/295, train_loss: 0.0571, step time: 1.0548\n",
      "152/295, train_loss: 0.0496, step time: 1.0451\n",
      "153/295, train_loss: 0.0821, step time: 1.0521\n",
      "154/295, train_loss: 0.0284, step time: 1.0383\n",
      "155/295, train_loss: 0.0662, step time: 1.0724\n",
      "156/295, train_loss: 0.0610, step time: 1.0323\n",
      "157/295, train_loss: 0.0605, step time: 1.0370\n",
      "158/295, train_loss: 0.0578, step time: 1.1085\n",
      "159/295, train_loss: 0.0465, step time: 1.0412\n",
      "160/295, train_loss: 0.0302, step time: 1.0321\n",
      "161/295, train_loss: 0.1112, step time: 1.0363\n",
      "162/295, train_loss: 0.0361, step time: 1.0351\n",
      "163/295, train_loss: 0.0535, step time: 1.0341\n",
      "164/295, train_loss: 0.0882, step time: 1.0359\n",
      "165/295, train_loss: 0.0596, step time: 1.0387\n",
      "166/295, train_loss: 0.0460, step time: 1.0833\n",
      "167/295, train_loss: 0.0644, step time: 1.0567\n",
      "168/295, train_loss: 0.0623, step time: 1.0350\n",
      "169/295, train_loss: 0.0870, step time: 1.0363\n",
      "170/295, train_loss: 0.0666, step time: 1.0376\n",
      "171/295, train_loss: 0.3711, step time: 1.0431\n",
      "172/295, train_loss: 0.0264, step time: 1.0632\n",
      "173/295, train_loss: 0.0711, step time: 1.0882\n",
      "174/295, train_loss: 0.0737, step time: 1.0422\n",
      "175/295, train_loss: 0.0392, step time: 1.0407\n",
      "176/295, train_loss: 0.0252, step time: 1.0354\n",
      "177/295, train_loss: 0.0324, step time: 1.0342\n",
      "178/295, train_loss: 0.0592, step time: 1.0321\n",
      "179/295, train_loss: 0.0481, step time: 1.0330\n",
      "180/295, train_loss: 0.0429, step time: 1.0443\n",
      "181/295, train_loss: 0.0448, step time: 1.0364\n",
      "182/295, train_loss: 0.0885, step time: 1.0378\n",
      "183/295, train_loss: 0.0568, step time: 1.0395\n",
      "184/295, train_loss: 0.0269, step time: 1.0711\n",
      "185/295, train_loss: 0.0367, step time: 1.1336\n",
      "186/295, train_loss: 0.0248, step time: 1.1084\n",
      "187/295, train_loss: 0.0678, step time: 1.0497\n",
      "188/295, train_loss: 0.0712, step time: 1.0354\n",
      "189/295, train_loss: 0.0769, step time: 1.0746\n",
      "190/295, train_loss: 0.0451, step time: 1.0332\n",
      "191/295, train_loss: 0.0598, step time: 1.0418\n",
      "192/295, train_loss: 0.0766, step time: 1.0438\n",
      "193/295, train_loss: 0.1013, step time: 1.0451\n",
      "194/295, train_loss: 0.0386, step time: 1.0352\n",
      "195/295, train_loss: 0.0893, step time: 1.0615\n",
      "196/295, train_loss: 0.0533, step time: 1.0338\n",
      "197/295, train_loss: 0.0308, step time: 1.0319\n",
      "198/295, train_loss: 0.3923, step time: 1.0369\n",
      "199/295, train_loss: 0.0331, step time: 1.0365\n",
      "200/295, train_loss: 0.0683, step time: 1.0577\n",
      "201/295, train_loss: 0.0374, step time: 1.1331\n",
      "202/295, train_loss: 0.0434, step time: 1.0335\n",
      "203/295, train_loss: 0.0397, step time: 1.0445\n",
      "204/295, train_loss: 0.0577, step time: 1.0388\n",
      "205/295, train_loss: 0.4162, step time: 1.0324\n",
      "206/295, train_loss: 0.0485, step time: 1.0678\n",
      "207/295, train_loss: 0.0746, step time: 1.0668\n",
      "208/295, train_loss: 0.0669, step time: 1.0483\n",
      "209/295, train_loss: 0.0854, step time: 1.0396\n",
      "210/295, train_loss: 0.0632, step time: 1.0677\n",
      "211/295, train_loss: 0.1241, step time: 1.0375\n",
      "212/295, train_loss: 0.0335, step time: 1.0373\n",
      "213/295, train_loss: 0.0525, step time: 1.0722\n",
      "214/295, train_loss: 0.0714, step time: 1.0341\n",
      "215/295, train_loss: 0.3652, step time: 1.0404\n",
      "216/295, train_loss: 0.0493, step time: 1.0395\n",
      "217/295, train_loss: 0.0961, step time: 1.0330\n",
      "218/295, train_loss: 0.3772, step time: 1.0433\n",
      "219/295, train_loss: 0.0368, step time: 1.0350\n",
      "220/295, train_loss: 0.0401, step time: 1.0335\n",
      "221/295, train_loss: 0.0905, step time: 1.0441\n",
      "222/295, train_loss: 0.3713, step time: 1.0534\n",
      "223/295, train_loss: 0.3794, step time: 1.0695\n",
      "224/295, train_loss: 0.0597, step time: 1.0500\n",
      "225/295, train_loss: 0.0429, step time: 1.0540\n",
      "226/295, train_loss: 0.0248, step time: 1.0647\n",
      "227/295, train_loss: 0.0587, step time: 1.1265\n",
      "228/295, train_loss: 0.0338, step time: 1.0418\n",
      "229/295, train_loss: 0.0965, step time: 1.0327\n",
      "230/295, train_loss: 0.0619, step time: 1.0409\n",
      "231/295, train_loss: 0.3582, step time: 1.0327\n",
      "232/295, train_loss: 0.0582, step time: 1.0354\n",
      "233/295, train_loss: 0.0548, step time: 1.0362\n",
      "234/295, train_loss: 0.0515, step time: 1.0360\n",
      "235/295, train_loss: 0.0252, step time: 1.0338\n",
      "236/295, train_loss: 0.0170, step time: 1.0488\n",
      "237/295, train_loss: 0.0407, step time: 1.0391\n",
      "238/295, train_loss: 0.0302, step time: 1.0352\n",
      "239/295, train_loss: 0.0459, step time: 1.0652\n",
      "240/295, train_loss: 0.0760, step time: 1.0434\n",
      "241/295, train_loss: 0.0492, step time: 1.0599\n",
      "242/295, train_loss: 0.0303, step time: 1.0563\n",
      "243/295, train_loss: 0.0442, step time: 1.0410\n",
      "244/295, train_loss: 0.0736, step time: 1.0377\n",
      "245/295, train_loss: 0.0620, step time: 1.0416\n",
      "246/295, train_loss: 0.0718, step time: 1.0446\n",
      "247/295, train_loss: 0.0630, step time: 1.0389\n",
      "248/295, train_loss: 0.0340, step time: 1.0359\n",
      "249/295, train_loss: 0.0528, step time: 1.0374\n",
      "250/295, train_loss: 0.3555, step time: 1.0728\n",
      "251/295, train_loss: 0.3682, step time: 1.0815\n",
      "252/295, train_loss: 0.0438, step time: 1.0351\n",
      "253/295, train_loss: 0.0498, step time: 1.0628\n",
      "254/295, train_loss: 0.0311, step time: 1.0352\n",
      "255/295, train_loss: 0.0264, step time: 1.0833\n",
      "256/295, train_loss: 0.0298, step time: 1.0331\n",
      "257/295, train_loss: 0.0484, step time: 1.0425\n",
      "258/295, train_loss: 0.0290, step time: 1.0674\n",
      "259/295, train_loss: 0.0498, step time: 1.0361\n",
      "260/295, train_loss: 0.0652, step time: 1.0476\n",
      "261/295, train_loss: 0.0347, step time: 1.0330\n",
      "262/295, train_loss: 0.3753, step time: 1.0347\n",
      "263/295, train_loss: 0.0379, step time: 1.0517\n",
      "264/295, train_loss: 0.3565, step time: 1.0386\n",
      "265/295, train_loss: 0.0247, step time: 1.0446\n",
      "266/295, train_loss: 0.0433, step time: 1.0350\n",
      "267/295, train_loss: 0.0718, step time: 1.0709\n",
      "268/295, train_loss: 0.0705, step time: 1.0386\n",
      "269/295, train_loss: 0.0343, step time: 1.0421\n",
      "270/295, train_loss: 0.0221, step time: 1.0450\n",
      "271/295, train_loss: 0.0313, step time: 1.1248\n",
      "272/295, train_loss: 0.0485, step time: 1.0379\n",
      "273/295, train_loss: 0.0545, step time: 1.0351\n",
      "274/295, train_loss: 0.0441, step time: 1.0363\n",
      "275/295, train_loss: 0.0344, step time: 1.0323\n",
      "276/295, train_loss: 0.3600, step time: 1.0774\n",
      "277/295, train_loss: 0.0557, step time: 1.0605\n",
      "278/295, train_loss: 0.0525, step time: 1.0330\n",
      "279/295, train_loss: 0.0421, step time: 1.0603\n",
      "280/295, train_loss: 0.0448, step time: 1.0439\n",
      "281/295, train_loss: 0.0474, step time: 1.0345\n",
      "282/295, train_loss: 0.0701, step time: 1.0567\n",
      "283/295, train_loss: 0.0379, step time: 1.0442\n",
      "284/295, train_loss: 0.0392, step time: 1.0615\n",
      "285/295, train_loss: 0.1120, step time: 1.0489\n",
      "286/295, train_loss: 0.0538, step time: 1.0350\n",
      "287/295, train_loss: 0.0401, step time: 1.0313\n",
      "288/295, train_loss: 0.0757, step time: 1.0492\n",
      "289/295, train_loss: 0.0455, step time: 1.0286\n",
      "290/295, train_loss: 0.0420, step time: 1.0277\n",
      "291/295, train_loss: 0.0729, step time: 1.0296\n",
      "292/295, train_loss: 0.1000, step time: 1.0276\n",
      "293/295, train_loss: 0.0776, step time: 1.0294\n",
      "294/295, train_loss: 0.1200, step time: 1.0310\n",
      "295/295, train_loss: 0.0238, step time: 1.0280\n",
      "epoch 97 average loss: 0.0769\n",
      "current epoch: 97 current mean dice: 0.7428 tc: 0.6936 wt: 0.8147 et: 0.7260\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 97 is: 386.3418\n",
      "----------\n",
      "epoch 98/100\n",
      "1/295, train_loss: 0.0439, step time: 1.1797\n",
      "2/295, train_loss: 0.0414, step time: 1.0922\n",
      "3/295, train_loss: 0.0909, step time: 1.1824\n",
      "4/295, train_loss: 0.0647, step time: 1.1172\n",
      "5/295, train_loss: 0.0285, step time: 1.0552\n",
      "6/295, train_loss: 0.0598, step time: 1.0562\n",
      "7/295, train_loss: 0.0404, step time: 1.0843\n",
      "8/295, train_loss: 0.0292, step time: 1.0300\n",
      "9/295, train_loss: 0.0330, step time: 1.0286\n",
      "10/295, train_loss: 0.0425, step time: 1.0339\n",
      "11/295, train_loss: 0.0654, step time: 1.0322\n",
      "12/295, train_loss: 0.0705, step time: 1.0380\n",
      "13/295, train_loss: 0.0626, step time: 1.0922\n",
      "14/295, train_loss: 0.0316, step time: 1.0773\n",
      "15/295, train_loss: 0.0657, step time: 1.0463\n",
      "16/295, train_loss: 0.0253, step time: 1.0312\n",
      "17/295, train_loss: 0.0768, step time: 1.0311\n",
      "18/295, train_loss: 0.0456, step time: 1.0382\n",
      "19/295, train_loss: 0.0413, step time: 1.0517\n",
      "20/295, train_loss: 0.0759, step time: 1.0613\n",
      "21/295, train_loss: 0.0361, step time: 1.0311\n",
      "22/295, train_loss: 0.0681, step time: 1.1605\n",
      "23/295, train_loss: 0.0524, step time: 1.0389\n",
      "24/295, train_loss: 0.0890, step time: 1.0442\n",
      "25/295, train_loss: 0.0463, step time: 1.0565\n",
      "26/295, train_loss: 0.0302, step time: 1.0557\n",
      "27/295, train_loss: 0.0869, step time: 1.0505\n",
      "28/295, train_loss: 0.0807, step time: 1.0326\n",
      "29/295, train_loss: 0.0433, step time: 1.0431\n",
      "30/295, train_loss: 0.0459, step time: 1.1648\n",
      "31/295, train_loss: 0.0206, step time: 1.0500\n",
      "32/295, train_loss: 0.3600, step time: 1.0496\n",
      "33/295, train_loss: 0.0478, step time: 1.0412\n",
      "34/295, train_loss: 0.0261, step time: 1.0561\n",
      "35/295, train_loss: 0.0393, step time: 1.0440\n",
      "36/295, train_loss: 0.0550, step time: 1.0295\n",
      "37/295, train_loss: 0.3598, step time: 1.0310\n",
      "38/295, train_loss: 0.0752, step time: 1.0711\n",
      "39/295, train_loss: 0.0286, step time: 1.0355\n",
      "40/295, train_loss: 0.0334, step time: 1.0366\n",
      "41/295, train_loss: 0.3718, step time: 1.0391\n",
      "42/295, train_loss: 0.0715, step time: 1.0540\n",
      "43/295, train_loss: 0.0448, step time: 1.0355\n",
      "44/295, train_loss: 0.0533, step time: 1.0362\n",
      "45/295, train_loss: 0.0264, step time: 1.0414\n",
      "46/295, train_loss: 0.0721, step time: 1.0349\n",
      "47/295, train_loss: 0.0418, step time: 1.0394\n",
      "48/295, train_loss: 0.0665, step time: 1.0544\n",
      "49/295, train_loss: 0.0348, step time: 1.0425\n",
      "50/295, train_loss: 0.0592, step time: 1.0570\n",
      "51/295, train_loss: 0.0273, step time: 1.0552\n",
      "52/295, train_loss: 0.0194, step time: 1.0336\n",
      "53/295, train_loss: 0.3646, step time: 1.0400\n",
      "54/295, train_loss: 0.0963, step time: 1.0476\n",
      "55/295, train_loss: 0.0560, step time: 1.0586\n",
      "56/295, train_loss: 0.0571, step time: 1.0950\n",
      "57/295, train_loss: 0.0305, step time: 1.0391\n",
      "58/295, train_loss: 0.0652, step time: 1.0593\n",
      "59/295, train_loss: 0.0543, step time: 1.0374\n",
      "60/295, train_loss: 0.0322, step time: 1.0442\n",
      "61/295, train_loss: 0.0530, step time: 1.0581\n",
      "62/295, train_loss: 0.0300, step time: 1.0693\n",
      "63/295, train_loss: 0.0269, step time: 1.0426\n",
      "64/295, train_loss: 0.0333, step time: 1.0416\n",
      "65/295, train_loss: 0.0334, step time: 1.0353\n",
      "66/295, train_loss: 0.0764, step time: 1.0380\n",
      "67/295, train_loss: 0.3733, step time: 1.0396\n",
      "68/295, train_loss: 0.0467, step time: 1.0527\n",
      "69/295, train_loss: 0.0301, step time: 1.0603\n",
      "70/295, train_loss: 0.0675, step time: 1.0482\n",
      "71/295, train_loss: 0.0796, step time: 1.0479\n",
      "72/295, train_loss: 0.0381, step time: 1.0380\n",
      "73/295, train_loss: 0.0619, step time: 1.0551\n",
      "74/295, train_loss: 0.0450, step time: 1.0766\n",
      "75/295, train_loss: 0.0479, step time: 1.0497\n",
      "76/295, train_loss: 0.0460, step time: 1.0770\n",
      "77/295, train_loss: 0.0588, step time: 1.0348\n",
      "78/295, train_loss: 0.0648, step time: 1.0574\n",
      "79/295, train_loss: 0.0353, step time: 1.0356\n",
      "80/295, train_loss: 0.3791, step time: 1.0359\n",
      "81/295, train_loss: 0.0308, step time: 1.0330\n",
      "82/295, train_loss: 0.0713, step time: 1.0594\n",
      "83/295, train_loss: 0.0566, step time: 1.0326\n",
      "84/295, train_loss: 0.0405, step time: 1.0360\n",
      "85/295, train_loss: 0.0673, step time: 1.0463\n",
      "86/295, train_loss: 0.0710, step time: 1.0456\n",
      "87/295, train_loss: 0.0493, step time: 1.0842\n",
      "88/295, train_loss: 0.0607, step time: 1.0774\n",
      "89/295, train_loss: 0.0396, step time: 1.0523\n",
      "90/295, train_loss: 0.3623, step time: 1.0354\n",
      "91/295, train_loss: 0.0356, step time: 1.0580\n",
      "92/295, train_loss: 0.0638, step time: 1.0521\n",
      "93/295, train_loss: 0.0456, step time: 1.0496\n",
      "94/295, train_loss: 0.0247, step time: 1.0507\n",
      "95/295, train_loss: 0.3739, step time: 1.0401\n",
      "96/295, train_loss: 0.0581, step time: 1.0432\n",
      "97/295, train_loss: 0.0237, step time: 1.0307\n",
      "98/295, train_loss: 0.0633, step time: 1.0444\n",
      "99/295, train_loss: 0.0999, step time: 1.0506\n",
      "100/295, train_loss: 0.0777, step time: 1.0403\n",
      "101/295, train_loss: 0.0497, step time: 1.0553\n",
      "102/295, train_loss: 0.0315, step time: 1.0660\n",
      "103/295, train_loss: 0.0440, step time: 1.0306\n",
      "104/295, train_loss: 0.0472, step time: 1.0335\n",
      "105/295, train_loss: 0.0219, step time: 1.0369\n",
      "106/295, train_loss: 0.0611, step time: 1.0340\n",
      "107/295, train_loss: 0.0353, step time: 1.0433\n",
      "108/295, train_loss: 0.3852, step time: 1.0356\n",
      "109/295, train_loss: 0.0265, step time: 1.0519\n",
      "110/295, train_loss: 0.0556, step time: 1.0403\n",
      "111/295, train_loss: 0.0781, step time: 1.0366\n",
      "112/295, train_loss: 0.0382, step time: 1.0389\n",
      "113/295, train_loss: 0.0547, step time: 1.0531\n",
      "114/295, train_loss: 0.3764, step time: 1.0387\n",
      "115/295, train_loss: 0.3714, step time: 1.0533\n",
      "116/295, train_loss: 0.0624, step time: 1.0754\n",
      "117/295, train_loss: 0.0358, step time: 1.0289\n",
      "118/295, train_loss: 0.0633, step time: 1.0349\n",
      "119/295, train_loss: 0.0419, step time: 1.0407\n",
      "120/295, train_loss: 0.0685, step time: 1.0455\n",
      "121/295, train_loss: 0.0839, step time: 1.0641\n",
      "122/295, train_loss: 0.0602, step time: 1.0534\n",
      "123/295, train_loss: 0.0268, step time: 1.0519\n",
      "124/295, train_loss: 0.0835, step time: 1.0348\n",
      "125/295, train_loss: 0.0434, step time: 1.1001\n",
      "126/295, train_loss: 0.0276, step time: 1.0622\n",
      "127/295, train_loss: 0.0286, step time: 1.0507\n",
      "128/295, train_loss: 0.0263, step time: 1.0559\n",
      "129/295, train_loss: 0.0251, step time: 1.0477\n",
      "130/295, train_loss: 0.0375, step time: 1.0327\n",
      "131/295, train_loss: 0.0523, step time: 1.0485\n",
      "132/295, train_loss: 0.0222, step time: 1.0477\n",
      "133/295, train_loss: 0.0659, step time: 1.0408\n",
      "134/295, train_loss: 0.0569, step time: 1.0544\n",
      "135/295, train_loss: 0.0375, step time: 1.0645\n",
      "136/295, train_loss: 0.3918, step time: 1.0856\n",
      "137/295, train_loss: 0.0249, step time: 1.0311\n",
      "138/295, train_loss: 0.0506, step time: 1.0370\n",
      "139/295, train_loss: 0.2047, step time: 1.0316\n",
      "140/295, train_loss: 0.0846, step time: 1.0328\n",
      "141/295, train_loss: 0.0623, step time: 1.0653\n",
      "142/295, train_loss: 0.3772, step time: 1.0443\n",
      "143/295, train_loss: 0.0747, step time: 1.0438\n",
      "144/295, train_loss: 0.0342, step time: 1.0316\n",
      "145/295, train_loss: 0.0391, step time: 1.0317\n",
      "146/295, train_loss: 0.0582, step time: 1.0368\n",
      "147/295, train_loss: 0.0739, step time: 1.0473\n",
      "148/295, train_loss: 0.0302, step time: 1.0582\n",
      "149/295, train_loss: 0.0249, step time: 1.0500\n",
      "150/295, train_loss: 0.0481, step time: 1.0392\n",
      "151/295, train_loss: 0.1049, step time: 1.0515\n",
      "152/295, train_loss: 0.0605, step time: 1.0495\n",
      "153/295, train_loss: 0.0333, step time: 1.1315\n",
      "154/295, train_loss: 0.0740, step time: 1.0332\n",
      "155/295, train_loss: 0.0397, step time: 1.0420\n",
      "156/295, train_loss: 0.0296, step time: 1.0316\n",
      "157/295, train_loss: 0.1119, step time: 1.0316\n",
      "158/295, train_loss: 0.0760, step time: 1.0450\n",
      "159/295, train_loss: 0.0566, step time: 1.0531\n",
      "160/295, train_loss: 0.0462, step time: 1.0363\n",
      "161/295, train_loss: 0.0268, step time: 1.0393\n",
      "162/295, train_loss: 0.0882, step time: 1.0915\n",
      "163/295, train_loss: 0.0388, step time: 1.0328\n",
      "164/295, train_loss: 0.0767, step time: 1.0443\n",
      "165/295, train_loss: 0.3581, step time: 1.0429\n",
      "166/295, train_loss: 0.0759, step time: 1.0376\n",
      "167/295, train_loss: 0.1123, step time: 1.0385\n",
      "168/295, train_loss: 0.0922, step time: 1.0620\n",
      "169/295, train_loss: 0.0420, step time: 1.0636\n",
      "170/295, train_loss: 0.0478, step time: 1.0470\n",
      "171/295, train_loss: 0.0507, step time: 1.0900\n",
      "172/295, train_loss: 0.0667, step time: 1.0320\n",
      "173/295, train_loss: 0.0805, step time: 1.0427\n",
      "174/295, train_loss: 0.0696, step time: 1.0622\n",
      "175/295, train_loss: 0.0709, step time: 1.0439\n",
      "176/295, train_loss: 0.0171, step time: 1.0365\n",
      "177/295, train_loss: 0.0884, step time: 1.0367\n",
      "178/295, train_loss: 0.0718, step time: 1.0591\n",
      "179/295, train_loss: 0.3566, step time: 1.0410\n",
      "180/295, train_loss: 0.0243, step time: 1.0565\n",
      "181/295, train_loss: 0.0504, step time: 1.0571\n",
      "182/295, train_loss: 0.0653, step time: 1.0550\n",
      "183/295, train_loss: 0.0487, step time: 1.0854\n",
      "184/295, train_loss: 0.1009, step time: 1.0323\n",
      "185/295, train_loss: 0.0784, step time: 1.0434\n",
      "186/295, train_loss: 0.0512, step time: 1.0316\n",
      "187/295, train_loss: 0.1039, step time: 1.0744\n",
      "188/295, train_loss: 0.0896, step time: 1.0444\n",
      "189/295, train_loss: 0.0220, step time: 1.0327\n",
      "190/295, train_loss: 0.0699, step time: 1.0408\n",
      "191/295, train_loss: 0.0419, step time: 1.0756\n",
      "192/295, train_loss: 0.0569, step time: 1.0377\n",
      "193/295, train_loss: 0.0286, step time: 1.0528\n",
      "194/295, train_loss: 0.0249, step time: 1.0615\n",
      "195/295, train_loss: 0.0482, step time: 1.0454\n",
      "196/295, train_loss: 0.0680, step time: 1.0382\n",
      "197/295, train_loss: 0.0265, step time: 1.0525\n",
      "198/295, train_loss: 0.0484, step time: 1.0393\n",
      "199/295, train_loss: 0.0334, step time: 1.0390\n",
      "200/295, train_loss: 0.0454, step time: 1.0799\n",
      "201/295, train_loss: 0.3763, step time: 1.0452\n",
      "202/295, train_loss: 0.3658, step time: 1.0393\n",
      "203/295, train_loss: 0.0495, step time: 1.0442\n",
      "204/295, train_loss: 0.0355, step time: 1.0330\n",
      "205/295, train_loss: 0.1240, step time: 1.0461\n",
      "206/295, train_loss: 0.0746, step time: 1.0421\n",
      "207/295, train_loss: 0.0543, step time: 1.0561\n",
      "208/295, train_loss: 0.0403, step time: 1.0322\n",
      "209/295, train_loss: 0.0407, step time: 1.0417\n",
      "210/295, train_loss: 0.0327, step time: 1.0789\n",
      "211/295, train_loss: 0.0680, step time: 1.0374\n",
      "212/295, train_loss: 0.0399, step time: 1.0587\n",
      "213/295, train_loss: 0.0459, step time: 1.0361\n",
      "214/295, train_loss: 0.0337, step time: 1.0359\n",
      "215/295, train_loss: 0.0591, step time: 1.0412\n",
      "216/295, train_loss: 0.0617, step time: 1.0378\n",
      "217/295, train_loss: 0.0760, step time: 1.0443\n",
      "218/295, train_loss: 0.0503, step time: 1.0490\n",
      "219/295, train_loss: 0.0544, step time: 1.0459\n",
      "220/295, train_loss: 0.0737, step time: 1.0597\n",
      "221/295, train_loss: 0.0924, step time: 1.0385\n",
      "222/295, train_loss: 0.0410, step time: 1.0627\n",
      "223/295, train_loss: 0.0224, step time: 1.0634\n",
      "224/295, train_loss: 0.0968, step time: 1.0782\n",
      "225/295, train_loss: 0.0711, step time: 1.0531\n",
      "226/295, train_loss: 0.0345, step time: 1.0372\n",
      "227/295, train_loss: 0.0500, step time: 1.0443\n",
      "228/295, train_loss: 0.0884, step time: 1.0859\n",
      "229/295, train_loss: 0.0604, step time: 1.0395\n",
      "230/295, train_loss: 0.0532, step time: 1.0359\n",
      "231/295, train_loss: 0.0728, step time: 1.0395\n",
      "232/295, train_loss: 0.0342, step time: 1.0533\n",
      "233/295, train_loss: 0.0882, step time: 1.0422\n",
      "234/295, train_loss: 0.0617, step time: 1.0346\n",
      "235/295, train_loss: 0.0383, step time: 1.0310\n",
      "236/295, train_loss: 0.0440, step time: 1.0562\n",
      "237/295, train_loss: 0.0595, step time: 1.0359\n",
      "238/295, train_loss: 0.0394, step time: 1.0540\n",
      "239/295, train_loss: 0.3697, step time: 1.0624\n",
      "240/295, train_loss: 0.0485, step time: 1.0637\n",
      "241/295, train_loss: 0.0495, step time: 1.0418\n",
      "242/295, train_loss: 0.0947, step time: 1.0464\n",
      "243/295, train_loss: 0.0377, step time: 1.0772\n",
      "244/295, train_loss: 0.0293, step time: 1.0515\n",
      "245/295, train_loss: 0.0731, step time: 1.0425\n",
      "246/295, train_loss: 0.4165, step time: 1.0548\n",
      "247/295, train_loss: 0.0438, step time: 1.0982\n",
      "248/295, train_loss: 0.0526, step time: 1.0516\n",
      "249/295, train_loss: 0.0612, step time: 1.0569\n",
      "250/295, train_loss: 0.0302, step time: 1.0333\n",
      "251/295, train_loss: 0.0432, step time: 1.0435\n",
      "252/295, train_loss: 0.0365, step time: 1.0649\n",
      "253/295, train_loss: 0.0449, step time: 1.0361\n",
      "254/295, train_loss: 0.1109, step time: 1.0349\n",
      "255/295, train_loss: 0.0578, step time: 1.0699\n",
      "256/295, train_loss: 0.0429, step time: 1.0407\n",
      "257/295, train_loss: 0.0483, step time: 1.0655\n",
      "258/295, train_loss: 0.0489, step time: 1.0415\n",
      "259/295, train_loss: 0.0526, step time: 1.0396\n",
      "260/295, train_loss: 0.0393, step time: 1.0398\n",
      "261/295, train_loss: 0.3765, step time: 1.0508\n",
      "262/295, train_loss: 0.0375, step time: 1.0380\n",
      "263/295, train_loss: 0.0242, step time: 1.0352\n",
      "264/295, train_loss: 0.0447, step time: 1.0440\n",
      "265/295, train_loss: 0.0335, step time: 1.0509\n",
      "266/295, train_loss: 0.0243, step time: 1.0419\n",
      "267/295, train_loss: 0.0224, step time: 1.0634\n",
      "268/295, train_loss: 0.0368, step time: 1.0417\n",
      "269/295, train_loss: 0.0370, step time: 1.1016\n",
      "270/295, train_loss: 0.0564, step time: 1.0506\n",
      "271/295, train_loss: 0.0311, step time: 1.1284\n",
      "272/295, train_loss: 0.3552, step time: 1.0383\n",
      "273/295, train_loss: 0.0524, step time: 1.0584\n",
      "274/295, train_loss: 0.1197, step time: 1.0508\n",
      "275/295, train_loss: 0.0719, step time: 1.0783\n",
      "276/295, train_loss: 0.0312, step time: 1.0349\n",
      "277/295, train_loss: 0.0373, step time: 1.0585\n",
      "278/295, train_loss: 0.0452, step time: 1.0513\n",
      "279/295, train_loss: 0.3686, step time: 1.0392\n",
      "280/295, train_loss: 0.0307, step time: 1.0729\n",
      "281/295, train_loss: 0.0473, step time: 1.0715\n",
      "282/295, train_loss: 0.0712, step time: 1.0428\n",
      "283/295, train_loss: 0.0565, step time: 1.0583\n",
      "284/295, train_loss: 0.0339, step time: 1.0327\n",
      "285/295, train_loss: 0.0510, step time: 1.0341\n",
      "286/295, train_loss: 0.0258, step time: 1.0322\n",
      "287/295, train_loss: 0.0457, step time: 1.0392\n",
      "288/295, train_loss: 0.0356, step time: 1.0381\n",
      "289/295, train_loss: 0.0449, step time: 1.0291\n",
      "290/295, train_loss: 0.0558, step time: 1.0284\n",
      "291/295, train_loss: 0.0590, step time: 1.0290\n",
      "292/295, train_loss: 0.0432, step time: 1.0282\n",
      "293/295, train_loss: 0.0696, step time: 1.0290\n",
      "294/295, train_loss: 0.0394, step time: 1.0284\n",
      "295/295, train_loss: 0.0659, step time: 1.0279\n",
      "epoch 98 average loss: 0.0768\n",
      "current epoch: 98 current mean dice: 0.7531 tc: 0.7005 wt: 0.8241 et: 0.7398\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 98 is: 384.9829\n",
      "----------\n",
      "epoch 99/100\n",
      "1/295, train_loss: 0.0878, step time: 1.1248\n",
      "2/295, train_loss: 0.0331, step time: 1.2054\n",
      "3/295, train_loss: 0.0509, step time: 1.0435\n",
      "4/295, train_loss: 0.0373, step time: 1.0487\n",
      "5/295, train_loss: 0.0330, step time: 1.0573\n",
      "6/295, train_loss: 0.1050, step time: 1.0523\n",
      "7/295, train_loss: 0.0328, step time: 1.0401\n",
      "8/295, train_loss: 0.0335, step time: 1.0553\n",
      "9/295, train_loss: 0.0719, step time: 1.0425\n",
      "10/295, train_loss: 0.0226, step time: 1.0504\n",
      "11/295, train_loss: 0.0302, step time: 1.0333\n",
      "12/295, train_loss: 0.0237, step time: 1.0532\n",
      "13/295, train_loss: 0.0564, step time: 1.0311\n",
      "14/295, train_loss: 0.0482, step time: 1.0374\n",
      "15/295, train_loss: 0.0681, step time: 1.0487\n",
      "16/295, train_loss: 0.0305, step time: 1.0370\n",
      "17/295, train_loss: 0.0666, step time: 1.0348\n",
      "18/295, train_loss: 0.0844, step time: 1.0461\n",
      "19/295, train_loss: 0.0201, step time: 1.0798\n",
      "20/295, train_loss: 0.0566, step time: 1.0442\n",
      "21/295, train_loss: 0.0716, step time: 1.0391\n",
      "22/295, train_loss: 0.0366, step time: 1.0559\n",
      "23/295, train_loss: 0.0463, step time: 1.0296\n",
      "24/295, train_loss: 0.0430, step time: 1.0295\n",
      "25/295, train_loss: 0.0885, step time: 1.0319\n",
      "26/295, train_loss: 0.0560, step time: 1.0331\n",
      "27/295, train_loss: 0.0221, step time: 1.1041\n",
      "28/295, train_loss: 0.0312, step time: 1.0335\n",
      "29/295, train_loss: 0.0478, step time: 1.0899\n",
      "30/295, train_loss: 0.0566, step time: 1.0384\n",
      "31/295, train_loss: 0.0569, step time: 1.0459\n",
      "32/295, train_loss: 0.0315, step time: 1.0520\n",
      "33/295, train_loss: 0.0285, step time: 1.0462\n",
      "34/295, train_loss: 0.0655, step time: 1.0613\n",
      "35/295, train_loss: 0.0407, step time: 1.0304\n",
      "36/295, train_loss: 0.0807, step time: 1.0352\n",
      "37/295, train_loss: 0.0240, step time: 1.0401\n",
      "38/295, train_loss: 0.0581, step time: 1.0755\n",
      "39/295, train_loss: 0.1194, step time: 1.0734\n",
      "40/295, train_loss: 0.3595, step time: 1.0425\n",
      "41/295, train_loss: 0.0449, step time: 1.0320\n",
      "42/295, train_loss: 0.0498, step time: 1.0397\n",
      "43/295, train_loss: 0.0331, step time: 1.0347\n",
      "44/295, train_loss: 0.0376, step time: 1.0451\n",
      "45/295, train_loss: 0.0736, step time: 1.0844\n",
      "46/295, train_loss: 0.0651, step time: 1.0443\n",
      "47/295, train_loss: 0.3764, step time: 1.0569\n",
      "48/295, train_loss: 0.0760, step time: 1.0311\n",
      "49/295, train_loss: 0.0682, step time: 1.0569\n",
      "50/295, train_loss: 0.1120, step time: 1.0401\n",
      "51/295, train_loss: 0.0270, step time: 1.0505\n",
      "52/295, train_loss: 0.0360, step time: 1.0368\n",
      "53/295, train_loss: 0.0358, step time: 1.0339\n",
      "54/295, train_loss: 0.0500, step time: 1.0594\n",
      "55/295, train_loss: 0.0327, step time: 1.0386\n",
      "56/295, train_loss: 0.0393, step time: 1.0746\n",
      "57/295, train_loss: 0.0537, step time: 1.0753\n",
      "58/295, train_loss: 0.0961, step time: 1.0408\n",
      "59/295, train_loss: 0.0706, step time: 1.0303\n",
      "60/295, train_loss: 0.0654, step time: 1.0503\n",
      "61/295, train_loss: 0.0621, step time: 1.0446\n",
      "62/295, train_loss: 0.0467, step time: 1.1034\n",
      "63/295, train_loss: 0.0416, step time: 1.0564\n",
      "64/295, train_loss: 0.0459, step time: 1.0649\n",
      "65/295, train_loss: 0.0285, step time: 1.0330\n",
      "66/295, train_loss: 0.0528, step time: 1.0632\n",
      "67/295, train_loss: 0.0391, step time: 1.0351\n",
      "68/295, train_loss: 0.0432, step time: 1.0338\n",
      "69/295, train_loss: 0.0657, step time: 1.0407\n",
      "70/295, train_loss: 0.3584, step time: 1.0321\n",
      "71/295, train_loss: 0.0731, step time: 1.0571\n",
      "72/295, train_loss: 0.3766, step time: 1.0544\n",
      "73/295, train_loss: 0.3919, step time: 1.0429\n",
      "74/295, train_loss: 0.0562, step time: 1.0682\n",
      "75/295, train_loss: 0.0253, step time: 1.0529\n",
      "76/295, train_loss: 0.0289, step time: 1.0398\n",
      "77/295, train_loss: 0.0408, step time: 1.0349\n",
      "78/295, train_loss: 0.0644, step time: 1.0833\n",
      "79/295, train_loss: 0.0281, step time: 1.0320\n",
      "80/295, train_loss: 0.0618, step time: 1.0560\n",
      "81/295, train_loss: 0.0522, step time: 1.0333\n",
      "82/295, train_loss: 0.0250, step time: 1.0819\n",
      "83/295, train_loss: 0.0531, step time: 1.1227\n",
      "84/295, train_loss: 0.0371, step time: 1.0438\n",
      "85/295, train_loss: 0.0883, step time: 1.0745\n",
      "86/295, train_loss: 0.0308, step time: 1.0470\n",
      "87/295, train_loss: 0.0334, step time: 1.0359\n",
      "88/295, train_loss: 0.0892, step time: 1.0417\n",
      "89/295, train_loss: 0.0505, step time: 1.0439\n",
      "90/295, train_loss: 0.0602, step time: 1.0434\n",
      "91/295, train_loss: 0.0335, step time: 1.0363\n",
      "92/295, train_loss: 0.0170, step time: 1.0529\n",
      "93/295, train_loss: 0.0569, step time: 1.0516\n",
      "94/295, train_loss: 0.0523, step time: 1.0427\n",
      "95/295, train_loss: 0.0309, step time: 1.0356\n",
      "96/295, train_loss: 0.0340, step time: 1.0425\n",
      "97/295, train_loss: 0.0339, step time: 1.0860\n",
      "98/295, train_loss: 0.0485, step time: 1.0599\n",
      "99/295, train_loss: 0.0304, step time: 1.0389\n",
      "100/295, train_loss: 0.0458, step time: 1.0360\n",
      "101/295, train_loss: 0.0838, step time: 1.0523\n",
      "102/295, train_loss: 0.0736, step time: 1.0870\n",
      "103/295, train_loss: 0.0725, step time: 1.0372\n",
      "104/295, train_loss: 0.0518, step time: 1.0397\n",
      "105/295, train_loss: 0.0486, step time: 1.0347\n",
      "106/295, train_loss: 0.0805, step time: 1.0442\n",
      "107/295, train_loss: 0.0694, step time: 1.0364\n",
      "108/295, train_loss: 0.0419, step time: 1.0674\n",
      "109/295, train_loss: 0.0417, step time: 1.0620\n",
      "110/295, train_loss: 0.0268, step time: 1.0398\n",
      "111/295, train_loss: 0.0290, step time: 1.0699\n",
      "112/295, train_loss: 0.0600, step time: 1.0336\n",
      "113/295, train_loss: 0.0593, step time: 1.0487\n",
      "114/295, train_loss: 0.0298, step time: 1.0401\n",
      "115/295, train_loss: 0.0479, step time: 1.1158\n",
      "116/295, train_loss: 0.0458, step time: 1.0356\n",
      "117/295, train_loss: 0.0716, step time: 1.0636\n",
      "118/295, train_loss: 0.0395, step time: 1.0909\n",
      "119/295, train_loss: 0.0505, step time: 1.0393\n",
      "120/295, train_loss: 0.0275, step time: 1.0362\n",
      "121/295, train_loss: 0.0301, step time: 1.0965\n",
      "122/295, train_loss: 0.0545, step time: 1.0618\n",
      "123/295, train_loss: 0.0893, step time: 1.0356\n",
      "124/295, train_loss: 0.0783, step time: 1.0342\n",
      "125/295, train_loss: 0.0583, step time: 1.0517\n",
      "126/295, train_loss: 0.0249, step time: 1.0521\n",
      "127/295, train_loss: 0.0660, step time: 1.0501\n",
      "128/295, train_loss: 0.0460, step time: 1.0342\n",
      "129/295, train_loss: 0.0381, step time: 1.0435\n",
      "130/295, train_loss: 0.0483, step time: 1.0340\n",
      "131/295, train_loss: 0.0706, step time: 1.0408\n",
      "132/295, train_loss: 0.0390, step time: 1.0514\n",
      "133/295, train_loss: 0.0669, step time: 1.0428\n",
      "134/295, train_loss: 0.0758, step time: 1.0305\n",
      "135/295, train_loss: 0.3682, step time: 1.0407\n",
      "136/295, train_loss: 0.0395, step time: 1.0674\n",
      "137/295, train_loss: 0.0438, step time: 1.0940\n",
      "138/295, train_loss: 0.0709, step time: 1.0546\n",
      "139/295, train_loss: 0.0595, step time: 1.0425\n",
      "140/295, train_loss: 0.3553, step time: 1.0370\n",
      "141/295, train_loss: 0.0615, step time: 1.0527\n",
      "142/295, train_loss: 0.0453, step time: 1.0343\n",
      "143/295, train_loss: 0.0420, step time: 1.0550\n",
      "144/295, train_loss: 0.0456, step time: 1.0835\n",
      "145/295, train_loss: 0.0770, step time: 1.0632\n",
      "146/295, train_loss: 0.0459, step time: 1.0332\n",
      "147/295, train_loss: 0.0389, step time: 1.0333\n",
      "148/295, train_loss: 0.0344, step time: 1.0461\n",
      "149/295, train_loss: 0.0220, step time: 1.0389\n",
      "150/295, train_loss: 0.0672, step time: 1.0556\n",
      "151/295, train_loss: 0.0623, step time: 1.0726\n",
      "152/295, train_loss: 0.0543, step time: 1.0875\n",
      "153/295, train_loss: 0.3696, step time: 1.0601\n",
      "154/295, train_loss: 0.0486, step time: 1.0370\n",
      "155/295, train_loss: 0.3617, step time: 1.0555\n",
      "156/295, train_loss: 0.0316, step time: 1.0610\n",
      "157/295, train_loss: 0.3714, step time: 1.0368\n",
      "158/295, train_loss: 0.0489, step time: 1.0344\n",
      "159/295, train_loss: 0.3791, step time: 1.0472\n",
      "160/295, train_loss: 0.0607, step time: 1.0781\n",
      "161/295, train_loss: 0.0375, step time: 1.0332\n",
      "162/295, train_loss: 0.0760, step time: 1.0424\n",
      "163/295, train_loss: 0.0275, step time: 1.0600\n",
      "164/295, train_loss: 0.0967, step time: 1.0485\n",
      "165/295, train_loss: 0.0576, step time: 1.0501\n",
      "166/295, train_loss: 0.0353, step time: 1.0322\n",
      "167/295, train_loss: 0.0335, step time: 1.0331\n",
      "168/295, train_loss: 0.0250, step time: 1.1533\n",
      "169/295, train_loss: 0.3658, step time: 1.0607\n",
      "170/295, train_loss: 0.0447, step time: 1.0627\n",
      "171/295, train_loss: 0.0303, step time: 1.0571\n",
      "172/295, train_loss: 0.1036, step time: 1.0317\n",
      "173/295, train_loss: 0.0492, step time: 1.0414\n",
      "174/295, train_loss: 0.0563, step time: 1.0526\n",
      "175/295, train_loss: 0.0484, step time: 1.0571\n",
      "176/295, train_loss: 0.0524, step time: 1.0447\n",
      "177/295, train_loss: 0.0365, step time: 1.0389\n",
      "178/295, train_loss: 0.0682, step time: 1.0355\n",
      "179/295, train_loss: 0.0398, step time: 1.0458\n",
      "180/295, train_loss: 0.0261, step time: 1.0611\n",
      "181/295, train_loss: 0.0369, step time: 1.0477\n",
      "182/295, train_loss: 0.0221, step time: 1.0758\n",
      "183/295, train_loss: 0.0352, step time: 1.0388\n",
      "184/295, train_loss: 0.0435, step time: 1.1708\n",
      "185/295, train_loss: 0.0512, step time: 1.0532\n",
      "186/295, train_loss: 0.0907, step time: 1.0373\n",
      "187/295, train_loss: 0.0926, step time: 1.0470\n",
      "188/295, train_loss: 0.0598, step time: 1.0620\n",
      "189/295, train_loss: 0.0421, step time: 1.0538\n",
      "190/295, train_loss: 0.1000, step time: 1.0819\n",
      "191/295, train_loss: 0.0382, step time: 1.0384\n",
      "192/295, train_loss: 0.0497, step time: 1.0421\n",
      "193/295, train_loss: 0.0248, step time: 1.0820\n",
      "194/295, train_loss: 0.0471, step time: 1.0375\n",
      "195/295, train_loss: 0.0444, step time: 1.0433\n",
      "196/295, train_loss: 0.0655, step time: 1.0648\n",
      "197/295, train_loss: 0.0269, step time: 1.0388\n",
      "198/295, train_loss: 0.0747, step time: 1.0374\n",
      "199/295, train_loss: 0.0434, step time: 1.0371\n",
      "200/295, train_loss: 0.0262, step time: 1.0360\n",
      "201/295, train_loss: 0.3762, step time: 1.0344\n",
      "202/295, train_loss: 0.0719, step time: 1.0343\n",
      "203/295, train_loss: 0.0709, step time: 1.0347\n",
      "204/295, train_loss: 0.0354, step time: 1.0358\n",
      "205/295, train_loss: 0.1248, step time: 1.0556\n",
      "206/295, train_loss: 0.2047, step time: 1.0724\n",
      "207/295, train_loss: 0.0398, step time: 1.0371\n",
      "208/295, train_loss: 0.0449, step time: 1.0396\n",
      "209/295, train_loss: 0.3735, step time: 1.0421\n",
      "210/295, train_loss: 0.0541, step time: 1.0645\n",
      "211/295, train_loss: 0.0458, step time: 1.1011\n",
      "212/295, train_loss: 0.0225, step time: 1.0997\n",
      "213/295, train_loss: 0.0343, step time: 1.0407\n",
      "214/295, train_loss: 0.0609, step time: 1.0509\n",
      "215/295, train_loss: 0.0479, step time: 1.0411\n",
      "216/295, train_loss: 0.3735, step time: 1.0405\n",
      "217/295, train_loss: 0.0567, step time: 1.0376\n",
      "218/295, train_loss: 0.0648, step time: 1.0652\n",
      "219/295, train_loss: 0.0292, step time: 1.0671\n",
      "220/295, train_loss: 0.0358, step time: 1.0888\n",
      "221/295, train_loss: 0.0357, step time: 1.0450\n",
      "222/295, train_loss: 0.0755, step time: 1.0341\n",
      "223/295, train_loss: 0.0335, step time: 1.0866\n",
      "224/295, train_loss: 0.0670, step time: 1.0351\n",
      "225/295, train_loss: 0.0531, step time: 1.0673\n",
      "226/295, train_loss: 0.0373, step time: 1.0333\n",
      "227/295, train_loss: 0.3714, step time: 1.0416\n",
      "228/295, train_loss: 0.0615, step time: 1.1117\n",
      "229/295, train_loss: 0.1111, step time: 1.0516\n",
      "230/295, train_loss: 0.0393, step time: 1.0385\n",
      "231/295, train_loss: 0.0436, step time: 1.0413\n",
      "232/295, train_loss: 0.0735, step time: 1.0362\n",
      "233/295, train_loss: 0.0441, step time: 1.0411\n",
      "234/295, train_loss: 0.0951, step time: 1.0375\n",
      "235/295, train_loss: 0.0711, step time: 1.0831\n",
      "236/295, train_loss: 0.0262, step time: 1.0361\n",
      "237/295, train_loss: 0.0262, step time: 1.0355\n",
      "238/295, train_loss: 0.4163, step time: 1.0420\n",
      "239/295, train_loss: 0.0755, step time: 1.0435\n",
      "240/295, train_loss: 0.0495, step time: 1.0695\n",
      "241/295, train_loss: 0.0398, step time: 1.0508\n",
      "242/295, train_loss: 0.0600, step time: 1.0475\n",
      "243/295, train_loss: 0.0627, step time: 1.0382\n",
      "244/295, train_loss: 0.3765, step time: 1.0339\n",
      "245/295, train_loss: 0.0774, step time: 1.0547\n",
      "246/295, train_loss: 0.0194, step time: 1.0419\n",
      "247/295, train_loss: 0.0406, step time: 1.0386\n",
      "248/295, train_loss: 0.0413, step time: 1.0362\n",
      "249/295, train_loss: 0.0869, step time: 1.0692\n",
      "250/295, train_loss: 0.0638, step time: 1.0741\n",
      "251/295, train_loss: 0.3857, step time: 1.0458\n",
      "252/295, train_loss: 0.0689, step time: 1.0509\n",
      "253/295, train_loss: 0.0699, step time: 1.0419\n",
      "254/295, train_loss: 0.0382, step time: 1.0518\n",
      "255/295, train_loss: 0.0428, step time: 1.0427\n",
      "256/295, train_loss: 0.0799, step time: 1.0426\n",
      "257/295, train_loss: 0.0775, step time: 1.0614\n",
      "258/295, train_loss: 0.0775, step time: 1.0341\n",
      "259/295, train_loss: 0.0573, step time: 1.1055\n",
      "260/295, train_loss: 0.0609, step time: 1.0483\n",
      "261/295, train_loss: 0.0436, step time: 1.0526\n",
      "262/295, train_loss: 0.1013, step time: 1.0580\n",
      "263/295, train_loss: 0.0742, step time: 1.0375\n",
      "264/295, train_loss: 0.0589, step time: 1.0387\n",
      "265/295, train_loss: 0.0468, step time: 1.0452\n",
      "266/295, train_loss: 0.0921, step time: 1.0422\n",
      "267/295, train_loss: 0.0494, step time: 1.0451\n",
      "268/295, train_loss: 0.0632, step time: 1.0483\n",
      "269/295, train_loss: 0.3566, step time: 1.0701\n",
      "270/295, train_loss: 0.1114, step time: 1.1098\n",
      "271/295, train_loss: 0.0349, step time: 1.0368\n",
      "272/295, train_loss: 0.0438, step time: 1.0642\n",
      "273/295, train_loss: 0.0263, step time: 1.0478\n",
      "274/295, train_loss: 0.0431, step time: 1.0631\n",
      "275/295, train_loss: 0.0285, step time: 1.0716\n",
      "276/295, train_loss: 0.0246, step time: 1.0459\n",
      "277/295, train_loss: 0.0456, step time: 1.0648\n",
      "278/295, train_loss: 0.0589, step time: 1.0661\n",
      "279/295, train_loss: 0.0592, step time: 1.0397\n",
      "280/295, train_loss: 0.0409, step time: 1.0366\n",
      "281/295, train_loss: 0.0549, step time: 1.0479\n",
      "282/295, train_loss: 0.0698, step time: 1.0412\n",
      "283/295, train_loss: 0.3597, step time: 1.0476\n",
      "284/295, train_loss: 0.0706, step time: 1.0469\n",
      "285/295, train_loss: 0.0850, step time: 1.0343\n",
      "286/295, train_loss: 0.0242, step time: 1.0380\n",
      "287/295, train_loss: 0.0770, step time: 1.0316\n",
      "288/295, train_loss: 0.0308, step time: 1.0304\n",
      "289/295, train_loss: 0.3643, step time: 1.0281\n",
      "290/295, train_loss: 0.0632, step time: 1.0277\n",
      "291/295, train_loss: 0.0525, step time: 1.0267\n",
      "292/295, train_loss: 0.0250, step time: 1.0286\n",
      "293/295, train_loss: 0.0882, step time: 1.0284\n",
      "294/295, train_loss: 0.0542, step time: 1.0294\n",
      "295/295, train_loss: 0.0258, step time: 1.0286\n",
      "epoch 99 average loss: 0.0768\n",
      "current epoch: 99 current mean dice: 0.7548 tc: 0.7046 wt: 0.8209 et: 0.7420\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 99 is: 386.2730\n",
      "----------\n",
      "epoch 100/100\n",
      "1/295, train_loss: 0.0440, step time: 1.1178\n",
      "2/295, train_loss: 0.0891, step time: 1.0640\n",
      "3/295, train_loss: 0.0360, step time: 1.0742\n",
      "4/295, train_loss: 0.3597, step time: 1.0617\n",
      "5/295, train_loss: 0.0741, step time: 1.0324\n",
      "6/295, train_loss: 0.0618, step time: 1.0828\n",
      "7/295, train_loss: 0.0770, step time: 1.0558\n",
      "8/295, train_loss: 0.0964, step time: 1.0555\n",
      "9/295, train_loss: 0.0955, step time: 1.0359\n",
      "10/295, train_loss: 0.0885, step time: 1.0371\n",
      "11/295, train_loss: 0.0532, step time: 1.0382\n",
      "12/295, train_loss: 0.0295, step time: 1.0390\n",
      "13/295, train_loss: 0.0249, step time: 1.0475\n",
      "14/295, train_loss: 0.0590, step time: 1.0722\n",
      "15/295, train_loss: 0.0569, step time: 1.0379\n",
      "16/295, train_loss: 0.0791, step time: 1.0367\n",
      "17/295, train_loss: 0.0374, step time: 1.0341\n",
      "18/295, train_loss: 0.0303, step time: 1.0408\n",
      "19/295, train_loss: 0.0264, step time: 1.0433\n",
      "20/295, train_loss: 0.0391, step time: 1.0483\n",
      "21/295, train_loss: 0.0882, step time: 1.0477\n",
      "22/295, train_loss: 0.0745, step time: 1.0690\n",
      "23/295, train_loss: 0.0539, step time: 1.0330\n",
      "24/295, train_loss: 0.0782, step time: 1.0335\n",
      "25/295, train_loss: 0.0202, step time: 1.0458\n",
      "26/295, train_loss: 0.0682, step time: 1.0623\n",
      "27/295, train_loss: 0.0869, step time: 1.1906\n",
      "28/295, train_loss: 0.3714, step time: 1.0524\n",
      "29/295, train_loss: 0.3855, step time: 1.0383\n",
      "30/295, train_loss: 0.0381, step time: 1.0951\n",
      "31/295, train_loss: 0.0312, step time: 1.0426\n",
      "32/295, train_loss: 0.0453, step time: 1.0323\n",
      "33/295, train_loss: 0.0618, step time: 1.0412\n",
      "34/295, train_loss: 0.1120, step time: 1.0452\n",
      "35/295, train_loss: 0.0250, step time: 1.0302\n",
      "36/295, train_loss: 0.0776, step time: 1.0621\n",
      "37/295, train_loss: 0.0331, step time: 1.0382\n",
      "38/295, train_loss: 0.3645, step time: 1.0694\n",
      "39/295, train_loss: 0.0648, step time: 1.0556\n",
      "40/295, train_loss: 0.0564, step time: 1.0614\n",
      "41/295, train_loss: 0.3765, step time: 1.0334\n",
      "42/295, train_loss: 0.0446, step time: 1.0466\n",
      "43/295, train_loss: 0.0652, step time: 1.0586\n",
      "44/295, train_loss: 0.0282, step time: 1.0410\n",
      "45/295, train_loss: 0.0221, step time: 1.0452\n",
      "46/295, train_loss: 0.0697, step time: 1.0642\n",
      "47/295, train_loss: 0.0719, step time: 1.0588\n",
      "48/295, train_loss: 0.0686, step time: 1.0615\n",
      "49/295, train_loss: 0.0458, step time: 1.0609\n",
      "50/295, train_loss: 0.0268, step time: 1.0297\n",
      "51/295, train_loss: 0.0668, step time: 1.0331\n",
      "52/295, train_loss: 0.3789, step time: 1.0541\n",
      "53/295, train_loss: 0.0602, step time: 1.0534\n",
      "54/295, train_loss: 0.0842, step time: 1.0423\n",
      "55/295, train_loss: 0.0468, step time: 1.0300\n",
      "56/295, train_loss: 0.0432, step time: 1.0519\n",
      "57/295, train_loss: 0.2047, step time: 1.0423\n",
      "58/295, train_loss: 0.0171, step time: 1.0529\n",
      "59/295, train_loss: 0.0699, step time: 1.0418\n",
      "60/295, train_loss: 0.0736, step time: 1.0673\n",
      "61/295, train_loss: 0.0461, step time: 1.0460\n",
      "62/295, train_loss: 0.0553, step time: 1.0296\n",
      "63/295, train_loss: 0.0274, step time: 1.0325\n",
      "64/295, train_loss: 0.0649, step time: 1.0342\n",
      "65/295, train_loss: 0.0494, step time: 1.0340\n",
      "66/295, train_loss: 0.0566, step time: 1.0554\n",
      "67/295, train_loss: 0.0393, step time: 1.0318\n",
      "68/295, train_loss: 0.3563, step time: 1.0404\n",
      "69/295, train_loss: 0.0264, step time: 1.0616\n",
      "70/295, train_loss: 0.0500, step time: 1.0447\n",
      "71/295, train_loss: 0.0580, step time: 1.0490\n",
      "72/295, train_loss: 0.0513, step time: 1.0331\n",
      "73/295, train_loss: 0.0433, step time: 1.0309\n",
      "74/295, train_loss: 0.3619, step time: 1.0322\n",
      "75/295, train_loss: 0.0330, step time: 1.0374\n",
      "76/295, train_loss: 0.4167, step time: 1.0526\n",
      "77/295, train_loss: 0.0488, step time: 1.0324\n",
      "78/295, train_loss: 0.0678, step time: 1.0450\n",
      "79/295, train_loss: 0.0531, step time: 1.0352\n",
      "80/295, train_loss: 0.0351, step time: 1.0390\n",
      "81/295, train_loss: 0.0768, step time: 1.0514\n",
      "82/295, train_loss: 0.0436, step time: 1.0704\n",
      "83/295, train_loss: 0.0963, step time: 1.0538\n",
      "84/295, train_loss: 0.0751, step time: 1.0655\n",
      "85/295, train_loss: 0.0252, step time: 1.0321\n",
      "86/295, train_loss: 0.3555, step time: 1.0374\n",
      "87/295, train_loss: 0.0280, step time: 1.0424\n",
      "88/295, train_loss: 0.0757, step time: 1.0995\n",
      "89/295, train_loss: 0.0437, step time: 1.0415\n",
      "90/295, train_loss: 0.0760, step time: 1.0511\n",
      "91/295, train_loss: 0.0449, step time: 1.0484\n",
      "92/295, train_loss: 0.0358, step time: 1.0325\n",
      "93/295, train_loss: 0.0444, step time: 1.0383\n",
      "94/295, train_loss: 0.0506, step time: 1.0477\n",
      "95/295, train_loss: 0.0456, step time: 1.0313\n",
      "96/295, train_loss: 0.0921, step time: 1.0413\n",
      "97/295, train_loss: 0.0494, step time: 1.0308\n",
      "98/295, train_loss: 0.0510, step time: 1.0417\n",
      "99/295, train_loss: 0.0433, step time: 1.0337\n",
      "100/295, train_loss: 0.0656, step time: 1.0675\n",
      "101/295, train_loss: 0.0589, step time: 1.0331\n",
      "102/295, train_loss: 0.0407, step time: 1.0434\n",
      "103/295, train_loss: 0.0524, step time: 1.0506\n",
      "104/295, train_loss: 0.0590, step time: 1.0367\n",
      "105/295, train_loss: 0.0438, step time: 1.0698\n",
      "106/295, train_loss: 0.3695, step time: 1.0521\n",
      "107/295, train_loss: 0.0241, step time: 1.0342\n",
      "108/295, train_loss: 0.1012, step time: 1.0583\n",
      "109/295, train_loss: 0.0327, step time: 1.0329\n",
      "110/295, train_loss: 0.0290, step time: 1.0335\n",
      "111/295, train_loss: 0.0383, step time: 1.0322\n",
      "112/295, train_loss: 0.0527, step time: 1.0401\n",
      "113/295, train_loss: 0.0368, step time: 1.0347\n",
      "114/295, train_loss: 0.0360, step time: 1.0699\n",
      "115/295, train_loss: 0.0395, step time: 1.0354\n",
      "116/295, train_loss: 0.0388, step time: 1.0391\n",
      "117/295, train_loss: 0.0470, step time: 1.0598\n",
      "118/295, train_loss: 0.0333, step time: 1.0557\n",
      "119/295, train_loss: 0.0523, step time: 1.0636\n",
      "120/295, train_loss: 0.0736, step time: 1.0962\n",
      "121/295, train_loss: 0.0682, step time: 1.0361\n",
      "122/295, train_loss: 0.1242, step time: 1.0386\n",
      "123/295, train_loss: 0.0332, step time: 1.0319\n",
      "124/295, train_loss: 0.0757, step time: 1.0384\n",
      "125/295, train_loss: 0.0652, step time: 1.0453\n",
      "126/295, train_loss: 0.0624, step time: 1.0721\n",
      "127/295, train_loss: 0.3767, step time: 1.0373\n",
      "128/295, train_loss: 0.0223, step time: 1.0575\n",
      "129/295, train_loss: 0.0368, step time: 1.0411\n",
      "130/295, train_loss: 0.0398, step time: 1.0405\n",
      "131/295, train_loss: 0.0581, step time: 1.0353\n",
      "132/295, train_loss: 0.0406, step time: 1.0426\n",
      "133/295, train_loss: 0.0539, step time: 1.0318\n",
      "134/295, train_loss: 0.0632, step time: 1.0507\n",
      "135/295, train_loss: 0.0459, step time: 1.0561\n",
      "136/295, train_loss: 0.0714, step time: 1.0317\n",
      "137/295, train_loss: 0.1000, step time: 1.0871\n",
      "138/295, train_loss: 0.0706, step time: 1.0394\n",
      "139/295, train_loss: 0.0484, step time: 1.0487\n",
      "140/295, train_loss: 0.0452, step time: 1.0438\n",
      "141/295, train_loss: 0.3678, step time: 1.0555\n",
      "142/295, train_loss: 0.0340, step time: 1.0405\n",
      "143/295, train_loss: 0.0226, step time: 1.0596\n",
      "144/295, train_loss: 0.0596, step time: 1.0945\n",
      "145/295, train_loss: 0.0392, step time: 1.0410\n",
      "146/295, train_loss: 0.0566, step time: 1.0463\n",
      "147/295, train_loss: 0.0664, step time: 1.0408\n",
      "148/295, train_loss: 0.3596, step time: 1.0501\n",
      "149/295, train_loss: 0.0740, step time: 1.0508\n",
      "150/295, train_loss: 0.0376, step time: 1.0747\n",
      "151/295, train_loss: 0.0651, step time: 1.0560\n",
      "152/295, train_loss: 0.0431, step time: 1.0382\n",
      "153/295, train_loss: 0.0308, step time: 1.0501\n",
      "154/295, train_loss: 0.0474, step time: 1.0630\n",
      "155/295, train_loss: 0.0593, step time: 1.0340\n",
      "156/295, train_loss: 0.0366, step time: 1.0395\n",
      "157/295, train_loss: 0.0481, step time: 1.0322\n",
      "158/295, train_loss: 0.0265, step time: 1.0334\n",
      "159/295, train_loss: 0.0415, step time: 1.0553\n",
      "160/295, train_loss: 0.0225, step time: 1.0651\n",
      "161/295, train_loss: 0.0482, step time: 1.0594\n",
      "162/295, train_loss: 0.0418, step time: 1.0503\n",
      "163/295, train_loss: 0.0403, step time: 1.0708\n",
      "164/295, train_loss: 0.0572, step time: 1.0572\n",
      "165/295, train_loss: 0.0474, step time: 1.0462\n",
      "166/295, train_loss: 0.0250, step time: 1.0344\n",
      "167/295, train_loss: 0.3764, step time: 1.0653\n",
      "168/295, train_loss: 0.0266, step time: 1.0352\n",
      "169/295, train_loss: 0.0246, step time: 1.0361\n",
      "170/295, train_loss: 0.0388, step time: 1.0362\n",
      "171/295, train_loss: 0.0802, step time: 1.0889\n",
      "172/295, train_loss: 0.0267, step time: 1.0512\n",
      "173/295, train_loss: 0.0892, step time: 1.0370\n",
      "174/295, train_loss: 0.0335, step time: 1.0315\n",
      "175/295, train_loss: 0.0315, step time: 1.0449\n",
      "176/295, train_loss: 0.0375, step time: 1.0677\n",
      "177/295, train_loss: 0.3658, step time: 1.1484\n",
      "178/295, train_loss: 0.0632, step time: 1.0443\n",
      "179/295, train_loss: 0.0838, step time: 1.0747\n",
      "180/295, train_loss: 0.0484, step time: 1.0633\n",
      "181/295, train_loss: 0.0712, step time: 1.0427\n",
      "182/295, train_loss: 0.0434, step time: 1.1838\n",
      "183/295, train_loss: 0.0313, step time: 1.0593\n",
      "184/295, train_loss: 0.0265, step time: 1.0375\n",
      "185/295, train_loss: 0.0729, step time: 1.0356\n",
      "186/295, train_loss: 0.0410, step time: 1.0366\n",
      "187/295, train_loss: 0.0247, step time: 1.0446\n",
      "188/295, train_loss: 0.0348, step time: 1.0373\n",
      "189/295, train_loss: 0.0569, step time: 1.0413\n",
      "190/295, train_loss: 0.0301, step time: 1.0423\n",
      "191/295, train_loss: 0.0399, step time: 1.0360\n",
      "192/295, train_loss: 0.0288, step time: 1.0687\n",
      "193/295, train_loss: 0.0237, step time: 1.0516\n",
      "194/295, train_loss: 0.0557, step time: 1.0386\n",
      "195/295, train_loss: 0.0306, step time: 1.0432\n",
      "196/295, train_loss: 0.3765, step time: 1.0565\n",
      "197/295, train_loss: 0.0634, step time: 1.0489\n",
      "198/295, train_loss: 0.0612, step time: 1.0751\n",
      "199/295, train_loss: 0.0453, step time: 1.0367\n",
      "200/295, train_loss: 0.0383, step time: 1.0906\n",
      "201/295, train_loss: 0.0331, step time: 1.0521\n",
      "202/295, train_loss: 0.0601, step time: 1.0393\n",
      "203/295, train_loss: 0.0499, step time: 1.0459\n",
      "204/295, train_loss: 0.0556, step time: 1.0473\n",
      "205/295, train_loss: 0.0609, step time: 1.0689\n",
      "206/295, train_loss: 0.0907, step time: 1.0492\n",
      "207/295, train_loss: 0.0195, step time: 1.0491\n",
      "208/295, train_loss: 0.0374, step time: 1.0365\n",
      "209/295, train_loss: 0.0343, step time: 1.0561\n",
      "210/295, train_loss: 0.0463, step time: 1.1324\n",
      "211/295, train_loss: 0.0326, step time: 1.0517\n",
      "212/295, train_loss: 0.0485, step time: 1.0931\n",
      "213/295, train_loss: 0.0420, step time: 1.0369\n",
      "214/295, train_loss: 0.1111, step time: 1.0529\n",
      "215/295, train_loss: 0.0262, step time: 1.0402\n",
      "216/295, train_loss: 0.0544, step time: 1.0717\n",
      "217/295, train_loss: 0.0707, step time: 1.0640\n",
      "218/295, train_loss: 0.0714, step time: 1.0600\n",
      "219/295, train_loss: 0.1049, step time: 1.0388\n",
      "220/295, train_loss: 0.0459, step time: 1.0327\n",
      "221/295, train_loss: 0.0288, step time: 1.0343\n",
      "222/295, train_loss: 0.0336, step time: 1.0380\n",
      "223/295, train_loss: 0.0427, step time: 1.0322\n",
      "224/295, train_loss: 0.0564, step time: 1.0330\n",
      "225/295, train_loss: 0.0613, step time: 1.0376\n",
      "226/295, train_loss: 0.0430, step time: 1.0400\n",
      "227/295, train_loss: 0.0598, step time: 1.0456\n",
      "228/295, train_loss: 0.0275, step time: 1.0355\n",
      "229/295, train_loss: 0.0486, step time: 1.0344\n",
      "230/295, train_loss: 0.0707, step time: 1.0365\n",
      "231/295, train_loss: 0.0360, step time: 1.0541\n",
      "232/295, train_loss: 0.0523, step time: 1.0326\n",
      "233/295, train_loss: 0.0757, step time: 1.0632\n",
      "234/295, train_loss: 0.0422, step time: 1.0459\n",
      "235/295, train_loss: 0.0386, step time: 1.0319\n",
      "236/295, train_loss: 0.0729, step time: 1.0361\n",
      "237/295, train_loss: 0.0334, step time: 1.0988\n",
      "238/295, train_loss: 0.3582, step time: 1.0704\n",
      "239/295, train_loss: 0.0507, step time: 1.0363\n",
      "240/295, train_loss: 0.3711, step time: 1.0436\n",
      "241/295, train_loss: 0.0923, step time: 1.0413\n",
      "242/295, train_loss: 0.0401, step time: 1.0536\n",
      "243/295, train_loss: 0.0304, step time: 1.0491\n",
      "244/295, train_loss: 0.0314, step time: 1.0393\n",
      "245/295, train_loss: 0.0845, step time: 1.0315\n",
      "246/295, train_loss: 0.0548, step time: 1.0556\n",
      "247/295, train_loss: 0.0708, step time: 1.0466\n",
      "248/295, train_loss: 0.0353, step time: 1.0448\n",
      "249/295, train_loss: 0.0308, step time: 1.0427\n",
      "250/295, train_loss: 0.0304, step time: 1.0430\n",
      "251/295, train_loss: 0.0487, step time: 1.0341\n",
      "252/295, train_loss: 0.1048, step time: 1.0430\n",
      "253/295, train_loss: 0.1122, step time: 1.0824\n",
      "254/295, train_loss: 0.0573, step time: 1.0349\n",
      "255/295, train_loss: 0.0620, step time: 1.0775\n",
      "256/295, train_loss: 0.0622, step time: 1.0896\n",
      "257/295, train_loss: 0.0416, step time: 1.0380\n",
      "258/295, train_loss: 0.0627, step time: 1.0428\n",
      "259/295, train_loss: 0.0687, step time: 1.0325\n",
      "260/295, train_loss: 0.0603, step time: 1.0350\n",
      "261/295, train_loss: 0.0715, step time: 1.0465\n",
      "262/295, train_loss: 0.0482, step time: 1.0341\n",
      "263/295, train_loss: 0.0258, step time: 1.0897\n",
      "264/295, train_loss: 0.0457, step time: 1.0424\n",
      "265/295, train_loss: 0.3735, step time: 1.0339\n",
      "266/295, train_loss: 0.0807, step time: 1.0398\n",
      "267/295, train_loss: 0.0369, step time: 1.0430\n",
      "268/295, train_loss: 0.0772, step time: 1.0422\n",
      "269/295, train_loss: 0.0651, step time: 1.0385\n",
      "270/295, train_loss: 0.0576, step time: 1.0628\n",
      "271/295, train_loss: 0.0884, step time: 1.0408\n",
      "272/295, train_loss: 0.0298, step time: 1.0371\n",
      "273/295, train_loss: 0.3915, step time: 1.0398\n",
      "274/295, train_loss: 0.0526, step time: 1.0304\n",
      "275/295, train_loss: 0.3735, step time: 1.0618\n",
      "276/295, train_loss: 0.0284, step time: 1.0484\n",
      "277/295, train_loss: 0.0698, step time: 1.0588\n",
      "278/295, train_loss: 0.0358, step time: 1.0590\n",
      "279/295, train_loss: 0.0496, step time: 1.0615\n",
      "280/295, train_loss: 0.0340, step time: 1.0579\n",
      "281/295, train_loss: 0.0222, step time: 1.0357\n",
      "282/295, train_loss: 0.0459, step time: 1.0366\n",
      "283/295, train_loss: 0.0244, step time: 1.0428\n",
      "284/295, train_loss: 0.0532, step time: 1.0766\n",
      "285/295, train_loss: 0.0668, step time: 1.0498\n",
      "286/295, train_loss: 0.0495, step time: 1.0429\n",
      "287/295, train_loss: 0.0883, step time: 1.0421\n",
      "288/295, train_loss: 0.1194, step time: 1.0568\n",
      "289/295, train_loss: 0.0775, step time: 1.0279\n",
      "290/295, train_loss: 0.0720, step time: 1.0280\n",
      "291/295, train_loss: 0.0404, step time: 1.0285\n",
      "292/295, train_loss: 0.0240, step time: 1.0279\n",
      "293/295, train_loss: 0.0674, step time: 1.0275\n",
      "294/295, train_loss: 0.0501, step time: 1.0313\n",
      "295/295, train_loss: 0.0337, step time: 1.0282\n",
      "epoch 100 average loss: 0.0768\n",
      "current epoch: 100 current mean dice: 0.7630 tc: 0.7172 wt: 0.8326 et: 0.7453\n",
      "best mean dice: 0.8045 at epoch: 29\n",
      "time consuming of epoch 100 is: 382.6379\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(cfg.epoch):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{cfg.epoch}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(output_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86dc0662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:47:30.203648Z",
     "iopub.status.busy": "2024-05-05T14:47:30.202526Z",
     "iopub.status.idle": "2024-05-05T14:47:30.208882Z",
     "shell.execute_reply": "2024-05-05T14:47:30.208003Z"
    },
    "papermill": {
     "duration": 2.343811,
     "end_time": "2024-05-05T14:47:30.210977",
     "exception": false,
     "start_time": "2024-05-05T14:47:27.867166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train completed, best_metric: 0.8045 at epoch: 29, total time: 38520.932084321976.\n"
     ]
    }
   ],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b5c51",
   "metadata": {
    "papermill": {
     "duration": 2.500331,
     "end_time": "2024-05-05T14:47:35.186723",
     "exception": false,
     "start_time": "2024-05-05T14:47:32.686392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot the loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c92dec87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:47:39.882280Z",
     "iopub.status.busy": "2024-05-05T14:47:39.881521Z",
     "iopub.status.idle": "2024-05-05T14:47:40.942946Z",
     "shell.execute_reply": "2024-05-05T14:47:40.941957Z"
    },
    "papermill": {
     "duration": 3.393333,
     "end_time": "2024-05-05T14:47:40.945326",
     "exception": false,
     "start_time": "2024-05-05T14:47:37.551993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
    "y = metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
    "y = metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
    "y = metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382ddd2",
   "metadata": {
    "papermill": {
     "duration": 2.350897,
     "end_time": "2024-05-05T14:47:45.779031",
     "exception": false,
     "start_time": "2024-05-05T14:47:43.428134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bff5c805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:47:50.608389Z",
     "iopub.status.busy": "2024-05-05T14:47:50.607608Z",
     "iopub.status.idle": "2024-05-05T14:47:56.630015Z",
     "shell.execute_reply": "2024-05-05T14:47:56.628861Z"
    },
    "papermill": {
     "duration": 8.349039,
     "end_time": "2024-05-05T14:47:56.632572",
     "exception": false,
     "start_time": "2024-05-05T14:47:48.283533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(output_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "                      \n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_data = val_ds[1]\n",
    "    val_input = val_data[\"image\"].unsqueeze(0).to(device)\n",
    "    val_output = inference(val_input)\n",
    "    val_output = post_trans(val_output[0])\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(val_data[\"image\"][i, :, :, val_input.shape[-1] // 2].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(val_data[\"label\"][i, :, :, val_input.shape[-1] // 2].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, val_input.shape[-1] // 2].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537e6c2",
   "metadata": {
    "papermill": {
     "duration": 2.517499,
     "end_time": "2024-05-05T14:48:01.726983",
     "exception": false,
     "start_time": "2024-05-05T14:47:59.209484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation on test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04144f5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:48:06.446533Z",
     "iopub.status.busy": "2024-05-05T14:48:06.446147Z",
     "iopub.status.idle": "2024-05-05T14:48:09.468163Z",
     "shell.execute_reply": "2024-05-05T14:48:09.467320Z"
    },
    "papermill": {
     "duration": 5.357472,
     "end_time": "2024-05-05T14:48:09.470407",
     "exception": false,
     "start_time": "2024-05-05T14:48:04.112935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=[\"bilinear\", \"nearest\"]\n",
    "        ),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=cfg.unetr.img_shape, mode=\"nearest\"),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_ds = BratsDataset(\n",
    "    root_dir=root_dir,\n",
    "    section=\"test\",\n",
    "    transform=transform,\n",
    "    cache_num=0\n",
    ")\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d5829b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:48:14.249759Z",
     "iopub.status.busy": "2024-05-05T14:48:14.248674Z",
     "iopub.status.idle": "2024-05-05T14:48:29.739044Z",
     "shell.execute_reply": "2024-05-05T14:48:29.737903Z"
    },
    "papermill": {
     "duration": 17.802673,
     "end_time": "2024-05-05T14:48:29.743278",
     "exception": false,
     "start_time": "2024-05-05T14:48:11.940605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric on test image:  0.7530734539031982\n",
      "metric_tc: 0.7018\n",
      "metric_wt: 0.8634\n",
      "metric_et: 0.7093\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(output_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_inputs, test_labels = (\n",
    "            test_data[\"image\"].to(device),\n",
    "            test_data[\"label\"].to(device),\n",
    "        )\n",
    "        test_outputs = inference(test_inputs)\n",
    "        test_outputs = [post_trans(i) for i in decollate_batch(test_outputs)]\n",
    "        dice_metric(y_pred=test_outputs, y=test_labels)\n",
    "        dice_metric_batch(y_pred=test_outputs, y=test_labels)\n",
    "\n",
    "    metric = dice_metric.aggregate().item()\n",
    "    metric_batch = dice_metric_batch.aggregate()\n",
    "\n",
    "    dice_metric.reset()\n",
    "    dice_metric_batch.reset()\n",
    "\n",
    "metric_tc, metric_wt, metric_et = metric_batch[0].item(), metric_batch[1].item(), metric_batch[2].item()\n",
    "\n",
    "print(\"Metric on test image: \", metric)\n",
    "print(f\"metric_tc: {metric_tc:.4f}\")\n",
    "print(f\"metric_wt: {metric_wt:.4f}\")\n",
    "print(f\"metric_et: {metric_et:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 751906,
     "sourceId": 1299795,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38647.207092,
   "end_time": "2024-05-05T14:48:35.490381",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-05T04:04:28.283289",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
